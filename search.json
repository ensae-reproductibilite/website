[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mise en production",
    "section": "",
    "text": "Cours dans le parcours data science en derni√®re ann√©e √† l‚ÄôENSAE construit par Romain Avouac et Lino Galiana.\nLes slides associ√©es au cours sont disponibles √† cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "index.html#mise-en-production-des-projets-de-data-science",
    "href": "index.html#mise-en-production-des-projets-de-data-science",
    "title": "Mise en production",
    "section": "",
    "text": "Cours dans le parcours data science en derni√®re ann√©e √† l‚ÄôENSAE construit par Romain Avouac et Lino Galiana.\nLes slides associ√©es au cours sont disponibles √† cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "chapters/portability.html",
    "href": "chapters/portability.html",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/portability.html#introduction",
    "href": "chapters/portability.html#introduction",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Introduction",
    "text": "Introduction\nPour illustrer l‚Äôimportance de travailler avec des environnements virtuels, mettons-nous √† la place d‚Äôun.e aspirant.e data scientist qui commencerait ses premiers projets.\nSelon toute vraisemblance, on va commencer par installer une distribution de Python ‚Äî souvent, via Anaconda ‚Äî sur son poste et commencer √† d√©velopper, projet apr√®s projet. S‚Äôil est n√©cessaire d‚Äôinstaller une librairie suppl√©mentaire, on le fera sans trop se poser de question. Puis, on passera au projet suivant en adoptant la m√™me d√©marche. Et ainsi de suite.\nCette d√©marche naturelle pr√©sentera l‚Äôavantage de permettre d‚Äôaller vite dans les exp√©rimentations. N√©anmoins, elle deviendra probl√©matique s‚Äôil devient n√©cessaire de partager son projet, ou de reprendre celui-ci dans le futur.\nDans cette approche, les diff√©rents packages qu‚Äôon va √™tre amen√© √† utiliser vont √™tre install√©s au m√™me endroit. Ceci peut appara√Ætre secondaire, apr√®s tout nous utilisons Python pour sa simplicit√© d‚Äôusage qui ne n√©cessite pas de passer des heures √† se poser des questions avant d‚Äô√©crire la moindre ligne de code, mais cela va finir par nous poser plusieurs probl√®mes :\n\nconflits de version : une application A peut d√©pendre de la version 1 d‚Äôun package l√† o√π une application B peut d√©pendre de la version 2 de ce m√™me package. Ces versions d‚Äôun m√™me package peuvent avoir des incompatibilit√©s2. Une seule application peut donc fonctionner dans cette configuration ;\nversion de Python fixe ‚Äî on ne peut avoir qu‚Äôune seule installation par syst√®me ‚Äî l√† o√π on voudrait pouvoir avoir des versions diff√©rentes selon le projet ;\nreproductiblit√© limit√©e : difficile de dire quel projet repose sur tel package, dans la mesure o√π ceux-ci s‚Äôaccumulent en un m√™me endroit au fil des projets ;\nportabilit√© limit√©e : cons√©quence du point pr√©c√©dent, il est difficile de fixer dans un fichier les d√©pendances sp√©cifiques √† un projet, et exclusivement celles-ci.\n\nLes environnements virtuels constituent une solution √† ces diff√©rents probl√®mes."
  },
  {
    "objectID": "chapters/portability.html#fonctionnement",
    "href": "chapters/portability.html#fonctionnement",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLe concept d‚Äôenvironnement virtuel est techniquement tr√®s simple. On peut lui donner la d√©finition suivante pour Python :\n\n‚Äúdossier auto-suffisant qui contient une installation de Python pour une version particuli√®re de Python ainsi que des packages additionnels et qui est isol√© des autres environnements existants.‚Äù\n\nOn peut donc simplement voir les environnements virtuels comme un moyen de faire cohabiter sur un m√™me syst√®me diff√©rentes installations de Python avec chacune leur propre liste de packages install√©s et leurs versions. D√©velopper dans des environnements virtuels vierges √† chaque d√©but de projet est une tr√®s bonne pratique pour accro√Ætre la reproductibilit√© des analyses."
  },
  {
    "objectID": "chapters/portability.html#impl√©mentations",
    "href": "chapters/portability.html#impl√©mentations",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Impl√©mentations",
    "text": "Impl√©mentations\nIl existe diff√©rentes impl√©mentations des environnements virtuels en Python, dont chacune ont leurs sp√©cificit√©s et leur communaut√© d‚Äôutilisateurs :\n\nL‚Äôimpl√©mentation standard en Python est venv.\nconda propose une impl√©mentation plus compl√®te.\n\nEn pratique pour les utilisateurs, ces impl√©mentations sont relativement proches. La diff√©rence conceptuelle majeure est que conda est √† la fois un package manager (comme pip) et un gestionnaire d‚Äôenvironnements virtuels (comme venv).\nPendant longtemps, conda en tant que package manager s‚Äôest av√©r√© tr√®s pratique en data science, dans la mesure o√π il g√©rait non seulement les d√©pendances Python mais aussi dans d‚Äôautres langages, comme des d√©pendances C, tr√®s utilis√©es par les principales librairies de data science et dont l‚Äôinstallation peut √™tre complexe sur certains syst√®mes d‚Äôexploitation. N√©anmoins, depuis quelques ann√©es, l‚Äôinstallation de packages par pip se fait de plus en plus par le biais de wheels qui sont des versions pr√©-compil√©es des librairies syst√®mes, propres √† chaque configuration syst√®me.\n\n\n\n\n\n\nUne diff√©rence conceptuelle entre pip et conda\n\n\n\n\n\nL‚Äôautre diff√©rence majeure avec pip est que Conda utilise une m√©thode plus avanc√©e ‚Äî et donc √©galement plus co√ªteuse en temps ‚Äî de r√©solution des d√©pendances.\nEn effet, diff√©rents packages peuvent sp√©cifier diff√©rentes versions d‚Äôun m√™me package dont ils d√©pendent tous les deux, ce qui provoque un conflit de version. Conda va par d√©faut appliquer un algorithme qui vise √† g√©rer au mieux ces conflits, l√† o√π pip va choisir une approche plus minimaliste3.\n\n\n\npip+venv pr√©sente l‚Äôavantage de la simplicit√©, conda de la fiabilit√©. Selon les projets, on privil√©giera l‚Äôun ou l‚Äôautre. N√©anmoins, si le projet est amen√© √† fonctionner de mani√®re isol√©e dans un conteneur, venv suffira amplement car l‚Äôisolation sera fournie par le conteneur comme nous le verrons ult√©rieurement.\n\n\n\n\n\n\nC‚Äôest diff√©rent en  ?\n\n\n\n\n\nOn lit souvent, notamment chez les afficionados de  que la gestion des environnements en Python est chaotique. C‚Äô√©tait vrai au d√©but des ann√©es 2010 mais c‚Äôest quelques peu exag√©r√© aujourd‚Äôhui.\nLa qualit√© sup√©rieure des outils  pour la gestion des d√©pendances ne saute pas aux yeux: renv est tr√®s int√©ressant mais ne permet pas de d√©finir la version de  :\n\nR version: renv tracks, but doesn‚Äôt help with, the version of R used with the packge. renv can‚Äôt easily help with this because it‚Äôs run inside of R, but you might find tools like rig helpful, as they make it easier to switch between multiple version of R on one computer.\n\nC‚Äôest, en fait, le probl√®me principal des outils  pour la reproductibilit√©. Pour les utiliser, il faut souvent se trouver dans une session , avec ses sp√©cificit√©s. Les outils  qui s‚Äôutilisent pas le biais de la ligne de commande offrent une robustesse plus importante. venv est certes d√©pendant de la version de  utilis√©e lors de la cr√©ation de l‚Äôenvironnement mais le fait de passer par le terminal permet de choisir la version de  qui servira √† cr√©er l‚Äôenvironnement. Quant √† conda, la version de  est d√©finie dans le environment.yml ce qui donne une grande libert√©.\n\n\n\nPuisqu‚Äôil n‚Äôy a pas de raison absolue d‚Äôimposer pip+venv ou conda, nous recommandons le pragmatisme. Personnellement, nous utilisons plut√¥t venv car nous travaillons principalement dans des microservices bas√©s sur des conteneurs et non sur des postes personnels, ce qui est l‚Äôapproche moderne dans le monde de la data science. Nous pr√©sentons n√©anmoins les deux approches par la suite. L‚Äôapplication fil rouge propose les deux approches, √† vous de choisir celle que vous d√©sirez privil√©gier."
  },
  {
    "objectID": "chapters/portability.html#guide-pratique-dutilisation-dun-environnement-virtuel",
    "href": "chapters/portability.html#guide-pratique-dutilisation-dun-environnement-virtuel",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Guide pratique d‚Äôutilisation d‚Äôun environnement virtuel",
    "text": "Guide pratique d‚Äôutilisation d‚Äôun environnement virtuel\n\nInstallation\n\nvenvconda\n\n\nvenv est un module inclus par d√©faut dans Python, ce qui le rend facilement accessible pour la gestion d‚Äôenvironnements virtuels.\nLes instructions pour utiliser venv, l‚Äôoutil de cr√©ation d‚Äôenvironnements virtuels int√©gr√© √† Python, sont d√©taill√©es dans la documentation officielle de Python.\n\n\n\nIllustration du principe (Source: dataquest)\n\n\n\n\nLes instructions √† suivre pour installer conda sont d√©taill√©es dans la documentation officielle. conda seul √©tant peu utile en pratique, il est g√©n√©ralement install√© dans le cadre de distributions. Les deux plus populaires sont :\n\nMiniconda : une distribution minimaliste contenant conda, Python ainsi qu‚Äôun petit nombre de packages techniques tr√®s utiles ;\nAnaconda : une distribution assez volumineuse contenant conda, Python, d‚Äôautres logiciels (R, Spyder, etc.) ainsi qu‚Äôun ensemble de packages utiles pour la data science (SciPy, NumPy, etc.).\n\n\nLe choix de la distribution importe assez peu en pratique, dans la mesure o√π nous allons de toute mani√®re utiliser des environnements virtuels vierges pour d√©velopper nos projets.\n\n\n\n\n\nCr√©er un environnement\n\nvenvconda\n\n\nPour commencer √† utiliser venv, commen√ßons par cr√©er un environnement vierge, nomm√© dev. Pour cr√©er un environnement virtuel, cela se fait en ligne de commande par le biais de Python. Cela signifie que la version de Python utilis√©e par cet environnement sera celle utilis√©e lors de la cr√©ation de celui-ci.\n\n\nterminal\n\n1$ python -m venv dev\n\n\n1\n\nSur un syst√®me Windows, ce sera python.exe -m venv dev\n\n\nCette commande cr√©e un dossier nomm√© dev/ contenant une installation Python isol√©e.\n\n\nExemple sur un syst√®me Linux\n\n\n\n\nExemple sur un syst√®me Linux\n\n\n\nCelle-ci est de la version de Python enregistr√©e par d√©faut dans le PATH, en l‚Äôoccurrence Python 3.11. Pour cr√©er un environnement virtuel avec une autre version de Python, il faudra d√©finir le chemin de mani√®re formelle, par exemple:\n\n\nterminal\n\n$ /chemin_local/python3.8 -m venv dev-old\n\n\n\nPour commencer √† utiliser conda, commen√ßons par cr√©er un environnement vierge, nomm√© dev, en sp√©cifiant la version de Python que l‚Äôon souhaite installer pour notre projet.\n\n\nterminal\n\n$ conda create -n dev python=3.9.7\n\nRetrieving notices: ...working... done\nChannels:\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/mamba/envs/dev\n\n  added / updated specs:\n    - python=3.9.7\n\n\nThe following packages will be downloaded:\n...\nThe following NEW packages will be INSTALLED:\n...\nProceed ([y]/n)? y\nDownloading and Extracting Packages\n...\nComme indiqu√© dans les logs, Conda a cr√©√© notre environnement et nous indique son emplacement sur le filesystem. En r√©alit√©, l‚Äôenvironnement n‚Äôest jamais vraiment vierge : Conda nous demande ‚Äî et il faut r√©pondre oui en tapant y ‚Äî d‚Äôinstaller un certain nombre de packages, qui sont ceux qui viennent avec la distribution Miniconda.\nOn peut v√©rifier que l‚Äôenvironnement a bien √©t√© cr√©√© en listant les environnements install√©s sur le syst√®me.\n\n\nterminal\n\n$ conda info --envs\n\n# conda environments:                                                                                                                             \n#                                                                                                                                                 \nbase                  *  /opt/mamba                                                                                                               \ndev                      /opt/mamba/envs/dev\n\n\n\n\n\nActiver un environnement\nComme plusieurs environnements peuvent coexister sur un m√™me syst√®me, il faut dire √† notre gestionnaire d‚Äôenvironnement d‚Äôactiver celui-ci. D√®s lors, ce sera celui-ci qui sera utilis√© implicitement lorsqu‚Äôon utilisera python, pip, etc. dans la ligne de commande active4.\n\nvenvconda\n\n\n\n\nterminal\n\n$ source dev/bin/activate\n\nvenv active l‚Äôenvironnement virtuel dev, indiqu√© par le changement du nom de l‚Äôenvironnement qui appara√Æt au d√©but de la ligne de commande dans le terminal. Une fois activ√©, dev devient temporairement notre environnement par d√©faut pour les op√©rations Python. Pour confirmer cela, nous pouvons utiliser la commande which pour d√©terminer l‚Äôemplacement de l‚Äôinterpr√©teur Python qui sera utilis√© pour ex√©cuter des scripts comme python mon-script.py.\n\n\nterminal\n\n(dev) $ which python\n\n/home/onyxia/work/dev/bin/python\n\n\n\n\nterminal\n\n$ conda activate dev\n\nConda nous indique que l‚Äôon travaille √† partir de maintenant dans l‚Äôenvironnement dev en indiquant son nom entre parenth√®ses au d√©but de la ligne de commandes. Autrement dit, dev devient pour un temps notre environnement par d√©faut. Pour s‚Äôen assurer, v√©rifions avec la commande which l‚Äôemplacement de l‚Äôinterpr√©teur Python qui sera utilis√© si on lance une commande du type python mon-script.py.\n\n\nterminal\n\n(dev) $ which python \n\n/opt/mamba/envs/dev/bin/python\n\n\n\nOn travaille bien dans l‚Äôenvironnement attendu : l‚Äôinterpr√©teur qui se lance n‚Äôest pas celui du syst√®me global, mais bien celui sp√©cifique √† notre environnement virtuel.\n\n\nLister les packages install√©s\nUne fois l‚Äôenvironnement activ√©, on peut lister les packages install√©s et leur version. Cela confirme qu‚Äôun certain nombre de packages sont install√©s par d√©faut lors de la cr√©ation d‚Äôun environnement virtuel.\n\nvenvconda\n\n\nOn part d‚Äôun environnement vraiment r√©duit √† l‚Äôos:\n\n\nterminal\n\n(dev) $ pip list\n\nPackage    Version\n---------- -------\npip        23.3.2\nsetuptools 69.0.3\nwheel      0.42.0\n\n\nL‚Äôenvironnement est assez minimaliste, quoique plus garni que lors de la cr√©ation d‚Äôun environnement virtuel par venv\n\n\nterminal\n\n(dev) $ conda list\n\n# packages in environment at /opt/mamba/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\nca-certificates           2023.11.17           hbcca054_0    conda-forge\n...\n\n\n\nPour se convaincre, on peut v√©rifier que Numpy est bien absent de notre environnement:\n\n\nterminal\n\n(dev) $ python -c \"import numpy as np\"\n\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'numpy'\n\n\nInstaller un package\nNotre environnement peut √™tre enrichi, lorsque n√©cessaire, avec l‚Äôinstallation d‚Äôun package par le biais de la ligne de commande. La proc√©dure est tr√®s similaire entre pip (pour les environnements venv) et conda.\n\n\n\n\n\n\nM√©langer pip et conda\n\n\n\n\n\nIl est techniquement possible d‚Äôinstaller des packages par le biais de pip en √©tant situ√© dans un environnement virtuel conda5. Ce n‚Äôest pas un probl√®me pour de l‚Äôexp√©rimentation et √ßa permet de d√©velopper rapidement.\nN√©anmoins, dans un environnement de production c‚Äôest une pratique √† √©viter.\n\nSoit on initialise un environnement conda autosuffisant avec un env.yml (voir plus bas) ;\nSoit on cr√©e un environnement venv et on fait exclusivement des pip install.\n\n\n\n\n\nvenvconda\n\n\n\n\nterminal\n\n(dev) $ pip install nom_du_package\n\n\n\n\n\nterminal\n\n(dev) $ conda install nom_du_package\n\n\n\n\nLa diff√©rence est que l√† o√π pip install va installer un package √† partir du r√©pertoire PyPI, conda install va chercher le package sur les r√©pertoires maintenus par les d√©veloppeurs de Conda6.\nInstallons par exemple le package phare de machine learning scikit-learn.\n\nvenvconda\n\n\n\n\nterminal\n\n(dev) $ conda install scikit-learn\n\npip install scikit-learn\nCollecting scikit-learn\n  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/5b/be/208f17ce87a5e55094b0e8ffd55b06919ab9b56e7e4ce2a64cd9095ec5d2/scikit_learn-1.4.0-1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading scikit_learn-1.4.0-1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting numpy&lt;2.0,&gt;=1.19.5 (from scikit-learn)\n  Obtaining dependency information for numpy&lt;2.0,&gt;=1.19.5 from https://files.pythonhosted.org/packages/5a/62/007b63f916aca1d27f5fede933fda3315d931\n...\nLes d√©pendances n√©cessaires (par exemple Numpy sont automatiquement install√©es). L‚Äôenvironnement s‚Äôenrichit donc:\n\n\nterminal\n\n(dev) $ pip list\n\nPackage       Version\n------------- -------\njoblib        1.3.2\nnumpy         1.26.3\npip           23.2.1\nscikit-learn  1.4.0\nscipy         1.12.0\nsetuptools    65.5.0\nthreadpoolctl 3.2.0\n\n\n\n\nterminal\n\n(dev) $ conda install scikit-learn\n\n\n\nVoir la sortie\n\nChannels:\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/mamba/envs/dev\n\n  added / updated specs:\n    - scikit-learn\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    joblib-1.3.2               |     pyhd8ed1ab_0         216 KB  conda-forge\n    libblas-3.9.0              |21_linux64_openblas          14 KB  conda-forge\n    libcblas-3.9.0             |21_linux64_openblas          14 KB  conda-forge\n    libgfortran-ng-13.2.0      |       h69a702a_3          23 KB  conda-forge\n    libgfortran5-13.2.0        |       ha4646dd_3         1.4 MB  conda-forge\n    liblapack-3.9.0            |21_linux64_openblas          14 KB  conda-forge\n    libopenblas-0.3.26         |pthreads_h413a1c8_0         5.3 MB  conda-forge\n    libstdcxx-ng-13.2.0        |       h7e041cc_3         3.7 MB  conda-forge\n    numpy-1.26.3               |   py39h474f0d3_0         6.6 MB  conda-forge\n    python_abi-3.9             |           4_cp39           6 KB  conda-forge\n    scikit-learn-1.4.0         |   py39ha22ef79_0         8.7 MB  conda-forge\n    scipy-1.12.0               |   py39h474f0d3_2        15.6 MB  conda-forge\n    threadpoolctl-3.2.0        |     pyha21a80b_0          20 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        41.6 MB\n\nThe following NEW packages will be INSTALLED:\n\n  joblib             conda-forge/noarch::joblib-1.3.2-pyhd8ed1ab_0 \n  libblas            conda-forge/linux-64::libblas-3.9.0-21_linux64_openblas \n  libcblas           conda-forge/linux-64::libcblas-3.9.0-21_linux64_openblas \n  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-13.2.0-h69a702a_3 \n  libgfortran5       conda-forge/linux-64::libgfortran5-13.2.0-ha4646dd_3 \n  liblapack          conda-forge/linux-64::liblapack-3.9.0-21_linux64_openblas \n  libopenblas        conda-forge/linux-64::libopenblas-0.3.26-pthreads_h413a1c8_0 \n  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.2.0-h7e041cc_3 \n  numpy              conda-forge/linux-64::numpy-1.26.3-py39h474f0d3_0 \n  python_abi         conda-forge/linux-64::python_abi-3.9-4_cp39 \n  scikit-learn       conda-forge/linux-64::scikit-learn-1.4.0-py39ha22ef79_0 \n  scipy              conda-forge/linux-64::scipy-1.12.0-py39h474f0d3_2 \n  threadpoolctl      conda-forge/noarch::threadpoolctl-3.2.0-pyha21a80b_0 \n\n\n\nDownloading and Extracting Packages:\n                                                                                                                                                  \nPreparing transaction: done                                                                                                                       \nVerifying transaction: done                                                                                                                       \nExecuting transaction: done \n\nL√† encore, conda nous demande d‚Äôinstaller d‚Äôautres packages, qui sont des d√©pendances de scikit-learn. Par exemple, la librairie de calcul scientifique NumPy.\n(dev) $ conda list\n# packages in environment at /opt/mamba/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\nca-certificates           2023.11.17           hbcca054_0    conda-forge\njoblib                    1.3.2              pyhd8ed1ab_0    conda-forge\nld_impl_linux-64          2.40                 h41732ed_0    conda-forge\nlibblas                   3.9.0           21_linux64_openblas    conda-forge\nlibcblas                  3.9.0           21_linux64_openblas    conda-forge\nlibffi                    3.4.2                h7f98852_5    conda-forge\nlibgcc-ng                 13.2.0               h807b86a_3    conda-forge\nlibgfortran-ng            13.2.0               h69a702a_3    conda-forge\nlibgfortran5              13.2.0               ha4646dd_3    conda-forge\nlibgomp                   13.2.0               h807b86a_3    conda-forge\nliblapack                 3.9.0           21_linux64_openblas    conda-forge\nlibopenblas               0.3.26          pthreads_h413a1c8_0    conda-forge\nlibsqlite                 3.44.2               h2797004_0    conda-forge\nlibstdcxx-ng              13.2.0               h7e041cc_3    conda-forge\nlibzlib                   1.2.13               hd590300_5    conda-forge\nncurses                   6.4                  h59595ed_2    conda-forge\nnumpy                     1.26.3           py39h474f0d3_0    conda-forge\nopenssl                   3.2.0                hd590300_1    conda-forge\npip                       23.3.2             pyhd8ed1ab_0    conda-forge\npython                    3.9.7           hf930737_3_cpython    conda-forge\npython_abi                3.9                      4_cp39    conda-forge\nreadline                  8.2                  h8228510_1    conda-forge\nscikit-learn              1.4.0            py39ha22ef79_0    conda-forge\nscipy                     1.12.0           py39h474f0d3_2    conda-forge\nsetuptools                69.0.3             pyhd8ed1ab_0    conda-forge\nsqlite                    3.44.2               h2c6b66d_0    conda-forge\nthreadpoolctl             3.2.0              pyha21a80b_0    conda-forge\ntk                        8.6.13          noxft_h4845f30_101    conda-forge\ntzdata                    2023d                h0c530f3_0    conda-forge\nwheel                     0.42.0             pyhd8ed1ab_0    conda-forge\nxz                        5.2.6                h166bdaf_0    conda-forge\nzlib                      1.2.13               hd590300_5    conda-forge\n\n\n\n\n\nExporter les sp√©cifications de l‚Äôenvironnement\nD√©velopper √† partir d‚Äôun environnement vierge est une bonne pratique de reproductibilit√© : en partant d‚Äôune base minimale, on s‚Äôassure que seuls les packages effectivement n√©cessaires au bon fonctionnement de notre application ont √©t√© install√©s au fur et √† mesure du projet.\nCela rend √©galement notre projet plus ais√© √† rendre portable. On peut exporter les sp√©cifications de l‚Äôenvironnement dans un fichier sp√©cial qui peut permettre de cr√©er un nouvel environnement similaire √† celui ayant servi initialement.\n\nvenvconda\n\n\n\n\nterminal\n\n(dev) $ pip freeze &gt; requirements.txt\n\n\n\nVoir le fichier requirements.txt g√©n√©r√©\n\n\n\nrequirements.txt\n\njoblib==1.3.2\nnumpy==1.26.3\nscikit-learn==1.4.0\nscipy==1.12.0\nthreadpoolctl==3.2.0\n\n\n\n\n\n\nterminal\n\n(dev) $ conda env export &gt; environment.yml\n\n\n\nVoir le fichier environment.yml g√©n√©r√©\n\n\n\nenvironment.yml\n\nname: dev\nchannels:\n  - conda-forge\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=2_gnu\n  - ca-certificates=2023.11.17=hbcca054_0\n  - joblib=1.3.2=pyhd8ed1ab_0\n  - ld_impl_linux-64=2.40=h41732ed_0\n  - libblas=3.9.0=21_linux64_openblas\n  - libcblas=3.9.0=21_linux64_openblas\n  - libffi=3.4.2=h7f98852_5\n  - libgcc-ng=13.2.0=h807b86a_3\n  - libgfortran-ng=13.2.0=h69a702a_3\n  - libgfortran5=13.2.0=ha4646dd_3\n  - libgomp=13.2.0=h807b86a_3\n  - liblapack=3.9.0=21_linux64_openblas\n  - libopenblas=0.3.26=pthreads_h413a1c8_0\n  - libsqlite=3.44.2=h2797004_0\n  - libstdcxx-ng=13.2.0=h7e041cc_3\n  - libzlib=1.2.13=hd590300_5\n  - ncurses=6.4=h59595ed_2\n  - numpy=1.26.3=py39h474f0d3_0\n  - openssl=3.2.0=hd590300_1\n  - pip=23.3.2=pyhd8ed1ab_0\n  - python=3.9.7=hf930737_3_cpython\n  - python_abi=3.9=4_cp39\n  - readline=8.2=h8228510_1\n  - scikit-learn=1.4.0=py39ha22ef79_0\n  - scipy=1.12.0=py39h474f0d3_2\n  - setuptools=69.0.3=pyhd8ed1ab_0\n  - sqlite=3.44.2=h2c6b66d_0\n  - threadpoolctl=3.2.0=pyha21a80b_0\n  - tk=8.6.13=noxft_h4845f30_101\n  - tzdata=2023d=h0c530f3_0\n  - wheel=0.42.0=pyhd8ed1ab_0\n  - xz=5.2.6=h166bdaf_0\n  - zlib=1.2.13=hd590300_5\nprefix: /opt/mamba/envs/dev\n\n\n\n\n\nCe fichier est mis par convention √† la racine du d√©p√¥t Git du projet. Ainsi, les personnes souhaitant tester l‚Äôapplication peuvent recr√©er le m√™me environnement Conda que celui qui a servi au d√©veloppement via la commande suivante.\n\nvenvconda\n\n\nOn refait la d√©marche pr√©c√©dente de cr√©ation d‚Äôun environnement vierge puis un pip install -r requirements.txt\n\n\nterminal\n\n$ python -m venv newenv\n$ source newenv/bin/activate\n\n\n\nterminal\n\n(newenv) $ pip install -r requirements.txt\n\n\n\nCela se fait en une seule commande:\n\n\nterminal\n\n$ $ conda env create -f environment.yml\n\n\n\n\n\n\nChanger d‚Äôenvironnement\n\nvenvconda\n\n\nPour changer d‚Äôenvironnement virtuel, il suffit d‚Äôen activer un autre.\n\n\nterminal\n\n(myenv) $ deactivate\n$ source anotherenv/bin/activate\n(anotherenv) $ which python\n/chemin/vers/anotherenv/bin/python\n\nPour quitter l‚Äôenvironnement virtuel actif, on utilise simplement la commande deactivate :\n\n\nterminal\n\n(anotherenv) $ deactivate\n$ \n\n\n\nPour changer d‚Äôenvironnement, il suffit d‚Äôen activer un autre.\n\n\nterminal\n\n(dev) $ conda activate base\n(base) $ which python\n/opt/mamba/bin/python\n\nPour sortir de tout environnement conda, on utilise la commande conda deactivate :\n\n\nterminal\n\n(base) $ conda deactivate\n$ \n\n\n\n\n\n\nAide-m√©moire\n\n\n\n\n\n\n\n\nvenv\nconda\nPrincipe\n\n\n\n\npython -m venv &lt;env_name&gt;\nconda create -n &lt;env_name&gt; python=&lt;python_version&gt;\nCr√©ation d‚Äôun environnement nomm√© &lt;env_name&gt; dont la version de Python est &lt;python_version&gt;\n\n\n\nconda info --envs\nLister les environnements\n\n\nsource &lt;env_name&gt;/bin/activate\nconda activate &lt;env_name&gt;\nUtiliser l‚Äôenvironnement &lt;env_name&gt; pour la session du terminal\n\n\npip list\nconda list\nLister les packages dans l‚Äôenvironnement actif\n\n\npip install &lt;pkg&gt;\nconda install &lt;pkg&gt;\nInstaller le package &lt;pkg&gt; dans l‚Äôenvironnement actif\n\n\npip freeze &gt; requirements.txt\nconda env export &gt; environment.yml\nExporter les sp√©cifications de l‚Äôenvironnement dans un fichier requirements.txt"
  },
  {
    "objectID": "chapters/portability.html#limites",
    "href": "chapters/portability.html#limites",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Limites",
    "text": "Limites\nD√©velopper dans des environnements virtuels est une bonne pratique, car cela accro√Æt la portabilit√© d‚Äôune application. N√©anmoins, il y a plusieurs limites √† leur utilisation :\n\nles librairies syst√®me n√©cessaires √† l‚Äôinstallation des packages ne sont pas g√©r√©es ;\nles environnements virtuels ne permettent pas toujours de g√©rer des projets faisant intervenir diff√©rents langages de programmation ;\ndevoir installer conda, Python, et les packages n√©cessaires √† chaque changement d‚Äôenvironnement peut √™tre assez long et p√©nible en pratique ;\ndans un environnement de production, g√©rer des environnements virtuels diff√©rents pour chaque projet peut s‚Äôav√©rer rapidement complexe pour les administrateurs syst√®me.\n\nLa technologie des conteneurs permet de r√©pondre √† ces diff√©rents probl√®mes."
  },
  {
    "objectID": "chapters/portability.html#introduction-1",
    "href": "chapters/portability.html#introduction-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Introduction",
    "text": "Introduction\nAvec les environnements virtuels, l‚Äôid√©e √©tait de permettre √† chaque utilisateur potentiel de notre projet d‚Äôinstaller sur son environnement d‚Äôex√©cution les packages n√©cessaires √† la bonne ex√©cution du projet.\nN√©anmoins, comme on l‚Äôa vu, cette approche ne garantit pas une reproductibilit√© parfaite et a l‚Äôinconv√©nient de demander beaucoup de gestion manuelle.\nChangeons de perspective : au lieu de distribuer une recette permettant √† l‚Äôutilisateur de recr√©er l‚Äôenvironnement n√©cessaire sur sa machine, ne pourrait-on pas directement distribuer √† l‚Äôutilisateur une machine contenant l‚Äôenvironnement pr√©-configur√© ?\nBien entendu, on ve pas configurer et envoyer des ordinateurs portables √† tous les utilisateurs potentiels d‚Äôun projet. On va donc essayer de livrer une version virtuelle de notre ordinateur. Il existe deux approches principales pour cela:\n\nLes machines virtuelles. Cette approche n‚Äôest pas nouvelle. Elle consiste √† recr√©er, sur un serveur, un environnement informatique complet (mat√©riel et syst√®me d‚Äôexploitation) qui r√©plique le comportement d‚Äôun v√©ritable ordinateur.\nLes conteneurs, une solution plus l√©g√®re pour empaqueter un environnement informatique afin de r√©pliquer le comportement d‚Äôune machine r√©elle."
  },
  {
    "objectID": "chapters/portability.html#fonctionnement-1",
    "href": "chapters/portability.html#fonctionnement-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLes machines virtuelles ont l‚Äôinconv√©nient d‚Äô√™tre assez lourdes, et complexes √† r√©pliquer et distribuer. Pour pallier ces diff√©rentes limites, les conteneurs se sont impos√©s au cours de la derni√®re d√©cennie. Toutes les infrastructures cloud modernes ont progressivement abandonn√© les machines virtuelles pour privil√©gier des conteneurs pour les raisons que nous allons √©voquer ult√©rieurement.\nComme les machines virtuelles, les conteneurs permettent d‚Äôempaqueter compl√®tement l‚Äôenvironnement (librairies syst√®mes, application, configuration) qui permet de faire tourner l‚Äôapplication. Mais √† l‚Äôinverse d‚Äôune machine virtuelle, le conteneur n‚Äôinclut pas de syst√®me d‚Äôexploitation propre, il utilise celui de la machine h√¥te qui l‚Äôex√©cute. Cela signifie que si on d√©sire reproduire le comportement d‚Äôune machine Windows, il n‚Äôest pas n√©cessaire d‚Äôavoir un gros serveur avec Windows. Il est tout √† fait possible d‚Äôavoir un serveur Linux, ce qui est la norme, et de r√©pliquer le comportement d‚Äôune machine Windows √† l‚Äôint√©rieur. A l‚Äôinverse, cela peut permettre de tester des configurations Linux ou Mac sur un ordinateur Windows. C‚Äôest le r√¥le du logiciel de conteneurisation qui fera la traduction entre les instructions voulues par le software et le syst√®me d‚Äôexploitation du hardware.\nLa technologie des conteneurs permet ainsi de garantir une tr√®s forte reproductibilit√© tout en restant suffisamment l√©g√®re pour permettre une distribution et un d√©ploiement simple aux utilisateurs. En effet, l‚Äôadh√©rence forte entre un syst√®me d‚Äôexploitation et un logiciel dans l‚Äôapproche des machines virtuelles rend plus complexe la mont√©e en charge d‚Äôun serveur. Si plus d‚Äôutilisateurs commencent √† utiliser l‚Äôapplication, il est n√©cessaire de s‚Äôassurer que des serveurs correspondant aux besoins de l‚Äôapplication (syst√®me d‚Äôexploitation, configurations techniques, etc.) soient disponibles. Avec les conteneurs, la mont√©e en charge est plus simple puisque les restrictions mat√©rielles sont moins fortes: ajouter un serveur Linux avec les logiciels ad√©quats peut √™tre suffisant.\n\n\n\nDiff√©rences entre l‚Äôapproche conteneurs (gauche) et l‚Äôapproche machines virtuelles (droite) (Source : docker.com )\n\n\nDu point de vue de l‚Äôutilisateur, la diff√©rence n‚Äôest pas toujours perceptible pour des besoins standards. L‚Äôutilisateur acc√©dera √† son application par une application d√©di√©e (un navigateur, un logiciel sp√©cialis√©‚Ä¶) et les calculs issus des op√©rations effectu√©es seront d√©port√©s sur les serveurs o√π est h√©berg√©e cette application. N√©anmoins, pour l‚Äôorganisation qui g√®re cette application, les conteneurs offriront plus de libert√© et de flexibilit√© comme nous l‚Äôavons √©voqu√©."
  },
  {
    "objectID": "chapters/portability.html#docker-limpl√©mentation-standard",
    "href": "chapters/portability.html#docker-limpl√©mentation-standard",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Docker , l‚Äôimpl√©mentation standard",
    "text": "Docker , l‚Äôimpl√©mentation standard\nComme nous l‚Äôavons √©voqu√©, le logiciel de conteneurisation fait office de couche tampon entre les applications et le syst√®me d‚Äôexploitation du serveur.\nComme pour les environnements virtuels, il existe diff√©rentes impl√©mentations de la technologie des conteneurs. En pratique, l‚Äôimpl√©mentation offerte par Docker est devenue largement pr√©dominante, au point qu‚Äôil est devenu courant d‚Äôutiliser de mani√®re interchangeable les termes ‚Äúconteneuriser‚Äù et ‚ÄúDockeriser‚Äù une application. C‚Äôest donc cette impl√©mentation que nous allons √©tudier et utiliser dans ce cours.\n\nInstallation et environnements bacs √† sable\nDocker  est un logiciel qui peut s‚Äôinstaller sur diff√©rents syst√®me d‚Äôexploitation. Les instructions sont d√©taill√©es dans la documentation officielle. Il est n√©cessaire d‚Äôavoir des droits administrateurs sur son poste pour pouvoir faire cette installation.\n\n\n\n\n\n\nBesoins en espace disque\n\n\n\n\n\nIl est √©galement recommand√© d‚Äôavoir de l‚Äôespace disque libre car certaines images (concept sur lequel nous reviendrons), une fois d√©compress√©es et construites, peuvent √™tre lourdes selon la richesse des librairies install√©es dessus. Elles peuvent rapidement prendre quelques Gigas d‚Äôespace disque.\nCeci est n√©anmoins √† comparer √† l‚Äôespace disque monstrueux que peut prendre un syst√®me d‚Äôexploitation complet (autour de 15GB pour Ubuntu ou Mac OS, 20GB par exemple pour Windows‚Ä¶). La distribution Linux la plus minimaliste (Alpine) ne fait que 3Mo compress√©e et 5Mo une fois d√©compress√©e.\n\n\n\nIl existe √©galement des environnements en ligne gratuits pouvant servir de bacs √† sable s‚Äôil n‚Äôest pas possible pour vous d‚Äôinstaller Docker.\nPlay with Docker permet de tester en ligne Docker comme on pourrait le faire sur une installation personnelle. N√©anmoins, ces services sont limit√©s: la taille maximale des images d√©compress√©es est limit√©e √† 2Go, les services connaissent des coupures en cas d‚Äôutilisation massive‚Ä¶\nComme nous le verrons ult√©rieurement, l‚Äôutilisation de Docker en interactif est pratique pour apprendre et exp√©rimenter. N√©anmoins, en pratique, on utilise principalement Docker par le biais de l‚Äôint√©gration continue via Github Actions ou Gitlab CI.\n\n\nPrincipes\n\n\n\nSource : k21academy.com\n\n\nUn conteneur Docker est mis √† disposition sous la forme d‚Äôune image, c‚Äôest √† dire d‚Äôun fichier binaire qui contient l‚Äôenvironnement n√©cessaire √† l‚Äôex√©cution de l‚Äôapplication. Celui-ci est mis √† disposition de tous sous une forme compress√©e sur un d√©p√¥t d‚Äôimages publiques (le plus connu est Dockerhub).\nAvant de mettre √† disposition une image, il est n√©cessaire de la construire (build). Pour cela on utilise un Dockerfile, un fichier texte qui contient la recette ‚Äî sous forme de commandes Linux ‚Äî de construction de l‚Äôenvironnement.\nUne fois l‚Äôimage construite, il est possible de faire deux actions:\n\nLa lancer (run) en local. Cela permet de tester l‚Äôapplication, √©ventuellement de la corriger en cas de mauvais fonctionnement. Le lancement de l‚Äôapplication permet de faire tourner l‚Äôimage dans un environnement isol√© qu‚Äôon appelle le conteneur (container), une instance vivante de l‚Äôimage en quelques sortes7.\nLa mettre √† disposition sur un d√©p√¥t public pour permettre √† d‚Äôautres (ou √† soi-m√™me) de la tester. L‚Äôimage va √™tre upload√©e (push) sur un d√©p√¥t (registry), public ou priv√©, depuis lequel les utilisateurs vont pouvoir t√©l√©charger l‚Äôimage (pull).\n\n\n\n\n\n\n\nMettre √† disposition son image Docker\n\n\n\n\n\nLe r√©pertoire d‚Äôimages publiques le plus connu est DockerHub. Il s‚Äôagit d‚Äôun r√©pertoire o√π n‚Äôimporte qui peut proposer une image Docker, associ√©e ou non √† un projet disponible sur Github ou Gitlab. Il est possible de mettre √† disposition de mani√®re manuelle des images mais, comme nous le montrerons dans le chapitre sur la mise en production, il est beaucoup plus pratique d‚Äôutiliser des fonctionalit√©s d‚Äôinteraction automatique entre DockerHub et un d√©p√¥t GitHub."
  },
  {
    "objectID": "chapters/portability.html#application",
    "href": "chapters/portability.html#application",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Application",
    "text": "Application\nAfin de pr√©senter l‚Äôutilisation de Docker en pratique, nous allons pr√©senter les diff√©rentes √©tapes permettant de ‚Äúdockeriser‚Äù une application web minimaliste construite avec le framework Python Flask8.\nLa structure de notre projet est la suivante.\n‚îú‚îÄ‚îÄ myflaskapp\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îú‚îÄ‚îÄ hello-world.py\n‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt\nLe script hello-world.py contient le code d‚Äôune application minimaliste, qui affiche simplement ‚ÄúHello, World!‚Äù sur une page web. Nous verrons dans l‚Äôapplication fil rouge comment construire une application interactive plus compl√®te.\n\n\nhello-world.py\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef hello_world():\n    return \"&lt;p&gt;Hello, World!&lt;/p&gt;\"\n\nPour faire tourner l‚Äôapplication, il nous faut donc √† la fois Python et le package Flask. Il est donc n√©cessaire de contr√¥ler l‚Äôenvironnement virtuel autour de Python ce qui va impliquer:\n\nInstaller Python ;\nInstaller les packages n√©cessaires √† l‚Äôex√©cution de notre code. En l‚Äôoccurrence, on n‚Äôa besoin que de Flask.\n\nSi la version de Python utilis√©e par notre application nous importe peu, il est plus simple d‚Äôadopter un environnement virtuel venv que conda. Nous allons donc proposer d‚Äôutiliser ceci ce qui tombe bien car nous avons d√©j√† notre requirements.txt qui prend la forme suivante:\n\n\nrequirements.txt\n\nFlask==2.1.1\n\nCes installations en deux temps (Python et packages n√©cessaires) doivent √™tre sp√©cifi√©es dans le Dockerfile (cf.¬†section suivante)."
  },
  {
    "objectID": "chapters/portability.html#dockerfile",
    "href": "chapters/portability.html#dockerfile",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Le Dockerfile",
    "text": "Le Dockerfile\nPour faire un plat, il faut une recette. Pour faire une image image Docker, il faut un Dockerfile.\nCe fichier texte contient une s√©rie de commandes qui permettent de construire l‚Äôimage. Ces fichiers peuvent √™tre plus ou moins complexes selon l‚Äôapplication que l‚Äôon cherche √† conteneuriser, mais leur structure est assez normalis√©e.\nL‚Äôid√©e est de partir d‚Äôune couche de base (une distribution Linux minimaliste) et y ajouter des couches en fonction des besoins de notre application.\nPour illustrer cela, analysons ligne √† ligne le Dockerfile n√©cessaire pour construire une image Docker de notre application Flask.\n\n\n\nDockerfile\n\n1FROM ubuntu:20.04\n\nRUN apt-get update -y && \\ \n2    apt-get install -y python3-pip python3-dev\n    \n3WORKDIR /app\n\n4COPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY . /app\n\n5ENV FLASK_APP=\"hello-world.py\"\n6EXPOSE 5000\n\n7CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n\n\n\n1\n\nFROM : sp√©cifie l‚Äôimage de base. Une image Docker h√©rite toujours d‚Äôune image de base. Ici, on choisit l‚Äôimage Ubuntu version 20.04, tout va donc se passer comme si l‚Äôon d√©veloppait sur une machine virtuelle vierge ayant pour syst√®me d‚Äôexploitation Ubuntu 20.04  ;\n\n2\n\nRUN : lance une commande Linux. Ici, on met d‚Äôabord √† jour la liste des packages t√©l√©chargeables via apt, puis on installe Python  ainsi que des librairies syst√®me n√©cessaires au bon fonctionnement de notre application ;\n\n3\n\nWORKDIR : sp√©cifie le r√©pertoire de travail de l‚Äôimage. Ainsi, toutes les commandes suivantes seront ex√©cut√©es depuis ce r√©pertoire. C‚Äôest l‚Äô√©quivalent Docker de la commande cd (voir Linux 101) ;\n\n4\n\nCOPY : copie un fichier local sur l‚Äôimage Docker. Cela est li√© √† la mani√®re dont fonctionne Docker. Pour ne pas polluer l‚Äôimage de fichiers non n√©cessaires (qui affecteront de mani√®re incidente sa configuration et le poids de celle-ci), par d√©faut, votre image ne contient pas de fichiers de votre projet. Si certains sont n√©cessaires pour construire l‚Äôimage, il faut explicitement le dire √† Docker. Ici, on copie d‚Äôabord le fichier requirements.txt du projet, qui sp√©cifie les d√©pendances Python de notre application, puis on les installe avec une commande RUN. La seconde instruction COPY copie le r√©pertoire du projet sur l‚Äôimage ;\n\n5\n\nENV : cr√©e une variable d‚Äôenvironnement qui sera accessible √† l‚Äôapplication dans le conteneur. Ici, on d√©finit une variable d‚Äôenvironnement attendue par Flask, qui sp√©cifie le nom du script permettant de lancer l‚Äôapplication ;\n\n6\n\nEXPOSE : informe Docker que le conteneur ‚Äú√©coute‚Äù sur le port 5000, qui est le port par d√©faut utilis√© par le serveur web de Flask. Ceci est li√© √† la nature du fonctionnement de Flask qui lance un localhost sur un port donn√©, en l‚Äôoccurrence le port 5000. ;\n\n7\n\nCMD : sp√©cifie la commande que doit ex√©cuter le conteneur lors de son lancement. Il s‚Äôagit d‚Äôune liste, qui contient les diff√©rentes parties de la commande sous forme de cha√Ænes de caract√®res. Ici, on lance la commande flask run qui sait automatiquement quelle application lancer du fait de la commande ENV sp√©cifi√©e pr√©c√©demment. On ajoute l‚Äôoption --host=0.0.0.0 pour que ce soit l‚Äôapplication d√©ploy√©e sur le localhost (notre application Flask) qui soit mis √† disposition de l‚Äôutilisateur final.\n\n\n\n\n\n\n\n\n\n\nChoisir l‚Äôimage de base\n\n\n\n\n\nDans l‚Äôid√©al, on essaie de partir d‚Äôune couche la plus petite possible pour limiter la taille de l‚Äôimage finalement obtenue. Il n‚Äôest en effet pas n√©cessaire d‚Äôutiliser une image disposant de  si on n‚Äôutilise que du .\nEn g√©n√©ral, les diff√©rents langages proposent des images de petite taille dans lequel un interpr√©teur est d√©j√† install√© et proprement configur√©. Dans cette application, on aurait par exemple pu utiliser l‚Äôimage python:3.9-slim-buster.\n\n\n\nAvec la premi√®re commande RUN du Dockerfile, nous installons Python mais aussi des librairies syst√®me n√©cessaires au bon fonctionnement de l‚Äôapplication. Mais comment les avons-nous trouv√©es ?\nPar essai et erreur. Lors de l‚Äô√©tape de build que l‚Äôon verra juste apr√®s, le moteur Docker va essayer de construire l‚Äôimage selon les sp√©cifications du Dockerfile, comme s‚Äôil partait d‚Äôun ordinateur vide contenant simplement Ubuntu 20.04. Si des librairies manquent, le processus de build devrait renvoyer une erreur, qui s‚Äôaffichera dans les logs de l‚Äôapplication, affich√©s par d√©faut dans la console. Quand on a de la chance, les logs d√©crivent explicitement les librairies syst√®me manquantes. Mais souvent, les messages d‚Äôerreur ne sont pas tr√®s explicites, et il faut alors les copier dans un moteur de recherche bien connu pour trouver la r√©ponse, souvent sur StackOverflow.\nIl est recommand√©, avant d‚Äôessayer de cr√©er une image Docker, de passer par l‚Äô√©tape interm√©diaire dans la d√©marche de reproductibilit√© qu‚Äôest la cr√©ation d‚Äôun script shell (.sh). Cette approche graduelle est illustr√©e dans l‚Äôapplication fil rouge.\n\n\n\n\n\n\nL‚Äôinstruction COPY\n\n\n\n\n\nLa recette pr√©sente dans le Dockerfile peut n√©cessiter l‚Äôutilisation de fichiers appartenant au dossier de travail. Pour que Docker les trouve dans son contexte, il est n√©cessaire d‚Äôintroduire une commande COPY. C‚Äôest un petit peu comme pour la cuisine: pour utiliser un produit dans une recette, il faut le sortir du frigo (fichier local) et le mettre sur la table.\n\n\n\nNous n‚Äôavons ici vu que les commandes Docker les plus fr√©quentes, il en existe beaucoup d‚Äôautres en pratique. N‚Äôh√©sitez pas √† consulter la documentation officielle pour comprendre leur utilisation."
  },
  {
    "objectID": "chapters/portability.html#build",
    "href": "chapters/portability.html#build",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Construction d‚Äôune image Docker",
    "text": "Construction d‚Äôune image Docker\nPour construire une image √† partir d‚Äôun Dockerfile, il suffit d‚Äôutiliser la commande docker build depuis la ligne de commande9. Il faut ensuite sp√©cifier deux √©l√©ments importants :\n\nle build context. Il faut indiquer √† Docker le chemin de notre projet, qui doit contenir le Dockerfile. En pratique, il est plus simple de se mettre dans le dossier du projet via la commande cd, puis de passer . comme build context pour indiquer √† Docker de build ‚Äúd‚Äôici‚Äù ;\nle tag, c‚Äôest √† dire le nom de l‚Äôimage. Tant que l‚Äôon utilise Docker en local, le tag importe peu. On verra par la suite que la structure du tag a de l‚Äôimportance lorsque l‚Äôon souhaite exporter ou importer une image Docker √† partir d‚Äôun d√©p√¥t distant.\n\nRegardons ce qui se passe en pratique lorsque l‚Äôon essaie de construire notre image. Le tag de celle-ci est myflaskapp:\n\n\nterminal\n\n$ docker build -t myflaskapp .\n\nSending build context to Docker daemon     47MB\nStep 1/8 : FROM ubuntu:20.04\n ---&gt; 825d55fb6340\nStep 2/8 : RUN apt-get update && apt-get install -y python3-pip python3-dev\n ---&gt; Running in 92b42d579cfa\n...\ndone.\nRemoving intermediate container 92b42d579cfa\n ---&gt; 8826d53e3c01\nStep 3/8 : WORKDIR /app\n ---&gt; Running in 153b32893c23\nRemoving intermediate container 153b32893c23\n ---&gt; 7b4d22021986\nStep 4/8 : COPY requirements.txt /app/requirements.txt\n...\nSuccessfully built 125bd8da70ff\nSuccessfully tagged myflaskapp:latest\nLe moteur Docker essaie de construire notre image s√©quentiellement √† partir des commandes sp√©cifi√©es dans le Dockerfile. S‚Äôil rencontre une erreur, la proc√©dure s‚Äôarr√™te, et il faut alors trouver la source du probl√®me dans les logs et adapter le Dockerfile en cons√©quence.\nSi tout se passe bien, Docker nous indique que le build a r√©ussi et l‚Äôimage est pr√™te √† √™tre utilis√©e. On peut v√©rifier que l‚Äôimage est bien disponible √† l‚Äôaide de la commande docker images.\n\n\nterminal\n\n$ docker images\n\nREPOSITORY                               TAG       IMAGE ID       CREATED          SIZE\nmyflaskapp                               latest    57d2f410a631   2 hours ago      433MB\nInt√©ressons nous un peu plus en d√©tail aux logs de l‚Äô√©tape de build üëÜÔ∏è.\nEntre les √©tapes, Docker affiche des suites de lettres et de chiffres un peu √©sot√©riques, et nous parle de conteneurs interm√©diaires. En fait, il faut voir une image Docker comme un empilement de couches (layers), qui sont elles-m√™mes des images Docker. Quand on h√©rite d‚Äôune image avec l‚Äôinstruction FROM, on sp√©cifie donc √† Docker la couche initiale, sur laquelle il va construire le reste de notre environnement. A chaque √©tape sa nouvelle couche, et √† chaque couche son hash, un identifiant unique fait de lettres et de chiffres.\nCela peut ressembler √† des d√©tails techniques, mais c‚Äôest en fait extr√™mement utile en pratique car cela permet √† Docker de faire du caching. Lorsqu‚Äôon d√©veloppe un Dockerfile, il est fr√©quent de devoir modifier ce dernier de nombreuses fois avant de trouver la bonne recette, et on aimerait bien ne pas avoir √† rebuild l‚Äôenvironnement complet √† chaque fois. Docker g√®re cela tr√®s bien : il cache chacune des couches interm√©diaires10.\nPar exemple, si l‚Äôon modifie la 5√®me commande du Dockerfile, Docker va utiliser le cache pour ne pas avoir √† recalculer les √©tapes pr√©c√©dentes, qui n‚Äôont pas chang√©. Cela s‚Äôappelle l‚Äô‚Äúinvalidation du cache‚Äù : d√®s lors qu‚Äôune √©tape du Dockerfile est modifi√©e, Docker va recalculer toutes les √©tapes suivantes, mais seulement celles-ci. Cons√©quence directe de cette observation : il faut toujours ordonner les √©tapes d‚Äôun Dockerfile de sorte √† ce qui est le plus susceptible d‚Äô√™tre souvent modifi√© soit √† la fin du fichier, et inversement.\nPour illustrer cela, regardons ce qui se passe si l‚Äôon modifie le nom du script qui lance l‚Äôapplication, et donc la valeur de la variable d‚Äôenvironnement FLASK_APP dans le Dockerfile.\n\n\nterminal\n\n$ docker build . -t myflaskapp\n\n\nSending build context to Docker daemon  4.096kB\nStep 1/10 : FROM ubuntu:20.04\n ---&gt; 825d55fb6340\nStep 2/10 : ENV DEBIAN_FRONTEND=noninteractive\n ---&gt; Using cache\n ---&gt; ea1c7c083ac9\nStep 3/10 : RUN apt-get update -y &&     apt-get install -y python3-pip python3-dev\n ---&gt; Using cache\n ---&gt; 078b8ac0e1cb\nStep 4/10 : WORKDIR /app\n ---&gt; Using cache\n ---&gt; cd19632825b3\nStep 5/10 : COPY requirements.txt /app/requirements.txt\n ---&gt; Using cache\n ---&gt; 271cd1686899\nStep 6/10 : RUN pip install -r requirements.txt\n ---&gt; Using cache\n ---&gt; 3ea406fdf383\nStep 7/10 : COPY . /app\n ---&gt; 3ce5bd3a9572\nStep 8/10 : ENV FLASK_APP=\"new.py\"\n ---&gt; Running in b378d16bb605\nRemoving intermediate container b378d16bb605\n ---&gt; e1f50490287b\nStep 9/10 : EXPOSE 5000\n ---&gt; Running in ab53c461d3de\nRemoving intermediate container ab53c461d3de\n ---&gt; 0b86eca40a80\nStep 10/10 : CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n ---&gt; Running in 340eec151a51\nRemoving intermediate container 340eec151a51\n ---&gt; 16d7a5b8db28\nSuccessfully built 16d7a5b8db28\nSuccessfully tagged myflaskapp:latest\n\nL‚Äô√©tape de build a pris quelques secondes au lieu de plusieurs minutes, et les logs montrent bien l‚Äôutilisation du cache faite par Docker : les √©tapes pr√©c√©dant le changement r√©utilisent les couches cach√©es, mais celle d‚Äôapr√®s sont recalcul√©es."
  },
  {
    "objectID": "chapters/portability.html#execution",
    "href": "chapters/portability.html#execution",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Ex√©cuter (run) une image Docker",
    "text": "Ex√©cuter (run) une image Docker\nL‚Äô√©tape de build a permis de cr√©er une image Docker. Une image doit √™tre vue comme un template : elle permet d‚Äôex√©cuter l‚Äôapplication sur n‚Äôimporte quel environnement d‚Äôex√©cution sur lequel un moteur Docker est install√©.\nEn l‚Äô√©tat, on a donc juste construit, mais rien lanc√© : notre application ne tourne pas encore. Pour cela, il faut cr√©er un conteneur, i.e.¬†une instance vivante de l‚Äôimage qui permet d‚Äôacc√©der √† l‚Äôapplication. Cela se fait via la commande docker run.\n\n\nterminal\n\n$ docker run -d -p 8000:5000 myflaskapp:latest\n6a2ab0d82d051a3829b182ede7b9152f7b692117d63fa013e7dfe6232f1b9e81\n\nD√©taillons la syntaxe de cette commande :\n\ndocker run tag : lance l‚Äôimage dont on fournit le tag. Le tag est de la forme repository/projet:version. Ici, il n‚Äôy a pas de repository puisque tout est fait en local ;\n-d : ‚Äúd√©tache‚Äù le conteneur du terminal qui le lance ;\n-p : effectue un mapping entre un port de la machine qui ex√©cute le conteneur, et le conteneur lui-m√™me. Notre conteneur √©coute sur le port 5000, et l‚Äôon veut que notre application soit expos√©e sur le port 8000 de notre machine.\n\nLorsque l‚Äôon ex√©cute docker run, Docker nous r√©pond simplement un hash qui identifie le conteneur que l‚Äôon a lanc√©. On peut v√©rifier qu‚Äôil tourne bien avec la commande docker ps, qui renvoie toutes les informations associ√©es au conteneur.\n\n\nterminal\n\n$ docker ps\n\nCONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS                                   NAMES\n6a2ab0d82d05   myflaskapp   \"flask run --host=0.‚Ä¶\"   7 seconds ago   Up 6 seconds   0.0.0.0:8000-&gt;5000/tcp, :::8000-&gt;5000/tcp   vigorous_kalam\nLes conteneurs peuvent √™tre utilis√©s pour r√©aliser des t√¢ches tr√®s diff√©rentes. Grossi√®rement, on peut distinguer deux situations :\n\nle conteneur effectue une t√¢che ‚Äúone-shot‚Äù, c‚Äôest √† dire une op√©ration qui a vocation √† s‚Äôeffectuer en un certain temps, suite √† quoi le conteneur peut s‚Äôarr√™ter ;\nle conteneur ex√©cute une application. Dans ce cas, on souhaite que le conteneur reste en vie aussi longtemps que l‚Äôon souhaite utiliser l‚Äôapplication en question.\n\nDans notre cas d‚Äôapplication, on se situe dans la seconde configuration puisque l‚Äôon veut ex√©cuter une application web. Lorsque l‚Äôapplication tourne, elle expose sur le localhost, accessible depuis un navigateur web ‚Äî en l‚Äôoccurence, √† l‚Äôadresse localhost:5000/. Les calculs sont effectu√©s sur un serveur local, et le navigateur sert d‚Äôinterface avec l‚Äôutilisateur ‚Äî comme lorsque vous utilisez un notebook Jupyter par exemple.\nFinalement, on a pu d√©velopper et ex√©cuter une application compl√®te sur notre environnement local, sans avoir eu √† installer quoi que ce soit sur notre machine personnelle, √† part Docker."
  },
  {
    "objectID": "chapters/portability.html#exp-docker",
    "href": "chapters/portability.html#exp-docker",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Exporter une image Docker",
    "text": "Exporter une image Docker\nJusqu‚Äô√† maintenant, toutes les commandes Docker que nous avons ex√©cut√©es se sont pass√©es en local. Ce mode de fonctionnement peut √™tre int√©ressant pour la phase de d√©veloppement et d‚Äôexp√©rimentation. Mais comme on l‚Äôa vu, un des gros avantages de Docker est la facilit√© de redistribution des images construites, qui peuvent ensuite √™tre utilis√©es par de nombreux utilisateurs pour faire tourner notre application. Pour cela, il nous faut uploader notre image sur un d√©p√¥t distant, √† partir duquel les utilisateurs pourront la t√©l√©charger.\nPlusieurs possibilit√©s existent selon le contexte de travail : une entreprise peut avoir un d√©p√¥t interne par exemple. Si le projet est open source, on peut utiliser le DockerHub.\nLe workflow pour uploader une image est le suivant :\n\ncr√©er un compte sur DockerHub ;\ncr√©er un projet (public) sur DockerHub, qui va h√©berger les images Docker du projet ;\nsur un terminal, utiliser docker login pour s‚Äôauthentifier au DockerHub ;\non va modifier le tag que l‚Äôon fournit lors du build pour sp√©cifier le chemin attendu. Dans notre cas : docker build -t compte/projet:version . ;\nuploader l‚Äôimage avec docker push compte/projet:version\n\n\n\nterminal\n\n$ docker push avouacr/myflaskapp:1.0.0\n\nThe push refers to repository [docker.io/avouacr/myflaskapp]\n71db96687fe6: Pushed \n624877ac887b: Pushed \nea4ab6b86e70: Pushed \nb5120a5bc48d: Pushed \n5fa484a3c9d8: Pushed \nc5ec52c98b31: Pushed \n1.0.0: digest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9 size: 1574"
  },
  {
    "objectID": "chapters/portability.html#imp-docker",
    "href": "chapters/portability.html#imp-docker",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Importer une image Docker",
    "text": "Importer une image Docker\nEn supposant que le d√©p√¥t utilis√© pour uploader l‚Äôimage est public, la proc√©dure que doit suivre un utilisateur pour la t√©l√©charger se r√©sume √† utiliser la commande docker pull compte/projet:version\n\n\nterminal\n\n$ docker pull avouacr/myflaskapp:1.0.0\n\n1.0.0: Pulling from avouacr/myflaskapp\ne0b25ef51634: Pull complete \nc0445e4b247e: Pull complete \n48ba4e71d1c2: Pull complete \nffd728caa80a: Pull complete \n906a95f00510: Pull complete \nd7d49b6e17ab: Pull complete \nDigest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9\nStatus: Downloaded newer image for avouacr/myflaskapp:1.0.0\ndocker.io/avouacr/myflaskapp:1.0.0\nDocker t√©l√©charge et extrait chacune des couches qui constituent l‚Äôimage (ce qui peut parfois √™tre long). L‚Äôutilisateur peut alors cr√©er un conteneur √† partir de l‚Äôimage, en utilisant docker run comme illustr√© pr√©c√©demment."
  },
  {
    "objectID": "chapters/portability.html#aide-m√©moire-1",
    "href": "chapters/portability.html#aide-m√©moire-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Aide-m√©moire",
    "text": "Aide-m√©moire\nVoici une premi√®re aide-m√©moire sur les principales commandes √† int√©grer dans un Dockerfile:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\nFROM &lt;image&gt;:&lt;tag&gt;\nUtiliser comme point de d√©part l‚Äôimage &lt;image&gt; ayant le tag &lt;tag&gt;\n\n\nRUN &lt;instructions&gt;\nUtiliser la suite d‚Äôinstructions &lt;instructions&gt; dans un terminal Linux. Pour passer plusieurs commandes dans un RUN, utiliser &&. Cette suite de commande peut avoir plusieurs lignes, dans ce cas, mettre \\ en fin de ligne\n\n\nCOPY &lt;source&gt; &lt;dest&gt;\nR√©cup√©rer le fichier pr√©sent dans le syst√®me de fichier local √† l‚Äôemplacement &lt;source&gt; pour que les instructions ult√©rieures puissent le trouver √† l‚Äôemplacement &lt;source&gt;\n\n\nADD &lt;source&gt; &lt;dest&gt;\nGlobalement, m√™me r√¥le que COPY\n\n\nENV MY_NAME=\"John Doe\"\nCr√©ation d‚Äôune variable d‚Äôenvironnement (qui devient disponible sous l‚Äôalias $MY_NAME)\n\n\nWORKDIR &lt;path&gt;\nD√©finir le working directory du conteuneur Docker dans le dossier &lt;path&gt;\n\n\nUSER &lt;username&gt;\nCr√©ation d‚Äôun utilisateur non root nomm√© &lt;username&gt;\n\n\nEXPOSE &lt;PORT_ID&gt;\nLorsqu‚Äôelle tournera, l‚Äôapplication sera disponible depuis le port &lt;PORT_ID&gt;\n\n\nCMD [\"executable\",\"param1\",\"param2\"]\nAu lancement de l‚Äôinstance Docker la commande executable (par exemple python3) sera lanc√©e avec les param√®tres additionnels fournis\n\n\n\nUne seconde aide-m√©moire pour les principales commandes Linux est disponible ci-dessous:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\ndocker build . -t &lt;tag&gt;\nConstruire l‚Äôimage Docker √† partir des fichiers dans le r√©pertoire courant (.) en l‚Äôidentifiant avec le tag &lt;tag&gt;\n\n\ndocker run -it &lt;tag&gt;\nLancer l‚Äôinstance docker identifi√©e par &lt;tag&gt;\n\n\ndocker images\nLister les images disponibles sur la machine et quelques unes de leurs propri√©t√©s (tags, volume, etc.)\n\n\ndocker system prune\nFaire un peu de m√©nage dans ses images Docker (bien r√©fl√©chir avant de faire tourner cette commande)"
  },
  {
    "objectID": "chapters/portability.html#footnotes",
    "href": "chapters/portability.html#footnotes",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNous reviendrons plus tard sur la mani√®re dont la mise √† disposition de packages sous forme pr√©compil√©e par le biais de wheels offre une solution √† ce probl√®me.‚Ü©Ô∏é\nS‚Äôil est impossible de suivre les √©volutions de tous les packages de la data science, il est recommand√© de faire une veille sur les principaux comme Pandas ou Scikit en suivant les release notes des versions majeures qui introduisent g√©n√©ralement des non-compatibilit√©s.‚Ü©Ô∏é\nLe solver de conda, qui est un algorithme de recherche de chemin optimal dans des graphes pour g√©rer les (in)compatibilit√©s de versions, est lourd √† mettre en oeuvre. Le projet mamba a permis d‚Äôoffrir une r√©impl√©mentation de Conda en C++ par le biais d‚Äôun solver plus efficace. Cela a permis de franchement acc√©l√©rer la vitesse d‚Äôinstallation des packages par le biais de conda. N√©anmoins, l‚Äôacc√®s de plus en plus fr√©quent √† des wheels a permis un retour en gr√¢ce des environnements virtuels impl√©ment√©s par venv au cours des derni√®res ann√©es.‚Ü©Ô∏é\nCela signifie que si on ouvre un nouveau terminal, il faudra √† nouveau activer cet environnement si on d√©sire l‚Äôutiliser. Si on d√©sire activer par d√©faut un environnement, il est possible de configurer le terminal pour qu‚Äôil active automatiquement un environnement sp√©cifique lors de son ouverture. Cela peut √™tre r√©alis√© en modifiant les fichiers de configuration du shell, par le biais par exemple du script .bashrc sur Linux.‚Ü©Ô∏é\nD‚Äôailleurs, si vous utilisez pip sur le SSPCloud, c‚Äôest ce que vous faites, sans vous en rendre compte.‚Ü©Ô∏é\nCes r√©pertoires sont, dans le langage conda, les canaux. Le canal par d√©faut est maintenu par les d√©veloppeurs dAnaconda. Cependant, pour en assurer la stabilit√©, ce canal a une forte inertie. La conda-forge a √©merg√© pour offrir plus de flexibilit√© aux d√©veloppeurs de package qui peuvent ainsi mettre √† disposition des versions plus r√©centes de leurs packages, comme sur PyPI.‚Ü©Ô∏é\nPar abus de langage, on m√©lange souvent les termes ‚Äúimage‚Äù et ‚Äúconteneur‚Äù. En pratique ces deux concepts sont tr√®s proches. Le second correspond √† la version vivante du premier.‚Ü©Ô∏é\nFlask est un framework permettant de d√©ployer, de mani√®re l√©g√®re, des applications reposant sur Python.‚Ü©Ô∏é\nSi vous √™tes sur Windows, les lignes de commande disponibles par d√©faut (cmd ou Powershell) sont peu pratiques. Il est recommand√© d‚Äôutiliser la ligne de commande de Git Bash (une √©mulation minimaliste d‚Äôune ligne de commande Linux) qui vous permettra de faire des op√©rations en ligne de commande.‚Ü©Ô∏é\nLe cache est tr√®s pratique pour une construction exp√©rimentale en local. Malheureusement, lorsqu‚Äôon passe par des services d‚Äôint√©gration continue, l‚Äôutilisation du cache est moins √©vidente car chaque run se fait sur une machine ind√©pendante de la pr√©c√©dente.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/linux-101.html",
    "href": "chapters/linux-101.html",
    "title": "Linux 101",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des syst√®mes d‚Äôexploitation (y compris avec Windows !). Mais comme il a la r√©putation d‚Äô√™tre aust√®re et complexe, on utilise plut√¥t des interfaces graphiques pour effectuer nos op√©rations informatiques quotidiennes.\nPourtant, avoir des notions quant √† l‚Äôutilisation d‚Äôun terminal est une vraie source d‚Äôautonomie, dans la mesure o√π celui-ci permet de g√©rer bien plus finement les commandes que l‚Äôon r√©alise. Pour les data scientists qui s‚Äôint√©ressent aux bonnes pratiques et √† la mise en production, sa ma√Ætrise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont g√©n√©ralement limit√©es par rapport √† l‚Äôutilisation du programme en ligne de commande. C‚Äôest par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de r√©aliser toutes les op√©rations permises par le logiciel ;\nmettre un projet de data science en production n√©cessite d‚Äôutiliser un serveur, qui le rend disponible en permanence √† son public potentiel. Or l√† o√π Windows domine le monde des ordinateurs personnels, une large majorit√© des serveurs et des infrastructures cloud fonctionnent sous Linux ;\nles principaux langages de programmation des data scientists (, , etc. ) visent √† simplifier l‚Äôex√©cution d‚Äôop√©rations complexes par le biais de la ligne de commande mais h√©ritent n√©anmoins de sa logique. Utiliser la ligne de commande permet de gagner en aisance dans son travail de programmation ;\nplus g√©n√©ralement, une utilisation r√©guli√®re du terminal est source d‚Äôune meilleure compr√©hension du fonctionnement d‚Äôun syst√®me de fichiers et de l‚Äôex√©cution des processus sur un ordinateur. Ces connaissances s‚Äôav√®rent tr√®s utiles dans la pratique quotidienne du data scientist, qui n√©cessite de plus en plus de d√©velopper dans diff√©rents environnements d‚Äôex√©cution.\n\nDans le cadre de ce cours, on s‚Äôint√©ressera particuli√®rement au terminal Linux puisque l‚Äô√©crasante majorit√©, si ce n‚Äôest l‚Äôensemble, des serveurs de mise en production s‚Äôappuient sur un syst√®me Linux.\n\n\n\nDiff√©rents environnements de travail peuvent √™tre utilis√©s pour apprendre √† se servir d‚Äôun terminal Linux :\n\nle SSP Cloud. Dans la mesure o√π les exemples de mise en production du cours seront illustr√©es sur cet environnement, nous recommandons de l‚Äôutiliser d√®s √† pr√©sent pour se familiariser. Le terminal est accessible √† partir de diff√©rents services (RStudio, Jupyter, etc.), mais nous recommandons d‚Äôutiliser le terminal d‚Äôun service VSCode, dans la mesure o√π se servir d‚Äôun IDE pour organiser notre code est en soi d√©j√† une bonne pratique ;\nKatacoda, un bac √† sable dans un syst√®me Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (√©mulation minimaliste d‚Äôun terminal Linux), qui est install√©e par d√©faut avec Git.\n\n\n\n\nLan√ßons un terminal pour pr√©senter son fonctionnement basique. On prend pour exemple le terminal d‚Äôun service VSCode lanc√© via le SSP Cloud (Application Menu tout en haut √† gauche de VSCode -&gt; Terminal -&gt; New Terminal). Voici √† quoi ressemble le terminal en question.\n\nD√©crivons d‚Äôabord les diff√©rentes inscriptions qui arrivent √† l‚Äôinitialisation :\n\n(base) : cette inscription n‚Äôest pas directement li√©e au terminal, elle provient du fait que l‚Äôon utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en d√©tail dans le chapitre sur la portabilit√© ;\ncoder@vscode-824991-64744dd6d8-zbgv5 : le nom de l‚Äôutilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que l‚Äôon verra l√† encore dans le chapitre sur la portabilit√©\n~/work : le chemin du r√©pertoire courant, i.e.¬†√† partir duquel va √™tre lanc√©e toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\n\nPour √©viter la lourdeur des images et permettre de copier/coller facilement les commandes, on repr√©sentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans l‚Äôexemple suivant. Les lignes commen√ßant par un $ sont celles avec lesquelles une commande est lanc√©e, et les lignes sans $ repr√©sentent le r√©sultat d‚Äôune commande. Attention √† ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement √† diff√©rencier celles-ci des r√©sultats.\n\n\nterminal\n\n$ echo \"une petite illustration\"\n\nune petite illustration\n\n\n\nLe terme filesystem (syst√®me de fichiers) d√©signe la mani√®re dont sont organis√©s les fichiers au sein d‚Äôun syst√®me d‚Äôexploitation. Cette structure est hi√©rarchique, en forme d‚Äôarbre :\n\nelle part d‚Äôun r√©pertoire racine (le dossier qui contient tous les autres) ;\ncontient des dossiers ;\nles dossiers peuvent contenir √† leur tout des dossiers (sous-dossiers) ou des fichiers.\n\nInt√©ressons nous √† la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations :\n\nla racine (root) sur Linux s‚Äôappelle /, l√† o√π elle s‚Äôappelle C:\\ par d√©faut sur Windows ;\nle r√©pertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un r√¥le essentiellement technique. Il est tout de m√™me utile d‚Äôen d√©crire les principaux :\n\n/bin : contient les binaires, i.e.¬†les programmes ex√©cutables ;\n/etc : contient les fichiers de configuration ;\n/home : contient l‚Äôensemble des dossiers et fichiers personnels des diff√©rents utilisateurs. Chaque utilisateur a un r√©pertoire dit ‚ÄúHOME‚Äù qui a pour chemin /home/&lt;username&gt; Ce r√©pertoire est souvent repr√©sent√© par le symbole ~. C‚Äô√©tait notamment le cas dans l‚Äôillustration du terminal VSCode ci-dessus, ce qui signifie qu‚Äôon se trouvait formellement dans le r√©pertoire /home/coder/work, coder √©tant l‚Äôutilisateur par d√©faut du service VSCode sur le SSP Cloud.\n\n\nChaque dossier ou fichier est repr√©sent√© par un chemin d‚Äôacc√®s, qui correspond simplement √† sa position dans le filesystem. Il existe deux moyens de sp√©cifier un chemin :\n\nen utilisant un chemin absolu, c‚Äôest √† dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconna√Æt donc un chemin absolu par le fait qu‚Äôil commence forc√©ment par /.\nen utilisant un chemin relatif, c‚Äôest √† dire en indiquant le chemin du dossier ou fichier relativement au r√©pertoire courant.\n\nComme tout ce qui touche de pr√®s ou de loin au terminal, la seule mani√®re de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront d‚Äôappliquer ces concepts √† des cas pratiques.\n\n\n\nLe r√¥le d‚Äôun terminal est de lancer des commandes. Ces commandes peuvent √™tre class√©es en trois grandes cat√©gories :\n\nnavigation au sein du filesystem\nmanipulations de fichiers (cr√©er, lire, modifier des dossiers/fichiers)\nlancement de programmes\n\n\n\nLorsque l‚Äôon lance un programme √† partir du terminal, celui-ci a pour r√©f√©rence le r√©pertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si l‚Äôon ex√©cute un script Python en se trouvant dans un certain r√©pertoire, tous les chemins des fichiers utilis√©s dans le script seront relatifs au r√©pertoire courant d‚Äôex√©cution ‚Äî √† moins d‚Äôutiliser uniquement des chemins absolus, ce qui n‚Äôest pas une bonne pratique en termes de reproductibilit√© puisque cela lie votre projet √† la structure de votre filesystem particulier.\nAinsi, la tr√®s grande majorit√© des op√©rations que l‚Äôon est amen√© √† r√©aliser dans un terminal consiste simplement √† se d√©placer au sein du filesystem. Les commandes principales pour naviguer et se rep√©rer dans le filesystem sont pr√©sent√©es dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez p√©nible de manipuler des chemins absolus, qui peuvent facilement √™tre tr√®s longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient √† se d√©placer √† partir du r√©pertoire courant. Pour se faire, voici quelques utilisations tr√®s fr√©quentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter d‚Äôun niveau dans l‚Äôarborescence (dossier parent)\n\n\ncd ~\nrevenir dans le r√©pertoire HOME de l‚Äôutilisateur courant\n\n\n\nLa premi√®re commande est l‚Äôoccasion de revenir sur une convention d‚Äô√©criture importante pour les chemins relatifs :\n\n. repr√©sente le r√©pertoire courant. Ainsi, cd . revient √† changer de r√©pertoire courant‚Ä¶ pour le r√©pertoire courant, ce qui bien s√ªr ne change rien. Mais le . est tr√®s utile pour la copie de fichiers (cf.¬†section suivante) ou encore lorsque l‚Äôon doit passer des param√®tres √† un programme (cf.¬†section Lancement de programmes) ;\n.. repr√©sente le r√©pertoire parent du r√©pertoire courant.\n\nCes diff√©rentes commandes constituent la tr√®s grande majorit√© des usages dans un terminal. Il est essentiel de les pratiquer jusqu‚Äô√† ce qu‚Äôelles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup d‚Äôautres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\nd√©placer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncr√©er (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncr√©er un fichier vide\n\n\n\nDans la mesure o√π il est g√©n√©ralement possible de r√©aliser toutes ces op√©rations √† l‚Äôaide d‚Äôinterfaces graphiques (notamment, l‚Äôexplorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se d√©placer dans le filesystem. Nous vous recommandons malgr√© tout de les pratiquer √©galement, et ce pour plusieurs raisons :\n\neffectuer un maximum d‚Äôop√©rations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ;\nen devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ;\nlorsque l‚Äôon est amen√© √† manipuler un terminal pour interagir avec un serveur, il n‚Äôy a souvent pas la moindre interface graphique, auquel cas il n‚Äôy a pas d‚Äôautre choix que d‚Äôop√©rer uniquement √† partir du terminal.\n\n\n\n\nLe r√¥le du terminal est de lancer des programmes. Lancer un programme se fait √† partir d‚Äôun fichier dit ex√©cutable, qui peut √™tre de deux formes :\n\nun binaire, i.e.¬†un programme dont le code n‚Äôest pas lisible par l‚Äôhumain ;\nun script, i.e.¬†un fichier texte contenant une s√©rie d‚Äôinstructions √† ex√©cuter. Le langage du terminal Linux est le shell, et les scripts associ√©s ont pour extension .sh.\n\nDans les deux cas, la syntaxe de lancement d‚Äôune commande est : le nom de l‚Äôex√©cutable, suivi d‚Äô√©ventuels param√®tres, s√©par√©s par des espaces. Par exemple, la commande python monscript.py ex√©cute le binaire python et lui passe comme unique argument le nom d‚Äôun script .py (contenu dans le r√©pertoire courant), qui va donc √™tre ex√©cut√© via Python. De la m√™me mani√®re, toutes les commandes vues pr√©c√©demment pour se d√©placer dans le filesystem ou manipuler des fichiers sont des ex√©cutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier √† copier et le chemin d‚Äôarriv√©e.\nDans les exemples de commandes pr√©c√©dents, les param√®tres √©taient pass√©s en mode positionnel : l‚Äôex√©cutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments n‚Äôest pas toujours fix√© √† l‚Äôavance, du fait de la pr√©sence de param√®tres optionnels. Ainsi, la plupart des ex√©cutables permettent le passage d‚Äôarguments optionnels, qui modifient le comportement de l‚Äôex√©cutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier √† un autre endroit du filesystem, mais peut-on copier un dossier et l‚Äôensemble de son contenu avec ? Nativement non, mais l‚Äôajout d‚Äôun param√®tre le permet : cp -R dossierdepart dossierarrivee permet de copier r√©cursivement le dossier et tout son contenu. Notons que les flags ont tr√®s souvent un √©quivalent en toute lettre, qui s‚Äô√©crit quant √† lui avec deux tirers. Par exemple, la commande pr√©c√©dente peut s‚Äô√©crire de mani√®re √©quivalente cp --recursive dossierdepart dossierarrivee. Il est fr√©quent de voir les deux syntaxes en pratique, parfois m√™me m√©lang√©es au sein d‚Äôune m√™me commande.\n\n\n\n\nComme tout langage de programmation, le langage shell permet d‚Äôassigner et d‚Äôutiliser des variables dans des commandes. Pour afficher le contenu d‚Äôune variable, on utilise la commande echo, qui est l‚Äô√©quivalent de la fonction print en Python ou en R.\n\n\nterminal\n\n$ MY_VAR=\"toto\"\n$ echo $MY_VAR\n\ntoto\nQuelques remarques importantes :\n\nla syntaxe pour la cr√©ation de variable est pr√©cise : aucun espace d‚Äôun c√¥t√© comme de l‚Äôautre du = ;\nen Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu √©crire MY_VAR=toto pour le m√™me r√©sultat. Par contre, si l‚Äôon veut assigner √† une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message d‚Äôerreur ;\npour acc√©der √† la valeur d‚Äôune variable, on la pr√©fixe d‚Äôun $.\n\nNotre objectif avec ce tutoriel n‚Äôest pas de savoir coder en shell, on ne va donc pas s‚Äôattarder sur les propri√©t√©s des variables. En revanche, introduire ce concept √©tait n√©cessaire pour en pr√©senter un autre, essentiel quant √† lui dans la pratique quotidienne du data scientist : les variables d‚Äôenvironnement. Pour faire une analogie ‚Äî un peu simpliste ‚Äî avec les langages de programmation, ce sont des sortes de variables ‚Äúglobales‚Äù, dans la mesure o√π elles vont √™tre accessibles √† tous les programmes lanc√©s √† partir d‚Äôun terminal, et vont modifier leur comportement.\nLa liste des variables d‚Äôenvironnement peut √™tre affich√©e √† l‚Äôaide de la commande env. Il y a g√©n√©ralement un grand nombre de variables d‚Äôenvironnement pr√©√©xistantes ; en voici un √©chantillon obtenu √† partir du terminal du service VSCode.\n\n\nterminal\n\n$ env\n\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la vari√©t√© des utilisations des variables d‚Äôenvironnements : - la variable $SHELL pr√©cise l‚Äôex√©cutable utilis√© pour lancer le terminal ; - la variable $HOME donne l‚Äôemplacement du r√©pertoire utilisateur. En fait, le symbole ~ que l‚Äôon a rencontr√© plus haut r√©f√©rence cette m√™me variable ; - la variable LANG sp√©cifie la locale, un concept qui permet de d√©finir la langue et l‚Äôencodage utilis√©s par d√©faut par Linux ; - la variable CONDA_PYTHON_EXE existe uniquement parce que l‚Äôon a install√© conda comme syst√®me de gestion de packages Python. C‚Äôest l‚Äôexistance de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous int√©resse.\nUne variable d‚Äôenvironnement essentielle, et que l‚Äôon est fr√©quemment amen√© √† modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concat√©nation de chemins absolus, s√©par√©s par :, qui sp√©cifie les dossiers dans lesquels Linux va chercher les ex√©cutables lorsque l‚Äôon lance une commande, ainsi que l‚Äôordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n\n\nterminal\n\n$ echo $PATH\n\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nL‚Äôordre de recherche est de gauche √† droite. C‚Äôest donc parce que le dossier /home/coder/local/bin/conda/bin est situ√© en premier que l‚Äôinterpr√©teur Python qui sera choisi lorsque l‚Äôon lance un script Python est celui issu de Conda, et non celui contenu par d√©faut dans /usr/bin par exemple.\nL‚Äôexistence et la configuration ad√©quate des variables d‚Äôenvironnement est essentielle pour le bon fonctionnement de nombreux outils tr√®s utilis√©s en data science, comme Git ou encore Spark par exemple. Il est donc n√©cessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration d‚Äôun serveur en cas de bug li√© √† une variable d‚Äôenvironnement manquante ou mal configur√©e.\n\n\n\nLa s√©curit√© est un enjeu central en Linux, qui permet une gestion tr√®s fine des permissions sur les diff√©rents fichiers et programmes.\nUne diff√©rence majeure par rapport √† d‚Äôautres syst√®mes d‚Äôexploitation, notamment Windows, est qu‚Äôaucun utilisateur n‚Äôa par d√©faut les droits complets d‚Äôadministrateur (root). Il n‚Äôest donc pas possible nativement d‚Äôacc√©der au parties sensibles du syst√®me, ou bien de lancer certains types de programme. Par exemple, si l‚Äôon essaie de lister les fichiers du dossier /root, on obtient une erreur.\n\n\nterminal\n\n$ ls /root\n\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines op√©rations telles que l‚Äôinstallation de binaires ou de packages n√©cessitent cependant des droits administrateurs. Dans ce cas, il est d‚Äôusage d‚Äôutiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de l‚Äôex√©cution de la commande.\n\n\nterminal\n\n$ sudo ls /root\n\nLe dossier /root √©tant vide, la commande ls renvoie une cha√Æne de caract√®res vide, mais nous n‚Äôavons plus de probl√®me de permission. Notons qu‚Äôune bonne pratique de s√©curit√©, en particulier dans les scripts shell que l‚Äôon peut √™tre amen√©s √† √©crire ou ex√©cuter, est de limiter l‚Äôutilisation de cette commande aux cas o√π elle s‚Äôav√®re n√©cessaire.\nUne autre subtilit√© concerne justement l‚Äôex√©cution de scripts shell. Par d√©faut, qu‚Äôil soit cr√©√© par l‚Äôutilisateur ou t√©l√©charg√© d‚Äôinternet, un script n‚Äôest pas ex√©cutable.\n\n\nterminal\n\n1$ touch test.sh\n2$ ./test.sh\n\n\n1\n\nCr√©er le script test.sh (vide)\n\n2\n\nEx√©cuter le script test.sh\n\n\nbash: ./test.sh: Permission denied\nC‚Äôest bien entendu une mesure de s√©curit√© pour √©viter l‚Äôex√©cution automatique de scripts potentiellement malveillants. Pour pouvoir ex√©cuter un tel script, il faut attribuer des droits d‚Äôex√©cution au fichier avec la commande chmod. Il devient alors possible d‚Äôex√©cuter le script classiquement.\n\n\nterminal\n\n1$ chmod +x test.sh\n2$ ./test.sh\n$ # Le script √©tant vide, il ne se passe rien\n\n\n1\n\nDonner des droits d‚Äôex√©cution au script test.sh\n\n2\n\nEx√©cuter le script test.sh\n\n\n\n\n\nMaintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell pr√©c√©demment √©voqu√©s. A l‚Äôinstar d‚Äôun script Python, un script shell permet d‚Äôautomatiser une s√©rie de commandes lanc√©es dans un terminal. Le but de ce tutoriel n‚Äôest pas de savoir √©crire des scripts shell complexes, travail g√©n√©ralement d√©volu aux les data engineers ou les sysadmin (administrateurs syst√®me), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces comp√©tences sont essentielles lorsque l‚Äôon se pr√©occupe de mise en production. A titre d‚Äôexemple, comme nous le verrons dans le chapitre sur la portabilit√©, il est fr√©quent d‚Äôutiliser un script shell comme entrypoint d‚Äôune image docker, afin de sp√©cifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement √† l‚Äôaide d‚Äôun script simple. Consid√©rons les commandes suivantes, que l‚Äôon met dans un fichier monscript.sh dans le r√©pertoire courant.\n\n\nterminal\n\n$ #!/bin/bash\n$ SECTION=$1\n$ CHAPTER=$2\n$ FORMATION_DIR=/home/coder/work/formation\n$ mkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\n$ touch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\n\nAnalysons la structure de ce script :\n\nla premi√®re ligne est classique, elle se nomme le shebang : elle indique au syst√®me quel interpr√©teur utiliser pour ex√©cuter ce script. Dans notre cas, et de mani√®re g√©n√©rale, on utilise bash (Bourne-Again SHell, l‚Äôimpl√©mentation moderne du shell) ;\nles lignes 2 et 3 assignent √† des variables les arguments pass√©s au script dans la commande. Par d√©faut, ceux-ci sont assign√©s √† des variables n o√π n est la position de l‚Äôargument, en commen√ßant √† 1 ;\nla ligne 4 assigne un chemin √† une variable\nla ligne 5 cr√©e le chemin complet, d√©fini √† partir des variables cr√©√©es pr√©c√©demment. Le param√®tre -p est important : il pr√©cise √† mkdir d‚Äôagir de mani√®re r√©cursive, c‚Äôest √† dire de cr√©er les dossiers interm√©diaires qui n‚Äôexistent pas encore ;\nla ligne 6 cr√©e un fichier texte vide dans le dossier cr√©√© avec la commande pr√©c√©dente.\n\nEx√©cutons maintenant ce script, en prenant soin de lui donner les permission ad√©quates au pr√©alable.\n\n\nterminal\n\n$ chmod +x monscript.sh\n$ bash monscript.sh section2 chapitre3\n$ ls formation/section1/chapitre2/\n\ntext.txt\nOp√©ration r√©ussie : le dossier a bien √©t√© cr√©√© et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash tr√®s bien r√©alis√©e.\n\n\n\nUne diff√©rence fondamentale entre Linux et Windows tient √† la mani√®re dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier ex√©cutable en .exe) sur le site du logiciel, et on l‚Äôex√©cute. En Linux, on passe g√©n√©ralement par un gestionnaire de packages qui va chercher les logiciels sur un r√©pertoire centralis√©, √† la mani√®re de pip en Python par exemple.\nPourquoi cette diff√©rence ? Une raison importante est que, contrairement √† Windows, il existe une multitude de distributions diff√©rentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent diff√©remment et peuvent avoir diff√©rentes versions. En utilisant le package manager (gestionnaire de paquets) propre √† la distribution en question, on s‚Äôassure de t√©l√©charger le logiciel adapt√© √† sa distribution. Dans ce cours, on fait le choix d‚Äôutiliser une distribution Debian et son gestionnaire de paquets associ√© apt. Debian est en effet un choix populaire pour les servers de part sa stabilit√© et sa simplicit√©, et sera √©galement famili√®re aux utilisateurs d‚ÄôUbuntu, distribution tr√®s populaire pour les ordinateurs personnels et qui est bas√©e sur Debian.\nL‚Äôutilisation d‚Äôapt est tr√®s simple. La seule difficult√© est de savoir le nom du paquet que l‚Äôon souhaite installer, ce qui n√©cessite en g√©n√©ral d‚Äôutiliser un moteur de recherche. L‚Äôinstallation de paquets est √©galement un cas o√π il faut utiliser sudo, puisque cela implique souvent l‚Äôacc√®s √† des r√©pertoires prot√©g√©s.\n\n\nterminal\n\n$ sudo apt install tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nD√©sinstaller un package est √©galement simple : c‚Äôest l‚Äôop√©ration inverse. Par s√©curit√©, le terminal vous demande si vous √™tes s√ªr de votre choix en vous demandant de tapper la lettre y ou la lettre n.¬†On peut passer automatiquement cette √©tape en ajoutant le param√®tre -y\n\n\nterminal\n\n$ sudo apt remove -y tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant d‚Äôinstaller un package, il est toujours pr√©f√©rable de mettre √† jour la base des packages, pour s‚Äôassurer qu‚Äôon obtiendra bien la derni√®re version.\n\n\nterminal\n\n$ sudo apt update\n\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date.\n\n\n\nOn l‚Äôa dit et redit : devenir √† l‚Äôaise avec le terminal Linux est essentiel et demande de la pratique. Il existe n√©anmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa premi√®re est l‚Äôautocompl√©tion. D√®s lors que vous √©crivez une commande contenant un nom d‚Äôex√©cutable, un chemin sur le filesystem, ou autre, n‚Äôh√©sitez pas √† utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majorit√© des cas, cela va vous faire gagner un temps pr√©cieux.\nUne seconde astuce, qui n‚Äôen est pas vraiment une, est de lire la documentation d‚Äôune commande lorsque l‚Äôon n‚Äôest pas s√ªr de sa syntaxe ou des param√®tres admissibles. Via le terminal, la documentation d‚Äôune commande peut √™tre affich√©e en ex√©cutant man suivie de la commande en question, par exemple : man cp. Comme il n‚Äôest pas toujours tr√®s pratique de lire de longs textes dans un petit terminal, on peut √©galement chercher la documentation d‚Äôune commande sur le site man7."
  },
  {
    "objectID": "chapters/linux-101.html#pourquoi-sint√©resser-au-terminal-linux",
    "href": "chapters/linux-101.html#pourquoi-sint√©resser-au-terminal-linux",
    "title": "Linux 101",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des syst√®mes d‚Äôexploitation (y compris avec Windows !). Mais comme il a la r√©putation d‚Äô√™tre aust√®re et complexe, on utilise plut√¥t des interfaces graphiques pour effectuer nos op√©rations informatiques quotidiennes.\nPourtant, avoir des notions quant √† l‚Äôutilisation d‚Äôun terminal est une vraie source d‚Äôautonomie, dans la mesure o√π celui-ci permet de g√©rer bien plus finement les commandes que l‚Äôon r√©alise. Pour les data scientists qui s‚Äôint√©ressent aux bonnes pratiques et √† la mise en production, sa ma√Ætrise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont g√©n√©ralement limit√©es par rapport √† l‚Äôutilisation du programme en ligne de commande. C‚Äôest par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de r√©aliser toutes les op√©rations permises par le logiciel ;\nmettre un projet de data science en production n√©cessite d‚Äôutiliser un serveur, qui le rend disponible en permanence √† son public potentiel. Or l√† o√π Windows domine le monde des ordinateurs personnels, une large majorit√© des serveurs et des infrastructures cloud fonctionnent sous Linux ;\nles principaux langages de programmation des data scientists (, , etc. ) visent √† simplifier l‚Äôex√©cution d‚Äôop√©rations complexes par le biais de la ligne de commande mais h√©ritent n√©anmoins de sa logique. Utiliser la ligne de commande permet de gagner en aisance dans son travail de programmation ;\nplus g√©n√©ralement, une utilisation r√©guli√®re du terminal est source d‚Äôune meilleure compr√©hension du fonctionnement d‚Äôun syst√®me de fichiers et de l‚Äôex√©cution des processus sur un ordinateur. Ces connaissances s‚Äôav√®rent tr√®s utiles dans la pratique quotidienne du data scientist, qui n√©cessite de plus en plus de d√©velopper dans diff√©rents environnements d‚Äôex√©cution.\n\nDans le cadre de ce cours, on s‚Äôint√©ressera particuli√®rement au terminal Linux puisque l‚Äô√©crasante majorit√©, si ce n‚Äôest l‚Äôensemble, des serveurs de mise en production s‚Äôappuient sur un syst√®me Linux."
  },
  {
    "objectID": "chapters/linux-101.html#environnement-de-travail",
    "href": "chapters/linux-101.html#environnement-de-travail",
    "title": "Linux 101",
    "section": "",
    "text": "Diff√©rents environnements de travail peuvent √™tre utilis√©s pour apprendre √† se servir d‚Äôun terminal Linux :\n\nle SSP Cloud. Dans la mesure o√π les exemples de mise en production du cours seront illustr√©es sur cet environnement, nous recommandons de l‚Äôutiliser d√®s √† pr√©sent pour se familiariser. Le terminal est accessible √† partir de diff√©rents services (RStudio, Jupyter, etc.), mais nous recommandons d‚Äôutiliser le terminal d‚Äôun service VSCode, dans la mesure o√π se servir d‚Äôun IDE pour organiser notre code est en soi d√©j√† une bonne pratique ;\nKatacoda, un bac √† sable dans un syst√®me Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (√©mulation minimaliste d‚Äôun terminal Linux), qui est install√©e par d√©faut avec Git."
  },
  {
    "objectID": "chapters/linux-101.html#introduction-au-terminal",
    "href": "chapters/linux-101.html#introduction-au-terminal",
    "title": "Linux 101",
    "section": "",
    "text": "Lan√ßons un terminal pour pr√©senter son fonctionnement basique. On prend pour exemple le terminal d‚Äôun service VSCode lanc√© via le SSP Cloud (Application Menu tout en haut √† gauche de VSCode -&gt; Terminal -&gt; New Terminal). Voici √† quoi ressemble le terminal en question.\n\nD√©crivons d‚Äôabord les diff√©rentes inscriptions qui arrivent √† l‚Äôinitialisation :\n\n(base) : cette inscription n‚Äôest pas directement li√©e au terminal, elle provient du fait que l‚Äôon utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en d√©tail dans le chapitre sur la portabilit√© ;\ncoder@vscode-824991-64744dd6d8-zbgv5 : le nom de l‚Äôutilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que l‚Äôon verra l√† encore dans le chapitre sur la portabilit√©\n~/work : le chemin du r√©pertoire courant, i.e.¬†√† partir duquel va √™tre lanc√©e toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\n\nPour √©viter la lourdeur des images et permettre de copier/coller facilement les commandes, on repr√©sentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans l‚Äôexemple suivant. Les lignes commen√ßant par un $ sont celles avec lesquelles une commande est lanc√©e, et les lignes sans $ repr√©sentent le r√©sultat d‚Äôune commande. Attention √† ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement √† diff√©rencier celles-ci des r√©sultats.\n\n\nterminal\n\n$ echo \"une petite illustration\"\n\nune petite illustration"
  },
  {
    "objectID": "chapters/linux-101.html#notions-de-filesystem",
    "href": "chapters/linux-101.html#notions-de-filesystem",
    "title": "Linux 101",
    "section": "",
    "text": "Le terme filesystem (syst√®me de fichiers) d√©signe la mani√®re dont sont organis√©s les fichiers au sein d‚Äôun syst√®me d‚Äôexploitation. Cette structure est hi√©rarchique, en forme d‚Äôarbre :\n\nelle part d‚Äôun r√©pertoire racine (le dossier qui contient tous les autres) ;\ncontient des dossiers ;\nles dossiers peuvent contenir √† leur tout des dossiers (sous-dossiers) ou des fichiers.\n\nInt√©ressons nous √† la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations :\n\nla racine (root) sur Linux s‚Äôappelle /, l√† o√π elle s‚Äôappelle C:\\ par d√©faut sur Windows ;\nle r√©pertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un r√¥le essentiellement technique. Il est tout de m√™me utile d‚Äôen d√©crire les principaux :\n\n/bin : contient les binaires, i.e.¬†les programmes ex√©cutables ;\n/etc : contient les fichiers de configuration ;\n/home : contient l‚Äôensemble des dossiers et fichiers personnels des diff√©rents utilisateurs. Chaque utilisateur a un r√©pertoire dit ‚ÄúHOME‚Äù qui a pour chemin /home/&lt;username&gt; Ce r√©pertoire est souvent repr√©sent√© par le symbole ~. C‚Äô√©tait notamment le cas dans l‚Äôillustration du terminal VSCode ci-dessus, ce qui signifie qu‚Äôon se trouvait formellement dans le r√©pertoire /home/coder/work, coder √©tant l‚Äôutilisateur par d√©faut du service VSCode sur le SSP Cloud.\n\n\nChaque dossier ou fichier est repr√©sent√© par un chemin d‚Äôacc√®s, qui correspond simplement √† sa position dans le filesystem. Il existe deux moyens de sp√©cifier un chemin :\n\nen utilisant un chemin absolu, c‚Äôest √† dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconna√Æt donc un chemin absolu par le fait qu‚Äôil commence forc√©ment par /.\nen utilisant un chemin relatif, c‚Äôest √† dire en indiquant le chemin du dossier ou fichier relativement au r√©pertoire courant.\n\nComme tout ce qui touche de pr√®s ou de loin au terminal, la seule mani√®re de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront d‚Äôappliquer ces concepts √† des cas pratiques."
  },
  {
    "objectID": "chapters/linux-101.html#lancer-des-commandes",
    "href": "chapters/linux-101.html#lancer-des-commandes",
    "title": "Linux 101",
    "section": "",
    "text": "Le r√¥le d‚Äôun terminal est de lancer des commandes. Ces commandes peuvent √™tre class√©es en trois grandes cat√©gories :\n\nnavigation au sein du filesystem\nmanipulations de fichiers (cr√©er, lire, modifier des dossiers/fichiers)\nlancement de programmes\n\n\n\nLorsque l‚Äôon lance un programme √† partir du terminal, celui-ci a pour r√©f√©rence le r√©pertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si l‚Äôon ex√©cute un script Python en se trouvant dans un certain r√©pertoire, tous les chemins des fichiers utilis√©s dans le script seront relatifs au r√©pertoire courant d‚Äôex√©cution ‚Äî √† moins d‚Äôutiliser uniquement des chemins absolus, ce qui n‚Äôest pas une bonne pratique en termes de reproductibilit√© puisque cela lie votre projet √† la structure de votre filesystem particulier.\nAinsi, la tr√®s grande majorit√© des op√©rations que l‚Äôon est amen√© √† r√©aliser dans un terminal consiste simplement √† se d√©placer au sein du filesystem. Les commandes principales pour naviguer et se rep√©rer dans le filesystem sont pr√©sent√©es dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez p√©nible de manipuler des chemins absolus, qui peuvent facilement √™tre tr√®s longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient √† se d√©placer √† partir du r√©pertoire courant. Pour se faire, voici quelques utilisations tr√®s fr√©quentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter d‚Äôun niveau dans l‚Äôarborescence (dossier parent)\n\n\ncd ~\nrevenir dans le r√©pertoire HOME de l‚Äôutilisateur courant\n\n\n\nLa premi√®re commande est l‚Äôoccasion de revenir sur une convention d‚Äô√©criture importante pour les chemins relatifs :\n\n. repr√©sente le r√©pertoire courant. Ainsi, cd . revient √† changer de r√©pertoire courant‚Ä¶ pour le r√©pertoire courant, ce qui bien s√ªr ne change rien. Mais le . est tr√®s utile pour la copie de fichiers (cf.¬†section suivante) ou encore lorsque l‚Äôon doit passer des param√®tres √† un programme (cf.¬†section Lancement de programmes) ;\n.. repr√©sente le r√©pertoire parent du r√©pertoire courant.\n\nCes diff√©rentes commandes constituent la tr√®s grande majorit√© des usages dans un terminal. Il est essentiel de les pratiquer jusqu‚Äô√† ce qu‚Äôelles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup d‚Äôautres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\nd√©placer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncr√©er (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncr√©er un fichier vide\n\n\n\nDans la mesure o√π il est g√©n√©ralement possible de r√©aliser toutes ces op√©rations √† l‚Äôaide d‚Äôinterfaces graphiques (notamment, l‚Äôexplorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se d√©placer dans le filesystem. Nous vous recommandons malgr√© tout de les pratiquer √©galement, et ce pour plusieurs raisons :\n\neffectuer un maximum d‚Äôop√©rations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ;\nen devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ;\nlorsque l‚Äôon est amen√© √† manipuler un terminal pour interagir avec un serveur, il n‚Äôy a souvent pas la moindre interface graphique, auquel cas il n‚Äôy a pas d‚Äôautre choix que d‚Äôop√©rer uniquement √† partir du terminal.\n\n\n\n\nLe r√¥le du terminal est de lancer des programmes. Lancer un programme se fait √† partir d‚Äôun fichier dit ex√©cutable, qui peut √™tre de deux formes :\n\nun binaire, i.e.¬†un programme dont le code n‚Äôest pas lisible par l‚Äôhumain ;\nun script, i.e.¬†un fichier texte contenant une s√©rie d‚Äôinstructions √† ex√©cuter. Le langage du terminal Linux est le shell, et les scripts associ√©s ont pour extension .sh.\n\nDans les deux cas, la syntaxe de lancement d‚Äôune commande est : le nom de l‚Äôex√©cutable, suivi d‚Äô√©ventuels param√®tres, s√©par√©s par des espaces. Par exemple, la commande python monscript.py ex√©cute le binaire python et lui passe comme unique argument le nom d‚Äôun script .py (contenu dans le r√©pertoire courant), qui va donc √™tre ex√©cut√© via Python. De la m√™me mani√®re, toutes les commandes vues pr√©c√©demment pour se d√©placer dans le filesystem ou manipuler des fichiers sont des ex√©cutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier √† copier et le chemin d‚Äôarriv√©e.\nDans les exemples de commandes pr√©c√©dents, les param√®tres √©taient pass√©s en mode positionnel : l‚Äôex√©cutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments n‚Äôest pas toujours fix√© √† l‚Äôavance, du fait de la pr√©sence de param√®tres optionnels. Ainsi, la plupart des ex√©cutables permettent le passage d‚Äôarguments optionnels, qui modifient le comportement de l‚Äôex√©cutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier √† un autre endroit du filesystem, mais peut-on copier un dossier et l‚Äôensemble de son contenu avec ? Nativement non, mais l‚Äôajout d‚Äôun param√®tre le permet : cp -R dossierdepart dossierarrivee permet de copier r√©cursivement le dossier et tout son contenu. Notons que les flags ont tr√®s souvent un √©quivalent en toute lettre, qui s‚Äô√©crit quant √† lui avec deux tirers. Par exemple, la commande pr√©c√©dente peut s‚Äô√©crire de mani√®re √©quivalente cp --recursive dossierdepart dossierarrivee. Il est fr√©quent de voir les deux syntaxes en pratique, parfois m√™me m√©lang√©es au sein d‚Äôune m√™me commande."
  },
  {
    "objectID": "chapters/linux-101.html#variables-denvironnement",
    "href": "chapters/linux-101.html#variables-denvironnement",
    "title": "Linux 101",
    "section": "",
    "text": "Comme tout langage de programmation, le langage shell permet d‚Äôassigner et d‚Äôutiliser des variables dans des commandes. Pour afficher le contenu d‚Äôune variable, on utilise la commande echo, qui est l‚Äô√©quivalent de la fonction print en Python ou en R.\n\n\nterminal\n\n$ MY_VAR=\"toto\"\n$ echo $MY_VAR\n\ntoto\nQuelques remarques importantes :\n\nla syntaxe pour la cr√©ation de variable est pr√©cise : aucun espace d‚Äôun c√¥t√© comme de l‚Äôautre du = ;\nen Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu √©crire MY_VAR=toto pour le m√™me r√©sultat. Par contre, si l‚Äôon veut assigner √† une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message d‚Äôerreur ;\npour acc√©der √† la valeur d‚Äôune variable, on la pr√©fixe d‚Äôun $.\n\nNotre objectif avec ce tutoriel n‚Äôest pas de savoir coder en shell, on ne va donc pas s‚Äôattarder sur les propri√©t√©s des variables. En revanche, introduire ce concept √©tait n√©cessaire pour en pr√©senter un autre, essentiel quant √† lui dans la pratique quotidienne du data scientist : les variables d‚Äôenvironnement. Pour faire une analogie ‚Äî un peu simpliste ‚Äî avec les langages de programmation, ce sont des sortes de variables ‚Äúglobales‚Äù, dans la mesure o√π elles vont √™tre accessibles √† tous les programmes lanc√©s √† partir d‚Äôun terminal, et vont modifier leur comportement.\nLa liste des variables d‚Äôenvironnement peut √™tre affich√©e √† l‚Äôaide de la commande env. Il y a g√©n√©ralement un grand nombre de variables d‚Äôenvironnement pr√©√©xistantes ; en voici un √©chantillon obtenu √† partir du terminal du service VSCode.\n\n\nterminal\n\n$ env\n\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la vari√©t√© des utilisations des variables d‚Äôenvironnements : - la variable $SHELL pr√©cise l‚Äôex√©cutable utilis√© pour lancer le terminal ; - la variable $HOME donne l‚Äôemplacement du r√©pertoire utilisateur. En fait, le symbole ~ que l‚Äôon a rencontr√© plus haut r√©f√©rence cette m√™me variable ; - la variable LANG sp√©cifie la locale, un concept qui permet de d√©finir la langue et l‚Äôencodage utilis√©s par d√©faut par Linux ; - la variable CONDA_PYTHON_EXE existe uniquement parce que l‚Äôon a install√© conda comme syst√®me de gestion de packages Python. C‚Äôest l‚Äôexistance de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous int√©resse.\nUne variable d‚Äôenvironnement essentielle, et que l‚Äôon est fr√©quemment amen√© √† modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concat√©nation de chemins absolus, s√©par√©s par :, qui sp√©cifie les dossiers dans lesquels Linux va chercher les ex√©cutables lorsque l‚Äôon lance une commande, ainsi que l‚Äôordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n\n\nterminal\n\n$ echo $PATH\n\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nL‚Äôordre de recherche est de gauche √† droite. C‚Äôest donc parce que le dossier /home/coder/local/bin/conda/bin est situ√© en premier que l‚Äôinterpr√©teur Python qui sera choisi lorsque l‚Äôon lance un script Python est celui issu de Conda, et non celui contenu par d√©faut dans /usr/bin par exemple.\nL‚Äôexistence et la configuration ad√©quate des variables d‚Äôenvironnement est essentielle pour le bon fonctionnement de nombreux outils tr√®s utilis√©s en data science, comme Git ou encore Spark par exemple. Il est donc n√©cessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration d‚Äôun serveur en cas de bug li√© √† une variable d‚Äôenvironnement manquante ou mal configur√©e."
  },
  {
    "objectID": "chapters/linux-101.html#permissions",
    "href": "chapters/linux-101.html#permissions",
    "title": "Linux 101",
    "section": "",
    "text": "La s√©curit√© est un enjeu central en Linux, qui permet une gestion tr√®s fine des permissions sur les diff√©rents fichiers et programmes.\nUne diff√©rence majeure par rapport √† d‚Äôautres syst√®mes d‚Äôexploitation, notamment Windows, est qu‚Äôaucun utilisateur n‚Äôa par d√©faut les droits complets d‚Äôadministrateur (root). Il n‚Äôest donc pas possible nativement d‚Äôacc√©der au parties sensibles du syst√®me, ou bien de lancer certains types de programme. Par exemple, si l‚Äôon essaie de lister les fichiers du dossier /root, on obtient une erreur.\n\n\nterminal\n\n$ ls /root\n\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines op√©rations telles que l‚Äôinstallation de binaires ou de packages n√©cessitent cependant des droits administrateurs. Dans ce cas, il est d‚Äôusage d‚Äôutiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de l‚Äôex√©cution de la commande.\n\n\nterminal\n\n$ sudo ls /root\n\nLe dossier /root √©tant vide, la commande ls renvoie une cha√Æne de caract√®res vide, mais nous n‚Äôavons plus de probl√®me de permission. Notons qu‚Äôune bonne pratique de s√©curit√©, en particulier dans les scripts shell que l‚Äôon peut √™tre amen√©s √† √©crire ou ex√©cuter, est de limiter l‚Äôutilisation de cette commande aux cas o√π elle s‚Äôav√®re n√©cessaire.\nUne autre subtilit√© concerne justement l‚Äôex√©cution de scripts shell. Par d√©faut, qu‚Äôil soit cr√©√© par l‚Äôutilisateur ou t√©l√©charg√© d‚Äôinternet, un script n‚Äôest pas ex√©cutable.\n\n\nterminal\n\n1$ touch test.sh\n2$ ./test.sh\n\n\n1\n\nCr√©er le script test.sh (vide)\n\n2\n\nEx√©cuter le script test.sh\n\n\nbash: ./test.sh: Permission denied\nC‚Äôest bien entendu une mesure de s√©curit√© pour √©viter l‚Äôex√©cution automatique de scripts potentiellement malveillants. Pour pouvoir ex√©cuter un tel script, il faut attribuer des droits d‚Äôex√©cution au fichier avec la commande chmod. Il devient alors possible d‚Äôex√©cuter le script classiquement.\n\n\nterminal\n\n1$ chmod +x test.sh\n2$ ./test.sh\n$ # Le script √©tant vide, il ne se passe rien\n\n\n1\n\nDonner des droits d‚Äôex√©cution au script test.sh\n\n2\n\nEx√©cuter le script test.sh"
  },
  {
    "objectID": "chapters/linux-101.html#les-scripts-shell",
    "href": "chapters/linux-101.html#les-scripts-shell",
    "title": "Linux 101",
    "section": "",
    "text": "Maintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell pr√©c√©demment √©voqu√©s. A l‚Äôinstar d‚Äôun script Python, un script shell permet d‚Äôautomatiser une s√©rie de commandes lanc√©es dans un terminal. Le but de ce tutoriel n‚Äôest pas de savoir √©crire des scripts shell complexes, travail g√©n√©ralement d√©volu aux les data engineers ou les sysadmin (administrateurs syst√®me), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces comp√©tences sont essentielles lorsque l‚Äôon se pr√©occupe de mise en production. A titre d‚Äôexemple, comme nous le verrons dans le chapitre sur la portabilit√©, il est fr√©quent d‚Äôutiliser un script shell comme entrypoint d‚Äôune image docker, afin de sp√©cifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement √† l‚Äôaide d‚Äôun script simple. Consid√©rons les commandes suivantes, que l‚Äôon met dans un fichier monscript.sh dans le r√©pertoire courant.\n\n\nterminal\n\n$ #!/bin/bash\n$ SECTION=$1\n$ CHAPTER=$2\n$ FORMATION_DIR=/home/coder/work/formation\n$ mkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\n$ touch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\n\nAnalysons la structure de ce script :\n\nla premi√®re ligne est classique, elle se nomme le shebang : elle indique au syst√®me quel interpr√©teur utiliser pour ex√©cuter ce script. Dans notre cas, et de mani√®re g√©n√©rale, on utilise bash (Bourne-Again SHell, l‚Äôimpl√©mentation moderne du shell) ;\nles lignes 2 et 3 assignent √† des variables les arguments pass√©s au script dans la commande. Par d√©faut, ceux-ci sont assign√©s √† des variables n o√π n est la position de l‚Äôargument, en commen√ßant √† 1 ;\nla ligne 4 assigne un chemin √† une variable\nla ligne 5 cr√©e le chemin complet, d√©fini √† partir des variables cr√©√©es pr√©c√©demment. Le param√®tre -p est important : il pr√©cise √† mkdir d‚Äôagir de mani√®re r√©cursive, c‚Äôest √† dire de cr√©er les dossiers interm√©diaires qui n‚Äôexistent pas encore ;\nla ligne 6 cr√©e un fichier texte vide dans le dossier cr√©√© avec la commande pr√©c√©dente.\n\nEx√©cutons maintenant ce script, en prenant soin de lui donner les permission ad√©quates au pr√©alable.\n\n\nterminal\n\n$ chmod +x monscript.sh\n$ bash monscript.sh section2 chapitre3\n$ ls formation/section1/chapitre2/\n\ntext.txt\nOp√©ration r√©ussie : le dossier a bien √©t√© cr√©√© et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash tr√®s bien r√©alis√©e."
  },
  {
    "objectID": "chapters/linux-101.html#gestionnaire-de-paquets",
    "href": "chapters/linux-101.html#gestionnaire-de-paquets",
    "title": "Linux 101",
    "section": "",
    "text": "Une diff√©rence fondamentale entre Linux et Windows tient √† la mani√®re dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier ex√©cutable en .exe) sur le site du logiciel, et on l‚Äôex√©cute. En Linux, on passe g√©n√©ralement par un gestionnaire de packages qui va chercher les logiciels sur un r√©pertoire centralis√©, √† la mani√®re de pip en Python par exemple.\nPourquoi cette diff√©rence ? Une raison importante est que, contrairement √† Windows, il existe une multitude de distributions diff√©rentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent diff√©remment et peuvent avoir diff√©rentes versions. En utilisant le package manager (gestionnaire de paquets) propre √† la distribution en question, on s‚Äôassure de t√©l√©charger le logiciel adapt√© √† sa distribution. Dans ce cours, on fait le choix d‚Äôutiliser une distribution Debian et son gestionnaire de paquets associ√© apt. Debian est en effet un choix populaire pour les servers de part sa stabilit√© et sa simplicit√©, et sera √©galement famili√®re aux utilisateurs d‚ÄôUbuntu, distribution tr√®s populaire pour les ordinateurs personnels et qui est bas√©e sur Debian.\nL‚Äôutilisation d‚Äôapt est tr√®s simple. La seule difficult√© est de savoir le nom du paquet que l‚Äôon souhaite installer, ce qui n√©cessite en g√©n√©ral d‚Äôutiliser un moteur de recherche. L‚Äôinstallation de paquets est √©galement un cas o√π il faut utiliser sudo, puisque cela implique souvent l‚Äôacc√®s √† des r√©pertoires prot√©g√©s.\n\n\nterminal\n\n$ sudo apt install tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nD√©sinstaller un package est √©galement simple : c‚Äôest l‚Äôop√©ration inverse. Par s√©curit√©, le terminal vous demande si vous √™tes s√ªr de votre choix en vous demandant de tapper la lettre y ou la lettre n.¬†On peut passer automatiquement cette √©tape en ajoutant le param√®tre -y\n\n\nterminal\n\n$ sudo apt remove -y tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant d‚Äôinstaller un package, il est toujours pr√©f√©rable de mettre √† jour la base des packages, pour s‚Äôassurer qu‚Äôon obtiendra bien la derni√®re version.\n\n\nterminal\n\n$ sudo apt update\n\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date."
  },
  {
    "objectID": "chapters/linux-101.html#tricks",
    "href": "chapters/linux-101.html#tricks",
    "title": "Linux 101",
    "section": "",
    "text": "On l‚Äôa dit et redit : devenir √† l‚Äôaise avec le terminal Linux est essentiel et demande de la pratique. Il existe n√©anmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa premi√®re est l‚Äôautocompl√©tion. D√®s lors que vous √©crivez une commande contenant un nom d‚Äôex√©cutable, un chemin sur le filesystem, ou autre, n‚Äôh√©sitez pas √† utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majorit√© des cas, cela va vous faire gagner un temps pr√©cieux.\nUne seconde astuce, qui n‚Äôen est pas vraiment une, est de lire la documentation d‚Äôune commande lorsque l‚Äôon n‚Äôest pas s√ªr de sa syntaxe ou des param√®tres admissibles. Via le terminal, la documentation d‚Äôune commande peut √™tre affich√©e en ex√©cutant man suivie de la commande en question, par exemple : man cp. Comme il n‚Äôest pas toujours tr√®s pratique de lire de longs textes dans un petit terminal, on peut √©galement chercher la documentation d‚Äôune commande sur le site man7."
  },
  {
    "objectID": "chapters/git.html",
    "href": "chapters/git.html",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/git.html#pourquoi-faire",
    "href": "chapters/git.html#pourquoi-faire",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi faire ?",
    "text": "Pourquoi faire ?\nLe d√©veloppement rapide de la data science au cours de ces derni√®res ann√©es s‚Äôest accompagn√©e d‚Äôune complexification substantielle des projets. Par ailleurs, les projets sont de plus en plus collaboratifs, que ce soit dans le cadre d‚Äô√©quipes dans un contexte professionnel ou bien pour des contributions √† des projets open-source. Naturellement, ces √©volutions doivent nous amener √† modifier nos mani√®res de travailler pour g√©rer cette complexit√© croissante et continuer √† produire de la valeur √† partir des projets de data science.\nPourtant, tout data scientist s‚Äôest parfois demand√© :\n\nquelle √©tait la bonne version d‚Äôun programme\nqui √©tait l‚Äôauteur d‚Äôun bout de code en particulier\nsi un changement √©tait important ou juste un essai\ncomment fusionner des programmes\netc.\n\nEt il n‚Äôest pas rare de perdre le fil des versions de son projet lorsque l‚Äôon garde trace de celles-ci de fa√ßon manuelle.\nExemple de contr√¥le de version fait ‚Äú√† la main‚Äù\n\nPourtant, il existe un outil informatique puissant afin de r√©pondre √† tous ces besoins : la gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer l‚Äôhistorique des modifications d‚Äôun ensemble de fichiers\nrevenir √† des versions pr√©c√©dentes d‚Äôun ou plusieurs fichiers\nrechercher les modifications qui ont pu cr√©er des erreurs\ntravailler simultan√©ment sur un m√™me fichier sans risque de perte\npartager ses modifications et r√©cup√©rer celles des autres\nproposer des modifications, les discuter, sans pour autant modifier la derni√®re version existante\nidentifier les auteurs et la date des modifications\n\nEn outre, ces outils fonctionnent avec tous les langages informatiques car ils reposent sur la comparaison des lignes et des caract√®res des programmes, ind√©pendamment du langage. En bref, c‚Äôest la bonne mani√®re pour partager des codes et travailler √† plusieurs sur un projet de data science. En r√©alit√©, il ne serait pas exag√©r√© de dire que l‚Äôutilisation du contr√¥le de version est la bonne pratique la plus fondamentale de tout projet faisant intervenir du code, et qu‚Äôelle conditionne largement toutes les autres."
  },
  {
    "objectID": "chapters/git.html#pourquoi-git",
    "href": "chapters/git.html#pourquoi-git",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi Git  ?",
    "text": "Pourquoi Git  ?\nPlusieurs logiciels de contr√¥le de version existent sur le march√©. En principe, le logiciel Git, d√©velopp√© initialement pour fournir une solution d√©centralis√©e et open-source dans le cadre du d√©veloppement du noyau Linux, est devenu largement h√©g√©monique. Aussi, toutes les application de ce cours s‚Äôeffectueront √† l‚Äôaide du logiciel Git."
  },
  {
    "objectID": "chapters/git.html#pourquoi-github",
    "href": "chapters/git.html#pourquoi-github",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi GitHub  ?",
    "text": "Pourquoi GitHub  ?\nTravailler de mani√®re collaborative avec Git implique de synchroniser son r√©pertoire local avec une copie distante, situ√©e sur un serveur h√©bergeant des projets Git. Ce serveur peut √™tre un serveur interne √† une organisation, ou bien √™tre fourni par un h√©bergeur externe. Les deux alternatives les plus populaires en la mati√®re sont GitHub et GitLab. Dans ce cours, nous utiliserons GitHub, qui est devenu au fil des ann√©es la r√©f√©rence pour l‚Äôh√©bergement des projets open-source. En pratique, les deux services sont relativement semblables, et tous les concepts pr√©sent√©s se retrouvent sous une forme similaire sur les deux plateformes."
  },
  {
    "objectID": "chapters/git.html#principes-et-commandes-usuelles",
    "href": "chapters/git.html#principes-et-commandes-usuelles",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Principes et commandes usuelles",
    "text": "Principes et commandes usuelles\nLe graphique suivant illustre les principes fondamentaux de Git.\nGit tout-en-un (Source) \nLorsqu‚Äôon utilise Git, il est important de bien distinguer ce qui se passe en local (sur son poste, sur le serveur sur lequel on travaille‚Ä¶) de ce qui se passe en remote, i.e.¬†en int√©ragissant avec un serveur distant. Comme le montre le graphique, l‚Äôessentiel du contr√¥le de version se passe en r√©alit√© en local.\nEn th√©orie, sur un projet individuel, il est m√™me possible de r√©aliser l‚Äôensemble du contr√¥le de version en mode hors-ligne. Pour cela, il suffit d‚Äôindiquer √† Git le projet (dossier) que l‚Äôon souhaite versionner en utilisant la commande git init. Cette commande a pour effet de cr√©er un dossier .git √† la racine du projet, dans lequel Git va stocker tout l‚Äôhistorique du projet (commits, branches, etc.) et permettre de naviguer entre les versions. A cause du . qui pr√©fixe son nom, ce dossier est g√©n√©ralement cach√© par d√©faut, ce qui n‚Äôest pas probl√©matique dans la mesure o√π il n‚Äôy a jamais besoin de le parcourir ou de le modifier √† la main en pratique. Retenez simplement que c‚Äôest la pr√©sence de ce dossier .git qui fait qu‚Äôun dossier est consid√©r√© comme un projet Git, et donc que vous pouvez utilisez les commandes usuelles de Git dans ce dossier √† l‚Äôaide d‚Äôun terminal : - git status : affiche les modifications du projet par rapport √† la version pr√©c√©dente ; - git add chemin_du_fichier : ajoute un fichier nouveau ou modifi√© √† la zone de staging de Git en vue d‚Äôun commit ; - git add -A : ajoute tous les fichiers nouveaux ou modifi√©s √† la zone de staging ; - git commit -m \"message de commit\" : cr√©e un commit, i.e.¬†une photographie des modifications (ajouts, modifications, suppressions) apport√©es au projet depuis la derni√®re version, et lui assigne un message d√©crivant ces changements. Les commits sont l‚Äôunit√© de base de l‚Äôhistorique du projet construit par Git.\nEn pratique, travailler uniquement en local n‚Äôest pas tr√®s int√©ressant. Pour pouvoir travailler de mani√®re collaborative, on va vouloir synchroniser les diff√©rentes copies locales du projet √† un r√©pertoire centralis√©, qui maintient de fait la ‚Äúsource de v√©rit√©‚Äù (single source of truth). M√™me sur un projet individuel, il fait sens de synchroniser son r√©pertoire local √† une copie distante pour assurer l‚Äôint√©grit√© du code de son projet en cas de probl√®me mat√©riel.\nEn g√©n√©ral, on va donc initialiser le projet dans l‚Äôautre sens : - cr√©er un nouveau projet sur GitHub - g√©n√©rer un jeton d‚Äôacc√®s (personal access token) - cloner le projet en local via la m√©thode HTTPS : git clone https://github.com/&lt;username&gt;/&lt;project_name&gt;.git\nLe projet clon√© est un projet Git ‚Äî il contient le dossier .git ‚Äî synchronis√© par d√©faut avec le r√©pertoire distant. On peut le v√©rifier avec la commande remote de Git :\n\n\nterminal\n\n$ git remote -v\n\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (fetch)\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (push)\nLe projet local est bien li√© au r√©pertoire distant sur GitHub, auquel Git donne par d√©faut le nom origin. Ce lien permet d‚Äôutiliser les commandes de synchronisation usuelles : - git pull : r√©cup√©rer les changements (fetch) sur le remote et les appliquer au projet local - git push : envoyer les changements locaux sur le remote"
  },
  {
    "objectID": "chapters/git.html#impl√©mentations",
    "href": "chapters/git.html#impl√©mentations",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Impl√©mentations",
    "text": "Impl√©mentations\nGit est un logiciel, qui peut √™tre t√©l√©charg√© sur le site officiel pour diff√©rents syst√®mes d‚Äôexploitation. Il existe cependant diff√©rentes mani√®res d‚Äôutiliser Git : - le client en ligne de commande : c‚Äôest l‚Äôimpl√©mentation standard, et donc la plus compl√®te. C‚Äôest celle qu‚Äôon utilisera dans ce cours. Le client Git est install√© par d√©faut sur les diff√©rents services du SSP Cloud (VSCode, RStudio, Jupyter, etc.) et peut donc √™tre utilis√© via n‚Äôimporte quel terminal. La documentation du SSP Cloud d√©taille la proc√©dure ; - des interfaces graphiques : elles facilitent la prise en main de Git via des guides visuels, mais ne permettent pas de r√©aliser toutes les op√©rations permises par Git - l‚Äôinterface native de RStudio pour les utilisateurs de R : tr√®s compl√®te et stable. La formation au travail collaboratif avec Git et RStudio pr√©sente son utilisation de mani√®re d√©taill√©e ; - le plugin Jupyter-git pour les utilisateurs de Python : elle impl√©mente les principales features de Git, mais s‚Äôav√®re assez instable √† l‚Äôusage."
  },
  {
    "objectID": "chapters/git.html#bonnes-pratiques",
    "href": "chapters/git.html#bonnes-pratiques",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\nLe contr√¥le de version est une bonne pratique de d√©veloppement en soi‚Ä¶ mais son utilisation admet elle m√™me des bonnes pratiques qui, lorsqu‚Äôelles sont appliqu√©es, permettent d‚Äôen tirer le plus grand profit.\n\nQue versionne-t-on ?\n\nUniquement des fichiers texte\nA chaque commit, Git calcule les diff√©rences entre les versions successives du projet, afin de ne pas avoir √† stocker une image compl√®te de ce dernier √† chaque fois. C‚Äôest ce qui permet aux projets Git de rester tr√®s l√©gers par d√©faut, et donc aux diff√©rentes op√©rations impliquant le remote (clone, push, pull..) d‚Äô√™tre tr√®s rapides.\nLa contrepartie de cette l√©g√®ret√© est une contrainte sur les types d‚Äôobjets que l‚Äôon doit versionner. Les diff√©rences sont calculables uniquement sur des fichiers de type texte : codes source, fichiers texte, fichiers de configuration non-sensibles‚Ä¶ Voici donc une liste non-exhaustive des extensions de fichier que l‚Äôon retrouve fr√©quemment dans un d√©p√¥t Git d‚Äôun projet R ou Python : .py, .R, .Rmd, .txt, .json, .xml, .yaml, .toml, et bien d‚Äôautres.\nEn revanche tous les fichiers binaires ‚Äî pour faire simple, tous les fichiers qui ne peuvent pas √™tre ouverts dans un √©diteur de texte basique sans produire une suite inintelligible de caract√®res ‚Äî n‚Äôont g√©n√©ralement pas destination √† se retrouver sur un d√©p√¥t Git. Du fait de leur formatage (binaire), Git ne peut pas calculer les diff√©rences entre versions pour ces fichiers et c‚Äôest donc le fichier entier qui est sauvegard√© dans l‚Äôhistorique √† chaque changement, ce qui peut tr√®s rapidement faire cro√Ætre la taille du d√©p√¥t. Pour √©viter de versionner ces fichiers par erreur, on va les ajouter au fichier .gitignore (cf.¬†supra).\n\n\nPas de donn√©es\nComme expliqu√© en introduction, le fil rouge de ce cours sur les bonnes pratiques est l‚Äôimportance de bien s√©parer code, donn√©es et environnement d‚Äôex√©cution afin de favoriser la reproductibilit√© des projets de data science. Ce principe doit s‚Äôappliquer √©galement √† l‚Äôusage du contr√¥le de version, et ce pour diff√©rentes raisons.\nA priori, inclure ces donn√©es dans un d√©p√¥t Git peut sembler une bonne id√©e en termes de reproductibilit√©. En machine learning par exemple, on est souvent amen√© √† r√©aliser de nombreuses exp√©rimentations √† partir d‚Äôun m√™me mod√®le appliqu√© √† diff√©rentes transformations des donn√©es initiales, transformations que l‚Äôon pourrait versionner. En pratique, il est g√©n√©ralement pr√©f√©rable de versionner le code qui permet de g√©n√©rer ces transformations et donc les exp√©rimentations associ√©es, dans la mesure o√π le suivi des versions des datasets peut s‚Äôav√©rer rapidement complexe. Pour de plus gros projets, des alternatives sp√©cifiques existent : c‚Äôest le champ du MLOps, domaine en constante expansion qui vise √† rendre les pipelines de machine learning plus reproductibles.\nEnfin, la structure m√™me de Git n‚Äôest techniquement pas faite pour le stockage de donn√©es. Si des petits datasets dans un format texte ne poseront pas de probl√®me, des donn√©es volumineuses (√† partir de plusieurs Mo) vont faire cro√Ætre la taille du d√©p√¥t et donc ralentir significativement les op√©rations de synchronisation avec le remote.\n\n\nPas d‚Äôinformations locales\nL√† encore en vertu du principe de s√©paration donn√©es / code/ environnement, les donn√©es locales, i.e.¬†sp√©cifiques √† l‚Äôenvironnement de travail sur lequel le code a √©t√© ex√©cut√©, n‚Äôont pas vocation √† √™tre versionn√©es. Par exemple, des fichiers de configuration sp√©cifiques √† un poste de travail, des chemins d‚Äôacc√®s sp√©cifiques √† un ordinateur donn√©, etc. Cela demande une plus grande rigueur lors de la construction du projet, mais garantit par l√† m√™me une meilleure reproductiblit√© pour les futurs utilisateurs du projet.\n\n\nPas d‚Äôoutputs\nLes outputs d‚Äôun projet (graphiques, publications, mod√®le entra√Æn√©‚Ä¶) n‚Äôont pas vocation √† √™tre versionn√©, en vertu des diff√©rents arguments pr√©sent√©s ci-dessus : - il ne s‚Äôagit g√©n√©ralement pas de fichiers de type texte ; - le code source du projet doit dans tous les cas permettre des les reg√©n√©rer √† l‚Äôidentique.\n\n\nUtiliser un .gitignore\nOn a list√© pr√©c√©demment un large √©ventail de fichiers qui n‚Äôont, par nature, pas vocation √† √™tre versionn√©. Bien entendu, faire attention √† ne pas ajouter ces diff√©rents fichiers au moment de chaque git add serait assez p√©nible. Git simplifie largement cette proc√©dure en nous donnant la possibilit√© de remplir un fichier .gitignore, situ√© √† la racine du projet, qui sp√©cifie l‚Äôensemble des fichiers et types de fichiers que l‚Äôon ne souhaite pas versionner dans le cadre du projet courant.\nDe mani√®re g√©n√©rale, il y a pour chaque langage des fichiers que l‚Äôon ne souhaitera jamais versionner. Pour en tenir compte, une premi√®re bonne pratique est de choisir le .gitignore associ√© au langage du projet lors de la cr√©ation du d√©p√¥t sur GitHub. Ce faisant, le projet est initialit√© avec un gitignore d√©j√† existant et pr√©-rempli de chemins et de types de fichiers qui ne sont pas √† versionner. Regardons un extrait du gitignore Python pour comprendre sa structure et son fonctionnement.\npip-log.txt\n__pycache__/\n*.log\nChaque ligne du gitignore sp√©cifie un √©l√©ment √† ignorer du contr√¥le de version, √©l√©ment qui peut √™tre un ficher/dossier ou bien une r√®gle concernant un ensemble de fichiers/dossiers. Sauf si sp√©cifi√© explicitement, les chemins sont relatifs √† la racine du projet. L‚Äôextrait du gitignore Python illustre les diff√©rentes possibilit√©s :\n\nligne 1 : ignore le fichier pip-log.txt ;\nligne 2 : ignore le dossier __pycache__/ ;\nligne 3 : ignore tous les fichiers dont l‚Äôextension est .log.\n\nDe nombreuses autres possiblit√©s existent, et sont d√©taill√©es par exemple dans la documentation de Git.\n\n\n\nMessages des commits\nLe commit est l‚Äôunit√© de temps de Git, et donc fondamentalement ce qui permet de remonter dans l‚Äôhistorique d‚Äôun projet. Afin de pouvoir b√©n√©ficier √† plein de cet avantage de Git, il est capital d‚Äôaccompagner ses commits de messages pertinents, en se pla√ßant dans la perspective que l‚Äôon peut √™tre amen√© plusieurs semaines ou mois plus tard √† vouloir retrouver du code dans l‚Äôhistorique de son projet. Les quelques secondes prises √† chaque commit pour r√©fl√©chir √† une description pertinente du bloc de modifications que l‚Äôon apporte au projet peuvent donc faire gagner un temps pr√©cieux √† la longue.\nDe nombreuses conventions existent pour r√©diger des messages de commit pertinents. Nous rappelons ici les plus importantes :\n\ncontenu : le message doit d√©tailler le pourquoi plut√¥t que le comment des modifications. Par exemple, plut√¥t que ‚ÄúAjoute le fichier test.py‚Äù, on pr√©f√©rera √©crire ‚ÄúAjout d‚Äôune s√©rie de tests unitaires‚Äù ;\nstyle : le message doit √™tre √† l‚Äôimp√©ratif et former une phrase (sans point √† la fin) ;\nlongueur : le message du commit doit √™tre court (&lt; 72 caract√®res). S‚Äôil n‚Äôest pas possible de trouver un message de cette taille qui r√©sume le commit, c‚Äôest g√©n√©ralement un signe que le commit regroupe trop de changements (cf.¬†point suivant). Le fait de devoir mettre des + ou des & / et dans un message de commit pour s√©parer les changements est √©galement un bon indicateur d‚Äôun commit trop gros.\n\n\n\nFr√©quence des commits\nDe mani√®re g√©n√©rale, il est conseill√© de r√©aliser des commits r√©guliers lorsque l‚Äôon travaille sur un projet. Une r√®gle simple que l‚Äôon peut par exemple appliquer est la suivante : d√®s lors qu‚Äôun ensemble de modifications forment un tout coh√©rent et peuvent √™tre r√©sum√©es par un message simple, il est temps d‚Äôen faire un commit. Cette approche a de nombreux avantages :\n\nsi l‚Äôon fait suivre chaque commit d‚Äôun push ‚Äî ce qui est conseill√© en pratique ‚Äî on s‚Äôassure de disposer r√©guli√®reemnt d‚Äôune copie de ses travaux, ce qui limite le risque de perte ;\nil est plus facile de revenir en arri√®re en cas d‚Äôerreur si les commits portent sur des changements cibl√©s et coh√©rents ;\nle processus de review d‚Äôune pull request est facilit√©, car les diff√©rents blocs de modification sont plus clairement s√©par√©s ;\ndans une approche d‚Äôint√©gration continue ‚Äî concept que l‚Äôon verra en d√©tail dans le chapitre sur la mise en production ‚Äî faire des commits et des PR r√©guli√®rement permet de d√©ployer de mani√®re continue les changements en production, et donc d‚Äôobtenir les feedbacks des utilisateurs et d‚Äôadapter plus rapidement si besoin."
  },
  {
    "objectID": "chapters/git.html#branches",
    "href": "chapters/git.html#branches",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Branches",
    "text": "Branches\n\nConcept\nLa possibilit√© de cr√©er des branches est l‚Äôune des fonctionnalit√©s majeures de Git. La cr√©ation d‚Äôune branche au sein d‚Äôun projet permet de diverger de la ligne principale de d√©veloppement (g√©n√©ralement appel√©e master, terme tendant √† dispara√Ætre au profit de celui de main) sans impacter cette ligne. Cela permet de s√©parer le nouveau d√©veloppement et de faire cohabiter plusieurs versions, pouvant √©voluer s√©par√©ment et pouvant √™tre facilement rassembl√©es si n√©cessaire.\nPour comprendre comment fonctionnent les branches, il nous faut revenir un peu plus en d√©tail sur la mani√®re dont Git stocke l‚Äôhistorique du projet. Comme nous l‚Äôavons vu, l‚Äôunit√© temporelle de Git est le commit, qui correspond √† une photographie √† un instant donn√© de l‚Äô√©tat du projet (snapshot). Chaque commit est uniquement identifi√© par un hash, une longue suite de caract√®res. La commande git log, qui liste les diff√©rents commits d‚Äôun projet, permet d‚Äôafficher ce hash ainsi que diverses m√©tadonn√©es (auteur, date, message) associ√©es au commit.\n\n\nterminal\n\n$ git log\n\ncommit e58b004d3b68bdf28093fe6ad6036b5d13216e55 (HEAD -&gt; master, origin/master, origin/HEAD)\nAuthor: Lino Galiana &lt;xxx@xxx.fr&gt;\nDate:   Tue Mar 22 14:34:04 2022 +0100\n\n    ajoute code √©quivalent python\n\n...\nUne branche est simplement un pointeur vers un commit. Dans l‚Äôexemple pr√©c√©dent, on a imprim√© les informations du dernier commit en date. La branche principale (master) pointe vers ce commit. Si l‚Äôon faisait un nouveau commit, le pointeur se d√©calerait et la branche master pointerait √† pr√©sent sur le nouveau commit.\n\n\nBranches locales\nDans ce contexte, cr√©er une nouvelle branche (en local) revient simplement √† cr√©er un nouveau pointeur vers un commit donn√©. Supposons que l‚Äôon cr√©e une branche testing √† partir du dernier commit.\n\n\nterminal\n\n1$ git branch testing\n2$ git branch\n\n\n1\n\nCr√©e une nouvelle branche\n\n2\n\nListe les branches existantes\n\n\n1* master\n2  testing\n\n1\n\nLa branche sur laquelle on se situe\n\n2\n\nLa nouvelle branche cr√©√©e\n\n\nLa figure suivante illustre l‚Äôeffet de cette cr√©ation sur l‚Äôhistorique Git.\n\nD√©sormais, deux branches (master et testing) pointent vers le m√™me commit. Si l‚Äôon effectue √† pr√©sent des commits sur la branche testing, on va diverger de la branche principale, ce qui permet de d√©velopper une nouvelle fonctionnalit√© sans risquer d‚Äôimpacter master.\nPour savoir sur quelle branche on se situe √† instant donn√© ‚Äî et donc sur quelle branche on va commiter ‚Äî Git utilise un pointeur sp√©cial, appel√© HEAD, qui pointe vers la branche courante. On comprend √† pr√©sent mieux la signification de HEAD -&gt; master dans l‚Äôoutput de la commande git log vu pr√©c√©demment. Cet √©l√©ment sp√©cifie la situation locale actuelle et signifie : on se situe actuellement sur la branche master, qui pointe sur le commit e58b004. Pour changer de branche, i.e.¬†d√©placer le HEAD, on utilise la commande git checkout. Par exemple, pour passer de la branche master sur laquelle on est par d√©faut √† la branche testing :\n\n\nterminal\n\n$ git checkout testing  # Changement de branche\n\nSwitched to branch 'testing'\nOn se situe d√©sormais sur la branche testing, sur laquelle on peut laisser libre cours √† sa cr√©ativit√© sans risquer d‚Äôimpacer la branche principale du projet. Mais que se passe-t-il si, pendant que l‚Äôon d√©veloppe sur testing, un autre membre du projet commit sur master ? On dit que les historiques ont diverg√©. La figure suivante illustre √† quoi ressemble √† pr√©sent l‚Äôhistorique du projet (et suppose que l‚Äôon est repass√© sur master).\n\nCette divergence n‚Äôest pas probl√©matique en soi : il est normal que les diff√©rentes parties et exp√©rimentations d‚Äôun projet avancent √† diff√©rents rythmes. La difficult√© est de savoir comment r√©concillier les diff√©rents changements si l‚Äôon d√©cide que la branche testing doit √™tre int√©gr√©e dans master. Deux situations peuvent survenir : - les modifications op√©r√©es en parall√®le sur les deux branches ne concernent pas les m√™mes fichiers ou les m√™mes parties des fichiers. Dans ce cas, Git est capable de fusionner (merge) les changements automatiquement et tout se passe sans encombre ; - dans le cas contraire, survient un merge conflict : les branches ont diverg√© de telle sorte qu‚Äôil n‚Äôest pas possible pour Git de fusionner les changements automatiquement. Il faut alors r√©soudre les conflits manuellement.\nLa r√©solution des conflits est une √©tape souvent douloureuse lors de l‚Äôapprentissage de Git. Aussi, nous conseillons dans la mesure du possible de ne pas fusionner des branches manuellement en local avec Git ‚Äî c‚Äôest d‚Äôailleurs pour cette raison que nous n‚Äôavons pas d√©taill√© les commandes pour le faire. Dans les sections suivantes, nous verrons comment une bonne organisation pr√©alable du travail en √©quipe, combin√©e aux outils collaboratifs fournis par GitHub, permet de rendre le processus de fusion des branches largement indolore.\n\n\nBranches remote\nRappellons que toutes les op√©rations que nous avons effectu√©es sur les branches dans cette section se sont pass√©s en local, le r√©pertoire distant est rest√© totalement inchang√©. Pour pouvoir collaborer sur une branche ou bien en faire une pull request (cf.¬†supra), il faut pousser la branche sur le r√©pertoire distant. La commande est simple : git push origin &lt;branche&gt;.\n\n\nterminal\n\n$ git push origin testing\n\nCounting objects: 24, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (15/15), done.\nWriting objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done.\nTotal 24 (delta 2), reused 0 (delta 0)\nTo https://github.com/linogaliana/ensae-reproductibilite-website\n * [new branch]      testing -&gt; testing"
  },
  {
    "objectID": "chapters/git.html#workflow-collaboratif",
    "href": "chapters/git.html#workflow-collaboratif",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Workflow collaboratif",
    "text": "Workflow collaboratif\nComme on l‚Äôa vu pr√©c√©demment, si le mod√®le des branches de Git semble id√©al pour g√©rer le travail collaboratif et asynchrone, il peut √©galement s‚Äôav√©rer rapidement complexe √† manipuler en l‚Äôabsence d‚Äôune bonne organisation du travail en √©quipe. De nombreux mod√®les (‚Äúworkflows‚Äù) existent en la mati√®re, avec des complexit√©s plus ou moins grandes selon la nature du projet. Nous conseillons d‚Äôadopter dans la plupart des cas un mod√®le tr√®s simple : le GitHub Flow.\nLe GitHub Flow est une m√©thode d‚Äôorganisation minimaliste du travail collaboratif, qui est progressivement devenue la norme dans les projets open-source. Elle est r√©sum√©e par la figure suivante, dont nous d√©taillons par la suite les diff√©rentes √©tapes.\n\n\nD√©finition des r√¥les des contributeurs\nDans tout projet collaboratif, une premi√®re √©tape essentielle est de bien d√©limiter les r√¥les des diff√©rents contributeurs. Les diff√©rents participants au projet ont en effet g√©n√©ralement des r√¥les diff√©rents dans l‚Äôorganisation, des niveaux diff√©rents de pratique de Git, etc. Il est important de refl√©ter ces diff√©rents r√¥les dans l‚Äôorganisation du travail collaboratif.\nSur les diff√©rents h√©bergeurs de projets Git, cela prend la forme de r√¥les que l‚Äôon attribue aux diff√©rents membres du porjet. Les mainteneurs sont les seuls √† pouvoir √©crire directement sur master. Les contributeurs sont quant √† eux tenus de d√©velopper sur des branches. Cela permet de prot√©ger la branche principale, qui doit rester une version propre et jamais compromise du projet.\nNotons que la possibilit√© de donner des r√¥les sur les projets GitHub n‚Äôest possible que dans le cadre d‚Äôorganisations (payantes), donc dans un contexte professionnel ou de projets open-source d‚Äôune certaine ampleur. Pour des petits projets, il est n√©cessaire de s‚Äôastreindre √† une certaine rigueur individuelle pour respecter cette organisation.\n\n\nD√©veloppement sur des branches de court-terme\nLes contributeurs d√©veloppent uniquement sur des branches. Il est d‚Äôusage de cr√©er une branche par fonctionnalit√©, en lui donnant un nom refl√©tant la fonctionnalit√© en cours de d√©veloppement (ex : ajout-tests-unitaires). Les diff√©rents contributeurs √† la fonctionnalit√© en cours de d√©veloppement font des commits sur la branche, en prenant bien soin de pull r√©guli√®rement les √©ventuels changements pour ne pas risquer de conflits de version. Pour la m√™me raison, il est pr√©f√©rable de faire des branches dites de court-terme, c‚Äôest √† dire propres √† une petite fonctionnalit√©, quite √† diviser une fonctionnalit√© en s√©ries d‚Äôimpl√©mentations. Cela permet de limiter les √©ventuels conflits √† g√©rer lors de la fusion finale de la branche avec master.\n\n\nPull Request\nUne fois la s√©rie de modifications termin√©e, vient le temps de rassembler les diff√©rents travaux, par l‚Äôinterm√©diaire de la fusion entre la branche et master. Il faut alors ‚Äúdemander‚Äù de fusionner (pull request) sur GitHub. Cela ouvre une page li√©e √† la pull request, qui rappelle les diff√©rents changements apport√©s et leurs auteurs, et permet d‚Äôentamer une discussion √† propos de ces changements.\n\n\nProcessus de review\nLes diff√©rents membres du projet peuvent donc analyser et commenter les changements, poser des questions, sugg√©rer des modifications, apporter d‚Äôautres contributions, etc. Il est par exemple possible de mentionner un membre de l‚Äô√©quipe par l‚Äôinterm√©diaire de @personne. Il est √©galement possible de proc√©der √† une code review, par exemple par un d√©veloppeur plus exp√©riment√©.\n\n\nR√©solution des √©ventuels conflits\nEn adoptant cette mani√®re de travailler, master ne sera modifi√©e que via des pull requests. Il ne devrait donc jamais y avoir le moindre conflit √† r√©gler sur master, les seuls conflits possibles se passent sur les branches. Par exemple, dans le cas o√π une autre pull request aurait √©t√© fusionn√©e sur master depuis l‚Äôouverture de la pull request en question.\nDans le cas d‚Äôun conflit √† g√©rer, le conflit doit √™tre r√©solu dans la branche et pas dans master. Voici la marche √† suivre :\n\nappliquez le conseil de survie : faites une copie de sauvegarde de votre clone\ndans votre clone, placez vous sur la branche en question : git checkout nom-de-la-branche\nfusionnez master dans la branche : git merge master\nr√©solvez les √©ventuels conflits dans les fichiers concern√©s\nfinalisez le commit de merge et poussez-le sur la branche remote, ce qui le fera appara√Ætre dans la pull request\n\n\n\nFusion de la branche\nSi tout est en ordre, la branche peut √™tre fusionn√©e. Seuls les mainteneurs, qui ont les droits sur master, peuvent fusionner la pull request. En termes d‚Äôhistorique du projet, deux choix sont possibles : - ‚ÄúCreate a merge commit‚Äù : tous les commits r√©alis√©s sur la branche appara√Ætront dans l‚Äôhistorique du projet ; - ‚ÄúSquash and merge‚Äù : les diff√©rents commits r√©alis√©s sur la branche seront rassembl√©s en un commit unique. Cette option est g√©n√©ralement pr√©f√©rable lorsqu‚Äôon utilise des branches de court-terme : elles permettent de garder l‚Äôhistorique plus lisible."
  },
  {
    "objectID": "chapters/git.html#utiliser-les-issues",
    "href": "chapters/git.html#utiliser-les-issues",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Utiliser les issues",
    "text": "Utiliser les issues\nLa mani√®re la plus simple de contribuer √† un projet open-source est d‚Äôouvrir une issue. Sur GitHub, cela se fait sur la page du projet, sous l‚Äôonglet Issue (cf.¬†documentation officielle). Les issues peuvent avoir diff√©rentes nature : - suggestion d‚Äôam√©lioration (sans code) - notification de bug - rapports d‚Äôexp√©rience - etc.\nLes issues sont une mani√®re tr√®s peu couteuse de contributer √† un projet, mais leur importance est capitale, dans la mesure o√π il est impossible pour les d√©veloppeurs d‚Äôun projet de penser en amont √† toutes les utilisations possibles et donc tous les bugs possibles d‚Äôune application."
  },
  {
    "objectID": "chapters/git.html#proposer-une-pull-request",
    "href": "chapters/git.html#proposer-une-pull-request",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Proposer une pull request",
    "text": "Proposer une pull request\nUne autre mani√®re, plus ambitieuse, de contribuer √† l‚Äôopen source est de proposer des pull requests. Concr√®tement, l‚Äôid√©e est de proposer une am√©lioration ou bien de r√©soudre un bug sous forme de code, que les mainteneurs du projet peuvent ensuite d√©cider d‚Äôint√©grer au code existant.\nLa proc√©dure pour proposer une pull request √† un projet sur lequel on n‚Äôa aucun droit est tr√®s similaire √† celle d√©crite ci-dessus dans le cas normal. La principale diff√©rence est que, du fait de l‚Äôabsence de droits, il est impossible de pousser une branche locale sur le r√©pertoire du projet. On va donc devoir cr√©er au pr√©alable un fork, i.e.¬†une copie du projet que l‚Äôon cr√©e dans son espace personnel sur GitHub. C‚Äôest sur cette copie que l‚Äôon va appliquer la proc√©dure d√©crite pr√©c√©demment, en prenant bien soin de travailler sur une branche et non sur master. Une fois les modifications pertinentes effectu√©es sur la branche du fork, GitHub propose de cr√©er une pull request sur le d√©p√¥t original. Cette pull request sera alors visible des mainteneurs du projet, qui pourront l‚Äô√©valuer et d√©cider d‚Äôadopter (ou non) les changements propos√©s."
  },
  {
    "objectID": "chapters/git.html#respecter-les-r√®gles-de-contribution",
    "href": "chapters/git.html#respecter-les-r√®gles-de-contribution",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Respecter les r√®gles de contribution",
    "text": "Respecter les r√®gles de contribution\nVouloir contribuer √† un projet open-source est tr√®s louable, mais ne peut pas pour autant se faire n‚Äôimporte comment. Un projet est constitu√© de personnes, qui ont d√©velopp√© ensemble une mani√®re de travailler, des standards de bonnes pratiques, etc. Pour s‚Äôassurer que sa contribution ne reste pas lettre morte, il est indispensable de s‚Äôimpr√©gner un minimum de la culture du projet.\nPour faciliter les contributions, les projets open-source sp√©cifient bien souvent la mani√®re dont les utilisateurs peuvent contribuer ainsi que le format attendu. En g√©n√©ral, ces r√®gles de contribution sont sp√©cifi√©es dans un fichier CONTRIBUTING.md situ√© √† la racine du projet GitHub, ou a d√©faut dans le README du projet. Il est essentiel de bien lire ce document s‚Äôil existe afin de s‚Äôassurer de proposer des contributions pertinentes."
  },
  {
    "objectID": "chapters/deployment.html",
    "href": "chapters/deployment.html",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/deployment.html#d√©finition",
    "href": "chapters/deployment.html#d√©finition",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "D√©finition",
    "text": "D√©finition\n\nL‚Äôint√©gration continue est un ensemble de pratiques utilis√©es en g√©nie logiciel consistant √† v√©rifier √† chaque modification de code source que le r√©sultat des modifications ne produit pas de r√©gression dans l‚Äôapplication d√©velopp√©e. [‚Ä¶] Elle permet d‚Äôautomatiser l‚Äôex√©cution des suites de tests et de voir l‚Äô√©volution du d√©veloppement du logiciel.\n\n\nLa livraison continue est une approche d‚Äôing√©nierie logicielle dans laquelle les √©quipes produisent des logiciels dans des cycles courts, ce qui permet de le mettre √† disposition √† n‚Äôimporte quel moment. Le but est de construire, tester et diffuser un logiciel plus rapidement.\n\n\nL‚Äôapproche CI/CD garantit une automatisation et une surveillance continues tout au long du cycle de vie d‚Äôun projet.\nCela pr√©sente de nombreux avantages:\n\non peut anticiper les contraintes de la mise en production gr√¢ce √† des environnements normalis√©s partant d‚Äôimage docker standardis√©es\non peut tester les changements apport√©s √† un livrable par un nouveau prototype\non peut d√©terminer tr√®s rapidement l‚Äôintroduction de bugs dans un projet."
  },
  {
    "objectID": "chapters/deployment.html#int√©gration-continue",
    "href": "chapters/deployment.html#int√©gration-continue",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Int√©gration continue",
    "text": "Int√©gration continue\n\nAutomatisation des phases de build et de test (data validation, model validation..)\nMise √† disposition des artifacts (forme ‚Äúmorte‚Äù)\n\n\nMise en oeuvre : GitHub Actions\nLes actions Github sont un ensemble de r√®gles qui se suivent au format YAML. Cela permet de d√©finir diff√©rentes √©tapes du processus avec, pour chaque √©tape, des √©l√©ments de configuration. Elles permettent de partir d‚Äôune configuration minimale (ici une machine ubuntu) et d‚Äôappliquer des instructions √† la suite, par √©tape.\n\n\nExemple d‚Äôaction impliquant exclusivement de l‚Äôint√©gration continue (application 14)\n\nname: Python package\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\"]\n\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n          cache: 'pip'\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install pylint\n          pip install -r requirements.txt\n      - name: Lint\n        run: |\n          pylint src --fail-under=6\n      - name: Test workflow\n        run: |\n          python main.py\n\n\n\n\n\n\n\nNote\n\n\n\nL‚Äôint√©gration continue est aussi au coeur des fonctionnalit√©s de Gitlab. Il s‚Äôagissait m√™me d‚Äôune fonctionnalit√© disponible des ann√©es avant Github.\nLa syntaxe est diff√©rente mais le principe est le m√™me : on part d‚Äôune machine √† la configuration minimale depuis des serveurs mis √† disposition et on teste la reproductibilit√© de son projet."
  },
  {
    "objectID": "chapters/deployment.html#d√©ploiement",
    "href": "chapters/deployment.html#d√©ploiement",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "D√©ploiement",
    "text": "D√©ploiement\n\nAvantages de d√©ployer sur une infra Cloud\nFonctionnement basique de Kubernetes\nGitOps\nImpl√©mentation : Kubernetes"
  },
  {
    "objectID": "chapters/deployment.html#orchestration",
    "href": "chapters/deployment.html#orchestration",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Orchestration",
    "text": "Orchestration\n\nMod√©lisation d‚Äôun pipeline data sous forme de DAG\nImpl√©mentation : argo-workflow\n\nUne chaine de production implique plusieurs √©tapes qui peuvent √©ventuellement n√©cessiter plusieurs langages. Ces √©tapes peuvent √™tre vues comme une suite de transformations.\nLa repr√©sentation de ces √©tapes peut √™tre faite √† l‚Äôaide des diagrammes acycliques dirig√©s (DAG):\n\nUn workflow complet sera ainsi reproductible si on peut, en ayant acc√®s aux inputs et √† l‚Äôensemble des r√®gles de transformation, reproduire exactement les outputs. Si les inputs ou le code changent, on peut √™tre en mesure de mettre √† jour les outputs, si possible sans faire retourner les parties du projet non concern√©s.\nUne premi√®re mani√®re de d√©velopper est l‚Äôapproche manuelle, qui est une t√¢che digne de Sisyphe:\n\nEcriture du code\nEx√©cution du code jusqu‚Äô√† sa fin\nD√©couverte d‚Äôune erreur ou mise √† jour du code ou des donn√©es\nRelance le code dans son ensemble\n\nPour √©viter ce cycle interminable, on est tent√© d‚Äô√©crire des bases interm√©diaires et de ne faire tourner qu‚Äôune partie du code. Cette approche, si elle a l‚Äôavantage de faire gagner du temps, est n√©anmoins dangereuse car on peut facilement oublier de mettre √† jour une base interm√©diaire qui a chang√© ou au contraire refaire tourner une partie du code qui n‚Äôa pas √©t√© mise √† jour.\nIl existe des m√©thodes plus fiables pour √©viter ces gestes manuels. Celles-ci sont inspir√©es de GNU Make et consistent √† cr√©er le chemin de d√©pendance de la chaine de production (lister l‚Äôenvironnement, les inputs et les outputs √† produire), √† d√©terminer les chemins affect√©s par un changement de code ou de donn√©es pour ne faire tourner √† nouveau que les √©tapes n√©cessaires."
  },
  {
    "objectID": "chapters/deployment.html#sp√©cificit√©s-li√©es-√†-la-mise-en-prod-de-mod√®les-de-ml",
    "href": "chapters/deployment.html#sp√©cificit√©s-li√©es-√†-la-mise-en-prod-de-mod√®les-de-ml",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Sp√©cificit√©s li√©es √† la mise en prod de mod√®les de ML",
    "text": "Sp√©cificit√©s li√©es √† la mise en prod de mod√®les de ML\n\nEntrainement : batch ou online\nBatch vs real-time prediction\nFeedback loops exp√©rimentation/d√©ploiement/monitoring\nExposer le mod√®le via une API"
  },
  {
    "objectID": "chapters/deployment.html#notions-dobservabilit√©",
    "href": "chapters/deployment.html#notions-dobservabilit√©",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Notions d‚Äôobservabilit√©",
    "text": "Notions d‚Äôobservabilit√©\n\nDrifts\nQuelles m√©triques monitorer"
  },
  {
    "objectID": "chapters/deployment.html#am√©lioration-continue",
    "href": "chapters/deployment.html#am√©lioration-continue",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Am√©lioration continue",
    "text": "Am√©lioration continue\n\nR√©entra√Ænement : p√©riodique vs continu"
  },
  {
    "objectID": "chapters/deployment.html#footnotes",
    "href": "chapters/deployment.html#footnotes",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPensons au cas r√©cent de ChatGPT, qui doit en partie son succ√®s au fait d‚Äôavoir r√©ussi √† exposer le mod√®le entra√Æn√© via une interface intuitive et r√©active.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/application.html",
    "href": "chapters/application.html",
    "title": "Application",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran.\nL‚Äôobjectif de cette mise en application est d‚Äôillustrer les diff√©rentes √©tapes qui s√©parent la phase de d√©veloppement d‚Äôun projet de celle de la mise en production. Elle permettra de mettre en pratique les diff√©rents concepts pr√©sent√©s tout au long du cours.\nCelle-ci est un tutoriel pas √† pas pour avoir un projet reproductible et disponible sous plusieurs livrables. Toutes les √©tapes ne sont pas indispensables √† tous les projets de data science.\nNous nous pla√ßons dans une situation initiale correspondant √† la fin de la phase de d√©veloppement d‚Äôun projet de data science. On a un notebook un peu monolithique, qui r√©alise les √©tapes classiques d‚Äôun pipeline de machine learning :\nL‚Äôobjectif est d‚Äôam√©liorer le projet de mani√®re incr√©mentale jusqu‚Äô√† pouvoir le mettre en production, en le valorisant sous une forme adapt√©e."
  },
  {
    "objectID": "chapters/application.html#√©tape-1-sassurer-que-le-script-sex√©cute-correctement",
    "href": "chapters/application.html#√©tape-1-sassurer-que-le-script-sex√©cute-correctement",
    "title": "Application",
    "section": "√âtape 1 : s‚Äôassurer que le script s‚Äôex√©cute correctement",
    "text": "√âtape 1 : s‚Äôassurer que le script s‚Äôex√©cute correctement\nOn va partir du fichier notebook.py qui reprend le contenu du notebook1 mais dans un script classique. Le travail de nettoyage en sera facilit√©.\nLa premi√®re √©tape est simple, mais souvent oubli√©e : v√©rifier que le code fonctionne correctement. Pour cela, nous recommandons de faire un aller-retour entre le script ouvert dans VSCode et un terminal pour le lancer.\n\n\n\n\n\n\nApplication 1: corriger les erreurs\n\n\n\n\nOuvrir dans VSCode le script titanic.py ;\nEx√©cuter le script en ligne de commande (python titanic.py)2 pour d√©tecter les erreurs ;\nCorriger les deux erreurs qui emp√™chent la bonne ex√©cution ;\nV√©rifier le fonctionnement du script en utilisant la ligne de commande:\n\n\n\nterminal\n\n$ python titanic.py\n\nLe code devrait afficher des sorties.\n\n\nAide sur les erreurs rencontr√©es\n\nLa premi√®re erreur rencontr√©e est une alerte FileNotFoundError, la seconde est li√©e √† un package.\n\nIl est maintenant temps de commit les changements effectu√©s avec Git3 :\n\n\nterminal\n\n$ git add titanic.py\n$ git commit -m \"Corrige l'erreur qui emp√™chait l'ex√©cution\"\n$ git push\n\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli1"
  },
  {
    "objectID": "chapters/application.html#√©tape-2-utiliser-un-linter-puis-un-formatter",
    "href": "chapters/application.html#√©tape-2-utiliser-un-linter-puis-un-formatter",
    "title": "Application",
    "section": "√âtape 2: utiliser un linter puis un formatter",
    "text": "√âtape 2: utiliser un linter puis un formatter\nOn va maintenant am√©liorer la qualit√© de notre code en appliquant les standards communautaires. Pour cela, on va utiliser le linter classique PyLint et le formatter Black.\n\n\n\n\n\n\nImportant\n\n\n\nPyLint et Black sont des packages Python qui s‚Äôutilisent principalement en ligne de commande.\nSi vous avez une erreur qui sugg√®re que votre terminal ne connait pas PyLint ou Black, n‚Äôoubliez pas d‚Äôex√©cuter la commande pip install pylint ou pip install black.\n\n\nLe linter renvoie alors une s√©rie d‚Äôirr√©gularit√©s, en pr√©cisant √† chaque fois la ligne de l‚Äôerreur et le message d‚Äôerreur associ√© (ex : mauvaise identation). Il renvoie finalement une note sur 10, qui estime la qualit√© du code √† l‚Äôaune des standards communautaires √©voqu√©s dans la partie Qualit√© du code.\n\n\n\n\n\n\nApplication 2: rendre lisible le script\n\n\n\n\nDiagnostiquer et √©valuer la qualit√© de titanic.py avec PyLint. Regarder la note obtenue.\nUtiliser black titanic.py --diff --color pour observer les changements de forme que va induire l‚Äôutilisation du formatter Black. Cette √©tape n‚Äôapplique pas les modifications, elle ne fait que vous les montrer.\nAppliquer le formatter Black\nR√©utiliser PyLint pour diagnostiquer l‚Äôam√©lioration de la qualit√© du script et le travail qui reste √† faire.\nComme la majorit√© du travail restant est √† consacrer aux imports:\n\nMettre tous les imports ensemble en d√©but de script\nRetirer les imports redondants en s‚Äôaidant des diagnostics de votre √©diteur\nR√©ordonner les imports si PyLint vous indique de le faire\nCorriger les derni√®res fautes formelles sugg√©r√©es par PyLint\n\nD√©limiter des parties dans votre code pour rendre sa structure plus lisible\n\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli2\n\n\n\n\n\n\n\n\n\nLe code est maintenant lisible, il obtient √† ce stade une note formelle proche de 10. Mais il n‚Äôest pas encore totalement intelligible ou fiable. Il y a notamment beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. N√©anmoins, avant cela, occupons-nous de mieux g√©rer certains param√®tres du script: jetons d‚ÄôAPI et chemin des fichiers."
  },
  {
    "objectID": "chapters/application.html#√©tape-3-gestion-des-param√®tres",
    "href": "chapters/application.html#√©tape-3-gestion-des-param√®tres",
    "title": "Application",
    "section": "√âtape 3: gestion des param√®tres",
    "text": "√âtape 3: gestion des param√®tres\nL‚Äôex√©cution du code et les r√©sultats obtenus d√©pendent de certains param√®tres d√©finis dans le code. L‚Äô√©tude de r√©sultats alternatifs, en jouant sur des variantes des (hyper)param√®tres, est √† ce stade compliqu√©e car il est n√©cessaire de parcourir le code pour trouver ces param√®tres. De plus, certains param√®tres personnels comme des jetons d‚ÄôAPI ou des mots de passe n‚Äôont pas vocation √† √™tre pr√©sents dans le code.\nIl est plus judicieux de consid√©rer ces param√®tres comme des variables d‚Äôentr√©e du script. Cela peut √™tre fait de deux mani√®res:\n\nAvec des arguments optionnels appel√©s depuis la ligne de commande (Application 3a). Cela peut √™tre pratique pour mettre en oeuvre des tests automatis√©s mais n‚Äôest pas forc√©ment pertinent pour toutes les variables. Nous allons montrer cet usage avec le nombre d‚Äôarbres de notre random forest ;\nEn utilisant un fichier de configuration dont les valeurs sont import√©es dans le script principal (Application 3b).\n\n\n\nUn exemple de d√©finition d‚Äôun argument pour l‚Äôutilisation en ligne de commande\n\n\n\nprenom.py\n\nimport argparse\nparser = argparse.ArgumentParser(description=\"Qui √™tes-vous?\")\nparser.add_argument(\n    \"--prenom\", type=str, default=\"Toto\", help=\"Un pr√©nom √† afficher\"\n)\nargs = parser.parse_args()\nprint(args.prenom)\n\nExemples d‚Äôutilisations en ligne de commande\n\n\nterminal\n\n$ python prenom.py\n$ python prenom.py --prenom \"Zinedine\"\n\n\n\n\n\n\n\n\nApplication 3a: Param√©trisation du script\n\n\n\n\nEn s‚Äôinspirant de l‚Äôexemple ci-dessus üëÜÔ∏è, cr√©er une variable n_trees qui peut √©ventuellement √™tre param√©tr√©e en ligne de commande et dont la valeur par d√©faut est 20 ;\nTester cette param√©trisation en ligne de commande avec la valeur par d√©faut puis 2, 10 et 50 arbres.\n\n\n\nL‚Äôexercice suivant permet de mettre en application le fait de param√©triser un script en utilisant des variables d√©finies dans un fichier YAML.\n\n\n\n\n\n\nApplication 3b: La configuration dans un fichier YAML\n\n\n\nNous allons mettre 4 param√®tres dans notre YAML. Celui-ci prendra la forme suivante:\n\n\nconfig.yaml\n\njeton_api: ####\ntrain_path: ####\ntest_path: ####\ntest_fraction: ####\n\nAvec #### des valeurs √† remplacer.\n\nCr√©er √† la racine du projet un fichier config.yaml √† partir du mod√®le üëÜÔ∏è ;\nRep√©rer les valeurs dans le code associ√©es et compl√©ter.\n\nMaintenant, nous allons exploiter ce fichier:\n\nPour √©viter d‚Äôavoir √† le faire plus tard, cr√©er une fonction import_yaml_config qui prend en argument le chemin d‚Äôun fichier YAML et renvoie le contenu de celui-ci en output. Vous pouvez suivre le conseil du chapitre sur la Qualit√© du code en adoptant le type hinting ;\n\n\n\nIndice si vous ne trouvez pas comment lire un fichier YAML\n\nSi le fichier s‚Äôappelle toto.yaml, vous pouvez l‚Äôimporter de cette mani√®re:\n\nwith open(\"toto.yaml\", \"r\", encoding=\"utf-8\") as stream:\n    dict_config = yaml.safe_load(stream)\n\n\n\nDans la fonction import_yaml_config, cr√©er une condition logique pour tenir compte du fait que le YAML de configuration peut ne pas exister4 ;\n\n\n\nIndice si vous ne savez comment conditionner la cr√©ation de la configuration √† l‚Äôexistence du fichier\n\nVoici la ligne qui peut vous aider. L‚Äôid√©al est d‚Äôins√©rer ceci dans import_yaml_config:\n\nCONFIG_PATH = 'config.yaml'\nconfig = {}\nif os.path.exists(CONFIG_PATH):\n    # lecture du fichier\n\n\n\nUtiliser le canevas de code suivant pour cr√©er les variables ad√©quates\n\n\nAPI_TOKEN = config.get(\"jeton_api\")\nTRAIN_PATH = config.get(\"train_path\", \"train.csv\")\nTEST_PATH = config.get(\"test_path\", \"test.csv\")\nTEST_FRACTION = config.get(\"test_fraction\", .1)\n\net remplacer dans le code ;\n\nTester en ligne de commande que l‚Äôex√©cution du fichier est toujours sans erreur et sinon corriger ;\nRefaire un diagnostic avec PyLint et corriger les √©ventuels messages ;\nCr√©er un fichier .gitignore (cf.¬†Chapitre Git). Ajouter dans ce fichier config.yaml car il ne faut pas committer ce fichier. Au passage ajouter __pycache__/ au .gitignore5, cela √©vitera d‚Äôavoir √† le faire ult√©rieurement ;\nCr√©er un fichier README.md o√π vous indiquez qu‚Äôil faut cr√©er un fichier config.yaml pour pouvoir utiliser l‚ÄôAPI.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli3"
  },
  {
    "objectID": "chapters/application.html#√©tape-4-privil√©gier-la-programmation-fonctionnelle",
    "href": "chapters/application.html#√©tape-4-privil√©gier-la-programmation-fonctionnelle",
    "title": "Application",
    "section": "√âtape 4 : Privil√©gier la programmation fonctionnelle",
    "text": "√âtape 4 : Privil√©gier la programmation fonctionnelle\nNous allons mettre en fonctions les parties importantes de l‚Äôanalyse. Ceci facilitera l‚Äô√©tape ult√©rieure de modularisation de notre projet.\nCet exercice √©tant chronophage, il n‚Äôest pas obligatoire de le r√©aliser en entier. L‚Äôimportant est de comprendre la d√©marche et d‚Äôadopter fr√©quemment une approche fonctionnelle6. Pour obtenir une chaine enti√®rement fonctionnalis√©e, vous pouvez reprendre le checkpoint.\n\n\n\n\n\n\nApplication 4: adoption des standards de programmation fonctionnelle\n\n\n\n\nCr√©er une fonction qui importe les donn√©es d‚Äôentra√Ænement (train.csv) et de test (test.csv) et renvoie des DataFrames Pandas ;\nEn fonction du temps disponible, cr√©er plusieurs fonctions pour r√©aliser les √©tapes de feature engineering:\n\nLa cr√©ation de la variable ‚ÄúTitle‚Äù peut √™tre automatis√©e en vertu du principe ‚Äúdo not repeat yourself‚Äù7.\nRegrouper ensemble les fillna et essayer de cr√©er une fonction g√©n√©ralisant l‚Äôop√©ration.\nLes label encoders peuvent √™tre transform√©s en deux fonctions: une premi√®re pour encoder une colonne puis une seconde qui utilise la premi√®re de mani√®re r√©p√©t√©e pour encoder plusieurs colonnes. Remarquez les erreurs de copier-coller que cela corrige\nFinaliser les derni√®res transformations avec des fonctions\n\nCr√©er une fonction qui r√©alise le split train/test de validation en fonction d‚Äôun param√®tre repr√©sentant la proportion de l‚Äô√©chantillon de test.\nCr√©er une fonction qui entra√Æne et √©value un classifieur RandomForest, et qui prend en param√®tre le nombre d‚Äôarbres (n_estimators). La fonction doit imprimer √† la fin la performance obtenue et la matrice de confusion.\nD√©placer toutes les fonctions ensemble, en d√©but de script.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLe fait d‚Äôappliquer des fonctions a d√©j√† am√©lior√© la fiabilit√© du processus en r√©duisant le nombre d‚Äôerreurs de copier-coller. N√©anmoins, pour vraiment fiabiliser le processus, il faudrait utiliser un pipeline de transformations de donn√©es.\nCeci n‚Äôest pas encore au programme du cours mais le sera dans une prochaine version.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli4\n\n\n\n\n\n\n\n\n\nCela ne se remarque pas encore vraiment car nous avons de nombreuses d√©finitions de fonctions mais notre chaine de production est beaucoup plus concise (le script fait environ 300 lignes dont 250 de d√©finitions de fonctions g√©n√©riques). Cette auto-discipline facilitera grandement les √©tapes ult√©rieures. Cela aurait √©t√© n√©anmoins beaucoup moins co√ªteux en temps d‚Äôadopter ces bons gestes de mani√®re plus pr√©coce."
  },
  {
    "objectID": "chapters/application.html#√©tape-1-modularisation",
    "href": "chapters/application.html#√©tape-1-modularisation",
    "title": "Application",
    "section": "√âtape 1 : modularisation",
    "text": "√âtape 1 : modularisation\nNous allons profiter de la modularisation pour adopter une structure applicative pour notre code. Celui-ci n‚Äô√©tant en effet plus lanc√© que la ligne de commande, on peut consid√©rer qu‚Äôon construit une application g√©n√©rique o√π un script principal (main.py) encapsule des √©l√©ments issus d‚Äôautres scripts.\n\n\n\n\n\n\nApplication 5: modularisation\n\n\n\n\nD√©placer les fonctions dans une s√©rie de fichiers d√©di√©s:\n\nimport_data.py: fonctions d‚Äôimport de donn√©es\nbuild_features.py: fonctions regroupant les √©tapes de feature engineering\ntrain_evaluate.py: fonctions d‚Äôentrainement et d‚Äô√©valuation du mod√®le\n\nSp√©cifier les d√©pendances (i.e.¬†les packages √† importer) dans les modules pour que ceux-ci puissent s‚Äôex√©cuter ind√©pendamment ;\nRenommer titanic.py en main.py pour suivre la convention de nommage des projets Python ;\nImporter les fonctions n√©cessaires √† partir des modules.\nV√©rifier que tout fonctionne bien en ex√©cutant le script main √† partir de la ligne de commande :\n\n\n\nterminal\n\n$ $ python main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli5"
  },
  {
    "objectID": "chapters/application.html#√©tape-2-adopter-une-architecture-standardis√©e-de-projet",
    "href": "chapters/application.html#√©tape-2-adopter-une-architecture-standardis√©e-de-projet",
    "title": "Application",
    "section": "√âtape 2 : adopter une architecture standardis√©e de projet",
    "text": "√âtape 2 : adopter une architecture standardis√©e de projet\nOn dispose maintenant d‚Äôune application Python fonctionnelle. N√©anmoins, le projet est certes plus fiable mais sa structuration laisse √† d√©sirer et il serait difficile de rentrer √† nouveau dans le projet dans quelques temps.\n\n\nEtat actuel du projet üôà\n\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ train.csv\n‚îú‚îÄ‚îÄ test.csv\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ config.yaml\n‚îú‚îÄ‚îÄ import_data.py\n‚îú‚îÄ‚îÄ build_features.py\n‚îú‚îÄ‚îÄ train_evaluate.py\n‚îú‚îÄ‚îÄ titanic.ipynb\n‚îî‚îÄ‚îÄ main.py\n\nComme cela est expliqu√© dans la partie Structure des projets, on va adopter une structure certes arbitraire mais qui va faciliter l‚Äôautodocumentation de notre projet. De plus, une telle structure va faciliter des √©volutions optionnelles comme la packagisation du projet. Passer d‚Äôune structure modulaire bien faite √† un package est quasi-imm√©diat en Python.\nOn va donc modifier l‚Äôarchitecture de notre projet pour la rendre plus standardis√©e. Pour cela, on va s‚Äôinspirer des structures cookiecutter qui g√©n√®rent des templates de projet. En l‚Äôoccurrence notre source d‚Äôinspiration sera le template datascience issu d‚Äôun effort communautaire.\n\n\n\n\n\n\nNote\n\n\n\nL‚Äôid√©e de cookiecutter est de proposer des templates que l‚Äôon utilise pour initialiser un projet, afin de b√¢tir √† l‚Äôavance une structure √©volutive. La syntaxe √† utiliser dans ce cas est la suivante :\n\n\nterminal\n\n$ pip install cookiecutter\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science\n\nIci, on a d√©j√† un projet, on va donc faire les choses dans l‚Äôautre sens : on va s‚Äôinspirer de la structure propos√©e afin de r√©organiser celle de notre projet selon les standards communautaires.\n\n\nEn s‚Äôinspirant du cookiecutter data science on va adopter la structure suivante:\n\n\nStructure recommand√©e\n\nensae-reproductibilite-application\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îî‚îÄ‚îÄ raw\n‚îÇ       ‚îú‚îÄ‚îÄ test.csv\n‚îÇ       ‚îî‚îÄ‚îÄ train.csv\n‚îú‚îÄ‚îÄ configuration\n‚îÇ   ‚îî‚îÄ‚îÄ config.yaml\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb\n‚îî‚îÄ‚îÄ src\n    ‚îú‚îÄ‚îÄ data\n    ‚îÇ   ‚îî‚îÄ‚îÄ import_data.py\n    ‚îú‚îÄ‚îÄ features\n    ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py\n    ‚îî‚îÄ‚îÄ models\n        ‚îî‚îÄ‚îÄ train_evaluate.py\n\n\n\n\n\n\n\nApplication 6: adopter une structure lisible\n\n\n\n\n(optionnel) Analyser et comprendre la structure de projet propos√©e par le template ;\nModifier l‚Äôarborescence du projet selon le mod√®le ;\nMettre √† jour l‚Äôimport des d√©pendances, le fichier de configuration et main.py avec les nouveaux chemins ;\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli6"
  },
  {
    "objectID": "chapters/application.html#√©tape-3-indiquer-lenvironnement-minimal-de-reproductibilit√©",
    "href": "chapters/application.html#√©tape-3-indiquer-lenvironnement-minimal-de-reproductibilit√©",
    "title": "Application",
    "section": "√âtape 3: indiquer l‚Äôenvironnement minimal de reproductibilit√©",
    "text": "√âtape 3: indiquer l‚Äôenvironnement minimal de reproductibilit√©\nLe script main.py n√©cessite un certain nombre de packages pour √™tre fonctionnel. Chez vous les packages n√©cessaires sont bien s√ªr install√©s mais √™tes-vous assur√© que c‚Äôest le cas chez la personne qui testera votre code ?\nAfin de favoriser la portabilit√© du projet, il est d‚Äôusage de ‚Äúfixer l‚Äôenvironnement‚Äù, c‚Äôest-√†-dire d‚Äôindiquer dans un fichier toutes les d√©pendances utilis√©es ainsi que leurs version. Nous proposons de cr√©er un fichier requirements.txt minimal, sur lequel nous reviendrons dans la partie consacr√©e aux environnements reproductibles.\nLe fichier requirements.txt est conventionnellement localis√© √† la racine du projet. Ici on ne va pas fixer les versions, on raffinera ce fichier ult√©rieurement.\n\n\n\n\n\n\nApplication 7: cr√©ation du requirements.txt\n\n\n\n\nCr√©er un fichier requirements.txt avec la liste des packages n√©cessaires\nAjouter une indication dans README.md sur l‚Äôinstallation des packages gr√¢ce au fichier requirements.txt\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli7"
  },
  {
    "objectID": "chapters/application.html#stockageS3",
    "href": "chapters/application.html#stockageS3",
    "title": "Application",
    "section": "√âtape 4 : stocker les donn√©es de mani√®re externe",
    "text": "√âtape 4 : stocker les donn√©es de mani√®re externe\n\n\n\n\n\n\nPour en savoir plus sur le syst√®me de stockage S3\n\n\n\n\n\nPour mettre en oeuvre cette √©tape, il peut √™tre utile de comprendre un peu comme fonctionne le SSP Cloud. Vous devrez suivre la documentation du SSP Cloud pour la r√©aliser. Une aide-m√©moire est √©galement disponible dans le cours de 2e ann√©e de l‚ÄôENSAE Python pour la data science.\n\n\n\nLe chapitre sur la structure des projets d√©veloppe l‚Äôid√©e qu‚Äôil est recommand√© de converger vers un mod√®le o√π environnements d‚Äôex√©cution, de stockage du code et des donn√©es sont conceptuellement s√©par√©s. Ce haut niveau d‚Äôexigence est un gain de temps important lors de la mise en production car au cours de cette derni√®re, le projet est amen√© √† √™tre ex√©cut√© sur une infrastructure informatique d√©di√©e qu‚Äôil est bon d‚Äôanticiper.\nA l‚Äôheure actuelle, les donn√©es sont stock√©es dans le d√©p√¥t. C‚Äôest une mauvaise pratique. En premier lieu, Git n‚Äôest techniquement pas bien adapt√© au stockage de donn√©es. Ici ce n‚Äôest pas tr√®s grave car il ne s‚Äôagit pas de donn√©es volumineuses et ces derni√®res ne sont pas modifi√©es au cours de notre chaine de traitement. La raison principale est que les donn√©es trait√©es par les data scientists sont g√©n√©ralement soumises √† des clauses de confidentialit√©s (RGPD, secret statistique‚Ä¶). Mettre ces donn√©es sous contr√¥le de version c‚Äôest prendre le risque de les divulguer √† un public non habilit√©. Il est donc recommand√© de privil√©gier des outils techniques adapt√©s au stockage de donn√©es.\nL‚Äôid√©al, dans notre cas, est d‚Äôutiliser une solution de stockage externe. On va utiliser pour cela MinIO, la solution de stockage de type S3 offerte par le SSP Cloud. Cela nous permettra de supprimer les donn√©es de Github tout en maintenant la reproductibilit√© de notre projet 8.\n\n\n\n\n\n\nApplication 8: utilisation d‚Äôun syst√®me de stockage distant\n\n\n\nA partir de la ligne de commande, utiliser l‚Äôutilitaire MinIO pour copier les donn√©es data/raw/train.csv et data/raw/test.csv vers votre bucket personnel, respectivement dans les dossiers ensae-reproductibilite/data/raw/train.csv et ensae-reproductibilite/data/raw/test.csv.\n\n\nIndice\n\nStructure √† adopter:\n\n\nterminal\n\n$ mc cp data/raw/train.csv s3/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/data/raw/train.csv\n$ mc cp data/raw/test.csv s3/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/data/raw/test.csv\n\nen modifiant l‚Äôemplacement de votre bucket personnel\n\nPour se simplifier la vie, on va utiliser des URL de t√©l√©chargement des fichiers (comme si ceux-ci √©taient sur n‚Äôimporte quel espace de stockage) plut√¥t que d‚Äôutiliser une librairie S3 compatible comme boto3 ou s3fs. Par d√©faut, le contenu de votre bucket est priv√©, seul vous y avez acc√®s. N√©anmoins, vous pouvez rendre accessible √† tous en lecture le contenu de votre bucket en faisant lui donnant des droits anonymes. Pour cela, en ligne de commande, faire:\n\n\nterminal\n\n$ mc anonymous set download s3/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/data/raw/\n\nen modifiant &lt;BUCKET_PERSONNEL&gt;. Les URL de t√©l√©chargement seront de la forme https://minio.lab.sspcloud.fr/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/data/raw/test.csv et https://minio.lab.sspcloud.fr/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/data/raw/train.csv\n\nModifier configuration/config.yaml pour utiliser directement les URL dans l‚Äôimport ;\nModifier les valeurs par d√©faut dans votre code ;\nSupprimer les fichiers .csv du dossier data de votre projet, on n‚Äôen a plus besoin vu qu‚Äôon les importe de l‚Äôext√©rieur ;\nV√©rifier le bon fonctionnement de votre application.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli8"
  },
  {
    "objectID": "chapters/application.html#√©tape-1-proposer-des-tests-unitaires-optionnel",
    "href": "chapters/application.html#√©tape-1-proposer-des-tests-unitaires-optionnel",
    "title": "Application",
    "section": "√âtape 1 : proposer des tests unitaires (optionnel)",
    "text": "√âtape 1 : proposer des tests unitaires (optionnel)\nNotre code comporte un certain nombre de fonctions g√©n√©riques. On peut vouloir tester leur usage sur des donn√©es standardis√©es, diff√©rentes de celles du Titanic.\nM√™me si la notion de tests unitaires prend plus de sens dans un package, nous pouvons proposer dans le projet des exemples d‚Äôutilisation de la fonction, ceci peut √™tre p√©dagogique.\nNous allons utiliser unittest pour effectuer des tests unitaires. Cette approche n√©cessite quelques notions de programmation orient√©e objet ou une bonne discussion avec ChatGPT.\n\n\n\n\n\n\nApplication 9: test unitaire (optionnel)\n\n\n\nDans le dossier tests/, cr√©er un fichier test_create_variable_title.py.\nEn s‚Äôinspirant de l‚Äôexemple de base, cr√©er une classe TestCreateVariableTitle qui effectue les op√©rations suivantes:\n\nCr√©ation d‚Äôune fonction test_create_variable_title_default_variable_name qui permet de comparer les objets suivants:\n\nCr√©ation d‚Äôun DataFrame de test :\n\ndf = pd.DataFrame({\n            'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)',\n                    'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath (Lily May Peel)',\n                    'Allen, Mr. William Henry', 'Moran, Mr. James',\n                    'McCarthy, Mr. Timothy J', 'Palsson, Master. Gosta Leonard',\n                    'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)',\n                    'Nasser, Mrs. Nicholas (Adele Achem)'],\n            'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],\n            'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n        })\n\nUtilisation de la fonction create_variable_title sur ce DataFrame\nComparaison au DataFrame attendu:\n\nexpected_result = pd.DataFrame({\n            'Title': ['Mr.', 'Mrs.', 'Miss.', 'Mrs.', 'Mr.', 'Mr.', 'Mr.', 'Master.', 'Mrs.', 'Mrs.'],\n            'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],\n            'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n        })\nEffectuer le test unitaire en ligne de commande avec unittest (python -m unittest tests/test_create_variable_title.py). Corriger le test unitaire en cas d‚Äôerreur.\nSi le temps le permet, proposer des variantes pour tenir compte de param√®tres (comme la variable variable_name) ou d‚Äôexceptions (comme la gestion du cas ‚ÄúDona‚Äù).\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLorsqu‚Äôon effectue des tests unitaires, on cherche g√©n√©ralement √† tester le plus de lignes possibles de son code. On parle de taux de couverture (coverage rate) pour d√©signer la statistique mesurant cela.\nCela peut s‚Äôeffectuer de la mani√®re suivante avec le package coverage:\n\n\nterminal\n\n$ coverage run -m unittest tests/test_create_variable_title.py\n$ coverage report -m\n\nName                                  Stmts   Miss  Cover   Missing\n-------------------------------------------------------------------\nsrc/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113\ntests/test_create_variable_title.py      21      1    95%   54\n-------------------------------------------------------------------\nTOTAL                                    55     22    60%\nLe taux de couverture est souvent mis en avant par les gros projets comme indicateur de leur qualit√©. Il existe d‚Äôailleurs des badges Github d√©di√©s."
  },
  {
    "objectID": "chapters/application.html#√©tape-2-transformer-son-projet-en-package-optionnel",
    "href": "chapters/application.html#√©tape-2-transformer-son-projet-en-package-optionnel",
    "title": "Application",
    "section": "√âtape 2 : transformer son projet en package (optionnel)",
    "text": "√âtape 2 : transformer son projet en package (optionnel)\nNotre projet est modulaire, ce qui le rend assez simple √† transformer en package, en s‚Äôinspirant de la structure du cookiecutter adapt√©, issu de cet ouvrage.\nOn va cr√©er un package nomm√© titanicml qui encapsule tout notre code et qui sera appel√© par notre script main.py. La structure attendue est la suivante:\n\n\nStructure vis√©e\n\nensae-reproductibilite-application\n‚îú‚îÄ‚îÄ docs                                    ‚îê \n‚îÇ   ‚îú‚îÄ‚îÄ main.py                             ‚îÇ \n‚îÇ   ‚îî‚îÄ‚îÄ notebooks                           ‚îÇ Package documentation and examples\n‚îÇ       ‚îî‚îÄ‚îÄ titanic.ipynb                   ‚îÇ \n‚îú‚îÄ‚îÄ configuration                           ‚îê Configuration (pas √† partager avec Git)\n‚îÇ   ‚îî‚îÄ‚îÄ config.yaml                         ‚îò \n‚îú‚îÄ‚îÄ README.md                                \n‚îú‚îÄ‚îÄ pyproject.toml                          ‚îê \n‚îú‚îÄ‚îÄ requirements.txt                        ‚îÇ\n‚îú‚îÄ‚îÄ titanicml                               ‚îÇ                \n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                         ‚îÇ Package source code, metadata\n‚îÇ   ‚îú‚îÄ‚îÄ data                                ‚îÇ and build instructions \n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ import_data.py                  ‚îÇ  \n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_create_variable_title.py   ‚îÇ   \n‚îÇ   ‚îú‚îÄ‚îÄ features                            ‚îÇ\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py               ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ models                              ‚îÇ\n‚îÇ       ‚îî‚îÄ‚îÄ train_evaluate.py               ‚îò\n‚îî‚îÄ‚îÄ tests                                   ‚îê\n    ‚îî‚îÄ‚îÄ test_create_variable_title.py       ‚îò Package tests\n\n\n\nRappel: structure actuelle\n\nensae-reproductibilite-application\n‚îú‚îÄ‚îÄ notebooks                                 \n‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb                  \n‚îú‚îÄ‚îÄ configuration                                 \n‚îÇ   ‚îî‚îÄ‚îÄ config.yaml                  \n‚îú‚îÄ‚îÄ main.py                              \n‚îú‚îÄ‚îÄ README.md                 \n‚îú‚îÄ‚îÄ requirements.txt                      \n‚îî‚îÄ‚îÄ src \n    ‚îú‚îÄ‚îÄ data                                \n    ‚îÇ   ‚îú‚îÄ‚îÄ import_data.py                    \n    ‚îÇ   ‚îî‚îÄ‚îÄ test_create_variable_title.py      \n    ‚îú‚îÄ‚îÄ features                           \n    ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py      \n    ‚îî‚îÄ‚îÄ models                          \n        ‚îî‚îÄ‚îÄ train_evaluate.py              \n\nIl existe plusieurs frameworks pour construire un package. Nous allons privil√©gier Poetry √† Setuptools.\n\n\n\n\n\n\nNote\n\n\n\nPour cr√©er la structure minimale d‚Äôun package, le plus simple est d‚Äôutiliser le cookiecutter adapt√©, issu de cet ouvrage.\nComme on a d√©j√† une structure tr√®s modulaire, on va plut√¥t recr√©er cette structure dans notre projet d√©j√† existant. En fait, il ne manque qu‚Äôun fichier essentiel, le principal distinguant un projet classique d‚Äôun package : pyproject.toml.\n\n\nterminal\n\n$ cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git\n\n\n\nD√©rouler pour voir les choix possibles\n\nauthor_name [Monty Python]: Daffy Duck\npackage_name [mypkg]: titanicml\npackage_short_description []: Impressive Titanic survival analysis\npackage_version [0.1.0]: \npython_version [3.9]: \nSelect open_source_license:\n1 - MIT\n2 - Apache License 2.0\n3 - GNU General Public License v3.0\n4 - Creative Commons Attribution 4.0\n5 - BSD 3-Clause\n6 - Proprietary\n7 - None\nChoose from 1, 2, 3, 4, 5, 6 [1]: \nSelect include_github_actions:\n1 - no\n2 - ci\n3 - ci+cd\nChoose from 1, 2, 3 [1]:\n\n\n\n\n\n\n\n\n\nApplication 10: packagisation (optionnel)\n\n\n\n\nRenommer le dossier titanicml pour respecter la nouvelle arborescence ;\nCr√©er un fichier pyproject.toml sur cette base ;\n\n\n\n\npyproject.toml\n\n[tool.poetry]\nname = \"titanicml\"\nversion = \"0.0.1\"\ndescription = \"Awesome Machine Learning project\"\nauthors = [\"Daffy Duck &lt;daffy.duck@fauxmail.fr&gt;\", \"Mickey Mouse\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.pytest.ini_options]\nlog_cli = true\nlog_cli_level = \"WARNING\"\nlog_cli_format = \"%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)\"\nlog_cli_date_format = \"%Y-%m-%d %H:%M:%S\"\n\n\n\nCr√©er le dossier docs et mettre les fichiers indiqu√©s dedans\nDans titanicml/, cr√©er un fichier __init__.py9\n\n\n\n\n__init__.py\n\nfrom .import_data import (\n    import_data, import_yaml_config\n)\nfrom .build_features import (\n    create_variable_title,\n    fill_na_titanic,\n    label_encoder_titanic,\n    check_has_cabin,\n    ticket_length\n)\nfrom .train_evaluate import random_forest_titanic\n\n__all__ = [\n    \"import_data\", \"import_yaml_config\",\n    \"create_variable_title\",\n    \"fill_na_titanic\",\n    \"label_encoder_titanic\",\n    \"check_has_cabin\",\n    \"ticket_length\",\n    \"random_forest_titanic\"\n]\n\n\n\nInstaller le package en local avec pip install -e .\nModifier le contenu de docs/main.py pour importer les fonctions de notre package titanicml et tester en ligne de commande notre fichier main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli10"
  },
  {
    "objectID": "chapters/application.html#anaconda",
    "href": "chapters/application.html#anaconda",
    "title": "Application",
    "section": "√âtape 1 : un environnement pour rendre le projet portable",
    "text": "√âtape 1 : un environnement pour rendre le projet portable\nPour qu‚Äôun projet soit portable, il doit remplir deux conditions:\n\nNe pas n√©cessiter de d√©pendance qui ne soient pas renseign√©es quelque part ;\nNe pas proposer des d√©pendances inutiles, qui ne sont pas utilis√©es dans le cadre du projet.\n\nLe prochain exercice vise √† mettre ceci en oeuvre. Comme expliqu√© dans le chapitre portabilit√©, le choix du gestionnaire d‚Äôenvironnement est laiss√© libre. Il est recommand√© de privil√©gier venv si vous d√©couvrez la probl√©matique de la portabilit√©.\n\nEnvironnement virtuel venvEnvironnement conda\n\n\nL‚Äôapproche la plus l√©g√®re est l‚Äôenvironnement virtuel. Nous avons en fait implicitement d√©j√† commenc√© √† aller vers cette direction en cr√©ant un fichier requirements.txt.\n\n\n\n\n\n\nApplication 11a: environnement virtuel venv\n\n\n\n\nEx√©cuter pip freeze en ligne de commande et observer la (tr√®s) longue liste de package\nCr√©er l‚Äôenvironnement virtuel titanic en s‚Äôinspirant de la documentation officielle10 ou du chapitre d√©di√©\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin install√©\nActiver l‚Äôenvironnement et v√©rifier l‚Äôinstallation de Python maintenant utilis√©e par votre machine \nV√©rifier directement depuis la ligne de commande que Python ex√©cute bien une commande11 avec:\n\n\n\nterminal\n\n$ python -c \"print('Hello')\"\n\n\nFaire la m√™me chose mais avec import pandas as pd\nInstaller les packages √† partir du requirements.txt. Tester √† nouveau import pandas as pd pour comprendre la diff√©rence.\nEx√©cuter pip freeze et comprendre la diff√©rence avec la situation pr√©c√©dente.\nV√©rifier que le script main.py fonctionne bien. Sinon ajouter les packages manquants dans le requirements.txt et reprendre de mani√®re it√©rative √† partir de la question 7.\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier √† Git.\n\n\n\nAide pour la question 4\n\nApr√®s l‚Äôactivation, vous pouvez v√©rifier quel python est utilis√© de cette mani√®re\n\n\nterminal\n\n(titanic) $ which python\n\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli11a\n\n\n\n\n\n\n\n\n\n\n\nLes environnements conda sont plus lourds √† mettre en oeuvre que les environnements virtuels mais peuvent permettre un contr√¥le plus formel des d√©pendances.\n\n\n\n\n\n\nApplication 11b: environnement conda\n\n\n\n\nEx√©cuter conda env export en ligne de commande et observer la (tr√®s) longue liste de package\nCr√©er un environnement titanic avec conda create\nActiver l‚Äôenvironnement et v√©rifier l‚Äôinstallation de Python maintenant utilis√©e par votre machine \nV√©rifier directement depuis la ligne de commande que Python ex√©cute bien une commande12 avec:\n\n\n\nterminal\n\n$ python -c \"print('Hello')\"\n\n\nFaire la m√™me chose mais avec import pandas as pd\nInstaller les packages qu‚Äôon avait list√© dans le requirements.txt pr√©c√©demment. Ne pas faire un pip install -r requirements.txt afin de privil√©gier conda install\nEx√©cuter √† nouveau conda env export et comprendre la diff√©rence avec la situation pr√©c√©dente13.\nV√©rifier que le script main.py fonctionne bien. Sinon installer les packages manquants et reprndre de mani√®re it√©rative √† partir de la question 7.\nQuand main.py fonctionne, faire conda env export &gt; environment.yml pour figer l‚Äôenvironnement de travail.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli11b"
  },
  {
    "objectID": "chapters/application.html#shell",
    "href": "chapters/application.html#shell",
    "title": "Application",
    "section": "√âtape 2: construire l‚Äôenvironnement de notre application via un script shell",
    "text": "√âtape 2: construire l‚Äôenvironnement de notre application via un script shell\nLes environnements virtuels permettent de mieux sp√©cifier les d√©pendances de notre projet, mais ne permettent pas de garantir une portabilit√© optimale. Pour cela, il faut recourir √† la technologie des conteneurs. L‚Äôid√©e est de construire une machine, en partant d‚Äôune base quasi-vierge, qui permette de construire √©tape par √©tape l‚Äôenvironnement n√©cessaire au bon fonctionnement de notre projet. C‚Äôest le principe des conteneurs Docker .\nLeur m√©thode de construction √©tant un peu difficile √† prendre en main au d√©but, nous allons passer par une √©tape interm√©diaire afin de bien comprendre le processus de production.\n\nNous allons d‚Äôabord cr√©er un script shell, c‚Äôest √† dire une suite de commandes Linux permettant de construire l‚Äôenvironnement √† partir d‚Äôune machine vierge ;\nNous transformerons celui-ci en Dockerfile dans un deuxi√®me temps. C‚Äôest l‚Äôobjet de l‚Äô√©tape suivante.\n\n\nEnvironnement virtuel venvEnvironnement conda\n\n\n\n\n\n\n\n\nApplication 12a : cr√©er un fichier d‚Äôinstallation de A √† Z\n\n\n\n\nCr√©er un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le d√©p√¥t\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11a avec git checkout appli11a\nVia l‚Äôexplorateur de fichiers, cr√©er le fichier install.sh √† la racine du projet avec le contenu suivant:\n\n\n\nScript √† cr√©er sous le nom install.sh\n\n\n\ninstall.sh\n\n#!/bin/bash\n# Install Python\napt-get -y update\napt-get install -y python3-pip python3-venv\n# Create empty virtual environment\npython3 -m venv titanic\nsource titanic/bin/activate\n# Install project dependencies\npip install -r requirements.txt\n\n\n\nChanger les permissions sur le script pour le rendre ex√©cutable\n\n\n\nterminal\n\n$ chmod +x install.sh\n\n\nEx√©cuter le script depuis la ligne de commande avec des droits de super-utilisateur (n√©cessaires pour installer des packages via apt)\n\n\n\nterminal\n\n$ sudo ./install.sh\n\n\nV√©rifier que le script main.py fonctionne correctement dans l‚Äôenvironnement virtuel cr√©√©\n\n\n\nterminal\n\n$ source titanic/bin/activate\n$ python3 main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli12a\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication 12b : cr√©er un fichier d‚Äôinstallation de A √† Z\n\n\n\n\nCr√©er un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le d√©p√¥t\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11b avec git checkout appli11b\nVia l‚Äôexplorateur de fichiers, cr√©er le fichier install.sh √† la racine du projet avec le contenu suivant:\n\n\n\nScript √† cr√©er sous le nom install.sh\n\n\n\ninstall.sh\n\napt-get -y update && apt-get -y install wget\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \\\n    rm -f Miniconda3-latest-Linux-x86_64.sh\nPATH=\"/miniconda/bin:${PATH}\"\n# Create environment\nconda create -n titanic pandas PyYAML scikit-learn -c conda-forge\nconda activate titanic\nPATH=\"/miniconda/envs/titanic/bin:${PATH}\"\npython main.py\n\n\n\nChanger les permissions sur le script pour le rendre ex√©cutable\n\n\n\nterminal\n\n$ chmod +x install.sh\n\n\nEx√©cuter le script depuis la ligne de commande avec des droits de super-utilisateur (n√©cessaires pour installer des packages via apt)\n\n\n\nterminal\n\n$ sudo ./install.sh\n\n\nV√©rifier que le script main.py fonctionne correctement dans l‚Äôenvironnement virtuel cr√©√©\n\n\n\nterminal\n\n$ conda activate titanic\n$ python3 main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli12b"
  },
  {
    "objectID": "chapters/application.html#docker",
    "href": "chapters/application.html#docker",
    "title": "Application",
    "section": "√âtape 3: conteneuriser l‚Äôapplication avec Docker",
    "text": "√âtape 3: conteneuriser l‚Äôapplication avec Docker\n\n\n\n\n\n\nNote\n\n\n\nCette application n√©cessite l‚Äôacc√®s √† une version interactive de Docker. Il n‚Äôy a pas beaucoup d‚Äôinstances en ligne disponibles.\nNous proposons deux solutions:\n\nInstaller Docker sur sa machine ;\nSe rendre sur l‚Äôenvironnement bac √† sable Play with Docker\n\nSinon, elle peut √™tre r√©alis√©e en essai-erreur par le biais des services d‚Äôint√©gration continue de Github  ou Gitlab . N√©anmoins, nous pr√©senterons l‚Äôutilisation de ces services plus tard, dans la prochaine partie.\n\n\nMaintenant qu‚Äôon sait que ce script pr√©paratoire fonctionne, on va le transformer en Dockerfile pour anticiper la mise en production. Comme la syntaxe Docker est l√©g√®rement diff√©rente de la syntaxe Linux classique (voir le chapitre portabilit√©), il va √™tre n√©cessaire de changer quelques instructions mais ceci sera tr√®s l√©ger.\nOn va tester le Dockerfile dans un environnement bac √† sable pour ensuite pouvoir plus facilement automatiser la construction de l‚Äôimage Docker.\n\n\n\n\n\n\nApplication 13: cr√©ation de l‚Äôimage Docker\n\n\n\nSe placer dans un environnement avec Docker, par exemple Play with Docker\n\nCr√©ation du Dockerfile\n\nDans le terminal Linux, cloner votre d√©p√¥t Github\nRepartir de la derni√®re version √† disposition. Par exemple, si vous avez privil√©gi√© l‚Äôenvironnement virtuel venv, ce sera:\n\n\n\nterminal\n\n$ git checkout appli12a\n\n\nCr√©er via la ligne de commande un fichier texte vierge nomm√© Dockerfile (la majuscule au d√©but du mot est importante)\n\n\n\nCommande pour cr√©er un Dockerfile vierge depuis la ligne de commande\n\n\n\nterminal\n\n$ touch Dockerfile\n\n\n\nOuvrir ce fichier via un √©diteur de texte et copier le contenu suivant dedans:\n\n\n\nPremier Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\nWORKDIR ${HOME}/titanic\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCMD [\"python3\", \"main.py\"]\n\n\n\n\nConstruire (build) l‚Äôimage\n\nUtiliser docker build pour cr√©er une image avec le tag my-python-app\n\n\n\nterminal\n\n$ docker build . -t my-python-app\n\n\nV√©rifier les images dont vous disposez. Vous devriez avoir un r√©sultat proche de celui-ci :\n\n\n\nterminal\n\n$ docker images\n\nREPOSITORY      TAG       IMAGE ID       CREATED              SIZE\nmy-python-app   latest    188957e16594   About a minute ago   879MB\n\n\nTester l‚Äôimage: d√©couverte du cache\nL‚Äô√©tape de build a fonctionn√©: une image a √©t√© construite.\nMais fait-elle effectivement ce que l‚Äôon attend d‚Äôelle ?\nPour le savoir, il faut passer √† l‚Äô√©tape suivante, l‚Äô√©tape de run.\n\n\nterminal\n\n$ docker run -it my-python-app\n\npython3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory\nLe message d‚Äôerreur est clair : Docker ne sait pas o√π trouver le fichier main.py. D‚Äôailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont n√©cessaires pour faire tourner le code, par exemple le dossier src.\n\nAvant l‚Äô√©tape CMD, copier les fichiers n√©cessaires sur l‚Äôimage afin que l‚Äôapplication dispose de tous les √©l√©ments n√©cessaires pour √™tre en mesure de fonctionner.\n\n\n\nNouveau Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\nWORKDIR ${HOME}/titanic\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY main.py .\nCOPY src ./src\nCMD [\"python3\", \"main.py\"]\n\n\n\nRefaire tourner l‚Äô√©tape de build\nRefaire tourner l‚Äô√©tape de run. A ce stade, la matrice de confusion doit fonctionner üéâ. Vous avez cr√©√© votre premi√®re application reproductible !\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIci, le cache permet d‚Äô√©conomiser beaucoup de temps. Par besoin de refaire tourner toutes les √©tapes, Docker agit de mani√®re intelligente en faisant tourner uniquement les √©tapes qui ont chang√©.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli13"
  },
  {
    "objectID": "chapters/application.html#√©tape-1-mise-en-place-de-tests-automatis√©s",
    "href": "chapters/application.html#√©tape-1-mise-en-place-de-tests-automatis√©s",
    "title": "Application",
    "section": "√âtape 1: mise en place de tests automatis√©s",
    "text": "√âtape 1: mise en place de tests automatis√©s\nAvant d‚Äôessayer de mettre en oeuvre la cr√©ation de notre image Docker de mani√®re automatis√©e, nous allons pr√©senter la logique de l‚Äôint√©gration continue en testant de mani√®re automatis√©e notre script main.py.\nPour cela, nous allons partir de la structure propos√©e dans l‚Äôaction officielle. La documentation associ√©e est ici. Des √©l√©ments succincts de pr√©sentation de la logique d√©clarative des actions Github sont disponibles dans le chapitre sur la mise en production. N√©anmoins, la meilleure √©cole pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et d‚Äôobserver les actions Github mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !\n\n\n\n\n\n\nApplication 14: premier script d‚Äôint√©gration continue\n\n\n\nA partir de l‚Äôexemple pr√©sent dans la documentation officielle de Github , on a d√©j√† une base de d√©part qui peut √™tre modifi√©e. Les questions suivantes permettront d‚Äôautomatiser les tests et le diagnostic qualit√© de notre code14\n\nCr√©er un fichier .github/workflows/test.yaml avec le contenu de l‚Äôexemple de la documentation\nAvec l‚Äôaide de la documentation, introduire une √©tape d‚Äôinstallation des d√©pendances. Utiliser le fichier requirements.txt pour installer les d√©pendances.\nUtiliser pylint pour v√©rifier la qualit√© du code. Ajouter l‚Äôargument --fail-under=6 pour renvoyer une erreur en cas de note trop basse15\nUtiliser une √©tape appelant notre application en ligne de commande (python main.py) pour tester que la matrice de confusion s‚Äôaffiche bien.\nAller voir votre test automatiser dans l‚Äôonglet Actions de votre d√©p√¥t sur Github\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli14\n\n\n\n\n\n\n\n\n\nMaintenant, nous pouvons observer que l‚Äôonglet Actions s‚Äôest enrichi. Chaque commit va entra√Æner une s√©ries d‚Äôactions automatis√©es.\nSi l‚Äôune des √©tapes √©choue, ou si la note de notre projet est mauvaise, nous aurons une croix rouge (et nous recevrons un mail). On pourra ainsi d√©tecter, en d√©veloppant son projet, les moments o√π on d√©grade la qualit√© du script afin de la r√©tablir imm√©diatemment."
  },
  {
    "objectID": "chapters/application.html#√©tape-2-automatisation-de-la-livraison-de-limage-docker",
    "href": "chapters/application.html#√©tape-2-automatisation-de-la-livraison-de-limage-docker",
    "title": "Application",
    "section": "√âtape 2: Automatisation de la livraison de l‚Äôimage Docker",
    "text": "√âtape 2: Automatisation de la livraison de l‚Äôimage Docker\nMaintenant, nous allons automatiser la mise √† disposition de notre image sur DockerHub (le lieu de partage des images Docker). Cela facilitera sa r√©utilisation mais aussi des valorisations ult√©rieures.\nL√† encore, nous allons utiliser une s√©rie d‚Äôactions pr√©-configur√©es.\nPour que Github puisse s‚Äôauthentifier aupr√®s de DockerHub, il va falloir d‚Äôabord interfacer les deux plateformes. Pour cela, nous allons utiliser un jeton (token) DockerHub que nous allons mettre dans un espace s√©curis√© associ√© √† votre d√©p√¥t Github.\n\n\n\n\n\n\nApplication 15a: configuration\n\n\n\n\nSe rendre sur https://hub.docker.com/ et cr√©er un compte. Il est recommand√© d‚Äôassocier ce compte √† votre compte Github.\nCr√©er un d√©p√¥t public application-correction\nAller dans les param√®tres de votre compte et cliquer, √† gauche, sur Security\nCr√©er un jeton personnel d‚Äôacc√®s, ne fermez pas l‚Äôonglet en question, vous ne pouvez voir sa valeur qu‚Äôune fois.\nDans le d√©p√¥t Github de votre projet, cliquer sur l‚Äôonglet Settings et cliquer, √† gauche, sur Secrets and variables puis dans le menu d√©roulant en dessous sur Actions. Sur la page qui s‚Äôaffiche, aller dans la section Repository secrets\nCr√©er un jeton DOCKERHUB_TOKEN √† partir du jeton que vous aviez cr√©√© sur Dockerhub. Valider\nCr√©er un deuxi√®me secret nomm√© DOCKERHUB_USERNAME ayant comme valeur le nom d‚Äôutilisateur que vous avez cr√©√© sur Dockerhub\n\n\n\nEtape optionnelle suppl√©mentaire si on met en production un site web\n\n\nDans le d√©p√¥t Github de votre projet, cliquer sur l‚Äôonglet Settings et cliquer, √† gauche, sur Actions. Donner les droits d‚Äô√©criture √† vos actions sur le d√©p√¥t du projet (ce sera n√©cessaire pour Github Pages)\n\n\n\n\n\nA ce stade, nous avons donn√© les moyens √† Github de s‚Äôauthentifier avec notre identit√© sur Dockerhub. Il nous reste √† mettre en oeuvre l‚Äôaction en s‚Äôinspirant de la documentation officielle. On ne va modifier que trois √©l√©ments dans ce fichier. Effectuer les actions suivantes:\n\n\n\n\n\n\nApplication 15b: automatisation de l‚Äôimage Docker\n\n\n\n\nEn s‚Äôinspirant de ce template, cr√©er le fichier .github/workflows/prod.yml qui va build et push l‚Äôimage sur le DockerHub. Il va √™tre n√©cessaire de changer l√©g√®rement ce mod√®le :\n\nRetirer la condition restrictive sur les commits pour lesquels sont lanc√©s cette automatisation. Pour cela, remplacer le contenu de on de sorte √† avoir on: [push]\nChanger le tag √† la fin pour mettre username/application-correction:latest o√π username est le nom d‚Äôutilisateur sur DockerHub;\nOptionnel: changer le nom de l‚Äôaction\n\nFaire un commit et un push de ces fichiers\n\nComme on est fier de notre travail, on va afficher √ßa avec un badge sur le README (partie optionnelle).\n\nSe rendre dans l‚Äôonglet Actions et cliquer sur une des actions list√©es.\nEn haut √† droite, cliquer sur ...\nS√©lectionner Create status badge\nR√©cup√©rer le code Markdown propos√©\nCopier dans votre README.md le code markdown propos√©\n\n\n\nCr√©er le badge\n\n\n\n\n\nMaintenant, il nous reste √† tester notre application dans l‚Äôespace bac √† sable ou en local, si Docker est install√©.\n\n\n\n\n\n\nApplication 15b (partie optionnelle): Tester l‚Äôapplication\n\n\n\n\nSe rendre sur l‚Äôenvironnement bac √† sable Play with Docker ou dans votre environnement Docker de pr√©dilection.\nR√©cup√©rer et lancer l‚Äôimage :\n\ndocker run -it username/application-correction:latest\nüéâ La matrice de confusion doit s‚Äôafficher ! Vous avez grandement facilit√© la r√©utilisation de votre image.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli15"
  },
  {
    "objectID": "chapters/application.html#√©tape-pr√©liminaire-cr√©ation-dun-pipeline-scikit",
    "href": "chapters/application.html#√©tape-pr√©liminaire-cr√©ation-dun-pipeline-scikit",
    "title": "Application",
    "section": "√âtape pr√©liminaire: cr√©ation d‚Äôun pipeline scikit",
    "text": "√âtape pr√©liminaire: cr√©ation d‚Äôun pipeline scikit\nLa mise en production n√©cessite d‚Äô√™tre exigeant sur la mise en oeuvre op√©rationnelle de notre pipeline. Nous avons n√©anmoins un pipeline un peu bancal car il requiert d‚Äô√™tre vigilant dans la mani√®re d‚Äôencha√Æner les √©tapes de preprocessing, d‚Äôentra√Ænement et d‚Äô√©valuation.\nQuand on utilise scikit, la bonne pratique est d‚Äôutiliser les pipelines qui s√©curisent les √©tapes de feature engineering n√©cessaires avant la mise en oeuvre d‚Äôun mod√®le, qu‚Äôil que ce soit pour l‚Äôentra√Ænement ou pour appliquer les m√™mes op√©rations avec les m√™mes param√®tres sur sur un nouveau jeu de donn√©es avant de faire un predict.\nOn va donc devoir refactoriser notre application pour utiliser un pipeline scikit. Les raisons sont expliqu√©es plus en d√©tail ici. Cela aura √©galement l‚Äôavantage de rendre les √©tapes de notre pipeline plus lisibles lorsqu‚Äôon passera √† l‚Äô√©tape d‚Äôindustrialisation avec MLFLow.\n\n\n\n\n\n\nApplication 16 (optionnelle): Un pipeline de machine learning\n\n\n\nCette application est optionnelle car elle rel√®ve plut√¥t d‚Äôun cours de machine learning que de cet enseignement. Les instructions sont donc minimales pour laisser de la marge de manoeuvre.\n\nSimplifier le code de split_train_test_titanic pour renvoyer deux DataFrames: train et test au lieu des 4 arrays Numpy comme jusqu‚Äô√† pr√©sent\nCr√©er une fonction build_pipeline dans src/models/train_evaluate.py qui :\n\nReprend les arguments de random_forest_titanic\nEffectue le preprocessing suivant pour les variables num√©riques (√† d√©finir): une imputation √† la valeur m√©diane, un MinMaxScaler ensuite\nEffectue le preprocessing suivant pour les variables cat√©gorielles (√† d√©finir): une imputation √† la valeur la plus fr√©quente, un one hot encoding ensuite\nD√©finit une random forest avec le nombre d‚Äôarbre donn√© en argument de la fonction\n\nModifier main.py pour que ce soit √† ce niveau qu‚Äôa lieu le d√©coupage en train/test, l‚Äôentrainement et l‚Äô√©valuation du mod√®le (qui est donc √† exfiltrer de src/models/train_evaluate.py). N‚Äôoubliez pas de retirer de ce script les √©tapes de preprocessing qui ne sont plus n√©cessaires.\nNettoyer le code de votre projet pour retirer les fonctions qui ne sont plus utilis√©es dans votre projet. Pour vous aider, vous pouvez utiliser vulture comme outil de diagnostic.\n\n\n\nterminal\n\n$ vulture main.py src/\n\nExemple de sortie\nmain.py:28: unused variable 'API_TOKEN' (60% confidence)\nsrc/features/build_features.py:39: unused function 'fill_na_titanic' (60% confidence)\nsrc/features/build_features.py:77: unused function 'label_encoder_titanic' (60% confidence)\nsrc/models/train_evaluate.py:3: unused import 'train_test_split' (90% confidence)\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli16"
  },
  {
    "objectID": "chapters/application.html#√©tape-1-d√©velopper-une-api-en-local",
    "href": "chapters/application.html#√©tape-1-d√©velopper-une-api-en-local",
    "title": "Application",
    "section": "√âtape 1: d√©velopper une API en local",
    "text": "√âtape 1: d√©velopper une API en local\nLe premier livrable devenu classique dans un projet impliquant du machine learning est la mise √† disposition d‚Äôun mod√®le par le biais d‚Äôune API (voir chapitre sur la mise en production). Le framework FastAPI va permettre de rapidement transformer notre application Python en une API fonctionnelle.\n\n\n\n\n\n\nSi vous prenez ce projet fil rouge en cours de route\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication 17: Mise √† disposition sous forme d‚ÄôAPI locale\n\n\n\n\nInstaller fastAPI et uvicorn puis les ajouter au requirements.txt\nRenommer le fichier main.py en train.py. Dans ce script, ajouter une sauvegarde du mod√®le apr√®s l‚Äôavoir entra√Æn√©, sous le format joblib.\nFaire tourner\n\n\n\nterminal\n\n$ python train.py\n\npour enregistrer en local votre mod√®le de production.\n\nModifier les appels √† main.py dans votre Dockerfile et vos actions Github sous peine d‚Äôessuyer des √©checs lors de vos actions Github apr√®s le prochain push.\nAjouter model.joblib au .gitignore car Git n‚Äôest pas fait pour ce type de fichiers.\n\nNous allons maintenant passer au d√©veloppement de l‚ÄôAPI. Comme d√©couvrir FastAPI n‚Äôest pas l‚Äôobjet de cet enseignement, nous donnons directement le mod√®le pour cr√©er l‚ÄôAPI. Si vous d√©sirez tester de vous-m√™mes, vous pouvez cr√©er votre fichier sans vous r√©f√©rer √† l‚Äôexemple\n\nCr√©er le fichier api.py permettant d‚Äôinitialiser l‚ÄôAPI:\n\n\n\nFichier api.py\n\nR√©cup√©rer le contenu sur cette page\n\n\nD√©ployer en local l‚ÄôAPI avec la commande\n\n\n\nterminal\n\n$ uvicorn api:app --reload --host \"0.0.0.0\" --port 5000\n\n\nA partir du README du service, se rendre sur l‚ÄôURL de d√©ploiement, ajouter /docs/ √† celui-ci et observer la documentation de l‚ÄôAPI\nSe servir de la documentation pour tester les requ√™tes /predict\nR√©cup√©rer l‚ÄôURL d‚Äôune des requ√™tes propos√©es. La tester dans le navigateur et depuis Python avec requests :\n\nimport request\nrequests.get(url).json()\n\nUne fois que vous avez test√©, vous pouvez tuer l‚Äôapplication en faisant CTRL+C. Retester votre bout de code Python et comprendre l‚Äôorigine du probl√®me.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli17"
  },
  {
    "objectID": "chapters/application.html#√©tape-2-d√©ployer-lapi-de-mani√®re-manuelle",
    "href": "chapters/application.html#√©tape-2-d√©ployer-lapi-de-mani√®re-manuelle",
    "title": "Application",
    "section": "√âtape 2: d√©ployer l‚ÄôAPI de mani√®re manuelle",
    "text": "√âtape 2: d√©ployer l‚ÄôAPI de mani√®re manuelle\nA ce stade, nous avons d√©ploy√© l‚ÄôAPI seulement localement, dans le cadre d‚Äôun terminal qui tourne en arri√®re-plan. C‚Äôest une mise en production manuelle, pas franchement p√©renne. Ce mode de d√©ploiement est tr√®s pratique pour la phase de d√©veloppement, afin de s‚Äôassurer que l‚ÄôAPI fonctionne comme attendu. Pour p√©renniser la mise en production, on va √©liminer l‚Äôaspect artisanal de celle-ci.\nIl est temps de passer √† l‚Äô√©tape de d√©ploiement, qui permettra √† notre API d‚Äô√™tre accessible via une URL sur le web et d‚Äôavoir un serveur, en arri√®re plan, qui effectuera les op√©rations pour r√©pondre √† une requ√™te. Pour se faire, on va utiliser les possibilit√©s offertes par Kubernetes, sur lequel est bas√© le SSP Cloud.\n\n\n\n\n\n\nApplication 18a: Dockeriser l‚ÄôAPI (int√©gration continue)\n\n\n\n\nPour rendre la structure du projet plus lisible, d√©placer api.py -&gt; api/main.py\nCr√©er un script api/run.sh √† la racine du projet qui lance le script train.py puis d√©ploie localement l‚ÄôAPI\n\n\n\nFichier run.sh\n\n\n\napi/run.sh\n\n#/bin/bash\npython3 train.py\nuvicorn api.main:app --reload --host \"0.0.0.0\" --port 5000\n\n\n\nDonner au script api/run.sh des permissions d‚Äôex√©cution : chmod +x api/run.sh\nChanger l‚Äôinstruction CMD du Dockerfile pour ex√©cuter le script api/run.sh au lancement du conteneur (CMD [\"bash\", \"-c\", \"./api/run.sh\"])\nCommit et push les changements\nUne fois le CI termin√©, r√©cup√©rer la nouvelle image dans votre environnement de test de Docker et v√©rifier que l‚ÄôAPI se d√©ploie correctement\n\n\n\nNous avons pr√©par√© la mise √† disposition de notre API mais √† l‚Äôheure actuelle elle n‚Äôest pas disponible de mani√®re ais√©e car il est n√©cessaire de lancer manuellement une image Docker pour pouvoir y acc√©der. Ce type de travail est la sp√©cialit√© de Kubernetes que nous allons utiliser pour g√©rer la mise √† disposition de notre API.\n\n\n\n\n\n\nApplication 18b: Mettre √† disposition l‚ÄôAPI (d√©ploiement manuel)\n\n\n\nCette partie n√©cessite d‚Äôavoir √† disposition une infrastructure cloud.\n\nCr√©er un dossier deployment √† la racine du projet qui va contenir les fichiers de configuration n√©cessaires pour d√©ployer sur un cluster Kubernetes\nEn vous inspirant de la documentation, y ajouter un premier fichier deployment.yaml qui va sp√©cifier la configuration du Pod √† lancer sur le cluster\n\n\n\nFichier deployment/deployment.yaml\n\n\n\ndeployment/deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n        image: linogaliana/application-correction:latest\n        ports:\n        - containerPort: 5000\n\n\n\nEn vous inspirant de la documentation, y ajouter un second fichier service.yaml qui va cr√©er une ressource Service permettant de donner une identit√© fixe au Pod pr√©c√©demment cr√©√© au sein du cluster\n\n\n\nFichier deployment/service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: titanic-service\nspec:\n  selector:\n    app: titanic\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 5000\n\n\nEn vous inspirant de la documentation, y ajouter un troisi√®me fichier ingress.yaml qui va cr√©er une ressource Ingress permettant d‚Äôexposer le service via une URL en dehors du cluster\n\n\n\nFichier deployment/ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: titanic-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n1    - # METTRE URL ICI\n  rules:\n  - host: # METTRE URL ICI &lt;1&gt;\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: titanic-service\n            port:\n              number: 80\n\n1\n\nMettez l‚ÄôURL auquel vous voulez exposer votre service. Sur le mod√®le de titanic.kub.sspcloud.fr (mais ne tentez pas celui-l√†, il est d√©j√† pris üòÉ)\n\n\n\n\nAppliquer ces fichiers de configuration sur le cluster : kubectl apply -f deployment/\nSi tout a correctement fonctionn√©, vous devriez pouvoir acc√©der depuis votre navigateur √† l‚ÄôAPI √† l‚ÄôURL sp√©cifi√©e dans le fichier deployment/ingress.yaml. Par exemple https://toto.kub.sspcloud.fr/ si vous avez mis celui-ci plus t√¥t (et https://toto.kub.sspcloud.fr/docs pour la documentation).\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli18\n\n\n\n\nOn peut remarquer quelques voies d‚Äôam√©lioration de notre approche qui seront ult√©rieurement trait√©es:\n\nl‚Äôentra√Ænement du mod√®le est effectu√© √† chaque lancement d‚Äôun nouveau conteneur. On relance donc autant de fois un entra√Ænement qu‚Äôon d√©ploie de conteneur pour r√©pondre √† nos utilisateurs. Ce sera l‚Äôobjet de la partie MLOps de fiabiliser et optimiser cette partie du pipeline.\nil est n√©cessaire de (re)lancer manuellement kubectl apply -f deployment/ √† chaque changement de notre code. Autrement dit, on am√©liore la fiabilit√© du lancement de notre API mais un lancement manuel est encore indispensable. Comme dans le reste de ce cours, on va essayer d‚Äô√©viter un geste manuel pouvant √™tre source d‚Äôerreur."
  },
  {
    "objectID": "chapters/application.html#etape-3-automatiser-le-d√©ploiement-d√©ploiement-en-continu",
    "href": "chapters/application.html#etape-3-automatiser-le-d√©ploiement-d√©ploiement-en-continu",
    "title": "Application",
    "section": "Etape 3: automatiser le d√©ploiement (d√©ploiement en continu)",
    "text": "Etape 3: automatiser le d√©ploiement (d√©ploiement en continu)\nQu‚Äôest-ce qui peut d√©clencher une √©volution n√©cessitant de mettre √† jour l‚Äôensemble de notre processus de production ?\nRegardons √† nouveau notre pipeline:\n\nLes inputs de notre pipeline sont donc:\n\nLa configuration. ;\nLes donn√©es. Nos donn√©es sont statiques et n‚Äôont pas vocation √† √©voluer. Si c‚Äô√©tait le cas, il faudrait en tenir compte dans notre automatisation. ;\nLe code. C‚Äôest l‚Äô√©l√©ment qui √©volue chez nous. On va donc faire en sorte qu‚Äô√† chaque mise √† jour de notre code (un push sur Github), les √©tapes ult√©rieures (production de l‚Äôimage Docker, etc.) vont se mettre √† compiler.\n\n\nGr√¢ce √† ArgoCD il est possible de d√©ployer un mod√®le de mani√®re continu, ainsi chaque modification d‚Äôun fichier pr√©sent dans le dossier kubernetes/. Cela va entrainer le red√©ploiement automatique en synchronisant avec notre d√©p√¥t Github.\nProjet exo appli19:\n\nLancer un service argoCD"
  },
  {
    "objectID": "chapters/application.html#etape-4-construire-un-site-web",
    "href": "chapters/application.html#etape-4-construire-un-site-web",
    "title": "Application",
    "section": "Etape 4: construire un site web",
    "text": "Etape 4: construire un site web"
  },
  {
    "objectID": "chapters/application.html#footnotes",
    "href": "chapters/application.html#footnotes",
    "title": "Application",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nL‚Äôexport dans un script .py a √©t√© fait directement depuis VSCode. Comme cela n‚Äôest pas vraiment l‚Äôobjet du cours, nous passons cette √©tape et fournissons directement le script expurg√© du texte interm√©diaire. Mais n‚Äôoubliez pas que cette d√©marche, fr√©quente quand on a d√©marr√© sur un notebook et qu‚Äôon d√©sire consolider en faisant la transition vers des scripts, n√©cessite d‚Äô√™tre attentif pour ne pas risquer de faire une erreur.‚Ü©Ô∏é\nIl est √©galement possible avec VSCode d‚Äôex√©cuter le script ligne √† ligne de mani√®re interactive ligne √† ligne (MAJ+ENTER). N√©anmoins, cela n√©cessite de s‚Äôassurer que le working directory de votre console interactive est le bon. Celle-ci se lance selon les param√®tres pr√©configur√©s de VSCode et les votres ne sont peut-√™tre pas les m√™mes que les notres. Vous pouvez changer le working directory dans le script en utilisant le package os mais peut-√™tre allez vous d√©couvrir ult√©rieurement qu‚Äôil y a de meilleures pratiques‚Ä¶‚Ü©Ô∏é\nEssayez de commit vos changements √† chaque √©tape de l‚Äôexercice, c‚Äôest une bonne habitude √† prendre.‚Ü©Ô∏é\nIci, le jeton d‚ÄôAPI n‚Äôest pas indispensable pour que le code fonctionne. Afin d‚Äô√©viter une erreur non n√©cessaire lorsqu‚Äôon automatisera le processus, on peut cr√©er une condition qui v√©rifie la pr√©sence ou non de ce fichier. Le script reste donc reproductible m√™me pour un utilisateur n‚Äôayant pas le fichier secrets.yaml.‚Ü©Ô∏é\nIl est normal d‚Äôavoir des dossiers __pycache__ qui tra√Ænent en local : ils se cr√©ent automatiquement √† l‚Äôex√©cution d‚Äôun script en Python. N√©anmoins, il ne faut pas associer ces fichiers √† Git, voil√† pourquoi on les ajoute au .gitignore.‚Ü©Ô∏é\nNous proposons ici d‚Äôadopter le principe de la programmation fonctionnelle. Pour encore fiabiliser un processus, il serait possible d‚Äôadopter le paradigme de la programmation orient√©e objet (POO). Celle-ci est plus rebutante et demande plus de temps au d√©veloppeur. L‚Äôarbitrage co√ªt-avantage est n√©gatif pour notre exemple, nous proposons donc de nous en passer. N√©anmoins, pour une mise en production r√©elle d‚Äôun mod√®le, il est recommand√© de l‚Äôadopter. C‚Äôest d‚Äôailleurs obligatoire avec des pipelines scikit.‚Ü©Ô∏é\nAu passage vous pouvez noter que mauvaises pratiques discutables, peuvent √™tre corrig√©es, notamment l‚Äôutilisation excessive de apply l√† o√π il serait possible d‚Äôutiliser des m√©thodes embarqu√©es par Pandas. Cela est plut√¥t de l‚Äôordre du bon style de programmation que de la qualit√© formelle du script. Ce n‚Äôest donc pas obligatoire mais c‚Äôest mieux.‚Ü©Ô∏é\nAttention, les donn√©es ont √©t√© committ√©es au moins une fois. Les supprimer du d√©p√¥t ne les efface pas de l‚Äôhistorique. Si cette erreur arrive, le mieux est de supprimer le d√©p√¥t en ligne, cr√©er un nouvel historique Git et partir de celui-ci pour des publications ult√©rieures sur Github. N√©anmoins l‚Äôid√©al serait de ne pas s‚Äôexposer √† cela. C‚Äôest justement l‚Äôobjet des bonnes pratiques de ce cours: un .gitignore bien construit et une s√©paration des environnements de stockage du code et des donn√©es seront bien plus efficaces pour vous √©viter ces probl√®mes que tout les conseils de vigilance que vous pourrez trouver ailleurs.‚Ü©Ô∏é\nLe fichier __init__.py indique √† Python que le dossier est un package. Il permet de proposer certaines configurations lors de l‚Äôimport du package. Il permet √©galement de contr√¥ler les objets export√©s (c‚Äôest-√†-dire mis √† disposition de l‚Äôutilisateur) par le package par rapport aux objets internes au package. En le laissant vide, nous allons utiliser ce fichier pour importer l‚Äôensemble des fonctions de nos sous-modules. Ce n‚Äôest pas la meilleure pratique mais un contr√¥le plus fin des objets export√©s demanderait un investissement qui ne vaut, ici, pas le co√ªt.‚Ü©Ô∏é\nSi vous d√©sirez aussi contr√¥ler la version de Python, ce qui peut √™tre important dans une perspective de portabilit√©, vous pouvez ajouter une option, par exemple -p python3.10. N√©anmoins nous n‚Äôallons pas nous embarasser de cette nuance pour la suite car nous pourrons contr√¥ler la version de Python plus finement par le biais de Docker.‚Ü©Ô∏é\nL‚Äôoption -c pass√©e apr√®s la commande python permet d‚Äôindiquer √† Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu‚Äôon va directement lui fournir.‚Ü©Ô∏é\nL‚Äôoption -c pass√©e apr√®s la commande python permet d‚Äôindiquer √† Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu‚Äôon va directement lui fournir.‚Ü©Ô∏é\nPour comparer les deux listes, vous pouvez utiliser la fonctionnalit√© de split du terminal sur VSCode pour comparer les outputs de conda env export en les mettant en face √† face.‚Ü©Ô∏é\nIl est tout √† fait normal de ne pas parvenir √† cr√©er une action fonctionnelle du premier coup. N‚Äôh√©sitez pas √† pusher votre code apr√®s chaque question pour v√©rifier que vous parvenez bien √† r√©aliser chaque √©tape. Sinon vous risquez de devoir corriger bout par bout un fichier plus cons√©quent.‚Ü©Ô∏é\nIl existe une approche alternative pour faire des tests r√©guliers: les hooks Git. Il s‚Äôagit de r√®gles qui doivent √™tre satisfaites pour que le fichier puisse √™tre committ√©. Cela assure que chaque commit remplisse des crit√®res de qualit√© afin d‚Äô√©viter le probl√®me de la procrastination.\nLa documentation de pylint offre des explications suppl√©mentaires. Ici, nous allons adopter une approche moins ambitieuse en demandant √† notre action de faire ce travail d‚Äô√©valuation de la qualit√© de notre code‚Ü©Ô∏é\nVous n‚Äô√™tes pas oblig√©s pour l‚Äô√©valuation de mettre en oeuvre les jalons de plusieurs parcours. N√©anmoins, vous d√©couvrirez que chaque nouveau pas en avant est moins co√ªteux que le pr√©c√©dent si vous avez mis en oeuvre les r√©flexes des bonnes pratiques.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/code-quality.html",
    "href": "chapters/code-quality.html",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran.\nCe chapitre constitue une introduction √† la question de la qualit√© du code, premier niveau dans l‚Äô√©chelle des bonnes pratiques. Celui-ci pr√©sente les enjeux de la qualit√© du code, les principes g√©n√©raux pour am√©liorer celui-ci et quelques outils ou gestes faciles √† mettre en ≈ìuvre pour am√©liorer la qualit√© du code. Ceux-ci sont approfondis dans l‚Äôapplication fil rouge."
  },
  {
    "objectID": "chapters/code-quality.html#lenjeu-dun-code-lisible-et-maintenable",
    "href": "chapters/code-quality.html#lenjeu-dun-code-lisible-et-maintenable",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "L‚Äôenjeu d‚Äôun code lisible et maintenable",
    "text": "L‚Äôenjeu d‚Äôun code lisible et maintenable\n\n‚ÄúThe code is read much more often than it is written.‚Äù\nGuido Van Rossum1\n\nLorsqu‚Äôon s‚Äôinitie √† la pratique de la data science, il est assez naturel de voir le code d‚Äôune mani√®re tr√®s fonctionnelle : je veux r√©aliser une t√¢che donn√©e ‚Äî par exemple un algorithme de classification ‚Äî et je vais donc assembler dans un notebook des bouts de code, souvent trouv√©s sur internet, jusqu‚Äô√† obtenir un projet qui r√©alise la t√¢che voulue. La structure du projet importe assez peu, tant qu‚Äôelle permet d‚Äôimporter correctement les donn√©es n√©cessaires √† la t√¢che en question.\nSi cette approche flexible et minimaliste fonctionne tr√®s bien lors de la phase d‚Äôapprentissage, il est malgr√© tout indispensable de s‚Äôen d√©tacher progressivement √† mesure que l‚Äôon progresse et que l‚Äôon est amen√© √† r√©aliser des projets plus professionnels ou bien √† int√©grer des projets collaboratifs. Autrement, on risque de produire un code complexe √† reprendre et √† faire √©voluer, ce qui pourrait conduire in√©vitablement √† son abandon.\nEn particulier, il est important de proposer, parmi les multiples mani√®res de r√©soudre un probl√®me informatique, une solution qui soit intelligible par d‚Äôautres personnes parlant le m√™me langage. Le code est en effet lu bien plus souvent qu‚Äôil n‚Äôest √©crit, c‚Äôest donc avant tout un outil de communication. De m√™me, la maintenance d‚Äôun code demande g√©n√©ralement beaucoup plus de moyens que sa phase de d√©veloppement initial. Il est donc important de penser en amont la qualit√© de son code et la structure de son projet de sorte √† le rendre maintenable dans le temps.\nAfin de faciliter la communication et r√©duire la douleur d‚Äôavoir √† faire √©voluer un code obscur, des tentatives plus ou moins institutionnalis√©es de d√©finir des conventions ont √©merg√©. Ces conventions d√©pendent naturellement du langage utilis√©, mais les principes sous-jacents s‚Äôappliquent de mani√®re universelle √† tout projet bas√© sur du code."
  },
  {
    "objectID": "chapters/code-quality.html#de-limportance-de-suivre-les-conventions",
    "href": "chapters/code-quality.html#de-limportance-de-suivre-les-conventions",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "De l‚Äôimportance de suivre les conventions",
    "text": "De l‚Äôimportance de suivre les conventions\nPython est un langage tr√®s lisible. Avec un peu d‚Äôeffort sur le nom des objets, sur la gestion des d√©pendances et sur la structure du programme, on peut tr√®s bien comprendre un script sans avoir besoin de l‚Äôex√©cuter. C‚Äôest l‚Äôune des principales forces du langage Python qui permet ainsi une acquisition rapide des bases et facilite l‚Äôappropriation d‚Äôun script.\nLa communaut√© Python a abouti √† un certain nombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard dans l‚Äô√©cosyst√®me Python. Les deux normes les plus connues sont :\n\nla norme PEP8 qui d√©finit un certain nombre de conventions relatives au code ;\nla norme PEP257 consacr√©e √† la documentation (docstrings).\n\nCes conventions vont au-del√† de la syntaxe. Un certain nombre de standards d‚Äôorganisation d‚Äôun projet ont √©merg√©, qui seront abord√©es dans le prochain chapitre.\n\n\n\n\n\n\nComparaison avec \n\n\n\n\n\nDans l‚Äôunivers , la formalisation a √©t√© moins organis√©e. Ce langage est plus permissif que Python sur certains aspects2. N√©anmoins, des standards ont √©merg√© r√©cemment, √† travers un certain nombre de style guides dont les plus connus sont le tidyverse style guide et le google style guide, MLR style guide‚Ä¶\nPour aller plus loin sur :\n\nLa formation Insee aux bonnes pratiques avec Git et  dont le parcours est tr√®s proche de celui de ce cours ;\nDes √©l√©ments compl√©mentaires dans la documentation collaborative utilitR ;\nCe post qui pointe vers un certain nombre de ressources sur le sujet.\n\n\n\n\nCes conventions sont arbitraires, dans une certaine mesure. Il est tout √† fait possible de trouver certaines conventions moins esth√©tiques que d‚Äôautres.\nCes conventions ne sont pas non plus immuables : les langages et leurs usages √©voluent, ce qui n√©cessite de mettre √† jour les conventions. Cependant, adopter dans la mesure du possible certains des r√©flexes pr√©conis√©s par ces conventions devrait am√©liorer la capacit√© √† √™tre compris par la communaut√©, augmenter les chances de b√©n√©ficier d‚Äôapport de celle-ci pour adapter le code, mais aussi r√©duire la difficult√© √† faire √©voluer un code.\nIl existe beaucoup de philosophies diff√©rentes sur le style de codage et, en fait, le plus important est la coh√©rence : si on choisit une convention, par exemple snake case (toto_a_la_plage) plut√¥t que camel case (totoALaPlage), le mieux est de s‚Äôy tenir."
  },
  {
    "objectID": "chapters/code-quality.html#un-bon-ide-un-premier-pas-vers-la-qualit√©",
    "href": "chapters/code-quality.html#un-bon-ide-un-premier-pas-vers-la-qualit√©",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Un bon IDE, un premier pas vers la qualit√©",
    "text": "Un bon IDE, un premier pas vers la qualit√©\nSans les outils automatis√©s de mise en forme du code, l‚Äôadoption des bonnes pratiques serait co√ªteuse en temps et donc difficile √† mettre en ≈ìuvre au quotidien. Ces outils, que ce soit par le biais de diagnostics ou de mise aux normes automatis√©e du code rendent de pr√©cieux services. Adopter les standards minimaux de qualit√© est plus ou moins instantan√© et √©conomise un temps pr√©cieux dans la vie d‚Äôun projet de data science. C‚Äôest un pr√©alable indispensable √† la mise en production, sur laquelle nous reviendrons ult√©rieurement.\nLe premier pas vers les bonnes pratiques est d‚Äôadopter un environnement de d√©veloppement adapt√©. VSCode est un tr√®s bon environnement comme nous le d√©couvrirons dans la partie pratique. Il propose tous les outils d‚Äôautocompl√©tion et de diagnostics usuels (contrairement √† Jupyter) et propose une grande gamme d‚Äôextensions pour enrichir les fonctionnalit√©s de l‚ÄôIDE de mani√®re contributive :\n\n\n\nExemple de diagnostics et d‚Äôactions propos√©s par VSCode\n\n\nN√©anmoins, les outils de d√©tection de code au niveau des IDE ne suffisent pas. En effet, ils n√©cessitent une composante manuelle qui peut √™tre chronophage et ainsi p√©nible √† appliquer r√©guli√®rement. Heureusement, il existe des outils automatis√©s de diagnostics et de mise en forme.\n\nLes outils automatis√©s pour le diagnostic et la mise en forme du code\nPython √©tant l‚Äôoutil de travail principal de milliers de data-scientists, un certain nombre d‚Äôoutils ont vu le jour pour r√©duire le temps n√©cessaire pour cr√©er un projet ou disposer d‚Äôun code fonctionnel. Ces outils permettent un gros gain de productivit√©, r√©duisent le temps pass√© √† effectuer des t√¢ches r√©barbatives et am√©liorent la qualit√© d‚Äôun projet en offrant des diagnostics, voire des correctifs √† des codes perfectibles.\nLes deux principaux types d‚Äôoutils sont les suivants :\n\nLinter : programme qui v√©rifie que le code est formellement conforme √† un certain guidestyle\n\nsignale des probl√®mes formels, sans corriger\n\nFormatter : programme qui reformate un code pour le rendre conforme √† un certain guidestyle\n\nmodifie directement le code\n\n\n\n\n\n\n\n\nExemples\n\n\n\n\n\n\nExemples d‚Äôerreurs rep√©r√©es par un linter :\n\nlignes de code trop longues ou mal indent√©es, parenth√®ses non √©quilibr√©es, noms de fonctions mal construits‚Ä¶\n\nExemples d‚Äôerreurs non rep√©r√©es par un linter :\n\nfonctions mal utilis√©es, arguments mal sp√©cifi√©s, structure du code incoh√©rente, code insuffisamment document√©‚Ä¶\n\n\n\n\n\n\n\nLes linters pour comprendre les mauvaises pratiques appliqu√©es\nLes linters sont des outils qui permettent d‚Äô√©valuer la qualit√© du code et son risque de provoquer une erreur (explicite ou silencieuse).\nVoici quelques exemples de probl√®mes que peuvent rencontrer les linters:\n\nles variables sont utilis√©es mais n‚Äôexistent pas (erreur)\nles variables inutilis√©es (inutiles)\nla mauvaise organisation du code (risque d‚Äôerreur)\nle non-respect des bonnes pratiques d‚Äô√©criture de code\nles erreurs de syntaxe (par exemple les coquilles)\n\nLa plupart des logiciels de d√©veloppement embarquent des fonctionnalit√©s de diagnostic (voire de suggestion de correctif). Il faut parfois les param√©trer dans les options (ils sont d√©sactiv√©s pour ne pas effrayer l‚Äôutilisateur avec des croix rouges partout). N√©anmoins, si on n‚Äôa pas appliqu√© les correctifs au fil de l‚Äôeau la masse des modifications √† mettre en ≈ìuvre peut √™tre effrayante.\nEn Python, les deux principaux linters sont PyLint et Flake8. Dans les exercices, nous proposons d‚Äôutiliser PyLint qui est pratique et p√©dagogique. Celui-ci s‚Äôutilise en ligne de commande, de la mani√®re suivante :\n$ pip install pylint\n$ pylint monscript.py #pour un fichier\n$ pylint src #pour tous les fichiers du dossier src\n\n\n\n\n\n\nTip\n\n\n\n\n\nL‚Äôun des int√©r√™ts d‚Äôutiliser PyLint est qu‚Äôon obtient une note, ce qui est assez instructif. Nous l‚Äôutiliserons dans l‚Äôapplication fil rouge pour comprendre la mani√®re dont chaque √©tape am√©liore la qualit√© du code.\nIl est possible de mettre en ≈ìuvre des pre commit hooks qui emp√™chent un commit n‚Äôayant pas une note minimale.\n\n\n\n\n\nLes formatters pour nettoyer en masse ses scripts\nLe formatter modifie directement le code. On peut faire un parall√®le avec le correcteur orthographique. Cet outil peut donc induire un changement substantiel du script afin de le rendre plus lisible.\nLe formater le plus utilis√©\nest Black. R√©cemment, Ruff, qui est √† la fois un linter et un formatter a √©merg√© pour int√©grer √† Black des diagnostics suppl√©mentaires, issus d‚Äôautres packages.\n\n\n\n\n\n\nNote\n\n\n\n\n\nPour signaler sur Github la qualit√© d‚Äôun projet utilisant Black, il est possible d‚Äôajouter un badge dans le README:\n\n\n\n\nIl est assez instructif de regarder le code modifi√© par les outils pour comprendre et corriger certains probl√®mes dans sa mani√®re de d√©velopper. Par exemple, √† la lecture de ce chapitre, vous allez certainement retenir en particulier certaines r√®gles qui tranchent avec vos pratiques actuelles. Vous pouvez alors essayer d‚Äôappliquer ces nouvelles r√®gles pendant un certain temps puis, lorsque celles-ci seront devenues naturelles, revenir √† ce guide et appliquer le processus √† nouveau. En proc√©dant ainsi de mani√®re incr√©mentale, vous am√©liorerez progressivement la qualit√© de vos projets sans avoir l‚Äôimpression de passer trop de temps sur des micro-d√©tails, au d√©triment des objectifs globaux du projet."
  },
  {
    "objectID": "chapters/code-quality.html#le-partage-une-d√©marche-favorable-√†-la-qualit√©-du-code",
    "href": "chapters/code-quality.html#le-partage-une-d√©marche-favorable-√†-la-qualit√©-du-code",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Le partage, une d√©marche favorable √† la qualit√© du code",
    "text": "Le partage, une d√©marche favorable √† la qualit√© du code\n\nL‚Äôopensource comme moyen pour am√©liorer la qualit√©\nEn ouvrant son code sur des forges opensource (cf.¬†chapitre Git), il est possible de recevoir des suggestions voire, des contributions de r√©-utilisateurs du code. Cependant, les vertus de l‚Äôouverture vont au-del√†. En effet, l‚Äôouverture se traduit g√©n√©ralement par des codes de meilleure qualit√©, mieux document√©s pour pouvoir √™tre r√©utilis√©s ou ayant simplement b√©n√©fici√© d‚Äôune attention accrue sur la qualit√© pour ne pas para√Ætre ridicule. M√™me en l‚Äôabsence de retour de (r√©)utilisateurs du code, le partage de code am√©liore la qualit√© des projets.\n\n\nLa revue de code\nLa revue de code s‚Äôinspire de la m√©thode du peer reviewing du monde acad√©mique pour am√©liorer la qualit√© du code Python. Dans une revue de code, le code √©crit par une personne est relu et √©valu√© par un ou plusieurs autres d√©veloppeurs afin d‚Äôidentifier les erreurs et les am√©liorations possibles. Cette pratique permet de d√©tecter les erreurs avant qu‚Äôelles ne deviennent des probl√®mes majeurs, d‚Äôassurer une coh√©rence dans le code, de garantir le respect des bonnes pratiques mais aussi d‚Äôam√©liorer la qualit√© du code en identifiant les parties du code qui peuvent √™tre simplifi√©es, optimis√©es ou refactoris√©es pour en am√©liorer la lisibilit√© et la maintenabilit√©.\nUn autre avantage de cette approche est qu‚Äôelle permet le partage de connaissances entre des personnes exp√©riment√©es et des personnes plus d√©butantes ce qui permet √† ces derni√®res de monter en comp√©tence. Github  et Gitlab  proposent des fonctionnalit√©s tr√®s pratiques pour la revue de code : discussions, suggestions de modifications‚Ä¶"
  },
  {
    "objectID": "chapters/code-quality.html#objectifs",
    "href": "chapters/code-quality.html#objectifs",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Objectifs",
    "text": "Objectifs\n\nFavoriser la concision pour r√©duire le risque d‚Äôerreur et rendre la d√©marche plus claire ;\nAm√©liorer la lisibilit√© ce qui est indispensable pour rendre la d√©marche intelligible par d‚Äôautres mais aussi pour soi, lorsqu‚Äôon reprend un code √©crit il y a quelques temps ;\nLimiter la redondance ce qui permet de simplifier un code (paradigme du don‚Äôt repeat yourself) ;\nLimite les risques d‚Äôerreurs li√©es aux copier/coller"
  },
  {
    "objectID": "chapters/code-quality.html#avantages-des-fonctions",
    "href": "chapters/code-quality.html#avantages-des-fonctions",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Avantages des fonctions",
    "text": "Avantages des fonctions\nLes fonctions ont de nombreux avantages par rapport √† de longs scripts :\n\nLimite les risques d‚Äôerreurs li√©s aux copier/coller\nRend le code plus lisible et plus compact\nUn seul endroit du code √† modifier lorsqu‚Äôon souhaite modifier le traitement\nFacilite la r√©utilisation et la documentation du code !\n\n\n\n\n\n\n\nR√®gle d‚Äôor\n\n\n\nIl faut utiliser une fonction d√®s qu‚Äôon utilise une m√™me portion de code plus de deux fois (don‚Äôt repeat yourself (DRY))\n\n\n\n\n\n\n\n\nR√®gles pour √©crire des fonctions pertinentes\n\n\n\n\nUne t√¢che = une fonction\nUne t√¢che complexe = un encha√Ænement de fonctions r√©alisant chacune une t√¢che simple\nLimiter l‚Äôutilisation de variables globales\n\n\n\nEn ce qui concerne l‚Äôinstallation des packages, nous allons voir dans les parties Structure de code et Portabilit√© qu‚Äôil ne faut pas g√©rer ceci dans le script mais dans un √©l√©ment √† part, relatif √† l‚Äôenvironnement d‚Äôex√©cution du projet3. De m√™me, ces parties pr√©senteront des conseils pratiques sur la gestion des jetons d‚Äôacc√®s √† des API ou bases de donn√©es qui ne doivent jamais √™tre inscrites dans un code.\nLes scripts trop longs ne sont pas une bonne pratique. Il est pr√©f√©rable de diviser l‚Äôensemble des scripts ex√©cutant une cha√Æne de production en ‚Äúmonades‚Äù, c‚Äôest-√†-dire en petites unit√©s coh√©rentes. Les fonctions sont un outil privil√©gi√© pour cela (en plus de limiter la redondance, et d‚Äô√™tre un outil privil√©gi√© pour documenter un code).\n\n\n\n\n\n\nExemple: privil√©gier les list comprehensions\n\n\n\n\n\nEn Python, il est recommand√© de privil√©gier les list comprehensions √† l‚Äôutilisation de boucles for indent√©es. Ces derni√®res sont en g√©n√©ral moins efficaces et surtout impliquent un nombre important de ligne de codes l√† o√π les compr√©hensions de listes sont beaucoup plus concises\nliste_nombres = range(10)\n\n# tr√®s mauvais\ny = []\nfor x in liste_nombres:\n    if x % 2 == 0:\n        y.append(x*x)\n\n# mieux\ny = [x*x for x in liste_nombres if x % 2 == 0]\n\n\n\n\nConseils pour la programmation\nDans le monde de la programmation en Python, il existe deux paradigmes diff√©rents :\n\nLa programmation fonctionnelle est une approche qui construit un code en encha√Ænant des fonctions, c‚Äôest-√†-dire des op√©rations plus ou moins standardis√©es ;\nLa programmation orient√©e objet (POO) consiste √† construire son code en d√©finissant des objets d‚Äôune certaine classe ayant des attributs (les caract√©ristiques intrins√®ques de l‚Äôobjet) et sur lequel on effectue des op√©rations ad hoc par le biais de m√©thodes qui encapsulent des op√©rations propres √† chaque classe.\n\n\n\nExemple de comparaison des deux paradigmes\n\nMerci ChatGPT pour l‚Äôexemple :\n\nclass AverageCalculator:\n    def __init__(self, numbers):\n        self.numbers = numbers\n\n    def calculate_average(self):\n        return sum(self.numbers) / len(self.numbers)\n\n# Utilisation\ncalculator = AverageCalculator([1, 2, 3, 4, 5])\nprint(\"Moyenne (POO):\", calculator.calculate_average())\n\ndef calculate_average(numbers):\n    return sum(numbers) / len(numbers)\n\n# Utilisation\nnumbers = [1, 2, 3, 4, 5]\nprint(\"Moyenne (PF):\", calculate_average(numbers))\n\nMoyenne (POO): 3.0\nMoyenne (PF): 3.0\n\n\n\nLa programmation fonctionnelle est plus intuitive que la POO et permet souvent de d√©velopper du code plus rapidement. La POO est une approche plus formaliste. Celle-ci est int√©ressante lorsqu‚Äôune fonction doit s‚Äôadapter au type d‚Äôobjet en entr√©e (par exemple aller chercher des poids diff√©rents selon le type de mod√®le Pytorch). Cela √©vite les codes spaghetti üçù inutilement complexes qui sont impossibles √† d√©bugger.\nN√©anmoins, il convient d‚Äô√™tre pragmatique. La programmation orient√©e objet peut √™tre plus complexe √† mettre en ≈ìuvre que la programmation fonctionnelle. Dans de nombreuses situations, cette derni√®re, si elle est bien faite, suffit largement. Il est utile lorsqu‚Äôon d√©veloppe dans le cadre d‚Äôun projet important d‚Äôadopter une approche dite de programmation d√©fensive. Il s‚Äôagit d‚Äôun principe de pr√©caution dans le paradigme de la programmation fonctionnelle qui vise √† limiter les situations impr√©vues en √©tant capable de g√©rer, par exemple, un argument d‚Äôune fonction inattendu ou un objet √† la structure diff√©rente de celle pour lequel le code a √©t√© pens√©.\n\n\n\n\n\n\nLe code spaghetti\n\n\n\nLe code spaghetti est un style d‚Äô√©criture qui favorise l‚Äôapparition du syndrome du plat de spaghettis : un code impossible √† d√©m√™ler parce qu‚Äôil fait un usage excessif de conditions, d‚Äôexceptions en tous sens, de gestion des √©v√©nements complexes. Il devient quasi impossible de savoir quelles ont √©t√© les conditions √† l‚Äôorigine de telle ou telle erreur sans ex√©cuter ligne √† ligne (et celles-ci sont excessivement nombreuses du fait de mauvaises pratiques de programmation) le programme.\nEn fait, la programmation spaghetti qualifie tout ce qui ne permet pas de d√©terminer le qui, le quoi et le comment. Le code est donc plus long √† mettre √† jour car cela n√©cessite de remonter un √† un le fil des renvois.\n\n\n\n\n\n\n\n\nUn exemple progressif pour comprendre\n\n\n\n\n\nüí° Supposons qu‚Äôon dispose d‚Äôune table de donn√©es qui utilise le code ‚àí99 pour repr√©senter les valeurs manquantes. On d√©sire remplacer l‚Äôensemble des ‚àí99 par des NA.\nVoici un code Python qui permet de se placer dans ce cas qui, malheureusement, arrive fr√©quemment.\n# On fixe la racine pour √™tre s√ªr de tous avoir le m√™me dataset\nnp.random.seed(1234)\n\n# On cr√©√© un dataframe\na = np.random.randint(1, 10, size = (5,6))\ndf = np.insert(\n    a,\n    np.random.choice(len(a), size=6),\n    -99,\n)\ndf = pd.DataFrame(df.reshape((6,6)), columns=[chr(x) for x in range(97, 103)])\nUn premier jet de code pourrait prendre la forme suivante :\n# Dupliquer les donn√©es\ndf2 = df.copy()\n# Remplacer les -99 par des NA\ndf2.loc[df2['a'] == -99,'a'] = np.nan\ndf2.loc[df2['b'] == -99,'b'] = np.nan\ndf2.loc[df2['c'] == -99,'c'] = np.nan\ndf2.loc[df2['d'] == -99,'d'] = np.nan\ndf2.loc[df2['e'] == -98,'e'] = np.nan\ndf2.loc[df2['f'] == -99,'e'] = np.nan\nQuelles sont les choses qui vous d√©rangent dans le code ci-dessus ?\n\n\nIndice üí° Regardez pr√©cis√©ment le code et le DataFrame, notamment les colonnes e et g.\n\nIl y a deux erreurs, difficiles √† d√©tecter:\n\ndf2.loc[df2['e'] == -98,'e'] = np.nan: une erreur de copier-coller sur la valeur de l‚Äôerreur ;\ndf2.loc[df2['f'] == -99,'e'] = np.nan: une erreur de copier-coller sur les colonnes en question\n\n\nOn peut noter au moins deux trois :\n\nLe code est long et r√©p√©titif, ce qui nuit √† sa lisibilit√© ;\nLe code est tr√®s d√©pendant de la structure des donn√©es (nom et nombre de colonnes) et doit √™tre adapt√© d√®s que celle-ci √©volue ;\nOn a introduit des erreurs humaines dans le code, difficiles √† d√©tecter.\n\nOn voit dans la premi√®re version de notre code qu‚Äôil y a une structure commune √† toutes nos lignes de la forme .[. == -99] = np.nan. Cette structure va servir de base √† notre fonction, en vue de g√©n√©raliser le traitement que nous voulons faire.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2['a'] = fix_missing(df['a'])\ndf2['b'] = fix_missing(df['b'])\ndf2['c'] = fix_missing(df['c'])\ndf2['d'] = fix_missing(df['d'])\ndf2['e'] = fix_missing(df['e'])\ndf2['f'] = fix_missing(df['f'])\nCette seconde version du code est meilleure que la premi√®re version, car on a r√©gl√© le probl√®me d‚Äôerreur humaine (il n‚Äôest plus possible de taper -98 au lieu de -99).\n\n\nMais voyez-vous le probl√®me qui persiste ?\n\nLe code reste long et r√©p√©titif, et n‚Äô√©limine pas encore toute possibilit√© d‚Äôerreur, car il est toujours possible de se tromper dans le nom des variables.\n\nLa prochaine √©tape consiste √† √©liminer ce risque d‚Äôerreur en combinant deux fonctions (ce qu‚Äôon appelle la combinaison de fonctions).\nLa premi√®re fonction fix_missing() sert √† r√©gler le probl√®me sur un vecteur. La seconde g√©n√©ralisera ce proc√©d√© √† toutes les colonnes. Comme Pandas permet une approche vectorielle, il est fr√©quent de construire des fonctions sur des vecteurs et les appliquer ensuite √† plusieurs colonnes.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2 = df2.apply(fix_missing)\nCette troisi√®me version du code a plusieurs avantages sur les deux autres versions :\n\nElle est plus concise et plus lisible ;\nSi on a un changement de code pour les valeurs manquantes, il suffit de le mettre √† un seul endroit ;\nElle fonctionne quel que soit le nombre de colonnes et le nom des colonnes ;\nOn ne peut pas traiter une colonne diff√©remment des autres par erreur.\n\nDe plus, le code est facilement g√©n√©ralisable.\nPar exemple, √† partir de la m√™me structure, √©crire le code qui permet de ne traiter que les colonnes a,b et e ne demande pas beaucoup d‚Äô√©nergie.\ndf2 = df.copy()\ndf2[['a','b','e']] = df2[['a','b','e']].apply(fix_missing)\n\n\n\nUn certain nombre de conseils sont pr√©sents dans le Hitchhiker‚Äôs Guide to Python qui vise √† faire conna√Ætre les pr√©ceptes du ‚ÄúZen of Python‚Äù (PEP 20). Ce post de blog illustre quelques uns de ces principes avec des exemples.\n\n\n\n\n\n\nLe Zen de Python\n\n\n\n\n\nLe ‚ÄúZen de Python‚Äù est une collection de principes pour la programmation en Python, √©crite par Tim Peters en 2004 sous la forme d‚Äôaphorismes. Ceux-ci mettent en lumi√®re la philosophie de conception du langage Python.\nVous pouvez retrouver ces conseils dans Python en tapant le code suivant:\n\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "chapters/code-quality.html#footnotes",
    "href": "chapters/code-quality.html#footnotes",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGuido Van Rossum est le cr√©ateur de , c‚Äôest donc quelqu‚Äôun qu‚Äôil est pertinent d‚Äô√©couter.‚Ü©Ô∏é\nPar exemple, en , il est possible d‚Äôutiliser &lt;- ou = pour l‚Äôassignation, on ne recontre pas d‚Äôerreur en cas de mauvaise indentation‚Ä¶‚Ü©Ô∏é\nNous pr√©senterons les deux approches principales en Python, leurs points commun et les points par lesquels ils diff√®rent : les environnements virtuels (g√©r√©s par un fichier requirements.txt) et les environnements conda (g√©r√©s par un fichier environment.yml)‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/evaluation.html",
    "href": "chapters/evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "L‚Äôobjectif g√©n√©ral de l‚Äô√©valuation de ce cours est de mettre en pratique les notions √©tudi√©es (bonnes pratiques de d√©veloppement et mise en production) de mani√®re appliqu√©e et r√©aliste, i.e.¬†√† travers un projet bas√© sur une probl√©matique ‚Äúm√©tier‚Äù et des donn√©es r√©elles. Pour cela, l‚Äô√©valuation sera en deux parties :\n\nPar groupe de 3 : un projet √† choisir parmi les 3 parcours (MLOps, app interactive / dashboard, publication reproductible + site web). Id√©alement, on choisira un projet r√©el, effectu√© par exemple dans le cadre d‚Äôun cours pr√©c√©dent et qui g√©n√®re un output propice √† une mise en production.\nSeul : effectuer une revue de code d‚Äôun autre projet. Comp√©tence essentielle et souvent attendue d‚Äôun data scientist, la revue de code sera l‚Äôoccasion de bien int√©grer les bonnes pratiques de d√©veloppement (cf.¬†checklist ci-dessous) et de faire un retour bienveillant sur un autre projet que celui de son groupe."
  },
  {
    "objectID": "chapters/evaluation.html#modalit√©s",
    "href": "chapters/evaluation.html#modalit√©s",
    "title": "Evaluation",
    "section": "",
    "text": "L‚Äôobjectif g√©n√©ral de l‚Äô√©valuation de ce cours est de mettre en pratique les notions √©tudi√©es (bonnes pratiques de d√©veloppement et mise en production) de mani√®re appliqu√©e et r√©aliste, i.e.¬†√† travers un projet bas√© sur une probl√©matique ‚Äúm√©tier‚Äù et des donn√©es r√©elles. Pour cela, l‚Äô√©valuation sera en deux parties :\n\nPar groupe de 3 : un projet √† choisir parmi les 3 parcours (MLOps, app interactive / dashboard, publication reproductible + site web). Id√©alement, on choisira un projet r√©el, effectu√© par exemple dans le cadre d‚Äôun cours pr√©c√©dent et qui g√©n√®re un output propice √† une mise en production.\nSeul : effectuer une revue de code d‚Äôun autre projet. Comp√©tence essentielle et souvent attendue d‚Äôun data scientist, la revue de code sera l‚Äôoccasion de bien int√©grer les bonnes pratiques de d√©veloppement (cf.¬†checklist ci-dessous) et de faire un retour bienveillant sur un autre projet que celui de son groupe."
  },
  {
    "objectID": "chapters/evaluation.html#checklist-des-bonnes-pratiques-de-d√©veloppement",
    "href": "chapters/evaluation.html#checklist-des-bonnes-pratiques-de-d√©veloppement",
    "title": "Evaluation",
    "section": "Checklist des bonnes pratiques de d√©veloppement",
    "text": "Checklist des bonnes pratiques de d√©veloppement\nLes bonnes pratiques de d√©veloppement ci-dessous sont les indispensables de ce cours. Elles doivent √™tre √† la fois appliqu√©es dans les projets de groupe, et √† la base de la revue de code individuelle.\n\nUtilisation de Git\n\nPr√©sence d‚Äôun fichier .gitignore adapt√© au langage et avec des r√®gles additionnelles pour respecter les bonnes pratiques de versioning\nTravail collaboratif : utilisation des branches et des pull requests\n\nPr√©sence d‚Äôun fichier README pr√©sentant le projet : contexte, objectif, comment l‚Äôutiliser ?\nPr√©sence d‚Äôun fichier LICENSE d√©clarant la licence (open-source) d‚Äôexploitation du projet.\nVersioning des packages : pr√©sence d‚Äôun fichier requirements.txt ou d‚Äôun fichier d‚Äôenvironnement environment.yml pour conda\nQualit√© du code\n\nRespect des standards communautaires : utiliser un linter et/ou un formatter\nModularit√© : un script principal qui appelle des modules\n\nStructure des projets\n\nRespect des standards communautaires (cookiecutter)\nModularit√© du projet selon le mod√®le √©voqu√© dans le cours:\n\nCode sur GitHub\nDonn√©es sur S3\nFichiers de configuration (secrets, etc.) √† part\n\n\n\n\n\n\nProposition de modularit√© du projet illustr√©e pour un projet mixte MLOps et dashboard"
  },
  {
    "objectID": "chapters/evaluation.html#projets",
    "href": "chapters/evaluation.html#projets",
    "title": "Evaluation",
    "section": "Projets",
    "text": "Projets\nVoici trois ‚Äúparcours‚Äù possibles afin de mettre en application les concepts et techniques du cours dans le cadre de projets appliqu√©s. Des projets qui sortiraient de ces parcours-types sont tout √† fait possibles et appr√©ci√©s, il suffit d‚Äôen discuter avec les auteurs du cours.\n\nParcours MLOps\n\n\n\n\n\n\nObjectif\n\n\n\nA partir d‚Äôun projet existant ou d‚Äôun projet type contest Kaggle, d√©velopper un mod√®le de ML r√©pondant √† une probl√©matique m√©tier, puis la d√©ployer sur une infrastructure de production conform√©ment aux principes du MLOps.\n\n\nEtapes :\n\nRespecter la checklist des bonnes pratiques de d√©veloppement ;\nD√©velopper un mod√®le de ML qui r√©pond √† un besoin m√©tier ;\nEntra√Æner le mod√®le via validation crois√©e, avec une proc√©dure de fine-tuning des hyperparam√®tres ;\nFormaliser le processus de fine-tuning de mani√®re reproductible via MLFlow ;\nConstruire une API avec Fastapi pour exposer le meilleur mod√®le ;\nCr√©er une image Docker pour mettre √† disposition l‚ÄôAPI ;\nD√©ployer l‚ÄôAPI sur le SSP Cloud ;\nIndustrialiser le d√©ploiement en mode GitOps avec ArgoCD\nG√©rer le monitoring de l‚Äôapplication : logs, dashboard de suivi des performances, etc.\n\n\n\nParcours dashboard / application interactive\n\n\n\n\n\n\nObjectif\n\n\n\n√† partir d‚Äôun projet existant ou d‚Äôun projet que vous construirez, d√©velopper une application interactive / un dashboard r√©pondant √† une probl√©matique m√©tier, puis la d√©ployer sur une infrastructure de production.\n\n\nEtapes :\n\nRespecter la checklist des bonnes pratiques de d√©veloppement\nD√©velopper une application interactive Streamlit ou un dashboard statique avec Quarto r√©pondant √† une probl√©matique m√©tier\nCr√©er une image Docker permettant d‚Äôexposer l‚Äôapplication en local\nD√©ployer l‚Äôapplication sur le SSP Cloud\nIndustrialiser le d√©ploiement en mode GitOps avec ArgoCD\nG√©rer le monitoring de l‚Äôapplication : logs, m√©triques de suivi des performances, etc.\n\n\n\nParcours publication reproductible\n\n\n\n\n\n\nObjectif\n\n\n\nA partir d‚Äôun projet existant ou d‚Äôun projet que vous construirez, r√©diger un rapport reproductible √† partir de donn√©es afin de r√©pondre √† une probl√©matique m√©tier, puis le mettre √† disposition √† travers un site web automatiquement g√©n√©r√© et publi√©.\n\n\nEtapes :\n\nRespecter la checklist des bonnes pratiques de d√©veloppement\nR√©diger un rapport reproductible avec Quarto qui fasse intervenir des donn√©es, du code, de la visualisation de donn√©es, du texte, etc.\nExposer le rapport sous la forme d‚Äôun site web via GitHub Actions\nCustomiser le th√®me, le CSS etc. pour mettre en valeur au maximum les r√©sultats de la publication et les messages principaux\nAutomatiser l‚Äôingestion des donn√©es en entr√©e pour que le site web se mette √† jour r√©guli√®rement\nMettre en place des tests automatis√©s de v√©rification des standards de qualit√© du code (linter), de d√©tection de fautes d‚Äôorthographes/de grammaire, etc.\nG√©n√©rer des slides au format quarto-revealjs afin de pr√©senter les principaux r√©sultats de la publication, et les exposer comme une page du site"
  },
  {
    "objectID": "chapters/evaluation.html#revue-de-code",
    "href": "chapters/evaluation.html#revue-de-code",
    "title": "Evaluation",
    "section": "Revue de code",
    "text": "Revue de code\nSur le projet d‚Äôun groupe diff√©rent du sien (attribu√© al√©atoirement au cours du semestre) :\n\nouvrir une pull request de revue de code via un fork (cf.¬†chapitre sur Git pour la proc√©dure)\ndonner une appr√©ciation g√©n√©rale de la conformit√© du projet √† la checklist des bonnes pratiques de d√©veloppement\nsugg√©rer des pistes d‚Äôam√©lioration du projet\n\nChaque groupe, ayant re√ßu des revues de code de son projet, pourra prendre en compte ces pistes d‚Äôam√©liorations dans la mesure du temps disponible, par le biais d‚Äôune autre pull request qui devra r√©f√©rencer celle de la revue de code. Cette derni√®re partie ne sera cependant pas strictement attendue, elle sera valoris√©e en bonus dans la notation finale."
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/introduction.html#origine",
    "href": "chapters/introduction.html#origine",
    "title": "Introduction",
    "section": "Origine",
    "text": "Origine\nLa notion de ‚Äúbonnes pratiques‚Äù qui nous int√©resse dans ce cours trouve son origine au sein de la communaut√© des d√©veloppeurs logiciels. Elle constitue une r√©ponse √† plusieurs constats :\n\nle ‚Äúcode est beaucoup plus souvent lu qu‚Äôil n‚Äôest √©crit‚Äù (Guido Van Rossum) ;\nla maintenance d‚Äôun code demande souvent (beaucoup) plus de ressources que son d√©veloppement initial ;\nla personne qui maintient une base de code a de fortes chances de ne pas √™tre celle qui l‚Äôa √©crite.\n\nFace √† ces constats, un ensemble de r√®gles informelles ont √©t√© conventionnellement accept√©es par la communaut√© des d√©veloppeurs comme produisant des logiciels plus fiables, √©volutifs et maintenables dans le temps. Comme toutes conventions de langue, certaines peuvent para√Ætre arbitraires. Ces r√®gles favorisent n√©anmoins la capacit√© √† communiquer du code, un aspect communautaire qui peut para√Ætre secondaire au premier abord mais qui est pourtant le principe ayant fait le succ√®s d‚Äôun langage open source en favorisant le partage d‚Äôexp√©rience et d‚Äôassistance.\n\n\n\n\n\n\nLa 12 Factor App\n\n\n\n\n\nR√©cemment, dans le contexte d‚Äôune √©volution des logiciels vers des applications web vivant dans le cloud, un certain nombre de ces bonnes pratiques ont √©t√© formalis√©es dans un manifeste : la 12 Factor App. Le d√©veloppement du cloud, c‚Äôest-√†-dire d‚Äôinfrastructures standardis√©es hors des syst√®mes d‚Äôinformation des d√©tenteurs de donn√©es, rend les besoins de bonnes pratiques plus pr√©gnant."
  },
  {
    "objectID": "chapters/introduction.html#pourquoi-sint√©resser-aux-bonnes-pratiques",
    "href": "chapters/introduction.html#pourquoi-sint√©resser-aux-bonnes-pratiques",
    "title": "Introduction",
    "section": "Pourquoi s‚Äôint√©resser aux bonnes pratiques ?",
    "text": "Pourquoi s‚Äôint√©resser aux bonnes pratiques ?\n\nEn quoi est-ce pertinent pour le data scientist, dont le r√¥le n‚Äôest pas de d√©velopper des applications mais de donner du sens aux donn√©es ?\n\nDu fait du d√©veloppement rapide de la data science et cons√©quemment de la croissance de la taille moyenne des projets, l‚Äôactivit√© du data scientist tend √† se rapprocher par certains aspects de celle du d√©veloppeur :\n\nles projets sur lesquels travaille le data scientist sont intenses en code ;\nil doit travailler de mani√®re collaborative au sein de projets de grande envergure ;\nil est de plus en plus amen√© √† travailler √† partir de donn√©es massives, ce qui n√©cessite de travailler sur des infrastructures big data informatiquement complexes ;\nil est amen√© √† interagir avec des profils informatiques pour d√©ployer ses mod√®les et les rendre accessibles √† des utilisateurs.\n\nAussi, il fait sens pour le data scientist moderne de s‚Äôint√©resser aux bonnes pratiques en vigueur dans la communaut√© des d√©veloppeurs. Bien entendu, celles-ci doivent √™tre adapt√©es aux sp√©cificit√©s des projets bas√©s sur des donn√©es. L‚Äôeffet b√©n√©fique de ces bonnes pratiques est que les projets les adoptant auront un co√ªt bien plus minimal pour √©voluer, ce qui les rend plus comp√©titif dans un √©cosyst√®me mouvant comme l‚Äôest la data science o√π les donn√©es, les outils et les attentes des utilisateurs sont en changements continuels."
  },
  {
    "objectID": "chapters/introduction.html#un-continuum-de-bonnes-pratiques",
    "href": "chapters/introduction.html#un-continuum-de-bonnes-pratiques",
    "title": "Introduction",
    "section": "Un continuum de bonnes pratiques",
    "text": "Un continuum de bonnes pratiques\nLa notion de bonnes pratiques ne doit pas √™tre vue de mani√®re binaire : il n‚Äôy a pas d‚Äôun c√¥t√© les projets qui les appliquent et de l‚Äôautre ceux qui ne les appliquent pas. Les bonnes pratiques ont un co√ªt, qu‚Äôil ne faut pas n√©gliger ‚Äî m√™me si leur application √©vite aussi des co√ªts futurs, notamment en terme de maintenance. Il faut donc plut√¥t voir les bonnes pratiques comme un spectre, sur lequel on vient positionner son projet en fonction de diff√©rents crit√®res, notamment du co√ªt-avantage √† avancer d‚Äôun niveau dans le spectre de la reproductibilit√©.\nLa d√©termination du seuil pertinent doit r√©sulter d‚Äôun arbitrage entre diff√©rents crit√®res li√©s au projet :\n\nambitions : le projet est-il amen√© √† √©voluer, prendre de l‚Äôampleur ? Est-il destin√© √† devenir collaboratif, que ce soit dans le cadre d‚Äôune √©quipe en organisation ou bien en open-source ? Les outputs du projet ont-ils vocation √† √™tre diffus√©s au grand public ?\nressources : quels sont les moyens humain du projet ? Pour un projet open-source, existe-t-il une communaut√© potentielle de contributeurs ?\ncontraintes : le projet a-t-il une √©ch√©ance proche ? Des exigences de qualit√© ont-elles √©t√© fix√©es ? Est-il destin√© √† la mise en production ? Existe-t-il des enjeux de s√©curit√© forts ?\npublic cible: √† quels profils s‚Äôadresse les diff√©rentes valorisations de donn√©es de ce projet ? Quel est leur niveau de technicit√© et le temps qu‚Äôelles vont consacrer √† suivre votre projet ?\n\nIl n‚Äôest donc pas question pour nous de sugg√©rer que tout projet de data science doit respecter toutes les bonnes pratiques pr√©sent√©es dans ce cours. Cela √©tant dit, nous sommes convaincus qu‚Äôil est important pour tout data scientist de r√©fl√©chir √† ces questions pour am√©liorer ces pratiques au fil du temps.\nEn particulier, nous pensons qu‚Äôil est possible de d√©finir un socle, i.e.¬†un ensemble minimal de bonnes pratiques qui apportent plus d‚Äôavantages qu‚Äôelles ne co√ªtent √† impl√©menter. Notre suggestion pour un tel socle est la suivante :\n\nContr√¥ler la qualit√© de son code en utilisant des outils d√©di√©s (cf.¬†chapitre Qualit√© du Code) ;\nAdopter une structure standardis√©e de projet en utilisant des templates pr√™ts √† l‚Äôemploi (cf.¬†chapitre Architecture des Projets) ;\nUtiliser Git pour versionner le code de ses projets, qu‚Äôils impliquent d‚Äôautres d√©veloppeurs ou seulement vous (cf.¬†chapitre Versionner son code et travailler collaborativement avec Git) ;\ncontr√¥ler les d√©pendances de son projet en d√©veloppant dans des environnements virtuels (cf.¬†chapitre Portabilit√©).\n\nLes √©tapes suivantes dans l‚Äô√©chelle de la reproductibilit√© d√©pendront de l‚Äôarbitrage co√ªt-avantage. L‚Äôadoption du socle minimal de reproductibilit√© facilitera √©norm√©ment l‚Äôavanc√©e ult√©rieure dans l‚Äôambition d‚Äôun projet.\nFaisons √† pr√©sent un tour d‚Äôhorizon des principes d√©fendus dans ce cours et de la progression logique de celui-ci."
  },
  {
    "objectID": "chapters/introduction.html#le-code-est-un-outil-de-communication",
    "href": "chapters/introduction.html#le-code-est-un-outil-de-communication",
    "title": "Introduction",
    "section": "Le code est un outil de communication",
    "text": "Le code est un outil de communication\nLa premi√®re bonne pratique √† adopter est de consid√©rer le code comme un outil de communication, et non simplement de mani√®re fonctionnelle. Un code ne sert pas seulement √† r√©aliser une t√¢che donn√©e, il a vocation √† √™tre diffus√©, r√©utilis√©, maintenu, que ce soit dans le contexte d‚Äôune √©quipe dans une organisation ou bien en open-source.\nPour favoriser cette communication du code, des conventions ont √©t√© developp√©es en mati√®re de qualit√© du code et de structuration des projets, qu‚Äôil est utile d‚Äôappliquer dans ses projets. Nous pr√©sentons ces conventions dans les chapitres Qualit√© du Code et Architecture des Projets.\nIl est pour les m√™mes raisons indispensable d‚Äôappliquer les principes du contr√¥le de version, qui permettent une documentation en continu des projets, ce qui accro√Æt fortement leur r√©utilisabilit√© et leur maintenabilit√© dans le temps. Nous proposons donc un chapitre de rappel sur l‚Äôutilisation du logiciel Git dans le chapitre Versionner son code et travailler collaborativement avec Git."
  },
  {
    "objectID": "chapters/introduction.html#travailler-de-mani√®re-collaborative",
    "href": "chapters/introduction.html#travailler-de-mani√®re-collaborative",
    "title": "Introduction",
    "section": "Travailler de mani√®re collaborative",
    "text": "Travailler de mani√®re collaborative\nLe data scientist, quel que soit son contexte de travail, est amen√© √† travailler dans le cadre de projets en √©quipe. Cela implique de d√©finir une organisation du travail ainsi que d‚Äôutiliser des outils permettant de collaborer sur un projet de mani√®re efficace et s√©curis√©e.\nNous pr√©sentons une mani√®re moderne de travailler collaborativement avec Git et GitHub dans le chapitre de rappel Versionner son code et travailler collaborativement avec Git. Les autres chapitres prendront pour acquis cette approche collaborative et la raffineront √† travers l‚Äôapproche DevOps4."
  },
  {
    "objectID": "chapters/introduction.html#maximiser-la-reproductibilit√©",
    "href": "chapters/introduction.html#maximiser-la-reproductibilit√©",
    "title": "Introduction",
    "section": "Maximiser la reproductibilit√©",
    "text": "Maximiser la reproductibilit√©\nLe troisi√®me pilier des bonnes pratiques pr√©sent√©es dans ce cours est la reproductibilit√©.\nUn projet est dit reproductible lorsque, avec le m√™me code et les m√™mes donn√©es, il est possible de reproduire les r√©sultats obtenus. Notons bien que le probl√®me de la reproductibilit√© est diff√©rent de celui de la r√©plicabilit√©. La r√©plicabilit√© est un concept scientifique, qui signifie qu‚Äôun m√™me proc√©d√© exp√©rimental donne des r√©sultats analogues sur des jeux de donn√©es diff√©rents. La reproductibilit√© est un concept technique : elle ne signifie pas que le protocole exp√©rimental est scientifiquement correct, mais qu‚Äôil a √©t√© sp√©cifi√© et diffus√© d‚Äôune mani√®re qui permet √† tous de reproduire les r√©sultats obtenus.\nLa notion de reproductibilit√© est le fil rouge de ce cours : toutes les notions vues dans les diff√©rents chapitres y contribuent. Le fait de produire du code et des projets qui respectent les conventions communautaires, comme le fait d‚Äôutiliser le contr√¥le de version, contribuent √† rendre le code plus lisible et document√©, et donc reproductible.\nIl faut n√©anmoins aller plus loin pour atteindre une v√©ritable reproductibilit√©, et r√©fl√©chir √† la notion d‚Äôenvironnement d‚Äôex√©cution. Un code n‚Äôest pas un objet autonome, il est toujours ex√©cut√© sur un environnement (ordinateur personnel, serveur, etc.), et ces environnements peuvent √™tre tr√®s diff√©rents (syst√®me d‚Äôexploitation, librairies install√©es, contraintes de s√©curit√©, etc.). C‚Äôest pourquoi il faut r√©fl√©chir √† la portabilit√© de son code, i.e.¬†sa capacit√© √† s‚Äôex√©cuter de mani√®re attendue sur diff√©rents environnements, ce qui sera l‚Äôobjet d‚Äôun chapitre √† part enti√®re."
  },
  {
    "objectID": "chapters/introduction.html#faciliter-la-mise-en-production",
    "href": "chapters/introduction.html#faciliter-la-mise-en-production",
    "title": "Introduction",
    "section": "Faciliter la mise en production",
    "text": "Faciliter la mise en production\nPour qu‚Äôun projet de data science cr√©e in fine de la valeur, il faut qu‚Äôil soit d√©ploy√© sous une forme valorisable de sorte √† toucher son public. Cela implique deux choses :\n\ntrouver le format de diffusion adapt√©, i.e.¬†qui valorise au mieux les r√©sultas obtenus aupr√®s des utilisateurs potentiels ;\nfaire transitionner le projet de l‚Äôenvironnement dans lequel il a √©t√© d√©velopp√© vers une infrastructure de production, i.e.¬†permettant un d√©ploiement robuste de l‚Äôoutput du projet afin que celui-ci soit disponible √† la demande.\n\nDans le chapitre D√©ployer et valoriser son projet de data science, nous proposons des pistes permettant de r√©pondre √† ces deux besoins. Nous pr√©sentons un certain nombre de formats standards (API, application, rapport automatis√©, site internet) qui permettent √† un projet de data science d‚Äô√™tre valoris√©, ainsi que les outils modernes qui permettent de les produire.\nNous d√©taillons ensuite les concepts essentiels du d√©ploiement sur une infrastructure de production, et illustrons ces derniers par des exemples de d√©ploiements dans un environnement cloud moderne.\nC‚Äôest en quelque sorte la r√©compense de l‚Äôapplication des bonnes pratiques : d√®s lors que l‚Äôon s‚Äôest donn√© la peine de produire un code et un projet appliquant des standards de qualit√©, que l‚Äôon a bien versionn√© son code, et que l‚Äôon a pris des mesures pour le rendre portable, le d√©ploiement du projet dans un environnement de production s‚Äôen trouve largement facilit√©."
  },
  {
    "objectID": "chapters/introduction.html#une-ouverture-√†-lindustrialisation-de-la-production",
    "href": "chapters/introduction.html#une-ouverture-√†-lindustrialisation-de-la-production",
    "title": "Introduction",
    "section": "Une ouverture √† l‚Äôindustrialisation de la production",
    "text": "Une ouverture √† l‚Äôindustrialisation de la production\nEn simplifiant la structure d‚Äôun projet, on facilite sa production en s√©rie. Dans le domaine de la data science, cela prendra par exemple la forme d‚Äôune industrialisation des entra√Ænements d‚Äôun mod√®le permettant de choisir le ‚Äúmeilleur‚Äù dans un ensemble beaucoup plus complet de mod√®les que ne le permettrait une approche artisanale.\nN√©anmoins, tout mod√®le apprend du pass√© et avoir un bon mod√®le aujourd‚Äôhui n‚Äôassure en rien que ce dernier sera pertinent demain, lorsqu‚Äôon aura r√©ellement besoin de celui-ci. Pour int√©grer cette dimension mouvante inh√©rante √† tout projet de data science, nous aurons l‚Äôoccasion de pr√©senter quelques principes du MLOps. Ce terme, qui est certes un buzz-word mais qui rassemble un ensemble pertinent de pratiques pour les data scientists, sera pr√©sent√© dans le chapitre consacr√©."
  },
  {
    "objectID": "chapters/introduction.html#chapitres-suppl√©mentaires",
    "href": "chapters/introduction.html#chapitres-suppl√©mentaires",
    "title": "Introduction",
    "section": "Chapitres suppl√©mentaires",
    "text": "Chapitres suppl√©mentaires\nPlusieurs outils pr√©sent√©s tout au long de ce cours, tels que les logiciels Git et Docker, impliquent l‚Äôutilisation du terminal ainsi que des connaissances de base du fonctionnement d‚Äôun syst√®me Linux. Dans le chapitre D√©mystifier le terminal Linux pour gagner en autonomie, nous pr√©sentons les connaissances essentielles des syst√®mes Linux qu‚Äôun data scientist doit poss√©der pour pouvoir √™tre autonome dans ses d√©ploiements et dans l‚Äôapplication des bonnes pratiques de d√©veloppement."
  },
  {
    "objectID": "chapters/introduction.html#approche-p√©dagogique",
    "href": "chapters/introduction.html#approche-p√©dagogique",
    "title": "Introduction",
    "section": "Approche p√©dagogique",
    "text": "Approche p√©dagogique\nLe parti pris de ce cours est que seule la pratique, et en particulier la confrontation √† des probl√®mes issus de projets r√©els, permet d‚Äôacqu√©rir efficacement des concepts informatiques. Aussi, une large part du cours consistera en l‚Äôapplication des notions √©tudi√©es √† des cas concrets. Chaque chapitre se concluera par des applications touchant √† des sujets r√©alistes de data science.\nUn exemple fil rouge illustre les progr√®s dans la conception d‚Äôun projet reproductible en appliquant successivement le contenu des chapitres de ce cours.\nPour l‚Äô√©valuation g√©n√©rale du cours, l‚Äôid√©e sera de partir d‚Äôun projet personnel, id√©alement termin√©, et de lui appliquer un maximum de bonnes pratiques pr√©sent√©es dans ce cours."
  },
  {
    "objectID": "chapters/introduction.html#langages",
    "href": "chapters/introduction.html#langages",
    "title": "Introduction",
    "section": "Langages",
    "text": "Langages\nLes principes pr√©sent√©s dans ce cours sont pour la plupart agnostiques du langage de programmation utilis√©.\nCe choix n‚Äôest pas qu‚Äô√©ditorial, c‚Äôest selon nous un aspect fondamental du sujet des bonnes pratiques. Trop souvent, des diff√©rences de langage entre les phases de d√©veloppement (notamment R ou Python) et de mise en production (ex : Java) √©rigent des murs artificiels qui r√©duisent fortement la capacit√© √† valoriser des projets de data science.\nA l‚Äôinverse, plus les diff√©rentes √©quipes qui forment le cycle de vie d‚Äôun projet s‚Äôaccordent pour appliquer le m√™me ensemble de bonnes pratiques, plus ces √©quipes d√©veloppent un langage commun, et plus les d√©ploiements en sont facilit√©s.\nUn exemple parlant est l‚Äôutilisation de la conteneurisation : si le data scientist met √† disposition une image Docker comme output de sa phase de d√©veloppement et que le data engineer s‚Äôoccupe de d√©ployer cette image sur une infrastructure d√©di√©e, le contenu m√™me de l‚Äôapplication en termes de langage importe finalement assez peu. Cet exemple, certes simpliste, illustre malgr√© tout l‚Äôenjeu des bonnes pratiques en mati√®re de communication au sein d‚Äôun projet.\nLes exemples pr√©sent√©s dans ce cours seront pour l‚Äôessentiel en Python. La raison principale est que ce langage, malgr√© ses d√©fauts, est enseign√© dans la majorit√© des cursus de data science mais aussi d‚Äôinformatique. Il peut faciliter la passerelle entre le monde des utilisateurs de donn√©es et celui des d√©veloppeurs informatiques, passerelle indispensable pour favoriser le dialogue entre ces deux profils, n√©cessaires tous deux pour un passage en production. Encore une fois, il est tout √† fait possible d‚Äôappliquer les m√™mes principes avec d‚Äôautres langages, et nous encourageons d‚Äôailleurs les √©tudiants √† s‚Äôessayer √† cet exercice formateur."
  },
  {
    "objectID": "chapters/introduction.html#environnement-dex√©cution",
    "href": "chapters/introduction.html#environnement-dex√©cution",
    "title": "Introduction",
    "section": "Environnement d‚Äôex√©cution",
    "text": "Environnement d‚Äôex√©cution\nA l‚Äôinstar du langage, les principes appliqu√©s dans ce cours sont agnostiques √† l‚Äôinfrastructure utilis√©e pour faire tourner les exemples propos√©s. Il est donc √† la fois possible et souhaitable d‚Äôappliquer les bonnes pratiques aussi bien √† un projet individuel d√©velopp√© sur un ordinateur personnel qu‚Äô√† un projet collaboratif visant √† √™tre d√©ploy√© sur une infrastructure de production d√©di√©e.\nCependant, nous choisissons comme environnement de r√©f√©rence tout au long de ce cours le SSP Cloud, une plateforme de services pour la data science d√©velopp√©e √† l‚ÄôInsee et accessible aux √©l√®ves des √©coles statistiques. Les raisons de ce choix sont multiples :\n\nl‚Äôenvironnement de d√©veloppement est normalis√© : les serveurs du SSP Cloud ont une configuration homog√®ne ‚Äî notamment, ils se basent sur une m√™me distribution Linux (Debian) ‚Äî ce qui garantit la reproductibilit√© des exemples pr√©sent√©s tout au long du cours ;\nvia un cluster Kubernetes sous-jacent, le SSP Cloud met √† disposition une infrastructure robuste permettant le d√©ploiement automatis√© d‚Äôapplications potentiellement intensives en donn√©es, ce qui permet de simuler un v√©ritable environnement de production ;\nle SSP Cloud est construit selon les standards les plus r√©cents des infrastructures data science, et permet donc d‚Äôacqu√©rir les bonnes pratiques de mani√®re organique :\n\nles services sont lanc√©s via des conteneurs, configur√©s par des images Docker. Cela permet de garantir une forte reproductibilit√© des d√©ploiements, au prix d‚Äôune phase de d√©veloppement un peu plus co√ªteuse ;\nle SSP Cloud est bas√© sur une approche dite cloud native : il est construit sur un ensemble modulaire de briques logicielles, qui permettent d‚Äôappliquer une s√©paration nette du code, des donn√©es, de la configuration et de l‚Äôenvironnement d‚Äôex√©cution, principe majeur des bonnes pratiques qui reviendra tout au long de ce cours.\n\n\nPour en savoir plus sur cette plateforme, vous pouvez consulter cette page."
  },
  {
    "objectID": "chapters/introduction.html#ressources-compl√©mentaires",
    "href": "chapters/introduction.html#ressources-compl√©mentaires",
    "title": "Introduction",
    "section": "Ressources compl√©mentaires",
    "text": "Ressources compl√©mentaires\n\nMissing semester du MIT"
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCelui que vous connaissez certainement le mieux est le Notebook Jupyter. Tr√®s pratique pour produire du code exploratoire ou pour transmettre un code avec de la documentation, nous aurons l‚Äôoccasion de d√©couvrir ses limites dans le cadre d‚Äôun projet collaboratif ou un projet √† grande √©chelle.‚Ü©Ô∏é\nNous aurons l‚Äôoccasion ult√©rieurement de d√©finir de mani√®re formelle cette notion centrale. En attendant, on peut entendre ce concept comme un environnement disponible en continu afin de mettre √† disposition une valorisation de donn√©es. Cela prend souvent la forme d‚Äôun serveur de production ou d‚Äôun cluster informatique qui doit √™tre disponible en continu.‚Ü©Ô∏é\nL‚Äôaspect tr√®s intriqu√© des notions de bonnes pratiques, de reproductibilit√© et de mise en production nous a d‚Äôailleurs longtemps fait h√©siter sur le nom √† donner √† ce cours. Parmi la shortlist des noms possibles, nous avions ‚ÄúBonnes pratiques en data science‚Äù ou ‚ÄúBonnes pratiques pour la reproductibilit√© en data science. N√©anmoins, les bonnes pratiques restent un moyen l√† o√π la mise en production est la finalit√©, nous avons ainsi privil√©gi√© le fait de mettre en avant cette derni√®re notion.‚Ü©Ô∏é\nD√©marche consistant √† automatiser et int√©grer la conception et la production des livrables avant la phase de mise en production. Comme les bonnes pratiques, cette approche issue √† l‚Äôorigine du monde du d√©veloppement logiciel est devenue incontournable pour les data scientists.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/mlops.html",
    "href": "chapters/mlops.html",
    "title": "MLOps",
    "section": "",
    "text": "Contenu √† venir"
  },
  {
    "objectID": "chapters/projects-architecture.html",
    "href": "chapters/projects-architecture.html",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/projects-architecture.html#d√©monstration-par-lexemple",
    "href": "chapters/projects-architecture.html#d√©monstration-par-lexemple",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "D√©monstration par l‚Äôexemple",
    "text": "D√©monstration par l‚Äôexemple\nVoici un exemple d‚Äôorganisation de projet, qui vous rappellera peut-√™tre des souvenirs :\n‚îú‚îÄ‚îÄ report.qmd\n‚îú‚îÄ‚îÄ correlation.png\n‚îú‚îÄ‚îÄ data.csv\n‚îú‚îÄ‚îÄ data2.csv\n‚îú‚îÄ‚îÄ fig1.png\n‚îú‚îÄ‚îÄ figure 2 (copy).png\n‚îú‚îÄ‚îÄ report.pdf\n‚îú‚îÄ‚îÄ partial data.csv\n‚îú‚îÄ‚îÄ script.R\n‚îî‚îÄ‚îÄ script_final.py\nSource : eliocamp.github.io\nLa structure du projet suivante rend compliqu√©e la compr√©hension du projet. Parmi les principales questions :\n\nQuelles sont les donn√©es en entr√©e de chaine ?\nDans quel ordre les donn√©es interm√©diaires sont-elles cr√©√©es ?\nQuel est l‚Äôobjet des productions graphiques ?\nTous les codes sont-ils utilis√©s dans ce projet ?\n\nEn structurant le dossier en suivant des r√®gles simples, par exemple en organisant le projet par des dossiers inputs, outputs, on am√©liore d√©j√† grandement la lisibilit√© du projet\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ raw\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.csv\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data2.csv\n‚îÇ   ‚îî‚îÄ‚îÄ derived\n‚îÇ       ‚îî‚îÄ‚îÄ partial data.csv\n‚îú‚îÄ‚îÄ src\n|   ‚îú‚îÄ‚îÄ script.py\n‚îÇ   ‚îú‚îÄ‚îÄ script_final.py\n‚îÇ   ‚îî‚îÄ‚îÄ report.qmd\n‚îî‚îÄ‚îÄ output\n    ‚îú‚îÄ‚îÄ fig1.png\n    ‚îú‚îÄ‚îÄ figure 2 (copy).png\n    ‚îú‚îÄ‚îÄ figure10.png\n    ‚îú‚îÄ‚îÄ correlation.png\n    ‚îî‚îÄ‚îÄ report.pdf\n\n\n\n\n\n\nNote\n\n\n\nComme Git est un pr√©requis, tout projet pr√©sente un fichier .gitignore (il est tr√®s important, surtout quand on manipule des donn√©es qui ne doivent pas se retrouver sur Github ou Gitlab).\nUn projet pr√©sente aussi un fichier README.md √† la racine, nous reviendrons dessus.\nUn projet qui utilise l‚Äôint√©gration continue contiendra √©galement des fichiers sp√©cifiques :\n\nsi vous utilisez Gitlab, les instructions sont stock√©es dans le fichier gitlab-ci.yml\nsi vous utilisez Github, cela se passe dans le dossier .github/workflows\n\n\n\nEn changeant simplement le nom des fichiers, on rend la structure du projet tr√®s lisible :\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ raw\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dpe_logement_202103.csv\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dpe_logement_202003.csv\n‚îÇ   ‚îî‚îÄ‚îÄ derived\n‚îÇ       ‚îî‚îÄ‚îÄ dpe_logement_merged_preprocessed.csv\n‚îú‚îÄ‚îÄ src\n|   ‚îú‚îÄ‚îÄ preprocessing.py\n‚îÇ   ‚îú‚îÄ‚îÄ generate_plots.py\n‚îÇ   ‚îî‚îÄ‚îÄ report.qmd\n‚îî‚îÄ‚îÄ output\n    ‚îú‚îÄ‚îÄ histogram_energy_diagnostic.png\n    ‚îú‚îÄ‚îÄ barplot_consumption_pcs.png\n    ‚îú‚îÄ‚îÄ correlation_matrix.png\n    ‚îî‚îÄ‚îÄ report.pdf\nMaintenant, le type de donn√©es en entr√©e de chaine est clair, le lien entre les scripts, les donn√©es interm√©diaires et les output est transparent."
  },
  {
    "objectID": "chapters/projects-architecture.html#vers-la-s√©paration-du-stockage-du-code-des-donn√©es-et-de-lenvironnement-dex√©cution",
    "href": "chapters/projects-architecture.html#vers-la-s√©paration-du-stockage-du-code-des-donn√©es-et-de-lenvironnement-dex√©cution",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Vers la s√©paration du stockage du code, des donn√©es et de l‚Äôenvironnement d‚Äôex√©cution",
    "text": "Vers la s√©paration du stockage du code, des donn√©es et de l‚Äôenvironnement d‚Äôex√©cution\nLa s√©paration du stockage du code et des donn√©es ainsi que de l‚Äôenvironnement d‚Äôex√©cution est importante pour plusieurs raisons.\nTout d‚Äôabord, cela permet de garantir la s√©curit√© et l‚Äôint√©grit√© des donn√©es. En s√©parant les donn√©es du code, il devient plus difficile pour n‚Äôimporte qui d‚Äôacc√©der aux informations sensibles stock√©es dans les donn√©es.\nEn s√©parant l‚Äôenvironnement d‚Äôex√©cution, il est possible de s‚Äôassurer que le code fonctionne de mani√®re coh√©rente et sans conflit avec d‚Äôautres programmes ex√©cut√©s sur le m√™me syst√®me ou n‚Äôest pas alt√©r√© par des configurations syst√®mes difficiles √† reproduire. Cette s√©paration facilite √©galement la portabilit√© et l‚Äôadaptation de l‚Äôapplication √† diff√©rentes plateformes, en permettant de modifier l‚Äôenvironnement d‚Äôex√©cution sans avoir √† modifier le code ou les donn√©es.\nLe prochain chapitre sera consacr√© √† la gestion des d√©pendances. Il illustrera la mani√®re dont environnement d‚Äôex√©cution et code d‚Äôun projet peuvent √™tre reli√©s afin de cr√©er de la portabilit√©.\nL‚Äôex√©cution d‚Äôun code peut d√©pendre d‚Äô√©l√©ments de configuration comme des jetons d‚Äôauthentification ou des mots de passe de connexion √† des services qui sont personnels. Ces √©l√©ments de configuration n‚Äôont pas vocation √† √™tre partag√©s par du code et il est recommand√© de les exclure du code. La meilleure mani√®re de transformer ces configurations en param√®tre est de les isoler dans un script s√©par√©, qui n‚Äôest pas partag√©, et utiliser les variables cr√©√©es √† cette occasion dans le reste du programme.\nLa mani√®re privil√©gi√©e de conserver ce type d‚Äôinformation est le format YAML. Ce format de fichier permet de stocker des informations de mani√®re hi√©rarchis√©e et flexible, mais de mani√®re plus lisible que le JSON. Ce format sera transform√© en dictionnaire Python ce qui permet des recherches facilit√©es.\nPrenons le YAML suivant :\n\n\nsecrets.yaml\n\ntoken:\n    api_insee: \"toto\"\n    api_github: \"tokengh\"\npwd:\n    base_pg: \"monmotdepasse\"\n\nL‚Äôimport de ce fichier se fait avec le package yaml de la mani√®re suivante :\nimport yaml\n\nwith open('secrets.yaml') as f:\n    secrets = yaml.safe_load(f)\n\n# utilisation du secret\njeton_insee = secrets['token']['api_insee']"
  },
  {
    "objectID": "chapters/projects-architecture.html#les-cookiecutters",
    "href": "chapters/projects-architecture.html#les-cookiecutters",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Les cookiecutters",
    "text": "Les cookiecutters\nEn Python il existe des mod√®les de structure de projets : les cookiecutters. Il s‚Äôagit de mod√®les d‚Äôarborescences de fichiers (fichiers Python mais √©galement tout type de fichiers) propos√©s par la communaut√© et t√©l√©chargeables comme point de d√©part d‚Äôun projet.\nL‚Äôid√©e de cookiecutter est de proposer des templates que l‚Äôon utilise pour initialiser un projet, afin de b√¢tir √† l‚Äôavance une structure √©volutive. On va s‚Äôinspirer de la structure du template datascience d√©velopp√© par la communaut√©. La syntaxe √† utiliser dans ce cas est la suivante :\n\n\nterminal\n\n$ $ pip install cookiecutter\n$ $ cookiecutter https://github.com/drivendata/cookiecutter-data-science\n\nLe mod√®le est personnalisable, notamment pour faciliter l‚Äôinteraction entre un syst√®me de stockage distant. L‚Äôarborescence g√©n√©r√©e est assez massive pour permettre une grande diversit√© de projet. Il n‚Äôest souvent pas n√©cessaire d‚Äôavoir toutes les composantes du cookiecutter.\n\n\nStructure compl√®te g√©n√©r√©e par le cookiecutter data science\n\n‚îú‚îÄ‚îÄ LICENSE\n‚îú‚îÄ‚îÄ Makefile           &lt;- Makefile with commands like `make data` or `make train`\n‚îú‚îÄ‚îÄ README.md          &lt;- The top-level README for developers using this project.\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ external       &lt;- Data from third party sources.\n‚îÇ   ‚îú‚îÄ‚îÄ interim        &lt;- Intermediate data that has been transformed.\n‚îÇ   ‚îú‚îÄ‚îÄ processed      &lt;- The final, canonical data sets for modeling.\n‚îÇ   ‚îî‚îÄ‚îÄ raw            &lt;- The original, immutable data dump.\n‚îÇ\n‚îú‚îÄ‚îÄ docs               &lt;- A default Sphinx project; see sphinx-doc.org for details\n‚îÇ\n‚îú‚îÄ‚îÄ models             &lt;- Trained and serialized models, model predictions, or model summaries\n‚îÇ\n‚îú‚îÄ‚îÄ notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.\n‚îÇ                         `1.0-jqp-initial-data-exploration`.\n‚îÇ\n‚îú‚îÄ‚îÄ references         &lt;- Data dictionaries, manuals, and all other explanatory materials.\n‚îÇ\n‚îú‚îÄ‚îÄ reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n‚îÇ   ‚îî‚îÄ‚îÄ figures        &lt;- Generated graphics and figures to be used in reporting\n‚îÇ\n‚îú‚îÄ‚îÄ requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n‚îÇ                         generated with `pip freeze &gt; requirements.txt`\n‚îÇ\n‚îú‚îÄ‚îÄ setup.py           &lt;- Make this project pip installable with `pip install -e`\n‚îú‚îÄ‚îÄ src                &lt;- Source code for use in this project.\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py    &lt;- Makes src a Python module\n‚îÇ   ‚îÇ\n‚îÇ   ‚îú‚îÄ‚îÄ data           &lt;- Scripts to download or generate data\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ make_dataset.py\n‚îÇ   ‚îÇ\n‚îÇ   ‚îú‚îÄ‚îÄ features       &lt;- Scripts to turn raw data into features for modeling\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py\n‚îÇ   ‚îÇ\n‚îÇ   ‚îú‚îÄ‚îÄ models         &lt;- Scripts to train models and then use trained models to make\n‚îÇ   ‚îÇ   ‚îÇ                 predictions\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict_model.py\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train_model.py\n‚îÇ   ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ visualization  &lt;- Scripts to create exploratory and results oriented visualizations\n‚îÇ       ‚îî‚îÄ‚îÄ visualize.py\n‚îÇ\n‚îî‚îÄ‚îÄ tox.ini            &lt;- tox file with settings for running tox; see tox.readthedocs.io\n\n\n\n\n\n\n\nTests unitaires\n\n\n\n\n\nLes tests unitaires sont des tests automatis√©s qui v√©rifient le bon fonctionnement d‚Äôune unit√© de code, comme une fonction ou une m√©thode. L‚Äôobjectif est de s‚Äôassurer que chaque unit√© de code fonctionne correctement avant d‚Äô√™tre int√©gr√©e dans le reste du programme.\nLes tests unitaires sont utiles lorsqu‚Äôon travaille sur un code de taille cons√©quente ou lorsqu‚Äôon partage son code √† d‚Äôautres personnes, car ils permettent de s‚Äôassurer que les modifications apport√©es ne cr√©ent pas de nouvelles erreurs.\nEn Python, on peut utiliser le package unittest pour √©crire des tests unitaires. Voici un exemple tir√© de ce site :\n# fichier test_str.py\nimport unittest\n\n\nclass ChaineDeCaractereTest(unittest.TestCase):\n\n    def test_reversed(self):\n        resultat = reversed(\"abcd\")\n        self.assertEqual(\"dcba\", \"\".join(resultat))\n\n    def test_sorted(self):\n        resultat = sorted(\"dbca\")\n        self.assertEqual(['a', 'b', 'c', 'd'], resultat)\n\n    def test_upper(self):\n        resultat = \"hello\".upper()\n        self.assertEqual(\"HELLO\", resultat)\n\n    def test_erreur\n\n\nif __name__ == '__main__':\n    unittest.main()\nPour v√©rifier que les tests fonctionnent, on ex√©cute ce script depuis la ligne de commande :\npython3 test_str.py\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nSi on √©crit des tests unitaires, il est important de les maintenir !\nPrendre du temps pour √©crire des tests unitaires qui ne sont pas maintenus et donc ne renvoie plus de diagnostics pertinents est du temps perdu."
  },
  {
    "objectID": "chapters/projects-architecture.html#transformer-son-projet-en-package-python",
    "href": "chapters/projects-architecture.html#transformer-son-projet-en-package-python",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Transformer son projet en package Python",
    "text": "Transformer son projet en package Python\nLe package est la structure aboutie d‚Äôun projet Python autosuffisant. Il s‚Äôagit d‚Äôune mani√®re formelle de contr√¥ler la reproductibilit√© d‚Äôun projet car :\n\nle package assure une gestion coh√©rente des d√©pendances\nle package offre une certaine structure pour la documentation\nle package facilite la r√©utilisation du code\nle package permet des √©conomies d‚Äô√©chelle, car on peut r√©utiliser l‚Äôun des packages pour un autre projet\nle package facilite le debuggage car il est plus facile d‚Äôidentifier une erreur quand elle est dans un package\n‚Ä¶\n\nEn Python, le package est une structure peu contraignante si on a adopt√© les bonnes pratiques de structuration de projet. √Ä partir de la structure modulaire pr√©c√©demment √©voqu√©e, il n‚Äôy a qu‚Äôun pas vers le package : l‚Äôajout d‚Äôun fichier pyproject.toml qui contr√¥le la construction du package (voir ici).\nIl existe plusieurs outils pour installer un package dans le syst√®me √† partir d‚Äôune structure de fichiers locale. Les deux principaux sont\n\nsetuptools\npoetry\n\nLe package fait la transition entre un code modulaire et un code portable, concept sur lequel nous reviendrons dans le prochain chapitre."
  },
  {
    "objectID": "chapters/projects-architecture.html#footnotes",
    "href": "chapters/projects-architecture.html#footnotes",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA cet √©gard, Python est beaucoup plus fiable que R. Dans R, si deux scripts utilisent des fonctions dont le nom est identique mais issues de packages diff√©rents, il y aura un conflit. En Python chaque module sera import√© comme un package en soi.‚Ü©Ô∏é"
  }
]