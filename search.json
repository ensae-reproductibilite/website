[
  {
    "objectID": "chapters/advanced-notions.html",
    "href": "chapters/advanced-notions.html",
    "title": "Des ressources pour aller plus loin dans l‚Äôindustrialisation de son projet",
    "section": "",
    "text": "Ce qu‚Äôon aurait aimer faire mais on n‚Äôavait pas encore eu le temps de l‚Äôint√©grer :\n\nMamba\nTests unitaires\nMonitoring\n\nLogging\n\nVersioning avanc√©\n\nData version control\nMLOps\n\nOptimisation de performance\n\nSin of early optimisation\nProfiling\nRessources\n\nR : Advanced R, R inferno\nPython : High Performance Python\n\n\nBonnes pratiques Dockerfile\nEnjeux de s√©curit√©"
  },
  {
    "objectID": "chapters/application.html",
    "href": "chapters/application.html",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "",
    "text": "L‚Äôobjectif de cette mise en application est d‚Äôillustrer les diff√©rentes √©tapes qui s√©parent la phase de d√©veloppement d‚Äôun projet de celle de la mise en production. Elle permettra de mettre en pratique les diff√©rents concepts pr√©sent√©s tout au long du cours.\nNous nous pla√ßons dans une situation initiale correspondant √† la fin de la phase de d√©veloppement d‚Äôun projet de data science. On a un notebook un peu monolithique, qui r√©alise les √©tapes classiques d‚Äôun pipeline de machine learning :\nL‚Äôobjectif est d‚Äôam√©liorer le projet de mani√®re incr√©mentale jusqu‚Äô√† pouvoir le mettre en production, en le valorisant sous une forme adapt√©e."
  },
  {
    "objectID": "chapters/application.html#etape-0-forker-le-d√©p√¥t-dexemple-et-cr√©er-une-branche-de-travail",
    "href": "chapters/application.html#etape-0-forker-le-d√©p√¥t-dexemple-et-cr√©er-une-branche-de-travail",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 0: forker le d√©p√¥t d‚Äôexemple et cr√©er une branche de travail",
    "text": "Etape 0: forker le d√©p√¥t d‚Äôexemple et cr√©er une branche de travail\n\nOuvrir un service VSCode sur le SSP Cloud. Vous pouvez aller dans la page My Services et cliquer sur New service. Sinon, vous pouvez lancer le service en cliquant directement ici.\nG√©n√©rer un jeton d‚Äôacc√®s (token) sur GitHub afin de permettre l‚Äôauthentification en ligne de commande √† votre compte. La proc√©dure est d√©crite ici. Garder le jeton g√©n√©r√© de c√¥t√©.\nForker le d√©p√¥t Github : https://github.com/linogaliana/ensae-reproductibilite-application\nCl√¥ner votre d√©p√¥t Github en utilisant le terminal depuis Visual Studio (Terminal > New Terminal) :\n\n$ git clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application.git\no√π <TOKEN> et <USERNAME> sont √† remplacer, respectivement, par le jeton que vous avez g√©n√©r√© pr√©c√©demment et votre nom d‚Äôutilisateur.\n\nSe placer avec le terminal dans le dossier en question :\n\n$ cd ensae-reproductibilite-application\n\nCr√©ez une branche nettoyage :\n\n$ git checkout -b nettoyage\nSwitched to a new branch 'nettoyage'"
  },
  {
    "objectID": "chapters/application.html#etape-1-sassurer-que-le-script-sex√©cute-correctement",
    "href": "chapters/application.html#etape-1-sassurer-que-le-script-sex√©cute-correctement",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 1 : s‚Äôassurer que le script s‚Äôex√©cute correctement",
    "text": "Etape 1 : s‚Äôassurer que le script s‚Äôex√©cute correctement\nOn va partir du fichier notebook.py qui reprend le contenu du notebook1 mais dans un script classique.\nLa premi√®re √©tape est simple, mais souvent oubli√©e : v√©rifier que le code fonctionne correctement.\n\n\n\n\n\n\nApplication 1: corriger les erreurs\n\n\n\n\nOuvrir dans VSCode le script titanic.py ;\nEx√©cuter le script ligne √† ligne pour d√©tecter les erreurs ;\nCorriger les deux erreurs qui emp√™chent la bonne ex√©cution ;\nV√©rifier le fonctionnement du script en utilisant la ligne de commande\n\npython titanic.py\n\n\nIl est maintenant temps de commit les changements effectu√©s avec Git2 :\n$ git add titanic.py\n$ git commit -m \"Corrige l'erreur qui emp√™chait l'ex√©cution\"\n$ git push\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\nScript checkpoint"
  },
  {
    "objectID": "chapters/application.html#etape-2-utiliser-un-linter-puis-un-formatter",
    "href": "chapters/application.html#etape-2-utiliser-un-linter-puis-un-formatter",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 2: utiliser un linter puis un formatter",
    "text": "Etape 2: utiliser un linter puis un formatter\nOn va maintenant am√©liorer la qualit√© de notre code en appliquant les standards communautaires. Pour cela, on va utiliser le linter classique PyLint.\n\n\n\n\n\n\nNote\n\n\n\nN‚Äôh√©sitez pas √† taper un code d‚Äôerreur sur un moteur de recherche pour obtenir plus d‚Äôinformations si jamais le message n‚Äôest pas clair !\n\n\nPour appliquer le linter √† un script .py, la syntaxe √† entrer dans le terminal est la suivante :\n$ pylint mon_script.py\n\n\n\n\n\n\nImportant\n\n\n\nPyLint et Black sont des packages Python qui s‚Äôutilisent principalement en ligne de commande.\nSi vous avez une erreur qui sugg√®re que votre terminal ne connait pas PyLint ou Black, n‚Äôoubliez pas d‚Äôex√©cuter la commande pip install pylint ou pip install black.\n\n\nLe linter renvoie alors une s√©rie d‚Äôirr√©gularit√©s, en pr√©cisant √† chaque fois la ligne de l‚Äôerreur et le message d‚Äôerreur associ√© (ex : mauvaise identation). Il renvoie finalement une note sur 10, qui estime la qualit√© du code √† l‚Äôaune des standards communautaires √©voqu√©s dans la partie Qualit√© du code.\n\n\n\n\n\n\nApplication 2: rendre lisible le script\n\n\n\n\nDiagnostiquer et √©valuer la qualit√© de titanic.py avec PyLint. Regarder la note obtenue.\nUtiliser black titanic.py --diff --color pour observer les changements de forme que va induire l‚Äôutilisation du formatter Black\nAppliquer le formatter Black\nR√©utiliser PyLint pour diagnostiquer l‚Äôam√©lioration de la qualit√© du script et le travail qui reste √† faire.\nComme la majorit√© du travail restant est √† consacrer aux imports:\n\nMettre tous les imports ensemble en d√©but de script\nRetirer les imports redondants en s‚Äôaidant des diagnostics de votre √©diteur\nR√©ordonner les imports si PyLint vous indique de le faire\nCorriger les derni√®res fautes formelles sugg√©r√©es par PyLint\n\nD√©limiter des parties dans votre code pour rendre sa structure plus lisible\n\n\n\nLe code est maintenant lisible, il obtient √† ce stade une note formelle proche de 10. Mais il n‚Äôest pas encore totalement intelligible ou fiable. Il y a notamment beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. N√©anmoins, avant cela, occupons-nous de mieux g√©rer certains param√®tres du script: jetons d‚ÄôAPI et chemin des fichiers.\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ntitanic.py"
  },
  {
    "objectID": "chapters/application.html#etape-3-gestion-des-param√®tres",
    "href": "chapters/application.html#etape-3-gestion-des-param√®tres",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 3: gestion des param√®tres",
    "text": "Etape 3: gestion des param√®tres\nL‚Äôex√©cution du code et les r√©sultats obtenus d√©pendent de certains param√®tres. L‚Äô√©tude de r√©sultats alternatifs, en jouant sur des variantes des param√®tres, est √† ce stade compliqu√©e car il est n√©cessaire de parcourir le code pour trouver ces param√®tres. De plus, certains param√®tres personnels comme des jetons d‚ÄôAPI ou des mots de passe n‚Äôont pas vocation √† √™tre pr√©sents dans le code.\nIl est plus judicieux de consid√©rer ces param√®tres comme des variables d‚Äôentr√©e du script. Cela peut √™tre fait de deux mani√®res:\n\nAvec des arguments optionnels appel√©s depuis la ligne de commande. Cela peut √™tre pratique pour mettre en oeuvre des tests automatis√©s3 mais n‚Äôest pas forc√©ment pertinent pour toutes les variables. Nous allons montrer cet usage avec le nombre d‚Äôarbres de notre random forest ;\nEn utilisant un fichier de configuration dont les valeurs sont import√©es dans le script principal. Nous allons le mettre en oeuvre pour deux types de fichiers: les √©l√©ments de configuration √† partager et ceux √† conserver pour soi mais pouvant servir.\n\n\n\n\n\n\n\nApplication 3: Param√©trisation du script\n\n\n\n\nEn s‚Äôinspirant de cette r√©ponse, cr√©er une variable n_trees qui peut √©ventuellement √™tre param√©tr√©e en ligne de commande et dont la valeur par d√©faut est 20.\nTester cette param√©trisation en ligne de commande avec la valeur par d√©faut puis 2, 10 et 50 arbres\nRep√©rer le jeton d‚ÄôAPI dans le code. Retirer le jeton d‚ÄôAPI du code et cr√©er √† la racine du projet un fichier YAML nomm√© secrets.yaml o√π vous √©crivez ce secret sous la forme key: value\nPour √©viter d‚Äôavoir √† le faire plus tard, cr√©er une fonction import_yaml_config qui prend en argument le chemin d‚Äôun fichier YAML et renvoie le contenu de celui-ci en output. Vous pouvez suivre le conseil du chapitre sur la Qualit√© du code en adoptant le type hinting.\nCr√©er la variable API_TOKEN ayant la valeur stock√©e dans secrets.yaml4.\nTester en ligne de commande que l‚Äôex√©cution du fichier est toujours sans erreur\nRefaire un diagnostic avec PyLint et corriger les √©ventuels messages.\nCr√©er un fichier config.yaml stockant trois informations: le chemin des donn√©es d‚Äôentra√Ænement, des donn√©es de test et la r√©partition train/test utilis√©e dans le code. Cr√©er les variables correspondantes dans le code apr√®s avoir utilis√© import_yaml_config\nCr√©er un fichier .gitignore. Ajouter dans ce fichier secrets.yaml car il ne faut pas committer ce fichier.\nCr√©er un fichier README.md o√π vous indiquez qu‚Äôil faut cr√©er un fichier secrets.yaml pour pouvoir utiliser l‚ÄôAPI.\n\n\n\nIndice si vous ne trouvez pas comment lire un fichier YAML\n\nSi le fichier s‚Äôappelle toto.yaml, vous pouvez l‚Äôimporter de cette mani√®re:\nwith open(\"toto.yaml\", \"r\", encoding=\"utf-8\") as stream:\n    dict_config = yaml.safe_load(stream)\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\ntitanic.py\nREADME.md\nconfig.yaml\nsecrets.yaml\n.gitignore"
  },
  {
    "objectID": "chapters/application.html#etape-4-adopter-la-programmation-fonctionnelle",
    "href": "chapters/application.html#etape-4-adopter-la-programmation-fonctionnelle",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 4 : Adopter la programmation fonctionnelle",
    "text": "Etape 4 : Adopter la programmation fonctionnelle\nNous allons mettre en fonctions les parties importantes de l‚Äôanalyse, et les mettre dans un module afin de pouvoir les importer directement depuis le notebook.\nCet exercice √©tant chronophage, il n‚Äôest pas obligatoire de le r√©aliser en entier. L‚Äôimportant est de comprendre la d√©marche et d‚Äôadopter fr√©quemment une approche fonctionnelle5. Pour obtenir une chaine enti√®rement fonctionnalis√©e, vous pouvez reprendre le checkpoint.\n\n\n\n\n\n\nApplication 4: adoption des standards de programmation fonctionnelle\n\n\n\n\nCr√©er une fonction qui importe les donn√©es d‚Äôentra√Ænement (train.csv) et de test (test.csv) et renvoie des DataFrames Pandas ;\nEn fonction du temps disponible, cr√©er plusieurs fonctions pour r√©aliser les √©tapes de feature engineering:\n\nLa cr√©ation de la variable ‚ÄúTitle‚Äù peut √™tre automatis√©e en vertu du principe ‚Äúdo not repeat yourself‚Äù6.\nRegrouper ensemble les fillna et essayer de cr√©er une fonction g√©n√©ralisant l‚Äôop√©ration.\nLes label encoders peuvent √™tre transform√©s en deux fonctions: une premi√®re pour encoder une colonne puis une seconde qui utilise la premi√®re de mani√®re r√©p√©t√©e pour encoder plusieurs colonnes. Remarquez les erreurs de copier-coller que cela corrige\nFinaliser les derni√®res transformations avec des fonctions\n\nCr√©er une fonction qui r√©alise le split train/test de validation en fonction d‚Äôun param√®tre repr√©sentant la proportion de l‚Äô√©chantillon de test.\nCr√©er une fonction qui entra√Æne et √©value un classifieur RandomForest, et qui prend en param√®tre le nombre d‚Äôarbres (n_estimators). La fonction doit imprimer √† la fin la performance obtenue et la matrice de confusion.\nD√©placer toutes les fonctions ensemble, en d√©but de script. \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLe fait d‚Äôappliquer des fonctions a d√©j√† am√©lior√© la fiabilit√© du processus en r√©duisant le nombre d‚Äôerreurs de copier-coller. N√©anmoins, pour vraiment fiabiliser le processus, il faudrait utiliser un pipeline de transformations de donn√©es.\nCeci n‚Äôest pas encore au programme du cours mais le sera dans une prochaine version.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\ntitanic.py\n\nLes autres fichiers inchang√©s:\n\nREADME.md\nconfig.yaml\nsecrets.yaml\n.gitignore"
  },
  {
    "objectID": "chapters/application.html#etape-1-modularisation",
    "href": "chapters/application.html#etape-1-modularisation",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 1 : modularisation",
    "text": "Etape 1 : modularisation\nFini le temps de l‚Äôexp√©rimentation : on va maintenant essayer de se passer compl√®tement du notebook. Pour cela, on va utiliser un main script, c‚Äôest √† dire un script qui reproduit l‚Äôanalyse en important et en ex√©cutant les diff√©rentes fonctions dans l‚Äôordre attendu.\n\n\n\n\n\n\nApplication 5: modularisation\n\n\n\n\nD√©placer les fonctions dans une s√©rie de fichiers d√©di√©s:\n\nimport_data.py: fonctions d‚Äôimport de donn√©es\nbuild_features.py: fonctions regroupant les √©tapes de feature engineering\ntrain_evaluate.py: fonctions d‚Äôentrainement et d‚Äô√©valuation du mod√®le\n\nSp√©cifier les d√©pendances (i.e.¬†les packages √† importer) dans les modules pour que ceux-ci puissent s‚Äôex√©cuter ind√©pendamment ;\nRenommer titanic.py en main.py pour suivre la convention de nommage des projets Python ;\nImporter les fonctions n√©cessaires √† partir des modules. ‚ö†Ô∏è Ne pas utiliser from XXX import *, ce n‚Äôest pas une bonne pratique !\nV√©rifier que tout fonctionne bien en ex√©cutant le script main √† partir de la ligne de commande :\n\n$ python main.py\n\n\nOn dispose maintenant d‚Äôune application Python fonctionnelle. N√©anmoins, le projet est certes plus fiable mais sa structuration laisse √† d√©sirer et il serait difficile de rentrer √† nouveau dans le projet dans quelques temps.\n\n\nEtat actuel du projet üôà\n\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ train.csv\n‚îú‚îÄ‚îÄ test.csv\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ config.yaml\n‚îú‚îÄ‚îÄ secrets.yaml\n‚îú‚îÄ‚îÄ import_data.py\n‚îú‚îÄ‚îÄ build_features.py\n‚îú‚îÄ‚îÄ train_evaluate.py\n‚îî‚îÄ‚îÄmain.py\n\nComme cela est expliqu√© dans la partie Structure des projets, on va adopter une structure certes arbitraire mais qui va faciliter l‚Äôautodocumentation de notre projet.\nDe plus, une telle structure va faciliter des √©volutions optionnelles comme la packagisation du projet. Passer d‚Äôune structure modulaire bien faite √† un package est quasi-imm√©diat en Python.\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\nbuild_features.py\nimport_data.py\ntrain_evaluate.py\nmain.py\n\nLes autres fichiers inchang√©s:\n\nREADME.md\nconfig.yaml\nsecrets.yaml\n.gitignore"
  },
  {
    "objectID": "chapters/application.html#etape-2-adopter-une-architecture-standardis√©e-de-projet",
    "href": "chapters/application.html#etape-2-adopter-une-architecture-standardis√©e-de-projet",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 2 : adopter une architecture standardis√©e de projet",
    "text": "Etape 2 : adopter une architecture standardis√©e de projet\nOn va maintenant modifier l‚Äôarchitecture de notre projet pour la rendre plus standardis√©e. Pour cela, on va s‚Äôinspirer des structures cookiecutter qui g√©n√®rent des templates de projet.\nOn va s‚Äôinspirer de la structure du template datascience d√©velopp√© par la communaut√©.\n\n\n\n\n\n\nNote\n\n\n\nL‚Äôid√©e de cookiecutter est de proposer des templates que l‚Äôon utilise pour initialiser un projet, afin de b√¢tir √† l‚Äôavance une structure √©volutive. La syntaxe √† utiliser dans ce cas est la suivante :\n$ pip install cookiecutter\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science\nIci, on a d√©j√† un projet, on va donc faire les choses dans l‚Äôautre sens : on va s‚Äôinspirer de la structure propos√©e afin de r√©organiser celle de notre projet selon les standards communautaires.\n\n\nEn s‚Äôinspirant du cookiecutter data science on va adopter la structure suivante:\nensae-reproductibilite-application\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îî‚îÄ‚îÄ raw\n‚îÇ       ‚îú‚îÄ‚îÄ test.csv\n‚îÇ       ‚îî‚îÄ‚îÄ train.csv\n‚îú‚îÄ‚îÄ configuration\n‚îÇ   ‚îú‚îÄ‚îÄ secrets.yaml\n‚îÇ   ‚îî‚îÄ‚îÄ config.yaml\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb\n‚îî‚îÄ‚îÄ src\n    ‚îú‚îÄ‚îÄ data\n    ‚îÇ   ‚îî‚îÄ‚îÄ import_data.py\n    ‚îú‚îÄ‚îÄ features\n    ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py\n    ‚îî‚îÄ‚îÄ models\n        ‚îî‚îÄ‚îÄ train_evaluate.py\n\n\n\n\n\n\nApplication 6: adopter une structure lisible\n\n\n\n\n(optionnel) Analyser et comprendre la structure de projet propos√©e par le template\nModifier l‚Äôarborescence du projet selon le mod√®le\nAdapter les scripts et les fichiers de configuration √† la nouvelle arborescence\nAjouter le dossier pycache au .gitignore7 et le dossier data\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\nbuild_features.py\nimport_data.py\ntrain_evaluate.py\nmain.py\n\nLes autres fichiers sont inchang√©s, √† l‚Äôexception de leur emplacement."
  },
  {
    "objectID": "chapters/application.html#conda-export",
    "href": "chapters/application.html#conda-export",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 6 : fixer l‚Äôenvironnement d‚Äôex√©cution",
    "text": "Etape 6 : fixer l‚Äôenvironnement d‚Äôex√©cution\nAfin de favoriser la portabilit√© du projet, il est d‚Äôusage de ‚Äúfixer l‚Äôenvironnement‚Äù, c‚Äôest √† dire d‚Äôindiquer dans un fichier toutes les d√©pendances utilis√©es ainsi que leurs version. Il est conventionnellement localis√© √† la racine du projet.\nSur le VSCode du SSP Cloud, on se situe dans un environnement conda. La commande pour exporter un environnement conda est la suivante :\n$ conda env export > environment.yml\nVous devriez √† pr√©sent avoir un fichier environement.yml √† la racine de votre projet, qui contient les d√©pendances et leurs versions.\n{{% box status=‚Äútip‚Äù title=‚ÄúNote‚Äù icon=‚Äúfa fa-hint‚Äù %}} En r√©alit√©, on aun peu trich√© : on a export√© l‚Äôenvironnement de base du VSCode SSP Cloud, qui contient beaucoup plus de packages que ceux utilis√©s par notre projet. On verra dans la Partie 2 de l‚Äôapplication comment fixer proprement les d√©pendances de notre projet. {{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#stockageS3",
    "href": "chapters/application.html#stockageS3",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 7 : stocker les donn√©es de mani√®re externe",
    "text": "Etape 7 : stocker les donn√©es de mani√®re externe\n{{% box status=‚Äúwarning‚Äù title=‚ÄúWarning‚Äù icon=‚Äúfa fa-exclamation-triangle‚Äù %}} Cette √©tape n‚Äôest pas facile. Vous devrez suivre la documentation du SSP Cloud pour la r√©aliser. Une aide-m√©moire est √©galement disponible dans le cours de Python pour les data-scientists {{% /box %}}\nComme on l‚Äôa vu dans le cours, les donn√©es ne sont pas cens√©es √™tre versionn√©es sur un projet Git. L‚Äôid√©al pour √©viter cela tout en maintenant la reproductibilit√© est d‚Äôutiliser une solution de stockage externe. On va utiliser pour cela MinIO, la solution de stockage de type S3 offerte par le SSP Cloud.\n\ncr√©er un dossier ensae-reproductibilite dans votre bucket personnel via l‚Äôinterface utilisateur\nmodifier votre fonction d‚Äôimport des donn√©es pour qu‚Äôelle r√©cup√®re les donn√©es √† partir de MinIO. Elle devra prendre en param√®tres le nom du bucket et le dossier dans lequel sont contenues les donn√©es sur MinIO.\nmodifier le main script pour appeler la fonction avec les param√®tres propres √† votre compte\nsupprimer les fichiers .csv du dossier data de votre projet, on n‚Äôen a plus besoin vu qu‚Äôon les importe de l‚Äôext√©rieur\nv√©rifier le bon fonctionnement de votre application"
  },
  {
    "objectID": "chapters/application.html#configyaml",
    "href": "chapters/application.html#configyaml",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 1: cr√©er un r√©pertoire de variables servant d‚Äôinput",
    "text": "Etape 1: cr√©er un r√©pertoire de variables servant d‚Äôinput\n\nEnjeu\nLors de l‚Äô√©tape 7, nous avons am√©lior√© la qualit√© du script en s√©parant stockage et code. Cependant, peut-√™tre avez-vous remarqu√© que nous avons introduit un nom de bucket personnel dans le script (voir le fichier main.py). Il s‚Äôagit typiquement du genre de petit vice cach√© d‚Äôun script qui peut g√©n√©rer une erreur: vous n‚Äôavez pas acc√®s au bucket en question donc si vous essayez de faire tourner ce script en l‚Äô√©tat, vous allez rencontrer une erreur.\nUne bonne pratique pour g√©rer ce type de configuration est d‚Äôutiliser un fichier YAML qui stocke de mani√®re hi√©rarchis√©e les variables globales 8.\nEn l‚Äôoccurrence, nous n‚Äôavons besoin que de deux √©l√©ments pour pouvoir d√©-personnaliser ce script :\n\nle nom du bucket\nl‚Äôemplacement dans le bucket\n\n\n\nApplication\nDans VSCode, cr√©er un fichier nomm√© config.yaml et le localiser √† la racine de votre d√©p√¥t. Voici, une proposition de hi√©rarchisation de l‚Äôinformation que vous devez adapter √† votre nom d‚Äôutilisateur :\ninput:\n  bucket: \"lgaliana\"\n  path: \"ensae-reproductibilite\"\nDans main.py, importer ce fichier et remplacer la ligne pr√©c√©demment √©voqu√©e par les valeurs du fichier. Tester en faisant tourner main.py"
  },
  {
    "objectID": "chapters/application.html#anaconda",
    "href": "chapters/application.html#anaconda",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 2 : cr√©er un environnement conda √† partir du fichier environment.yml",
    "text": "Etape 2 : cr√©er un environnement conda √† partir du fichier environment.yml\nL‚Äôenvironnement conda cr√©√© avec conda env export (√©tape 6) contient √©norm√©ment de d√©pendances, dont de nombreuses qui ne nous sont pas n√©cessaires (il en serait de m√™me avec pip freeze). Nous n‚Äôavons en effet besoin que des packages pr√©sents dans la section import de nos scripts et les d√©pendances n√©cessaires pour que ces packages soient fonctionnels.\nVous allez chercher √† obtenir un environment.yml beaucoup plus parcimonieux que celui g√©n√©r√© par conda env export\n\n{{% panel name=‚ÄúApproche g√©n√©rale üê®‚Äù %}}\nLe tableau r√©capitulatif pr√©sent dans la partie portabilit√© peut √™tre utile dans cette partie. L‚Äôid√©e est de partir from scratch et figer l‚Äôenvironnement qui permet d‚Äôavoir une appli fonctionnelle.\n\nCr√©er un environnement vide avec Python 3.10 \nActiver cet environnement\nInstaller en ligne de commande avec pip les packages n√©cessaires pour faire tourner votre code\n\n\n\nFaire un pip freeze > requirements.txt ou conda env export > environment.yml (privil√©gier la deuxi√®me option)\nRetirer la section prefix (si elle est pr√©sente) et changer la section name en monenv\n\n{{% /panel %}}\n{{% panel name=‚ÄúApproche fain√©ante ü¶•‚Äù %}}\nNous allons g√©n√©rer une version plus minimaliste gr√¢ce √† l‚Äôutilitaire pipreqs\n\nInstaller pipreqs en pip install\nEn ligne de commande, depuis la racine du projet, faire pipreqs\nOuvrir le requirements.txt automatiquement g√©n√©r√©. Il est beaucoup plus minimal que celui que vous obtiendriez avec pip freeze ou l‚Äôenvironment.yml obtenu √† l‚Äô√©tape 6.\nRemplacer toute la section dependencies du environment.yml par le contenu du requirements.txt (‚ö†Ô∏è ne pas oublier l‚Äôindentation et le tiret en d√©but de ligne)\n‚ö†Ô∏è Modifier le tiret √† scikit learn. Il ne faut pas un underscore mais un tiret\nAjouter la version de python (par exemple python=3.10.0) au d√©but de la section dependencies\nRetirer la section prefix du fichier environment.yml (si elle est pr√©sente) et changer le contenu de la section name en monenv\nCr√©er l‚Äôenvironnement (voir le tableau r√©capitulatif dans la partie portabilit√©)\n\n\n{{% /panel %}}\n{{% /panelset %}}\nMaintenant, il reste √† tester si tout fonctionne bien dans notre environnement plus minimaliste:\n\nActiver l‚Äôenvironnement\nTester votre script en ligne de commande\nFaire un commit quand vous √™tes contents"
  },
  {
    "objectID": "chapters/application.html#docker",
    "href": "chapters/application.html#docker",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 3: conteneuriser avec Docker ",
    "text": "Etape 3: conteneuriser avec Docker \n\nPr√©liminaire\n\nSe rendre sur l‚Äôenvironnement bac √† sable Play with Docker\nDans le terminal Linux, cloner votre d√©p√¥t Github \nCr√©er via la ligne de commande un fichier Dockerfile. Il y a plusieurs mani√®res de proc√©der, en voici un exemple:\n\necho \"#Dockerfile pour reproduire mon super travail\" > Dockerfile\n\nOuvrir ce fichier via l‚Äô√©diteur propos√© par l‚Äôenvironnement bac √† sable.\n\n\n\nCr√©ation d‚Äôun premier Dockerfile\n\n1Ô∏è‚É£ Comme couche de d√©part, partir d‚Äôune image l√©g√®re comme ubuntu:20.04\n2Ô∏è‚É£ Dans une deuxi√®me couche, faire un apt get -y update et installer wget qui va √™tre n√©cessaire pour t√©l√©charger Miniconda depuis la ligne de commande\n3Ô∏è‚É£ Dans la troisi√®me couche, nous allons installer Miniconda :\n\nT√©l√©charger la derni√®re version de Miniconda avec wget depuis l‚Äôurl de t√©l√©chargement direct https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nInstaller Miniconda dans le chemin /home/coder/local/bin/conda\nEffacer le fichier d‚Äôinstallation pour lib√©rer de la place sur l‚Äôimage\n\n4Ô∏è‚É£ En quatri√®me couche, on va installer mamba pour acc√©l√©rer l‚Äôinstallation des packages dans notre environnement.\n5Ô∏è‚É£ En cinqui√®me couche, nous allons cr√©er l‚Äôenvironnement conda:\n\nUtiliser COPY pour que Docker soit en mesure d‚Äôutiliser le fichier environment.yml (sinon Docker renverra une erreur)\nCr√©er l‚Äôenvironnement vide monenv (pr√©sentant uniquement Python 3.10) avec la commande conda ad√©quate\nMettre √† jour l‚Äôenvironnement en utilisant environment.yml avec mamba\n\n6Ô∏è‚É£ Utiliser ENV pour ajouter l‚Äôenvironnement monenv au PATH et utiliser le fix suivant:\n\nRUN echo \"export PATH=$PATH\" >> /home/coder/.bashrc  # Temporary fix while PATH gets overwritten by code-server\n\n7Ô∏è‚É£ Exposer sur le port 5000\n8Ô∏è‚É£ En derni√®re √©tape, utiliser CMD pour reproduire le comportement de python main.py\n\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint: mamba‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}} mamba est une alternative √† conda pour installer des packages dans un environnement Miniconda/Anaconda. mamba n‚Äôest pas obligatoire, conda peut suffire. Cependant, mamba est beaucoup plus rapide que conda pour installer des packages √† installer ; il s‚Äôagit donc d‚Äôun utilitaire tr√®s pratique. {{% /box %}}\n\n{{% panel name=‚ÄúIndications suppl√©mentaires‚Äù %}}\nCliquer sur les onglets ci-dessus üëÜ pour b√©n√©ficier d‚Äôindications suppl√©mentaires, pour vous aider. Cependant, essayez de ne pas les consulter imm√©diatement: n‚Äôh√©sitez pas √† t√¢tonner.\n{{% /panel %}}\n{{% panel name=‚ÄúInstallation de Miniconda‚Äù %}}\n# INSTALL MINICONDA -------------------------------\nARG CONDA_DIR=/home/coder/local/bin/conda\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nRUN bash Miniconda3-latest-Linux-x86_64.sh -b -p $CONDA_DIR\nRUN rm -f Miniconda3-latest-Linux-x86_64.sh\n{{% /panel %}}\n{{% panel name=‚ÄúInstallation de mamba‚Äù %}}\nENV PATH=\"/home/coder/local/bin/conda/bin:${PATH}\"\nRUN conda install mamba -n base -c conda-forge\n{{% /panel %}}\n{{% panel name=‚ÄúCr√©ation de l‚Äôenvironnement‚Äù %}}\nCOPY environment.yml .\nRUN conda create -n monenv python=3.10\nRUN mamba env update -n monenv -f environment.yml\n{{% /panel %}}\n\n\n\nConstruire l‚Äôimage\nMaintenant, nous avons d√©fini notre recette. Il nous reste √† faire notre plat et √† le go√ªter\n\nUtiliser docker build pour cr√©er une image avec le tag my-python-app\nV√©rifier les images dont vous disposez. Vous devriez avoir un r√©sultat proche de celui-ci\n\n\nREPOSITORY      TAG       IMAGE ID       CREATED         SIZE\nmy-python-app   latest    c0dfa42d8520   6 minutes ago   2.23GB\nubuntu          20.04     825d55fb6340   6 days ago      72.8MB\n\n\nTester l‚Äôimage: d√©couverte du cache\nIl ne reste plus qu‚Äô√† go√ªter la recette et voir si le plat est bon.\nUtiliser docker run avec l‚Äôoption it pour pouvoir appeler l‚Äôimage depuis son tag\n\n‚ö†Ô∏è üí£ üî• Docker ne sait pas o√π trouver le fichier main.py. D‚Äôailleurs, il ne connait pas d‚Äôautres fichiers de notre application qui sont n√©cessaires pour faire tourner le code: config.yaml et le dossier src\n\nAvant l‚Äô√©tape EXPOSE utiliser plusieurs ADD et/ou COPY pour que l‚Äôapplication dispose de tous les √©l√©ments minimaux pour √™tre en mesure de fonctionner\nRefaire tourner docker run \n\n{{% box status=‚Äútip‚Äù title=‚ÄúNote‚Äù icon=‚Äúfa fa-hint‚Äù %}} Ici, le cache permet d‚Äô√©conomiser beaucoup de temps. Par besoin de refaire tourner toutes les √©tapes, Docker agit de mani√®re intelligente en faisant tourner uniquement les nouvelles √©tapes. {{% /box %}}\n\n\nCorriger une faille de reproductibilit√©\nVous devriez rencontrer une erreur li√©e √† la variable d‚Äôenvironnement AWS_ENDPOINT_URL. C‚Äôest normal, elle est inconnue de cet environnement minimaliste. D‚Äôailleurs, Docker n‚Äôa aucune raison de conna√Ætre votre espace de stockage sur le S3 du SSP-Cloud si vous ne lui dites pas. Donc cet environnement ne sait pas comment acc√©der aux fichiers pr√©sents dans votre minio.\nVous allez r√©gler ce probl√®me avec les √©tapes suivantes, :\n\n1Ô∏è‚É£ Naviguer dans l‚Äôinterface du SSP-Cloud pour retrouver les liens d‚Äôacc√®s direct de vos fichiers\n2Ô∏è‚É£ Dans VSCode, les mettre dans config.yaml (faire de nouvelles cl√©s)\n3Ô∏è‚É£ Dans VSCode, modifier la fonction d‚Äôimport pour s‚Äôadapter √† ce changement.\n4Ô∏è‚É£ Faire un commit et pusher les fichiers\n5Ô∏è‚É£ Dans l‚Äôenvironnement bac √† sable, faire un pull pour r√©cup√©rer ces modifications\n6Ô∏è‚É£ Tester √† nouveau le build (l√† encore le cache est bien pratique !)\n\n\nüéâ A ce stade, la matrice de confusion doit fonctionner. Vous avez cr√©√© votre premi√®re application reproductible !"
  },
  {
    "objectID": "chapters/application.html#etape-pr√©liminaire",
    "href": "chapters/application.html#etape-pr√©liminaire",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape pr√©liminaire",
    "text": "Etape pr√©liminaire\nPour ne pas risquer de tout casser sur notre branche master, nous allons nous placer sur une branche nomm√©e dev:\n\nsi dans l‚Äô√©tape suivante vous appliquez la m√©thode la plus simple, vous allez pouvoir la cr√©er depuis l‚Äôinterface de Github ;\nsi vous utilisez l‚Äôautre m√©thode, vous allez devoir la cr√©er en local ( via la commande git checkout -b dev)"
  },
  {
    "objectID": "chapters/application.html#etape-1-mise-en-place-de-tests-automatis√©s",
    "href": "chapters/application.html#etape-1-mise-en-place-de-tests-automatis√©s",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 1: mise en place de tests automatis√©s",
    "text": "Etape 1: mise en place de tests automatis√©s\nAvant d‚Äôessayer de mettre en oeuvre la cr√©ation de notre image Docker de mani√®re automatis√©e, nous allons pr√©senter la logique de l‚Äôint√©gration continue en g√©n√©ralisant les √©valuations de qualit√© du code avec le linter\n\n{{% panel name=‚ÄúUtilisation d‚Äôun template Github üê±‚Äù %}}\nMethode la plus simple: utilisation d‚Äôun template Github\nSi vous cliquez sur l‚Äôonglet Actions de votre d√©p√¥t, Github vous propose des workflows standardis√©s reli√©s √† Python. Choisir l‚Äôoption Python Package using Anaconda.\nwarning: Nous n‚Äôallons modifier que deux √©l√©ments de ce fichier.\n1Ô∏è‚É£ La derni√®re √©tape (Test with pytest) ne nous est pas n√©cessaire car nous n‚Äôavons pas de tests unitaires Nous allons donc remplacer celle-ci par l‚Äôutilisation de pylint pour avoir une note de qualit√© du package.\n\nUtiliser pylint √† cette √©tape pour noter les scripts ;\nVous pouvez fixer un score minimal √† 5 (option --fail-under=5)\n\n2Ô∏è‚É£ Mettre entre guillements la version de Python pour que celle-ci soit reconnue.\n3Ô∏è‚É£ Enfin, finaliser la cr√©ation de ce script:\n\nEn cliquant sur le bouton Start Commit, choisir la m√©thode Create a new branch for this commit and start a pull request en nommant la branche dev\nCr√©er la Pull Request en lui donnant un nom signifiant\n\n{{% /panel %}}\n{{% panel name=‚ÄúM√©thode manuelle‚Äù %}}\n‚ö†Ô∏è On est plut√¥t sur une m√©thode de gal√©rien. Il vaut mieux privil√©gier l‚Äôautre approche\nOn va √©diter depuis VisualStudio nos fichiers.\n\nCr√©er une branche dev en ligne de commande\nCr√©er un dossier .github/workflows via la ligne de commande ou l‚Äôexplorateur de fichier \nCr√©er un fichier .github/workflows/quality.yml.\n\nNous allons construire, par √©tape, une version simplifi√©e du Dockerfile pr√©sent dans ce post et dans celui-ci\n1Ô∏è‚É£ D‚Äôabord, d√©finissons des param√®tres pour indiquer √† Github quand faire tourner notre script:\n\nCommencez par nommer votre workflow par exemple Python Linting avec la cl√© name\nNous allons faire tourner ce workflow dans la branche master et dans la branche actuelle (dev). Ici, nous laissons de c√¥t√© les autres √©l√©ments (par exemple le fait de faire tourner √† chaque pull request). La cl√© on est d√©di√©e √† cet usage\n\n2Ô∏è‚É£ Ensuite, d√©fnissons le contexte d‚Äôex√©cution des t√¢ches (jobs) de notre script dans les options de la partie build:\n\nUtilisons une machine ubuntu-latest. Nous verrons plus tard comment am√©liorer cela.\n\n3Ô∏è‚É£ Nous allons ensuite m√©langer des √©tapes pr√©-d√©finies (des actions du marketplace) et des instructions que nous faisons :\n\nLe runner Github doit r√©cup√©rer le contenu de notre d√©p√¥t, pour cela utiliser l‚Äôaction checkout. Par rapport √† l‚Äôexemple, il convient d‚Äôajouter, pour le moment, un param√®tre ref avec le nom de la branche (par exemple dev)\nOn installe ensuite Python avec l‚Äôaction setup-python Pas besoin d‚Äôinstaller Python, on va utiliser l‚Äôoption conda-incubator/setup-miniconda@v2\nPour installer Python et l‚Äôenvironnement conda, on va plut√¥t utiliser l‚Äôastuce de ce blog avec l‚Äôoption conda-incubator/setup-miniconda@v2\nOn utilise ensuite flake8 et pylint (option --fail-under=5) pour effectuer des diagnostics de qualit√©\n\nIl ne reste plus qu‚Äô√† faire un commit et esp√©rer que cela fonctionne. Cela devrait donner le fichier suivant :\nname: Python Linting\non:\n  push:\n    branches: [master, dev]\njobs:\n  build:\n    runs-on: ubuntu-latest    \n    steps:\n      - uses: actions/checkout@v3\n        with:\n          ref: \"dev\"\n      - uses: conda-incubator/setup-miniconda@v2\n        with:\n          activate-environment: monenv\n          environment-file: environment.yml\n          python-version: '3.10'\n          auto-activate-base: false\n      - shell: bash -l {0}\n        run: |\n          conda info\n          conda list\n      - name: Lint with flake8\n        run: |\n          pip install flake8\n          flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics\n          flake8 src --count --max-complexity=10 --max-line-length=79 --statistics\n      - name: Lint with Pylint\n        run: |\n          pip install pylint\n          pylint src\n{{% /panel %}}\n\nMaintenant, nous pouvons observer que l‚Äôonglet Actions s‚Äôest enrichi. Chaque commit va entra√Æner une action pour tester nos scripts.\nSi la note est mauvaise, nous aurons une croix rouge (et nous recevrons un mail). On pourra ainsi d√©tecter, en d√©veloppant son projet, les moments o√π on d√©grade la qualit√© du script afin de la r√©tablir imm√©diatemment.\n{{% box status=‚Äúhint‚Äù title=‚ÄúUn linter sous forme de hook pre-commit‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nGit offre une fonctionalit√© int√©ressante lorsqu‚Äôon est puriste: les hooks. Il s‚Äôagit de r√®gles qui doivent √™tre satisfaites pour que le fichier puisse √™tre committ√©. Cela assurera que chaque commit remplisse des crit√®res de qualit√© afin d‚Äô√©viter le probl√®me de la procrastination.\nLa documentation de pylint offre des explications suppl√©mentaires.\n{{% /box %}}"
  },
  {
    "objectID": "chapters/application.html#etape-2-automatisation-de-la-livraison-de-limage-docker",
    "href": "chapters/application.html#etape-2-automatisation-de-la-livraison-de-limage-docker",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 2: Automatisation de la livraison de l‚Äôimage Docker",
    "text": "Etape 2: Automatisation de la livraison de l‚Äôimage Docker\nMaintenant, nous allons automatiser la mise √† disposition de notre image sur DockerHub. Cela facilitera sa r√©utilisation mais aussi des valorisations ult√©rieures.\nL√† encore, nous allons utiliser une s√©rie d‚Äôactions pr√©-configur√©es.\n1Ô∏è‚É£ Pour que Github puisse s‚Äôauthentifier aupr√®s de DockerHub, il va falloir d‚Äôabord interfacer les deux plateformes. Pour cela, nous allons utiliser un jeton (token) DockerHub que nous allons mettre dans un espace s√©curis√© associ√© √† votre d√©p√¥t Github. Cette d√©marche sera l√† m√™me ult√©rieurement lorsque nous connecterons notre d√©p√¥t √† un autre service tiers, √† savoir Netlify:\n\nSe rendre sur https://hub.docker.com/ et cr√©er un compte.\nAller dans les param√®tres (https://hub.docker.com/settings/general) et cliquer, √† gauche, sur Security\nCr√©er un jeton personnel d‚Äôacc√®s, ne fermez pas l‚Äôonglet en question, vous ne pouvez voir sa valeur qu‚Äôune fois.\nDans votre d√©p√¥t Github, cliquer sur l‚Äôonglet Settings et cliquer, √† gauche, sur Actions. Sur la page qui s‚Äôaffiche, cliquer sur New repository secret\nDonner le nom DOCKERHUB_TOKEN √† ce jeton et copier la valeur. Valider\nCr√©er un deuxi√®me secret nomm√© DOCKERHUB_USERNAME ayant comme valeur le nom d‚Äôutilisateur que vous avez cr√©√© sur Dockerhub\n\n2Ô∏è‚É£ A ce stade, nous avons donn√© les moyens √† Github de s‚Äôauthentifier avec notre identit√© sur Dockerhub. Il nous reste √† mettre en oeuvre l‚Äôaction en s‚Äôinspirant de https://github.com/docker/build-push-action/#usage. On ne va modifier que trois √©l√©ments dans ce fichier. Effectuer les actions suivantes:\n\nCr√©er depuis VSCode un fichier .github/workflows/docker.yml et coller le contenu du template dedans ;\nChanger le nom en un titre plus signifiant (par exemple ‚ÄúProduction de l‚Äôimage Docker‚Äù)\nAjouter master et dev √† la liste des branches sur lesquelles tourne le pipeline ;\nChanger le tag √† la fin pour mettre <username>/ensae-repro-docker:latest o√π username est le nom d‚Äôutilisateur sur DockerHub;\nFaire un commit et un push de ces fichiers\n\n4Ô∏è‚É£ Comme on est fier de notre travail, on va afficher √ßa avec un badge sur le README. Pour cela, on se rend dans l‚Äôonglet Actions et on clique sur un des scripts en train de tourner.\n\nEn haut √† droite, on clique sur ...\nS√©lectionner Create status badge\nR√©cup√©rer le code Markdown propos√©\nCopier dans le README depuis VSCode\nFaire de m√™me pour l‚Äôautre workflow\n\n5Ô∏è‚É£ Maintenant, il nous reste √† tester notre application dans l‚Äôespace bac √† sable:\n\nSe rendre sur l‚Äôenvironnement bac √† sable\nCr√©er un fichier Dockerfile ne contenant que l‚Äôimport et le d√©ploiement de l‚Äôappli:\n\nFROM <username>/ensae-repro-docker:latest\n\nEXPOSE 5000\nCMD [\"python\", \"main.py\"]\n\nComme pr√©c√©demment, faire un build\nTester l‚Äôimage avec run\n\nüéâ La matrice de confusion doit s‚Äôafficher ! Vous avez grandement facilit√© la r√©utilisation de votre image."
  },
  {
    "objectID": "chapters/application.html#etape-3-cr√©ation-dun-rapport-automatique",
    "href": "chapters/application.html#etape-3-cr√©ation-dun-rapport-automatique",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 3: cr√©ation d‚Äôun rapport automatique",
    "text": "Etape 3: cr√©ation d‚Äôun rapport automatique\nMaintenant, nous allons cr√©er et d√©ployer un site web pour valoriser notre travail. Cela va impliquer trois √©tapes:\n\nTester en local le logiciel quarto et cr√©er un rapport minimal qui sera compil√© par quarto ;\nEnrichir l‚Äôimage docker avec le logiciel quarto ;\nCompiler le document en utilisant cette image sur les serveurs de Github ;\nD√©ployer ce rapport minimal pour le rendre disponible √† tous sur le web.\n\nLe but est de proposer un rapport minimal qui illustre la performance du mod√®le est la feature importance. Pour ce dernier √©l√©ment, le rapport qui sera propos√© utilise shap qui est une librairie d√©di√©e √† l‚Äôinterpr√©tabilit√© des mod√®les de machine learning\n\n1. Rapport minimal en local\n1Ô∏è‚É£ La premi√®re √©tape consiste √† installer quarto sur notre machine Linux sur laquelle tourne VSCode:\n\nDans un terminal, installer quarto avec les commandes suivantes:\n\nQUARTO_VERSION=\"0.9.287\"\nwget \"https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb\"\nsudo apt install \"./quarto-${QUARTO_VERSION}-linux-amd64.deb\"\n\nS‚Äôassurer qu‚Äôon travaille bien depuis l‚Äôenvironnement conda monenv. Sinon l‚Äôactiver\n\n2Ô∏è‚É£ Il va √™tre n√©cessaire d‚Äôenrichir l‚Äôenvironnement conda. Certaines d√©pendances sont n√©cessaires pour que quarto fonctionne bien avec Python (jupyter, nbclient‚Ä¶) alors que d‚Äôautres ne sont n√©cessaires que parce qu‚Äôils sont utilis√©s dans le document (seaborn, shap‚Ä¶). Changer la section dependencies avec la liste suivante:\ndependencies:\n  - python=3.10.0\n  - ipykernel==6.13.0\n  - jupyter==1.0.0\n  - matplotlib==3.5.1\n  - nbconvert==6.5.0\n  - nbclient==0.6.0\n  - nbformat==5.3.0\n  - pandas==1.4.1\n  - PyYAML==6.0\n  - s3fs==2022.2.0\n  - scikit-learn==1.0.2\n  - seaborn==0.11.2\n  - shap==0.40.0\n3Ô∏è‚É£ Cr√©er un fichier nomm√© report.qmd\n\n\n---\ntitle: \"Comprendre les facteurs de survie sur le Titanic\"\nsubtitle: \"Un rapport innovant\"\nformat:\n  html:\n    self-contained: true\n  ipynb: default\njupyter: python3\n---\n\n\n\nVoici un rapport pr√©sentant quelques intuitions issues d'un mod√®le \n_random forest_ sur le jeu de donn√©es `Titanic` entra√Æn√© et \nd√©ploy√© de mani√®re automatique. \n\nIl est possible de t√©l√©charger cette page sous format `Jupyter Notebook` <a href=\"report.ipynb\" download>ici</a>\n\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nimport main\nX_train = main.X_train\ny_train = main.y_train\ntraining_data = main.training_data\nrdmf = RandomForestClassifier(n_estimators=20)\nrdmf.fit(X_train, y_train)\n```\n\n# Feature importance\n\nLa @fig-feature-importance repr√©sente l'importance des variables :\n\n```python\nfeature_imp = pd.Series(rdmf.feature_importances_, index=training_data.iloc[:,1:].columns).sort_values(ascending=False)\n```\n\n```python\n#| label: fig-feature-importance\n#| fig-cap: \"Feature importance\"\nplt.figure(figsize=(10,6))\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.tight_layout()\nplt.show()\n```\n\nCelle-ci peut √©galement √™tre obtenue gr√¢ce √† la librairie\n`shap`:\n\n```python\n#| echo : true\nimport shap\nshap_values = shap.TreeExplainer(rdmf).shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\", feature_names = training_data.iloc[:,1:].columns)\n```\n\nOn peut √©galement utiliser cette librairie pour\ninterpr√©ter la pr√©diction de notre mod√®le:\n\n\n```python\n# explain all the predictions in the test set\nexplainer = shap.TreeExplainer(rdmf)\n# Calculate Shap values\nchoosen_instance = main.X_test[15]\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance, feature_names = training_data.iloc[:,1:].columns)\n```\n\n# Qualit√© pr√©dictive du mod√®le\n\nLa matrice de confusion est pr√©sent√©e sur la\n@fig-confusion\n\n```python\n#| label: fig-confusion\n#| fig-cap: \"Matrice de confusion\"\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(main.y_test, rdmf.predict(main.X_test))\nplt.figure(figsize=(8,5))\nsns.heatmap(conf_matrix, annot=True)\nplt.title('Confusion Matrix')\nplt.tight_layout()\n```\n\nOu, sous forme de tableau:\n\n\n```python\npd.DataFrame(conf_matrix, columns=['Predicted','Observed'], index = ['Predicted','Observed']).to_html()\n```\n4Ô∏è‚É£ On va tenter de compiler ce document\n\nLe compiler en local avec la commande quarto render report.qmd\nVous devriez rencontrer l‚Äôerreur suivante:\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [1], in <cell line: 6>()\n      4 from sklearn.ensemble import RandomForestClassifier\n      5 import main\n----> 6 X_train = main.X_train\n      7 y_train = main.y_train\n      8 training_data = main.training_data\n\nAttributeError: module 'main' has no attribute 'X_train'\nAttributeError: module 'main' has no attribute 'X_train'\n\nRefactoriser main.py pour que toutes les op√©rations, √† l‚Äôexception du print de la matrice de confusion ne soient plus dans la section __main__ afin qu‚Äôils soient syst√©matiquement ex√©cut√©s.\nTenter √† nouveau quarto render report.qmd\nDeux fichiers ont √©t√© g√©n√©r√©s:\n\nun Notebook que vous pouvez ouvrir et dont vous pouvez ex√©cuter des cellules\nun fichier HTML que vous pouvez t√©l√©charger et ouvrir\n\n\n5Ô∏è‚É£ On a d√©j√† un r√©sultat assez esth√©tique en ce qui concerne la page HTML. Cependant, on peut se dire que certains param√®tres par d√©faut, comme l‚Äôaffichage des blocs de code, ne conviennent pas au public cibl√©. De m√™me, certains param√®tres de style, comme l‚Äôaffichage des tableaux peuvent ne pas convenir √† notre charte graphique. On va rem√©dier √† cela en deux √©tapes:\n\nenrichir le header d‚Äôoptions globales contr√¥lant le comportement de quarto\ncr√©er un fichier CSS pour avoir de beaux tableaux\n\n6Ô∏è‚É£ Changer la section format du header avec les options suivantes:\nformat:\n  html:\n    echo: false\n    code-fold: true\n    self-contained: true\n    code-summary: \"Show the code\"\n    warning: false\n    message: false\n    theme:\n      - cosmo\n      - css/custom.scss\n  ipynb: default\n7Ô∏è‚É£ Cr√©er le fichier css/custom.scss avec le contenu suivant:\n/*-- scss:rules --*/\n\ntable {\n    border-collapse: collapse;\n    margin: 25px 0;\n    font-size: 0.9em;\n    font-family: sans-serif;\n    min-width: 400px;\n    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);  \n}\n\nthead tr {\n    background-color: #516db0;\n    color: #ffffff;\n    text-align: center;\n}\n\nth, td {\n    padding: 12px 15px;\n}\n\ntbody tr {\n    border-bottom: 1px solid #dddddd;\n}\n\ntbody tr:nth-of-type(even) {\n    background-color: #f3f3f3;\n}\n\ntbody tr:last-of-type {\n    border-bottom: 2px solid #516db0;\n}\n\ntbody tr.active-row {\n    font-weight: bold;\n    color: #009879;\n}\n8Ô∏è‚É£ Compiler √† nouveau et observer le changement d‚Äôesth√©tique du HTML\n9Ô∏è‚É£ Commit des nouveaux fichier report.qmd, custom.scss et des fichiers d√©j√† existants.\n{{% box status=‚Äúhint‚Äù title=‚ÄúUn linter sous forme de hook pre-commit‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}}\nOn ne commit pas les output, ici le notebook et le fichier html. Les mettre sur le d√©p√¥t Github n‚Äôest pas la bonne mani√®re de les mettre √† disposition. On va le voir, on va utiliser l‚Äôapproche CI/CD pour cela.\nId√©alement, on ajoute au .gitignore les fichiers concern√©s, ici report.ipynb et report.html\n{{% /box %}}\n\n\n3. Enrichir l‚Äôimage Docker\nOn va vouloir mettre √† jour notre image pour automatiser, √† terme, la production de nos livrables (le notebook et la page web).\nPour cela, il est n√©cessaire que notre image int√®gre le logiciel quarto.\n1Ô∏è‚É£ A partir du script pr√©c√©dent d‚Äôinstallation de quarto, enrichir l‚Äôimage Docker9\n\n\n\n4. Automatisation avec Github Actions\n1Ô∏è‚É£ Cr√©er un nouveau fichier .github/workflows.report.yml\nSi les d√©pendances et l‚Äôimage ont bien √©t√© enrichis, cette √©tape est quasi directe avec\n\n{{% panel name=‚ÄúVersion autonome üöó‚Äù %}}\n\nDonner comme nom Deploy as website\nEffectuer cette action √† chaque push sur les branches main, master et dev\nLe job doit tourner sur une machine ubuntu\nCependant, il convient d‚Äôutiliser comme container votre image Docker\nLes steps:\n\nR√©cup√©rer le contenu du dossier avec checkout\nFaire un quarto render\nR√©cup√©rer le notebook sous forme d‚Äôartefact\n\n\n{{% /panel %}}\n{{% panel name=‚ÄúVersion guid√©e :map:‚Äù %}}\nname: Deploy as website\n\non:\n  push:\n    branches:\n      - main\n      - master\n      - dev\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    container: linogaliana/ensae-repro-docker:latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Render site\n        run: quarto render report.qmd\n      - uses: actions/upload-artifact@v1\n        with:\n          name: Report\n          path: report.ipynb\n{{% /panel %}}\n\nSi vous √™tes fier de vous, vous pouvez ajouter le badge de ce workflow sur le README üòé\nCette √©tape nous a permis d‚Äôautomatiser la construction de nos livrables. Mais la mise √† disposition de ce livrable est encore assez manuelle: il faut aller chercher √† la main la derni√®re version du notebook pour la partager.\nOn va am√©liorer cela en d√©ployant automatiquement un site web pr√©sentant en page d‚Äôaccueil notre rapport et permettant le t√©l√©chargement du notebook."
  },
  {
    "objectID": "chapters/application.html#etape-4-d√©ploiement-de-ce-rapport-automatique-sur-le-web",
    "href": "chapters/application.html#etape-4-d√©ploiement-de-ce-rapport-automatique-sur-le-web",
    "title": "Appliquer les concepts √©tudi√©s √† un projet de data science",
    "section": "Etape 4: D√©ploiement de ce rapport automatique sur le web",
    "text": "Etape 4: D√©ploiement de ce rapport automatique sur le web\n1Ô∏è‚É£ Dans un premier temps, nous allons connecter notre d√©p√¥t Github au service tiers Netlify\n\nAller sur https://www.netlify.com/ et faire Sign up (utiliser son compte Github)\nDans la page d‚Äôaccueil de votre profil, vous pouvez cliquer sur Add new site > Import an existing project\nCliquer sur Github. S‚Äôil y a des autorisations √† donner, les accorder. Rechercher votre projet dans la liste de vos projets Github\nCliquer sur le nom du projet et laisser les param√®tres par d√©faut (nous allons modifier par la suite)\nCliquer sur Deploy site\n\n2Ô∏è‚É£ A ce stade, votre d√©ploiement devrait √©chouer. C‚Äôest normal, vous essayez de d√©ployer depuis master qui ne comporte pas de html. Mais le rapport n‚Äôest pas non plus pr√©sent dans la branche dev. En fait, aucune branche ne comporte le rapport: celui-ci est g√©n√©r√© dans votre pipeline mais n‚Äôest jamais pr√©sent dans le d√©p√¥t car il s‚Äôagit d‚Äôun output. On va d√©sactiver le d√©ploiement automatique pour privil√©gier un d√©ploiement depuis Github Actions:\n\nAller dans Site Settings puis, √† gauche, cliquer sur Build and Deploy\nDans la section Build settings, cliquer sur Stop builds et valider\n\nOn vient de d√©sactiver le d√©ploiement automatique par d√©faut. On va faire communiquer notre d√©p√¥t Github et Netlify par le biais de l‚Äôint√©gration continue.\n3Ô∏è‚É£ Pour cela, il faut cr√©er un jeton Netlify pour que les serveurs de Github, lorsqu‚Äôils disposent d‚Äôun rapport, puissent l‚Äôenvoyer √† Netlify pour la mise sur le web. Il va √™tre n√©cessaire de cr√©er deux variables d‚Äôenvironnement pour connecter Github et Netlify: l‚Äôidentifiant du site et le token\n\nPour le token :\n\nCr√©er un jeton en cliquant, en haut √† droite, sur l‚Äôicone de votre profil. Aller dans User settings. A gauche, cliquer sur Applications et cr√©er un jeton personnel d‚Äôacc√®s avec un nom signifiant (par exemple PAT_ENSAE_reproductibilite)\nMettre de c√¥t√© (conseil : garder l‚Äôonglet ouvert)\n\nPour l‚Äôidentifiant du site:\n\ncliquer sur Site Settings dans les onglets en haut\nGarder l‚Äôonglet ouvert pour copier la valeur quand n√©cessaire\n\nIl est maintenant n√©cessaire d‚Äôaller dans le d√©p√¥t Github et de cr√©er les secrets (Settings > Secrets > Actions):\n\nCr√©er le secret NETLIFY_AUTH_TOKEN en collant la valeur du jeton d‚Äôauthentification Netlify\nCr√©er le secret NETLIFY_SITE_ID en collant l‚Äôidentifiant du site\n\n\n4Ô∏è‚É£ Nous avons effectu√© toutes les configurations n√©cessaires. On va maintenant mettre √† jour l‚Äôint√©gration continue afin de mettre √† disposition sur le web notre rapport. On va utiliser l‚Äôinterface en ligne de commande (CLI) de Netlify. Celle-ci attend que le site web se trouve dans un dossier public et que la page d‚Äôaccueil soit nomm√©e index.html:\n\n{{% panel name=‚ÄúVision d‚Äôensemble‚Äù %}}\n\nune installation de npm\nune √©tape de d√©ploiement via la CLI de netlify\n\n- name: Install npm\n  uses: actions/setup-node@v2\n  with:\n    node-version: '14'\n- name: Deploy to Netlify\n  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets\n  env:\n    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n  run: |\n    mkdir -p public\n    mv report.html public/index.html\n    mv report.ipynb public/report.ipynb\n    npm install --unsafe-perm=true netlify-cli -g\n    netlify init\n    netlify deploy --prod --dir=\"public\" --message \"Deploy master\"\n{{% /panel %}}\n{{% panel name=‚ÄúD√©tails npm‚Äù %}}\n\n\nname: Install npm uses: actions/setup-node@v2 with: node-version: ‚Äò14‚Äô\n\n\nnpm est le gestionnaire de paquet de JS. Il est n√©cessaire de le configurer, ce qui est fait automatiquement gr√¢ce √† l‚Äôaction actions/setup-node@v2\n{{% /panel %}}\n{{% panel name=‚ÄúD√©tails Netlify CLI‚Äù %}}\n\nOn rappelle √† Github Actions nos param√®tres d‚Äôauthentification sous forme de variables d‚Äôenvironnement. Cela permet de les garder secr√®tes\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo‚Äôs secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install ‚Äìunsafe-perm=true netlify-cli -g netlify init netlify deploy ‚Äìprod ‚Äìdir=‚Äúpublic‚Äù ‚Äìmessage ‚ÄúDeploy master‚Äù\n\n\n\nOn d√©place les rapports de la racine vers le dossier public\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo‚Äôs secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install ‚Äìunsafe-perm=true netlify-cli -g netlify init netlify deploy ‚Äìprod ‚Äìdir=‚Äúpublic‚Äù ‚Äìmessage ‚ÄúDeploy master‚Äù\n\n\n\nOn installe et initialise Netlify\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo‚Äôs secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install ‚Äìunsafe-perm=true netlify-cli -g netlify init netlify deploy ‚Äìprod ‚Äìdir=‚Äúpublic‚Äù ‚Äìmessage ‚ÄúDeploy master‚Äù\n\n\n\nOn d√©ploie sur l‚Äôurl par d√©faut (-- prod) depuis le dossier public\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo‚Äôs secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install ‚Äìunsafe-perm=true netlify-cli -g netlify init netlify deploy ‚Äìprod ‚Äìdir=‚Äúpublic‚Äù ‚Äìmessage ‚ÄúDeploy master‚Äù\n\n\n{{% /panel %}}\n\nAu bout de quelques minutes, le rapport est disponible en ligne sur l‚ÄôURL Netlify (par exemple https://spiffy-florentine-c913b9.netlify.app)"
  },
  {
    "objectID": "chapters/code-quality.html",
    "href": "chapters/code-quality.html",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "",
    "text": "‚ÄúThe code is read much more often than it is written.‚Äù\nGuido Van Rossum1\n\nLorsque l‚Äôon s‚Äôinitie √† la pratique de la data science, il est assez naturel de voir le code d‚Äôune mani√®re tr√®s fonctionnelle : je veux r√©aliser une t√¢che donn√©e ‚Äî par exemple un algorithme de classification ‚Äî et je vais donc assembler dans un notebook des bouts de code, souvent trouv√©s sur internet, jusqu‚Äô√† obtenir un projet qui r√©alise la t√¢che voulue. La structure du projet importe assez peu, tant qu‚Äôelle permet d‚Äôimporter correctement les donn√©es n√©cessaires √† la t√¢che en question.\nSi cette approche flexible et minimaliste fonctionne tr√®s bien lors de la phase d‚Äôapprentissage, il est malgr√© tout indispensable de s‚Äôen d√©tacher progressivement √† mesure que l‚Äôon progresse et que l‚Äôon peut √™tre amen√© √† r√©aliser des projets plus professionnels ou bien √† int√©grer des projets collaboratifs.\nEn particulier, il est important de proposer, parmi les multiples mani√®res de r√©soudre un probl√®me informatique, une solution qui soit intelligible par d‚Äôautres personnes parlant le langage. Le code est en effet lu bien plus souvent qu‚Äôil n‚Äôest √©crit, c‚Äôest donc avant tout un outil de communication. De m√™me, la maintenance d‚Äôun code demande g√©n√©ralement beaucoup plus de moyens que sa phase de d√©veloppement initial, il est donc important de penser en amont la qualit√© de son code et la structure de son projet de sorte √† le rendre au maximum maintenable dans le temps.\nAfin de faciliter la communication et r√©duire la douleur d‚Äôavoir √† faire √©voluer un code obscur, des tentatives plus ou moins institutionnalis√©es de d√©finir des conventions ont √©merg√©. Ces conventions d√©pendent naturellement du langage utilis√©, mais les principes sous-jacents s‚Äôappliquent de mani√®re universelle √† tout projet bas√© sur du code.\n\n\n\nPython est un langage tr√®s lisible. Avec un peu d‚Äôeffort sur le nom des objets, sur la gestion des d√©pendances et sur la structure du programme, on peut tr√®s bien comprendre un script sans avoir besoin de l‚Äôex√©cuter. C‚Äôest l‚Äôune des principales forces du langage Python qui permet ainsi une acquisition rapide des bases et facilite l‚Äôappropriation d‚Äôun script.\nLa communaut√© Python a abouti √† un certain nombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard dans l‚Äô√©cosyst√®me Python. Les deux normes les plus connues sont :\n\nla norme PEP8 qui d√©finit un certain nombre de conventions relatives au code\nla norme PEP257 consacr√©e √† la documentation (docstrings).\n\n\n\n\n\n\n\nNote\n\n\n\nDans l‚Äôunivers R, la formalisation a √©t√© moins organis√©e. Ce langage est plus permissif que Python sur certains aspects2. N√©anmoins, des standards ont √©merg√©, √† travers un certain nombre de style guides dont les plus connus sont le tidyverse style guide et le google style guide3 (voir ce post qui pointe vers un certain nombre de ressources sur le sujet).\n\n\nCes conventions sont arbitraires, dans une certaine mesure. Il est tout √† fait possible de trouver certaines conventions moins esth√©tiques que d‚Äôautres.\nCes conventions ne sont pas non plus immuables: les langages et leurs usages √©voluent, ce qui n√©cessite de mettre √† jour les conventions. Cependant, adopter dans la mesure du possible certains des r√©flexes pr√©conis√©s par ces conventions devrait am√©liorer la capacit√© √† √™tre compris par la communaut√©, augmenter les chances de b√©n√©ficier d‚Äôapport de celle-ci pour adapter le code mais aussi r√©duire la difficult√© √† faire √©voluer un code.\nIl existe beaucoup de philosophies diff√©rentes sur le style de codage et, en fait, le plus important est la coh√©rence : si on choisit une convention, par exemple snake case plut√¥t que camel case, le mieux est de s‚Äôy tenir.\nLes conventions vont au-del√† de la syntaxe. Un certain nombre de standards d‚Äôorganisation d‚Äôun projet ont √©merg√©, qui seront abord√©es dans le prochain chapitre."
  },
  {
    "objectID": "chapters/code-quality.html#lisibilite",
    "href": "chapters/code-quality.html#lisibilite",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Lisibilit√©",
    "text": "Lisibilit√©\nUn code √©crit avec des noms de variables et de fonctions explicites est autant, voire plus, informatif que les commentaires qui l‚Äôaccompagnent4. C‚Äôest pourquoi il est essentiel de respecter des conventions pour le choix des noms des objets afin d‚Äôassurer la lisibilit√© des programmes.\nUn certain nombre de conseils sont pr√©sents dans le Hitchhiker‚Äôs Guide to Python qui vise √† faire conna√Ætre les pr√©ceptes du ‚ÄúZen of Python‚Äù (PEP 20). Ce post de blog illustre quelques uns de ces principes avec des exemples. Vous pouvez retrouver ces conseils dans Python en tapant le code suivant:\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\nIl est conseill√© de suivre ces deux principes lorsqu‚Äôon commence √† programmer des fonctions (ce qui, comme cela est √©voqu√© par la suite, est toujours recommand√©).\n\nFaire attention au type d‚Äôobjet renvoy√© par Python. Ce langage ne propose pas de typage fort, il est donc possible qu‚Äôune fonction renvoie des objets de nature diff√©rente selon les cas5. Cela peut amener √† des surprises lorsqu‚Äôon utilise une telle fonction dans un code. Il est recommand√© d‚Äô√©viter ce comportement en proposant des fonctions diff√©rentes si l‚Äôoutput d‚Äôune fonction est de nature diff√©rente. Ce principe de pr√©caution (mais aussi d‚Äôinformation) renvoie au paradigme de la programmation d√©fensive.\n\n\nPrivil√©gier la programmation orient√©e objet lorsqu‚Äôune fonction doit s‚Äôadapter au type d‚Äôobjet en entr√©e (par exemple aller chercher des √©l√©ments diff√©rents pour un objet lm ou un objet glm). Cela √©vite les codes spaghetti üçù inutilement complexes qui sont impossibles √† d√©bugger.\n\n\n\n\n\n\n\nType hinting\n\n\n\nPython propose une fonctionalit√© assez plaisante qui est le type hinting (doc officielle et tutoriel sur realpython.com).\nCette approche permet d‚Äôindiquer le type d‚Äôargument attendu par une fonction et celui qui sera renvoy√© par la fonction. Par exemple, la personne ayant √©crit la fonction suivante\ndef calcul_moyenne(df: pd.DataFrame, col : str = \"y\") -> pd.DataFrame:\n    return df[col].mean()\npropose d‚Äôutiliser deux types d‚Äôinputs (un DataFrame Pandas et une chaine de caract√®re) et indique qu‚Äôelle renverra un DataFrame Pandas. A noter que c‚Äôest indicatif, non contraignant. En effet, le code ci-dessus fonctionnera si on fournit en argument col une liste puisque Pandas sait g√©rer cela √† l‚Äô√©tape df[col].mean().\nLe type hinting est un √©l√©ment d‚Äôautodocumentation puisque gr√¢ce √† ces hints le code suffit √† faire comprendre la volont√© de la personne l‚Äôayant √©crit.\n\n\n\n\n\n\n\n\nLe code spaghetti\n\n\n\nLe code spaghetti est un style d‚Äô√©criture qui favorise l‚Äôapparition du syndrome du plat de spaghettis : un code impossible √† d√©m√©ler parce qu‚Äôil fait un usage excessif de conditions, d‚Äôexceptions en tous sens, de gestion des √©v√©nements complexes. Il devient quasi-impossible de savoir quelles ont √©t√© les conditions √† l‚Äôorigine de telle ou telle erreur sans ex√©cuter ligne √† ligne (et celles-ci sont excessivement nombreuses du fait de mauvaises pratiques de programmation) le programme.\nEn fait, la programmation spaghetti qualifie tout ce qui ne permet pas de d√©terminer le qui, le quoi et le comment. Le code est donc plus long √† mettre √† jour car cela n√©cessite de remonter un √† un le fil des renvois."
  },
  {
    "objectID": "chapters/code-quality.html#concision",
    "href": "chapters/code-quality.html#concision",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Concision",
    "text": "Concision\nUn code reproductible doit pouvoir s‚Äôex√©cuter de mani√®re lin√©aire. S‚Äôil provoque une erreur, il est important de pouvoir identifier l‚Äôinstruction responsable pour pouvoir debugger. Comme une d√©monstration math√©matique, un code intelligible doit viser la concision et la simplicit√©. Les codes tr√®s longs sont souvent signes de r√©p√©titions et sont difficiles √† d√©bugger.\nLes scripts trop longs ne sont pas une bonne pratique. Il est pr√©f√©rable de diviser l‚Äôensemble des scripts ex√©cutant une cha√Æne de production en ‚Äúmonades‚Äù, c‚Äôest-√†-dire en petites unit√©s coh√©rentes. Les fonctions sont un outil privil√©gi√© pour cela (en plus de limiter la redondance, et d‚Äô√™tre un outil privil√©gi√© pour documenter un code).\n\n\n\n\n\n\nExemple: privil√©gier les list comprehensions\n\n\n\n\n\nEn Python, il est recommand√© de privil√©gier les list comprehensions √† l‚Äôutilisation de boucles for indent√©es. Ces derni√®res sont en g√©n√©ral moins efficaces et surtout impliquent un nombre important de ligne de codes l√† o√π les compr√©hensions de listes sont beaucoup plus concises\nliste_nombres = range(10)\n\n# tr√®s mauvais\ny = []\nfor x in liste_nombres:\n    if x % 2 == 0:\n        y.append(x*x)\n\n# mieux\ny = [x*x for x in liste_nombres if x % 2 == 0]\n\n\n\n\n\n\n\n\n\nR√®gle d‚Äôor\n\n\n\nIl faut utiliser une fonction d√®s qu‚Äôon utilise une m√™me portion de code plus de deux fois (don‚Äôt repeat yourself (DRY))\n\n\nLes fonctions ont de nombreux avantages par rapport √† de longs scripts:\n\nLimite les risques d‚Äôerreurs li√©s aux copier/coller\nRend le code plus lisible et plus compact\nUn seul endroit du code √† modifier lorsqu‚Äôon souhaite modifier le traitement\nFacilite la r√©utilisation et la documentation du code !\n\n\n\n\n\n\n\nR√®gles pour √©crire des fonctions pertinentes\n\n\n\n\nUne t√¢che = une fonction\nUne t√¢che complexe = un encha√Ænement de fonctions r√©alisant chacune une t√¢che simple\nLimiter l‚Äôutilisation de variables globales."
  },
  {
    "objectID": "chapters/code-quality.html#coherence",
    "href": "chapters/code-quality.html#coherence",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Coh√©rence du script",
    "text": "Coh√©rence du script\nLister les d√©pendances est important:\n\npour des raisons techniques: le logiciel doit savoir o√π aller chercher les fonctions utilis√©es dans un script pour avoir un code fonctionnel ;\npour des raisons conventionnelles: les utilisateurs doivent comprendre les d√©pendances √† installer pour √™tre en mesure de r√©utiliser le code.\n\nLes imports se mettent conventionnellement en d√©but de script, qu‚Äôil s‚Äôagisse d‚Äôimport de packages dans leur ensemble ou seulement de certaines fonctions:\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nDans le premier cas, on fait ensuite r√©f√©rence aux fonctions en les faisant pr√©c√©der du nom du package :\npd.DataFrame([0,1])\nCela permet de dire √† Python d‚Äôaller chercher dans le namespace pd (alias pour pandas qui est lui-m√™me un ensemble de scripts enregistr√©s sur le disque) la fonction DataFrame.\n\n\n\n\n\n\nQuelques conseils compl√©mentaires.\n\n\n\n\n\nEn premier lieu, il convient d‚Äôadopter les m√™mes standards que la communaut√© pour les noms de package.\n# bien\nimport numpy as np\n\n# trompeur\nimport numpy as pd\nIl faut √©galement faire attention aux namespaces pour √©viter les conflits entre fonctions. Cela implique de ne pas importer l‚Äôensemble des fonctions d‚Äôun package de la mani√®re suivante:\nfrom numpy import *\nfrom math import *\nDans ce cas, on va se retrouver avec des conflits potentiels entre les fonctions du package numpy et du package math qui portent le m√™me nom (floor par exemple).\n\n\n\nEn ce qui concerne l‚Äôinstallation des packages, nous allons voir dans les parties Structure de code et Portabilit√© qu‚Äôil ne faut pas g√©rer ceci dans le script mais dans un √©l√©ment √† part, relatif √† l‚Äôenvironnement d‚Äôex√©cution du projet6."
  },
  {
    "objectID": "chapters/code-quality.html#redondance",
    "href": "chapters/code-quality.html#redondance",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Limiter la redondance",
    "text": "Limiter la redondance\nUn bon principe √† suivre est ‚Äúdon‚Äôt repeat yourself !‚Äù (DRY). Celui-ci r√©duit la charge de code √† √©crire, √† comprendre et √† tenir √† jour.\n\nCe post donne quelques bonnes pratiques pour r√©duire la redondance des codes.\n\n\n\n\n\n\nUn exemple progressif pour comprendre\n\n\n\n\n\nüí° Supposons qu‚Äôon dispose d‚Äôune table de donn√©es qui utilise le code ‚àí99 pour repr√©senter les valeurs manquantes. On d√©sire remplacer l‚Äôensemble des ‚àí99 par des NA.\nVoici un code Python qui permet de se placer dans ce cas qui, malheureusement, arrive fr√©quemment.\n# On fixe la racine pour √™tre s√ªr de tous avoir le m√™me dataset\nnp.random.seed(1234)\n\n# On cr√©√© un dataframe\na = np.random.randint(1, 10, size = (5,6))\ndf = np.insert(\n    a,\n    np.random.choice(len(a), size=6),\n    -99,\n)\ndf = pd.DataFrame(df.reshape((6,6)), columns=[chr(x) for x in range(97, 103)])\nUn premier jet de code pourrait prendre la forme suivante:\n# Dupliquer les donn√©es\ndf2 = df.copy()\n# Remplacer les -99 par des NA\ndf2.loc[df2['a'] == -99,'a'] = np.nan\ndf2.loc[df2['b'] == -99,'b'] = np.nan\ndf2.loc[df2['c'] == -99,'c'] = np.nan\ndf2.loc[df2['d'] == -99,'d'] = np.nan\ndf2.loc[df2['e'] == -98,'e'] = np.nan\ndf2.loc[df2['f'] == -99,'e'] = np.nan\nQuelles sont les choses qui vous d√©rangent dans le code ci-dessus?\n\n\nIndice üí° Regardez pr√©cis√©ment le code et le DataFrame, notamment les colonnes e et g.\n\nIl y a deux erreurs, difficiles √† d√©tecter:\n\ndf2.loc[df2['e'] == -98,'e'] = np.nan: une erreur de copier-coller sur la valeur de l‚Äôerreur ;\ndf2.loc[df2['f'] == -99,'e'] = np.nan: une erreur de copier-coller sur les colonnes en question\n\n\nOn peut noter au moins deux trois :\n\nLe code est long et r√©p√©titif, ce qui nuit √† sa lisibilit√©;\nLe code est tr√®s d√©pendant de la structure des donn√©es (nom et nombre de colonnes) et doit √™tre adapt√© d√®s que celle-ci √©volue;\nOn a introduit des erreurs humaines dans le code, difficiles √† d√©tecter.\n\nOn voit dans la premi√®re version de notre code qu‚Äôil y a une structure commune √† toutes nos lignes de la forme .[. == -99] = np.nan. Cette structure va servir de base √† notre fonction, en vue de g√©n√©raliser le traitement que nous voulons faire.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2['a'] = fix_missing(df['a'])\ndf2['b'] = fix_missing(df['b'])\ndf2['c'] = fix_missing(df['c'])\ndf2['d'] = fix_missing(df['d'])\ndf2['e'] = fix_missing(df['e'])\ndf2['f'] = fix_missing(df['f'])\nCette seconde version du code est meilleure que la premi√®re version, car on a r√©gl√© le probl√®me d‚Äôerreur humaine (il n‚Äôest plus possible de taper -98 au lieu de -99).\n\n\nMais voyez-vous le probl√®me qui persiste ?\n\nLe code reste long et r√©p√©titif, et n‚Äô√©limine pas encore toute possibilit√© d‚Äôerreur, car il est toujours possible de se tromper dans le nom des variables.\n\nLa prochaine √©tape consiste √† √©liminer ce risque d‚Äôerreur en combinant deux fonctions (ce qu‚Äôon appelle la combinaison de fonctions).\nLa premi√®re fonction fix_missing() sert √† r√©gler le probl√®me sur un vecteur. La seconde g√©n√©ralisera ce proc√©d√© √† toutes les colonnes. Comme Pandas permet une approche vectorielle, il est fr√©quent de construire des fonctions sur des vecteurs et les appliquer ensuite √† plusieurs colonnes.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2 = df2.apply(fix_missing)\nCette troisi√®me version du code a plusieurs avantages sur les deux autres versions:\n\nElle est plus concise et plus lisible;\nSi on a un changement de code pour les valeurs manquantes, il suffit de le mettre √† un seul endroit;\nElle fonctionne quels que soient le nombre de colonnes et le nom des colonnes;\nOn ne peut pas traiter une colonne diff√©remment des autres par erreur.\n\nDe plus, le code est facilement g√©n√©ralisable.\nPar exemple, √† partir de la m√™me structure, √©crire le code qui permet de ne traiter que les colonnes a,b et e ne demande pas beaucoup d‚Äô√©nergie.\ndf2 = df.copy()\ndf2[['a','b','e']] = df2[['a','b','e']].apply(fix_missing)"
  },
  {
    "objectID": "chapters/code-quality.html#documentation",
    "href": "chapters/code-quality.html#documentation",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "(Auto)documentation",
    "text": "(Auto)documentation\nUn code sans aucun commentaire est tr√®s difficile √† s‚Äôapproprier (y compris pour la personne qui l‚Äôa r√©dig√© et qui y revient quelques semaines plus tard). Cependant, un code pr√©sentant trop de commentaires est √©galement illisible et refl√®te g√©n√©ralement un d√©faut de conception du code qui n‚Äôest pas assez explicite.\nLa documentation vise √† pr√©senter la d√©marche g√©n√©rale, √©ventuellement √† travers des exemples, mais aussi √† expliciter certains √©l√©ments du code (une op√©ration qui n‚Äôest pas √©vidente, des arguments de fonction, etc.). La documentation se m√©lange donc aux instructions visant √† √™tre ex√©cut√©es mais s‚Äôen distingue. Ces principes sont h√©rit√©s du paradigme de la ‚Äúprogrammation lettr√©e‚Äù (Literate programming) dont l‚Äôun des avocats √©tait Donald Knuth.\n\n‚ÄúJe crois que le temps est venu pour une am√©lioration significative de la documentation des programmes, et que le meilleur moyen d‚Äôy arriver est de consid√©rer les programmes comme des ≈ìuvres litt√©raires. D‚Äôo√π mon titre, ¬´ programmation lettr√©e¬´ .\nNous devons changer notre attitude traditionnelle envers la construction des programmes : au lieu de consid√©rer que notre t√¢che principale est de dire √† un ordinateur ce qu‚Äôil doit faire, appliquons-nous plut√¥t √† expliquer √† des √™tres humains ce que nous voulons que l‚Äôordinateur fasse.\nCelui qui pratique la programmation lettr√©e peut √™tre vu comme un essayiste, qui s‚Äôattache principalement √† exposer son sujet dans un style visant √† l‚Äôexcellence. Tel un auteur, il choisit , avec soin, le dictionnaire √† la main, les noms de ses variables et en explique la signification pour chacune d‚Äôelles. Il cherche donc √† obtenir un programme compr√©hensible parce que ses concepts sont pr√©sent√©s dans le meilleur ordre possible. Pour cela, il utilise un m√©lange de m√©thodes formelles et informelles qui se compl√®tent‚Äù\nDonald Knuth, Literate Programming (source)\n\nCela peut amener √† distinguer deux types de documentation:\n\nUne documentation g√©n√©rale de type Jupyter Notebook ou Quarto Markdown qui pr√©sente certes du code ex√©cut√© mais dont l‚Äôobjet principal est de pr√©senter une d√©marche ou des r√©sultats ;\nUne documentation de la d√©marche plus proche du code dont l‚Äôun des exemples sont les docstrings Python (ou son √©quivalent R, la documentation Roxygen).\n\nLes deux grands principes de la documentation au sein d‚Äôun script sont les suivants:\n\nIl est pr√©f√©rable de documenter le pourquoi plut√¥t que le comment. Le ‚Äúcomment‚Äù devrait √™tre compr√©hensible √† la lecture du code ;\nPrivil√©gier l‚Äôautodocumentation via des nommages pertinents\n\n\n\n\n\n\n\nComment bien documenter un script ?\n\n\n\n\nMinimum üö¶ : commentaire au d√©but du script pour d√©crire ce qu‚Äôil fait\nBien üëç : commenter les parties ‚Äúd√©licates‚Äù du code\nId√©al üí™ : documenter ses fonctions avec la syntaxe des docstrings."
  },
  {
    "objectID": "chapters/code-quality.html#outils-et-m√©thodes-pour-am√©liorer-un-code",
    "href": "chapters/code-quality.html#outils-et-m√©thodes-pour-am√©liorer-un-code",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Outils et m√©thodes pour am√©liorer un code",
    "text": "Outils et m√©thodes pour am√©liorer un code\nL‚Äôapprentissage par coeur de ces r√®gles ou faire des aller-retour en continu entre le code et les manuels de r√®gles serait quelques peu r√©barbatif.\nPour faire le parall√®le avec le langage naturel, on n‚Äôa pas toujours le b√©cherelle ou le dictionnaire sous les yeux. Les √©diteurs de texte ou les smartphones embarquent des correcteurs orthographiques qui identifient voire corrigent directement le texte √©crit.\nIl existe le m√™me type d‚Äôoutils pour les langages de programmation. Python √©tant l‚Äôoutil de travail principal de milliers de data-scientists, un certain nombre d‚Äôoutils ont vu le jour pour r√©duire le temps n√©cessaire pour cr√©er un projet ou disposer d‚Äôun code fonctionnel. Ces outils permettent un gros gain de productivit√©, r√©duisent le temps pass√© √† effectuer des t√¢ches r√©barbatives et am√©liorent la qualit√© d‚Äôun projet en offrant des diagnostics voire des correctifs √† des codes perfectibles.\nLes principaux outils sont les suivants:\n\nlinter : programme qui v√©rifie que le code est formellement conforme √† un certain guidestyle\n\nsignale probl√®mes formels, sans corriger\n\nformatter : programme qui reformate un code pour le rendre conforme √† un certain guidestyle\n\nmodifie directement le code\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nExemples d‚Äôerreurs rep√©r√©es par un linter :\n\nlignes de code trop longues ou mal indent√©es, parenth√®ses non √©quilibr√©es, noms de fonctions mal construits‚Ä¶\n\nExemples d‚Äôerreurs non rep√©r√©es par un linter :\n\nfonctions mal utilis√©es, arguments mal sp√©cifi√©s, structure du code incoh√©rente, code insuffisamment document√©‚Ä¶"
  },
  {
    "objectID": "chapters/code-quality.html#linters",
    "href": "chapters/code-quality.html#linters",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Linters",
    "text": "Linters\nLes linters sont des outils qui permettent d‚Äô√©valuer la qualit√© du code et son risque de provoquer une erreur (explicite ou silencieuse).\nVoici quelques exemples de probl√®mes que peuvent rencontrer les linters:\n\nles variables sont utilis√©es mais n‚Äôexistent pas (erreur)\nles variables inutilis√©es (inutiles)\nla mauvaise organisation du code (risque d‚Äôerreur)\nle non respect des bonnes pratiques d‚Äô√©criture de code\nles erreurs de syntaxe (par exemple les coquilles)\n\nLa plupart des logiciels de d√©veloppement embarquent des fonctionalit√©s de diagnostic (voire de suggestion de correctif). Il faut parfois les param√©trer dans les options (ils sont d√©sactiv√©s pour ne pas effrayer l‚Äôutilisateur avec des croix rouges partout).\nEn Python, les deux principaux linters sont PyLint et Flake8. Dans les exercices, nous proposons d‚Äôutiliser PyLint qui est pratique.\n\n\n\n\n\n\nTip\n\n\n\nL‚Äôun des int√©r√™ts d‚Äôutiliser PyLint est qu‚Äôon obtient une note, ce qui est assez instructif. Nous l‚Äôutiliserons dans l‚Äôapplication fil rouge pour comprendre la mani√®re dont chaque √©tape am√©liore la qualit√© du code.\nIl est possible de mettre en oeuvre des pre commit hook qui emp√™chent un commit n‚Äôayant pas une note minimale."
  },
  {
    "objectID": "chapters/code-quality.html#formaters",
    "href": "chapters/code-quality.html#formaters",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Formaters",
    "text": "Formaters\nLe formater modifie directement le code. On peut faire un parall√®le avec le correcteur orthographique.\nCet outil peut donc induire un changement substantiel du script afin de le rendre plus lisible.\nLe formater le plus utilis√© est Black.\n\n\n\n\n\n\nNote\n\n\n\nPour signaler sur Github la qualit√© d‚Äôun projet utilisant Black, il est possible d‚Äôajouter un badge dans le README:\n\n\n\nCode style: black"
  },
  {
    "objectID": "chapters/code-quality.html#lopensource-comme-moyen-pour-am√©liorer-la-qualit√©",
    "href": "chapters/code-quality.html#lopensource-comme-moyen-pour-am√©liorer-la-qualit√©",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "L‚Äôopensource comme moyen pour am√©liorer la qualit√©",
    "text": "L‚Äôopensource comme moyen pour am√©liorer la qualit√©\nEn ouvrant son code, il est possible de recevoir des suggestions voire des contributions de r√©utilisateurs du code. Cependant, les vertus de l‚Äôouverture vont au-del√†.\nEn effet, l‚Äôouverture se traduit g√©n√©ralement par des codes de meilleur qualit√©, mieux document√©s pour pouvoir √™tre r√©utilis√©s ou ayant simplement b√©n√©fici√© d‚Äôune attention accrue sur la qualit√© pour ne pas para√Ætre ridicule. M√™me en l‚Äôabsence de retour de (r√©)utilisateurs du code, le partage de code am√©liore la qualit√© des projets."
  },
  {
    "objectID": "chapters/code-quality.html#revue-de-code",
    "href": "chapters/code-quality.html#revue-de-code",
    "title": "Am√©liorer la qualit√© de son code",
    "section": "Revue de code",
    "text": "Revue de code\nLa revue de code s‚Äôinspire de la m√©thode du peer reviewing du monde acad√©mique pour am√©liorer la qualit√© du code Python. Dans une revue de code, le code √©crit par une personne est relu et √©valu√© par un ou plusieurs autres d√©veloppeurs afin d‚Äôidentifier les erreurs et les am√©liorations possibles. Cette pratique permet de d√©tecter les erreurs avant qu‚Äôelles ne deviennent des probl√®mes majeurs, d‚Äôassurer une coh√©rence dans le code, de garantir le respect des bonnes pratiques mais aussi d‚Äôam√©liorer la qualit√© du code en identifiant les parties du code qui peuvent √™tre simplifi√©es, optimis√©es ou refactoris√©es pour en am√©liorer la lisibilit√© et la maintenabilit√©.\nUn autre avantage de cette approche est qu‚Äôelle permet le partage de connaissances entre des personnes exp√©riment√©es et des personnes plus d√©butantes ce qui permet √† ces derni√®res de monter en comp√©tence. Github et Gitlab proposent des fonctionnalit√©s tr√®s pratiques pour la revue de code: discussions, suggestions de modifications‚Ä¶"
  },
  {
    "objectID": "chapters/deployment.html",
    "href": "chapters/deployment.html",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "",
    "text": "Dans les chapitres pr√©c√©dents, nous avons explor√© la mani√®re dont une structure de projet et de code ad√©quate facilite la r√©utilisation d‚Äôun projet. Cependant, le code est rarement, en soi, le produit final mais un moyen. Le code peut servir √† mettre en oeuvre une application, √† effectuer des traitements sur une base de donn√©es pour un papier ou un rapport, etc. La fr√©quence de r√©-utilisation du code peut elle-m√™me √™tre variable: certains projets vont √™tre utilis√©s quotidiennement alors que d‚Äôautres ne le seront qu‚Äô√† des √©ch√©ances diverses.\nComme tout produit, un projet a un cycle de vie. Pour faire simple, on peut s√©parer celui-ci en trois phases:\n\nla phase de d√©veloppement correspond √† l‚Äô√©criture du code et √† des exploitations exploratoires ;\nla phase de mise en production correspond √† l‚Äôadaptation du prototype √† des contraintes n√©cessaires pour qu‚Äôun projet produise un output √† la demande ;\nla phase de maintenance correspond √† la situation o√π on ne met plus en oeuvre de nouvelles fonctionalit√©s mais o√π on s‚Äôassure qu‚Äôun projet informatique continue de produire les output d√©sir√©s malgr√© l‚Äô√©volution du contexte.\n\nEn pratique, la distinction entre ces moments d‚Äôun projet peut √™tre floue. Par exemple, gr√¢ce √† Git, on peut ainsi mettre en oeuvre de nouvelles fonctionalit√©s (protypage correspond √† la phase de d√©veloppement) parall√®les √† celles d√©j√† existantes (phase de maintenance). N√©anmoins, ces diff√©rences conceptuelles sont int√©ressantes pour appr√©hender les contraintes diff√©rentes de ces phases d‚Äôun projet.\n\n\nLes bonnes pratiques mises en oeuvre jusqu‚Äô√† pr√©sent avaient pour objectif de faciliter la compr√©hension et la r√©utilisation d‚Äôun projet. Elles sont donc particuli√®rement appropri√©es pour r√©duire le co√ªt en temps de la mise en production et de la maintenance d‚Äôun projet. Ces co√ªts pourraient amener un projet, m√™me utile, √† √™tre abandonn√©.\n\n\n\nDe m√™me, l‚Äôemphase du chapitre pr√©c√©dent sur la portabilit√© vise √† faciliter la mise en production. En effet, en cr√©ant un environnement normalis√© qui cr√©√© des conditions simples pour reproduire certains output, on √©vite un hiatus entre le protypage et la mise en production."
  },
  {
    "objectID": "chapters/deployment.html#pipelines-de-donn√©es",
    "href": "chapters/deployment.html#pipelines-de-donn√©es",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Pipelines de donn√©es",
    "text": "Pipelines de donn√©es\nUne chaine de production implique plusieurs √©tapes qui peuvent √©ventuellement n√©cessiter plusieurs langages. Ces √©tapes peuvent √™tre vues comme des transformations √† la chaine d‚Äôun ou plusieurs inputs afin de produire un ou plusieurs output.\nLa repr√©sentation de ces √©tapes peut √™tre faite √† l‚Äôaide des diagrammes acycliques dirig√©s (DAG):\n\nUn workflow complet sera ainsi reproductible si on peut, en ayant acc√®s aux inputs et √† l‚Äôensemble des r√®gles de transformation reproduire exactement les outputs. Si les inputs ou le code change, on peut √™tre en mesure de mettre √† jour les outputs, si possible sans faire retourner les parties du projet non concern√©s.\nUne premi√®re mani√®re de d√©velopper est l‚Äôapproche manuelle, qui est une t√¢che digne de Sisyphe:\n\nEcriture du code\nEx√©cution du code jusqu‚Äô√† sa fin\nD√©couverte d‚Äôune erreur ou mise √† jour du code ou des donn√©es\nRelance le code dans son ensemble\n\nPour √©viter ce cycle interminable, on est tent√© d‚Äô√©crire des bases interm√©diaires et de ne faire tourner qu‚Äôune partie du code. Cette approche, si elle a l‚Äôavantage de faire gagner du temps, est n√©anmoins dangereuse car on peut facilement oublier de mettre √† jour une base interm√©diaire qui a chang√© ou au contraire refaire tourner une partie du code qui n‚Äôa pas √©t√© mise √† jour.\nIl existe des m√©thodes plus fiables pour √©viter ces gestes manuels. Celles-ci sont inspir√©es de GNU Make et consistent √† cr√©er le chemin de d√©pendance de la chaine de production (lister l‚Äôenvironnement, les inputs et les outputs √† produire), √† d√©terminer les chemins affect√©s par un changement de code ou de donn√©es pour ne faire tourner √† nouveau que les √©tapes n√©cessaires.\nLes impl√©mentations en Python et R sont nombreuses. Parmi celles-ci, on peut mettre en valeur\n\nsnakemake pour Python\ntargets pour R"
  },
  {
    "objectID": "chapters/deployment.html#orchestration",
    "href": "chapters/deployment.html#orchestration",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Orchestration",
    "text": "Orchestration\nCertains outils vont plus loin:\n\nargo\nmlflow\nairflow"
  },
  {
    "objectID": "chapters/deployment.html#d√©finition",
    "href": "chapters/deployment.html#d√©finition",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "D√©finition",
    "text": "D√©finition\n\nL‚Äôint√©gration continue est un ensemble de pratiques utilis√©es en g√©nie logiciel consistant √† v√©rifier √† chaque modification de code source que le r√©sultat des modifications ne produit pas de r√©gression dans l‚Äôapplication d√©velopp√©e. [‚Ä¶] Elle permet d‚Äôautomatiser l‚Äôex√©cution des suites de tests et de voir l‚Äô√©volution du d√©veloppement du logiciel.\n\n\nLa livraison continue est une approche d‚Äôing√©nierie logicielle dans laquelle les √©quipes produisent des logiciels dans des cycles courts, ce qui permet de le mettre √† disposition √† n‚Äôimporte quel moment. Le but est de construire, tester et diffuser un logiciel plus rapidement."
  },
  {
    "objectID": "chapters/deployment.html#avantages",
    "href": "chapters/deployment.html#avantages",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Avantages",
    "text": "Avantages\nL‚Äôapproche CI/CD garantit une automatisation et une surveillance continues tout au long du cycle de vie d‚Äôun projet.\nCela pr√©sente de nombreux avantages:\n\non peut anticiper les contraintes de la mise en production gr√¢ce √† des environnements normalis√©s partant d‚Äôimage docker standardis√©es\non peut tester les changements apport√©s √† un livrable par un nouveau prototype\non peut d√©terminer tr√®s rapidement l‚Äôintroduction de bugs dans un projet"
  },
  {
    "objectID": "chapters/deployment.html#mise-en-oeuvre",
    "href": "chapters/deployment.html#mise-en-oeuvre",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Mise en oeuvre",
    "text": "Mise en oeuvre\nL‚Äôid√©e de l‚Äôapproche CI/CD est ainsi d‚Äôassocier chaque changement de code (commit) √† l‚Äôex√©cution de scripts automatis√©s. Bien que mis en oeuvre de mani√®re diff√©rente, Gitlab et Github proposent tous les deux ce type de fonctionalit√©s.\nLes actions Github sont un ensemble de r√®gles qui se suivent au format YAML. Cela permet de d√©finir diff√©rentes √©tapes du processus avec, pour chaque √©tape, des √©l√©ments de configuration.\nVoici, par exemple, l‚Äôaction qui sert de mod√®le √† modifier dans l‚Äôexercice d‚Äôapplication\nname: Python Package using Conda\n\non: [push]\n\njobs:\n  build-linux:\n    runs-on: ubuntu-latest\n    strategy:\n      max-parallel: 5\n\n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python 3.10\n      uses: actions/setup-python@v3\n      with:\n        python-version: 3.10\n    - name: Add conda to system path\n      run: |\n        # $CONDA is an environment variable pointing to the root of the miniconda directory\n        echo $CONDA/bin >> $GITHUB_PATH\n    - name: Install dependencies\n      run: |\n        conda env update --file environment.yml --name base\n    - name: Lint with flake8\n      run: |\n        conda install flake8\n        # stop the build if there are Python syntax errors or undefined names\n        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide\n        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n    - name: Test with pytest\n      run: |\n        conda install pytest\n        pytest"
  },
  {
    "objectID": "chapters/deployment.html#quest-ce-quun-site-web",
    "href": "chapters/deployment.html#quest-ce-quun-site-web",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Qu‚Äôest-ce qu‚Äôun site web ?",
    "text": "Qu‚Äôest-ce qu‚Äôun site web ?\nPour simplifier, on peut voir un site web comme la combinaison de trois √©l√©ments:\n\nune arborescence de fichiers HTML qui pr√©sentent le contenu du site dans un balisage lourd\ndes fichiers CSS qui g√®rent la mise en forme1\ndes fonctions javascript\n\nIl existe √©norm√©ment d‚Äôoutils aujourd‚Äôhui qui permettent, sans connaissance en HTML, CSS ou JS, de cr√©er un site web. Dans le domaine de la data-science, le format Markdown (fichiers .md) s‚Äôest impos√©.\nMarkdown est un syst√®me d‚Äô√©dition dot√© d‚Äôune syntaxe simplifi√©e souvent utilis√© pour faire de la documentation de projet. Le format est utilis√© sur de nombreux sites internet, notamment Gitlab et Stackoverflow. L‚Äôextension de ce type de fichier est .md. Markdown pr√©sente plusieurs avantages:\n\nil est facile d‚Äôinclure des blocs de code informatique et des √©quations math√©matiques dans un document Markdown ;\nle formatage de blocs de texte ou de code est simple et tr√®s bien fait (et beaucoup plus l√©ger qu‚Äôen LaTeX par exemple) ;\nil existe des outils de conversion de Markdown en HTML tr√®s bien faits.\n\nPlus d‚Äô√©l√©ments sur la logique de Markdown et ses int√©r√™ts sont disponibles dans le chapitre R Markdown de la documentation utilitR"
  },
  {
    "objectID": "chapters/deployment.html#comment-faire-un-site-web",
    "href": "chapters/deployment.html#comment-faire-un-site-web",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Comment faire un site web ?",
    "text": "Comment faire un site web ?\nIl existe historiquement plusieurs approches dans l‚Äô√©cosyst√®me de la data-science, selon le langage utilis√© et l‚Äôoutput d√©sir√©.\nSi on part de fichiers qui pr√©sentent des blocs de code qui ne n√©cessitent pas d‚Äô√™tre ex√©cut√©s, l‚Äô√©cosyst√®me le plus riche est Hugo. Celui-ci permet de g√©n√©rer des sites web √† l‚Äôarchitecture complexe √† partir d‚Äôune arborescence de .md. C‚Äôest l‚Äôapproche adopt√©e pour ce site web\nCependant, il est souvent n√©cessaire d‚Äôex√©cuter des bouts de code pour tester les exemples pr√©sent√©s, ou g√©n√©rer des tableaux ou sorties graphiques. Pour cela, historiquement, il existe deux paradigmes:\n\nJupyterBook qui vise √† g√©n√©rer des sites web et des notebooks √† partir de fichiers markdown. Le notebook n‚Äôest donc pas le produit de d√©part (cf.¬†XXXX) mais un livrable.\nR Markdown: l‚Äô√©cosyst√®me le plus riche avec de nombreux mod√®les de documents customisables (documents HTML ou articles PDF, sites web de documentation avec bookdown, blogs avec blogdown, dashboards, etc.) Le principe de R Markdown est d‚Äôoffrir une surcouche √† Markdown pour que les blocs de code soient ex√©cut√©s afin de cr√©er un output reproductible.\n\nR Markdown est un syst√®me d‚Äôune grande richesse. Initialement pens√© pour les utilisateurs de R, ce paradigme permet maintenant de cr√©er des documents executant d‚Äôautres langages, notamment du Python (le cours de python de 2e ann√©e de l‚ÄôENSAE est test√© et construit gr√¢ce √† R Markdown).\nQuarto est le petit nouveau dans cet √©cosyst√®me, amen√© √† devenir un outil standard des data-scientists comme peuvent l‚Äô√™tre, aujourd‚Äôhui, les Notebooks Jupyter. Successeur de R Markdown, il vise √† am√©liorer l‚Äôaspect universel des documents produits en n‚Äôobligeant plus √† utiliser R pour compiler le document qui ne le n√©cessitent pas. C‚Äôest un outil particuli√®rement adapt√© aux utilisateurs de Python\n\nUn document quarto h√©rite des principes de base d‚Äôun document R Markdown, notamment la structure. Il comporte ainsi deux parties principales :\n\nL‚Äôen-t√™te (YAML header) qui g√®re les √©l√©ments de style et les param√®tres globaux;\nLe contenu qui g√®re le fond et permet d‚Äôalterner librement texte et code :\n\nLes blocs de texte brut mis en forme selon la syntaxe markdown\nLes blocs de code sont pr√©sent√©s dans des chunks identifi√©s par un langage (ici python) qui seront ex√©cut√©s de mani√®re lin√©aire et produiront l‚Äôoutput d√©sir√© (ici une figure matplotlib)."
  },
  {
    "objectID": "chapters/deployment.html#comment-mettre-√†-disposition-un-site-web",
    "href": "chapters/deployment.html#comment-mettre-√†-disposition-un-site-web",
    "title": "D√©ployer et valoriser son projet de data science",
    "section": "Comment mettre √† disposition un site web ?",
    "text": "Comment mettre √† disposition un site web ?\nMettre √† jour manuellement un site web apr√®s l‚Äôavoir compil√© est une t√¢che p√©nible et source d‚Äôerreur dans un projet actif de data-science. L‚Äôautomatisation issue de l‚Äôapproche CI/CD permet un gain de confort aussi dans ce domaine.\nSupposons qu‚Äôon ait mis en oeuvre une routine pour automatiser la construction d‚Äôun site web √† partir de fichiers Markdown. Comment les mettre √† disposition ?\nIl existe plusieurs mani√®res de d√©ployer automatiquement un site web :\n\nGitlab pages ;\nGithub pages ;\nNetlify.\n\nCes trois services sont gratuits. Ils consistent √† mettre √† disposition un DNS sur lequel des fichiers HTML peuvent √™tre mis √† disposition automatiquement pour assurer que chaque commit\nL‚Äôexercice d‚Äôapplication supposant l‚Äôutilisation de Github, il pr√©sentera Netlify qui est plus pratique que Github Pages2. Le r√©sultat de cette chaine de production d‚Äôun site web reproductible est accessible √† l‚Äôurl https://spiffy-florentine-c913b9.netlify.app/"
  },
  {
    "objectID": "chapters/git.html",
    "href": "chapters/git.html",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "",
    "text": "Le d√©veloppement rapide de la data science au cours de ces derni√®res ann√©es s‚Äôest accompagn√©e d‚Äôune complexification substantielle des projets. Par ailleurs, les projets sont de plus en plus collaboratifs, que ce soit dans le cadre d‚Äô√©quipes dans un contexte professionnel ou bien pour des contributions √† des projets open-source. Naturellement, ces √©volutions doivent nous amener √† modifier nos mani√®res de travailler pour g√©rer cette complexit√© croissante et continuer √† produire de la valeur √† partir des projets de data science.\nPourtant, tout data scientist s‚Äôest parfois demand√© :\n\nquelle √©tait la bonne version d‚Äôun programme\nqui √©tait l‚Äôauteur d‚Äôun bout de code en particulier\nsi un changement √©tait important ou juste un essai\ncomment fusionner des programmes\netc.\n\nEt il n‚Äôest pas rare de perdre le fil des versions de son projet lorsque l‚Äôon garde trace de celles-ci de fa√ßon manuelle.\nExemple de contr√¥le de version fait ‚Äú√† la main‚Äù\n\nPourtant, il existe un outil informatique puissant afin de r√©pondre √† tous ces besoins : la gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer l‚Äôhistorique des modifications d‚Äôun ensemble de fichiers\nrevenir √† des versions pr√©c√©dentes d‚Äôun ou plusieurs fichiers\nrechercher les modifications qui ont pu cr√©er des erreurs\ntravailler simultan√©ment sur un m√™me fichier sans risque de perte\npartager ses modifications et r√©cup√©rer celles des autres\nproposer des modifications, les discuter, sans pour autant modifier la derni√®re version existante\nidentifier les auteurs et la date des modifications\n\nEn outre, ces outils fonctionnent avec tous les langages informatiques car ils reposent sur la comparaison des lignes et des caract√®res des programmes, ind√©pendamment du langage. En bref, c‚Äôest la bonne mani√®re pour partager des codes et travailler √† plusieurs sur un projet de data science. En r√©alit√©, il ne serait pas exag√©r√© de dire que l‚Äôutilisation du contr√¥le de version est la bonne pratique la plus fondamentale de tout projet faisant intervenir du code, et qu‚Äôelle conditionne largement toutes les autres.\n\n\n\nPlusieurs logiciels de contr√¥le de version existent sur le march√©. En principe, le logiciel Git, d√©velopp√© initialement pour fournir une solution d√©centralis√©e et open-source dans le cadre du d√©veloppement du noyau Linux, est devenu largement h√©g√©monique. Aussi, toutes les application de ce cours s‚Äôeffectueront √† l‚Äôaide du logiciel Git.\n\n\n\nTravailler de mani√®re collaborative avec Git implique de synchroniser son r√©pertoire local avec une copie distante, situ√©e sur un serveur h√©bergeant des projets Git. Ce serveur peut √™tre un serveur interne √† une organisation, ou bien √™tre fourni par un h√©bergeur externe. Les deux alternatives les plus populaires en la mati√®re sont GitHub et GitLab. Dans ce cours, nous utiliserons GitHub, qui est devenu au fil des ann√©es la r√©f√©rence pour l‚Äôh√©bergement des projets open-source. En pratique, les deux services sont relativement semblables, et tous les concepts pr√©sent√©s se retrouvent sous une forme similaire sur les deux plateformes."
  },
  {
    "objectID": "chapters/git.html#principes-et-commandes-usuelles",
    "href": "chapters/git.html#principes-et-commandes-usuelles",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Principes et commandes usuelles",
    "text": "Principes et commandes usuelles\nLe graphique suivant illustre les principes fondamentaux de Git.\nGit tout-en-un (Source) \nLorsqu‚Äôon utilise Git, il est important de bien distinguer ce qui se passe en local (sur son poste, sur le serveur sur lequel on travaille‚Ä¶) de ce qui se passe en remote, i.e.¬†en int√©ragissant avec un serveur distant. Comme le montre le graphique, l‚Äôessentiel du contr√¥le de version se passe en r√©alit√© en local.\nEn th√©orie, sur un projet individuel, il est m√™me possible de r√©aliser l‚Äôensemble du contr√¥le de version en mode hors-ligne. Pour cela, il suffit d‚Äôindiquer √† Git le projet (dossier) que l‚Äôon souhaite versionner en utilisant la commande git init. Cette commande a pour effet de cr√©er un dossier .git √† la racine du projet, dans lequel Git va stocker tout l‚Äôhistorique du projet (commits, branches, etc.) et permettre de naviguer entre les versions. A cause du . qui pr√©fixe son nom, ce dossier est g√©n√©ralement cach√© par d√©faut, ce qui n‚Äôest pas probl√©matique dans la mesure o√π il n‚Äôy a jamais besoin de le parcourir ou de le modifier √† la main en pratique. Retenez simplement que c‚Äôest la pr√©sence de ce dossier .git qui fait qu‚Äôun dossier est consid√©r√© comme un projet Git, et donc que vous pouvez utilisez les commandes usuelles de Git dans ce dossier √† l‚Äôaide d‚Äôun terminal : - git status : affiche les modifications du projet par rapport √† la version pr√©c√©dente ; - git add chemin_du_fichier : ajoute un fichier nouveau ou modifi√© √† la zone de staging de Git en vue d‚Äôun commit ; - git add -A : ajoute tous les fichiers nouveaux ou modifi√©s √† la zone de staging ; - git commit -m \"message de commit\" : cr√©e un commit, i.e.¬†une photographie des modifications (ajouts, modifications, suppressions) apport√©es au projet depuis la derni√®re version, et lui assigne un message d√©crivant ces changements. Les commits sont l‚Äôunit√© de base de l‚Äôhistorique du projet construit par Git.\nEn pratique, travailler uniquement en local n‚Äôest pas tr√®s int√©ressant. Pour pouvoir travailler de mani√®re collaborative, on va vouloir synchroniser les diff√©rentes copies locales du projet √† un r√©pertoire centralis√©, qui maintient de fait la ‚Äúsource de v√©rit√©‚Äù (single source of truth). M√™me sur un projet individuel, il fait sens de synchroniser son r√©pertoire local √† une copie distante pour assurer l‚Äôint√©grit√© du code de son projet en cas de probl√®me mat√©riel.\nEn g√©n√©ral, on va donc initialiser le projet dans l‚Äôautre sens : - cr√©er un nouveau projet sur GitHub - g√©n√©rer un jeton d‚Äôacc√®s (personal access token) - cloner le projet en local via la m√©thode HTTPS : git clone https://github.com/<username>/<project_name>.git\nLe projet clon√© est un projet Git ‚Äî il contient le dossier .git ‚Äî synchronis√© par d√©faut avec le r√©pertoire distant. On peut le v√©rifier avec la commande remote de Git :\n$ git remote -v\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (fetch)\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (push)\nLe projet local est bien li√© au r√©pertoire distant sur GitHub, auquel Git donne par d√©faut le nom origin. Ce lien permet d‚Äôutiliser les commandes de synchronisation usuelles : - git pull : r√©cup√©rer les changements (fetch) sur le remote et les appliquer au projet local - git push : envoyer les changements locaux sur le remote"
  },
  {
    "objectID": "chapters/git.html#impl√©mentations",
    "href": "chapters/git.html#impl√©mentations",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Impl√©mentations",
    "text": "Impl√©mentations\nGit est un logiciel, qui peut √™tre t√©l√©charg√© sur le site officiel pour diff√©rents syst√®mes d‚Äôexploitation. Il existe cependant diff√©rentes mani√®res d‚Äôutiliser Git : - le client en ligne de commande : c‚Äôest l‚Äôimpl√©mentation standard, et donc la plus compl√®te. C‚Äôest celle qu‚Äôon utilisera dans ce cours. Le client Git est install√© par d√©faut sur les diff√©rents services du SSP Cloud (VSCode, RStudio, Jupyter, etc.) et peut donc √™tre utilis√© via n‚Äôimporte quel terminal. La documentation du SSP Cloud d√©taille la proc√©dure ; - des interfaces graphiques : elles facilitent la prise en main de Git via des guides visuels, mais ne permettent pas de r√©aliser toutes les op√©rations permises par Git - l‚Äôinterface native de RStudio pour les utilisateurs de R : tr√®s compl√®te et stable. La formation au travail collaboratif avec Git et RStudio pr√©sente son utilisation de mani√®re d√©taill√©e ; - le plugin Jupyter-git pour les utilisateurs de Python : elle impl√©mente les principales features de Git, mais s‚Äôav√®re assez instable √† l‚Äôusage."
  },
  {
    "objectID": "chapters/git.html#bonnes-pratiques",
    "href": "chapters/git.html#bonnes-pratiques",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\nLe contr√¥le de version est une bonne pratique de d√©veloppement en soi‚Ä¶ mais son utilisation admet elle m√™me des bonnes pratiques qui, lorsqu‚Äôelles sont appliqu√©es, permettent d‚Äôen tirer le plus grand profit.\n\nQue versionne-t-on ?\n\nUniquement des fichiers texte\nA chaque commit, Git calcule les diff√©rences entre les versions successives du projet, afin de ne pas avoir √† stocker une image compl√®te de ce dernier √† chaque fois. C‚Äôest ce qui permet aux projets Git de rester tr√®s l√©gers par d√©faut, et donc aux diff√©rentes op√©rations impliquant le remote (clone, push, pull..) d‚Äô√™tre tr√®s rapides.\nLa contrepartie de cette l√©g√®ret√© est une contrainte sur les types d‚Äôobjets que l‚Äôon doit versionner. Les diff√©rences sont calculables uniquement sur des fichiers de type texte : codes source, fichiers texte, fichiers de configuration non-sensibles‚Ä¶ Voici donc une liste non-exhaustive des extensions de fichier que l‚Äôon retrouve fr√©quemment dans un d√©p√¥t Git d‚Äôun projet R ou Python : .py, .R, .Rmd, .txt, .json, .xml, .yaml, .toml, et bien d‚Äôautres.\nEn revanche tous les fichiers binaires ‚Äî pour faire simple, tous les fichiers qui ne peuvent pas √™tre ouverts dans un √©diteur de texte basique sans produire une suite inintelligible de caract√®res ‚Äî n‚Äôont g√©n√©ralement pas destination √† se retrouver sur un d√©p√¥t Git. Du fait de leur formatage (binaire), Git ne peut pas calculer les diff√©rences entre versions pour ces fichiers et c‚Äôest donc le fichier entier qui est sauvegard√© dans l‚Äôhistorique √† chaque changement, ce qui peut tr√®s rapidement faire cro√Ætre la taille du d√©p√¥t. Pour √©viter de versionner ces fichiers par erreur, on va les ajouter au fichier .gitignore (cf.¬†supra).\n\n\nPas de donn√©es\nComme expliqu√© en introduction, le fil rouge de ce cours sur les bonnes pratiques est l‚Äôimportance de bien s√©parer code, donn√©es et environnement d‚Äôex√©cution afin de favoriser la reproductibilit√© des projets de data science. Ce principe doit s‚Äôappliquer √©galement √† l‚Äôusage du contr√¥le de version, et ce pour diff√©rentes raisons.\nA priori, inclure ces donn√©es dans un d√©p√¥t Git peut sembler une bonne id√©e en termes de reproductibilit√©. En machine learning par exemple, on est souvent amen√© √† r√©aliser de nombreuses exp√©rimentations √† partir d‚Äôun m√™me mod√®le appliqu√© √† diff√©rentes transformations des donn√©es initiales, transformations que l‚Äôon pourrait versionner. En pratique, il est g√©n√©ralement pr√©f√©rable de versionner le code qui permet de g√©n√©rer ces transformations et donc les exp√©rimentations associ√©es, dans la mesure o√π le suivi des versions des datasets peut s‚Äôav√©rer rapidement complexe. Pour de plus gros projets, des alternatives sp√©cifiques existent : c‚Äôest le champ du MLOps, domaine en constante expansion qui vise √† rendre les pipelines de machine learning plus reproductibles.\nEnfin, la structure m√™me de Git n‚Äôest techniquement pas faite pour le stockage de donn√©es. Si des petits datasets dans un format texte ne poseront pas de probl√®me, des donn√©es volumineuses (√† partir de plusieurs Mo) vont faire cro√Ætre la taille du d√©p√¥t et donc ralentir significativement les op√©rations de synchronisation avec le remote.\n\n\nPas d‚Äôinformations locales\nL√† encore en vertu du principe de s√©paration donn√©es / code/ environnement, les donn√©es locales, i.e.¬†sp√©cifiques √† l‚Äôenvironnement de travail sur lequel le code a √©t√© ex√©cut√©, n‚Äôont pas vocation √† √™tre versionn√©es. Par exemple, des fichiers de configuration sp√©cifiques √† un poste de travail, des chemins d‚Äôacc√®s sp√©cifiques √† un ordinateur donn√©, etc. Cela demande une plus grande rigueur lors de la construction du projet, mais garantit par l√† m√™me une meilleure reproductiblit√© pour les futurs utilisateurs du projet.\n\n\nPas d‚Äôoutputs\nLes outputs d‚Äôun projet (graphiques, publications, mod√®le entra√Æn√©‚Ä¶) n‚Äôont pas vocation √† √™tre versionn√©, en vertu des diff√©rents arguments pr√©sent√©s ci-dessus : - il ne s‚Äôagit g√©n√©ralement pas de fichiers de type texte ; - le code source du projet doit dans tous les cas permettre des les reg√©n√©rer √† l‚Äôidentique.\n\n\nUtiliser un .gitignore\nOn a list√© pr√©c√©demment un large √©ventail de fichiers qui n‚Äôont, par nature, pas vocation √† √™tre versionn√©. Bien entendu, faire attention √† ne pas ajouter ces diff√©rents fichiers au moment de chaque git add serait assez p√©nible. Git simplifie largement cette proc√©dure en nous donnant la possibilit√© de remplir un fichier .gitignore, situ√© √† la racine du projet, qui sp√©cifie l‚Äôensemble des fichiers et types de fichiers que l‚Äôon ne souhaite pas versionner dans le cadre du projet courant.\nDe mani√®re g√©n√©rale, il y a pour chaque langage des fichiers que l‚Äôon ne souhaitera jamais versionner. Pour en tenir compte, une premi√®re bonne pratique est de choisir le .gitignore associ√© au langage du projet lors de la cr√©ation du d√©p√¥t sur GitHub. Ce faisant, le projet est initialit√© avec un gitignore d√©j√† existant et pr√©-rempli de chemins et de types de fichiers qui ne sont pas √† versionner. Regardons un extrait du gitignore Python pour comprendre sa structure et son fonctionnement.\npip-log.txt\n__pycache__/\n*.log\nChaque ligne du gitignore sp√©cifie un √©l√©ment √† ignorer du contr√¥le de version, √©l√©ment qui peut √™tre un ficher/dossier ou bien une r√®gle concernant un ensemble de fichiers/dossiers. Sauf si sp√©cifi√© explicitement, les chemins sont relatifs √† la racine du projet. L‚Äôextrait du gitignore Python illustre les diff√©rentes possibilit√©s :\n\nligne 1 : ignore le fichier pip-log.txt ;\nligne 2 : ignore le dossier __pycache__/ ;\nligne 3 : ignore tous les fichiers dont l‚Äôextension est .log.\n\nDe nombreuses autres possiblit√©s existent, et sont d√©taill√©es par exemple dans la documentation de Git.\n\n\n\nMessages des commits\nLe commit est l‚Äôunit√© de temps de Git, et donc fondamentalement ce qui permet de remonter dans l‚Äôhistorique d‚Äôun projet. Afin de pouvoir b√©n√©ficier √† plein de cet avantage de Git, il est capital d‚Äôaccompagner ses commits de messages pertinents, en se pla√ßant dans la perspective que l‚Äôon peut √™tre amen√© plusieurs semaines ou mois plus tard √† vouloir retrouver du code dans l‚Äôhistorique de son projet. Les quelques secondes prises √† chaque commit pour r√©fl√©chir √† une description pertinente du bloc de modifications que l‚Äôon apporte au projet peuvent donc faire gagner un temps pr√©cieux √† la longue.\nDe nombreuses conventions existent pour r√©diger des messages de commit pertinents. Nous rappelons ici les plus importantes :\n\ncontenu : le message doit d√©tailler le pourquoi plut√¥t que le comment des modifications. Par exemple, plut√¥t que ‚ÄúAjoute le fichier test.py‚Äù, on pr√©f√©rera √©crire ‚ÄúAjout d‚Äôune s√©rie de tests unitaires‚Äù ;\nstyle : le message doit √™tre √† l‚Äôimp√©ratif et former une phrase (sans point √† la fin) ;\nlongueur : le message du commit doit √™tre court (< 72 caract√®res). S‚Äôil n‚Äôest pas possible de trouver un message de cette taille qui r√©sume le commit, c‚Äôest g√©n√©ralement un signe que le commit regroupe trop de changements (cf.¬†point suivant). Le fait de devoir mettre des + ou des & / et dans un message de commit pour s√©parer les changements est √©galement un bon indicateur d‚Äôun commit trop gros.\n\n\n\nFr√©quence des commits\nDe mani√®re g√©n√©rale, il est conseill√© de r√©aliser des commits r√©guliers lorsque l‚Äôon travaille sur un projet. Une r√®gle simple que l‚Äôon peut par exemple appliquer est la suivante : d√®s lors qu‚Äôun ensemble de modifications forment un tout coh√©rent et peuvent √™tre r√©sum√©es par un message simple, il est temps d‚Äôen faire un commit. Cette approche a de nombreux avantages :\n\nsi l‚Äôon fait suivre chaque commit d‚Äôun push ‚Äî ce qui est conseill√© en pratique ‚Äî on s‚Äôassure de disposer r√©guli√®reemnt d‚Äôune copie de ses travaux, ce qui limite le risque de perte ;\nil est plus facile de revenir en arri√®re en cas d‚Äôerreur si les commits portent sur des changements cibl√©s et coh√©rents ;\nle processus de review d‚Äôune pull request est facilit√©, car les diff√©rents blocs de modification sont plus clairement s√©par√©s ;\ndans une approche d‚Äôint√©gration continue ‚Äî concept que l‚Äôon verra en d√©tail dans le chapitre sur la [mise en production]() ‚Äî faire des commits et des PR r√©guli√®rement permet de d√©ployer de mani√®re continue les changements en production, et donc d‚Äôobtenir les feedbacks des utilisateurs et d‚Äôadapter plus rapidement si besoin."
  },
  {
    "objectID": "chapters/git.html#branches",
    "href": "chapters/git.html#branches",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Branches",
    "text": "Branches\n\nConcept\nLa possibilit√© de cr√©er des branches est l‚Äôune des fonctionnalit√©s majeures de Git. La cr√©ation d‚Äôune branche au sein d‚Äôun projet permet de diverger de la ligne principale de d√©veloppement (g√©n√©ralement appel√©e master, terme tendant √† dispara√Ætre au profit de celui de main) sans impacter cette ligne. Cela permet de s√©parer le nouveau d√©veloppement et de faire cohabiter plusieurs versions, pouvant √©voluer s√©par√©ment et pouvant √™tre facilement rassembl√©es si n√©cessaire.\nPour comprendre comment fonctionnent les branches, il nous faut revenir un peu plus en d√©tail sur la mani√®re dont Git stocke l‚Äôhistorique du projet. Comme nous l‚Äôavons vu, l‚Äôunit√© temporelle de Git est le commit, qui correspond √† une photographie √† un instant donn√© de l‚Äô√©tat du projet (snapshot). Chaque commit est uniquement identifi√© par un hash, une longue suite de caract√®res. La commande git log, qui liste les diff√©rents commits d‚Äôun projet, permet d‚Äôafficher ce hash ainsi que diverses m√©tadonn√©es (auteur, date, message) associ√©es au commit.\n$ git log\ncommit e58b004d3b68bdf28093fe6ad6036b5d13216e55 (HEAD -> master, origin/master, origin/HEAD)\nAuthor: Lino Galiana <xxx@xxx.fr>\nDate:   Tue Mar 22 14:34:04 2022 +0100\n\n    ajoute code √©quivalent python\n\n...\nUne branche est simplement un pointeur vers un commit. Dans l‚Äôexemple pr√©c√©dent, on a imprim√© les informations du dernier commit en date. La branche principale (master) pointe vers ce commit. Si l‚Äôon faisait un nouveau commit, le pointeur se d√©calerait et la branche master pointerait √† pr√©sent sur le nouveau commit.\n\n\nBranches locales\nDans ce contexte, cr√©er une nouvelle branche (en local) revient simplement √† cr√©er un nouveau pointeur vers un commit donn√©. Supposons que l‚Äôon cr√©e une branche testing √† partir du dernier commit.\n$ git branch testing  # Cr√©e une nouvelle branche\n$ git branch  # Liste les branches existantes\n* master  # La branche sur laquelle on se situe\n  testing  # La nouvelle branche cr√©√©e\nLa figure suivante illustre l‚Äôeffet de cette cr√©ation sur l‚Äôhistorique Git.\n\nD√©sormais, deux branches (master et testing) pointent vers le m√™me commit. Si l‚Äôon effectue √† pr√©sent des commits sur la branche testing, on va diverger de la branche principale, ce qui permet de d√©velopper une nouvelle fonctionnalit√© sans risquer d‚Äôimpacter master.\nPour savoir sur quelle branche on se situe √† instant donn√© ‚Äî et donc sur quelle branche on va commiter ‚Äî Git utilise un pointeur sp√©cial, appel√© HEAD, qui pointe vers la branche courante. On comprend √† pr√©sent mieux la signification de HEAD -> master dans l‚Äôoutput de la commande git log vu pr√©c√©demment. Cet √©l√©ment sp√©cifie la situation locale actuelle et signifie : on se situe actuellement sur la branche master, qui pointe sur le commit e58b004. Pour changer de branche, i.e.¬†d√©placer le HEAD, on utilise la commande git checkout. Par exemple, pour passer de la branche master sur laquelle on est par d√©faut √† la branche testing :\n$ git checkout testing  # Changement de branche\nSwitched to branch 'testing'\nOn se situe d√©sormais sur la branche testing, sur laquelle on peut laisser libre cours √† sa cr√©ativit√© sans risquer d‚Äôimpacer la branche principale du projet. Mais que se passe-t-il si, pendant que l‚Äôon d√©veloppe sur testing, un autre membre du projet commit sur master ? On dit que les historiques ont diverg√©. La figure suivante illustre √† quoi ressemble √† pr√©sent l‚Äôhistorique du projet (et suppose que l‚Äôon est repass√© sur master).\n\nCette divergence n‚Äôest pas probl√©matique en soi : il est normal que les diff√©rentes parties et exp√©rimentations d‚Äôun projet avancent √† diff√©rents rythmes. La difficult√© est de savoir comment r√©concillier les diff√©rents changements si l‚Äôon d√©cide que la branche testing doit √™tre int√©gr√©e dans master. Deux situations peuvent survenir : - les modifications op√©r√©es en parall√®le sur les deux branches ne concernent pas les m√™mes fichiers ou les m√™mes parties des fichiers. Dans ce cas, Git est capable de fusionner (merge) les changements automatiquement et tout se passe sans encombre ; - dans le cas contraire, survient un merge conflict : les branches ont diverg√© de telle sorte qu‚Äôil n‚Äôest pas possible pour Git de fusionner les changements automatiquement. Il faut alors r√©soudre les conflits manuellement.\nLa r√©solution des conflits est une √©tape souvent douloureuse lors de l‚Äôapprentissage de Git. Aussi, nous conseillons dans la mesure du possible de ne pas fusionner des branches manuellement en local avec Git ‚Äî c‚Äôest d‚Äôailleurs pour cette raison que nous n‚Äôavons pas d√©taill√© les commandes pour le faire. Dans les sections suivantes, nous verrons comment une bonne organisation pr√©alable du travail en √©quipe, combin√©e aux outils collaboratifs fournis par GitHub, permet de rendre le processus de fusion des branches largement indolore.\n\n\nBranches remote\nRappellons que toutes les op√©rations que nous avons effectu√©es sur les branches dans cette section se sont pass√©s en local, le r√©pertoire distant est rest√© totalement inchang√©. Pour pouvoir collaborer sur une branche ou bien en faire une pull request (cf.¬†supra), il faut pousser la branche sur le r√©pertoire distant. La commande est simple : git push origin <branche>.\n$ git push origin testing\nCounting objects: 24, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (15/15), done.\nWriting objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done.\nTotal 24 (delta 2), reused 0 (delta 0)\nTo https://github.com/linogaliana/ensae-reproductibilite-website\n * [new branch]      testing -> testing"
  },
  {
    "objectID": "chapters/git.html#workflow-collaboratif",
    "href": "chapters/git.html#workflow-collaboratif",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Workflow collaboratif",
    "text": "Workflow collaboratif\nComme on l‚Äôa vu pr√©c√©demment, si le mod√®le des branches de Git semble id√©al pour g√©rer le travail collaboratif et asynchrone, il peut √©galement s‚Äôav√©rer rapidement complexe √† manipuler en l‚Äôabsence d‚Äôune bonne organisation du travail en √©quipe. De nombreux mod√®les (‚Äúworkflows‚Äù) existent en la mati√®re, avec des complexit√©s plus ou moins grandes selon la nature du projet. Nous conseillons d‚Äôadopter dans la plupart des cas un mod√®le tr√®s simple : le GitHub Flow.\nLe GitHub Flow est une m√©thode d‚Äôorganisation minimaliste du travail collaboratif, qui est progressivement devenue la norme dans les projets open-source. Elle est r√©sum√©e par la figure suivante, dont nous d√©taillons par la suite les diff√©rentes √©tapes.\n\n\nD√©finition des r√¥les des contributeurs\nDans tout projet collaboratif, une premi√®re √©tape essentielle est de bien d√©limiter les r√¥les des diff√©rents contributeurs. Les diff√©rents participants au projet ont en effet g√©n√©ralement des r√¥les diff√©rents dans l‚Äôorganisation, des niveaux diff√©rents de pratique de Git, etc. Il est important de refl√©ter ces diff√©rents r√¥les dans l‚Äôorganisation du travail collaboratif.\nSur les diff√©rents h√©bergeurs de projets Git, cela prend la forme de r√¥les que l‚Äôon attribue aux diff√©rents membres du porjet. Les mainteneurs sont les seuls √† pouvoir √©crire directement sur master. Les contributeurs sont quant √† eux tenus de d√©velopper sur des branches. Cela permet de prot√©ger la branche principale, qui doit rester une version propre et jamais compromise du projet.\nNotons que la possibilit√© de donner des r√¥les sur les projets GitHub n‚Äôest possible que dans le cadre d‚Äôorganisations (payantes), donc dans un contexte professionnel ou de projets open-source d‚Äôune certaine ampleur. Pour des petits projets, il est n√©cessaire de s‚Äôastreindre √† une certaine rigueur individuelle pour respecter cette organisation.\n\n\nD√©veloppement sur des branches de court-terme\nLes contributeurs d√©veloppent uniquement sur des branches. Il est d‚Äôusage de cr√©er une branche par fonctionnalit√©, en lui donnant un nom refl√©tant la fonctionnalit√© en cours de d√©veloppement (ex : ajout-tests-unitaires). Les diff√©rents contributeurs √† la fonctionnalit√© en cours de d√©veloppement font des commits sur la branche, en prenant bien soin de pull r√©guli√®rement les √©ventuels changements pour ne pas risquer de conflits de version. Pour la m√™me raison, il est pr√©f√©rable de faire des branches dites de court-terme, c‚Äôest √† dire propres √† une petite fonctionnalit√©, quite √† diviser une fonctionnalit√© en s√©ries d‚Äôimpl√©mentations. Cela permet de limiter les √©ventuels conflits √† g√©rer lors de la fusion finale de la branche avec master.\n\n\nPull Request\nUne fois la s√©rie de modifications termin√©e, vient le temps de rassembler les diff√©rents travaux, par l‚Äôinterm√©diaire de la fusion entre la branche et master. Il faut alors ‚Äúdemander‚Äù de fusionner (pull request) sur GitHub. Cela ouvre une page li√©e √† la pull request, qui rappelle les diff√©rents changements apport√©s et leurs auteurs, et permet d‚Äôentamer une discussion √† propos de ces changements.\n\n\nProcessus de review\nLes diff√©rents membres du projet peuvent donc analyser et commenter les changements, poser des questions, sugg√©rer des modifications, apporter d‚Äôautres contributions, etc. Il est par exemple possible de mentionner un membre de l‚Äô√©quipe par l‚Äôinterm√©diaire de @personne. Il est √©galement possible de proc√©der √† une code review, par exemple par un d√©veloppeur plus exp√©riment√©.\n\n\nR√©solution des √©ventuels conflits\nEn adoptant cette mani√®re de travailler, master ne sera modifi√©e que via des pull requests. Il ne devrait donc jamais y avoir le moindre conflit √† r√©gler sur master, les seuls conflits possibles se passent sur les branches. Par exemple, dans le cas o√π une autre pull request aurait √©t√© fusionn√©e sur master depuis l‚Äôouverture de la pull request en question.\nDans le cas d‚Äôun conflit √† g√©rer, le conflit doit √™tre r√©solu dans la branche et pas dans master. Voici la marche √† suivre :\n\nappliquez le conseil de survie : faites une copie de sauvegarde de votre clone\ndans votre clone, placez vous sur la branche en question : git checkout nom-de-la-branche\nfusionnez master dans la branche : git merge master\nr√©solvez les √©ventuels conflits dans les fichiers concern√©s\nfinalisez le commit de merge et poussez-le sur la branche remote, ce qui le fera appara√Ætre dans la pull request\n\n\n\nFusion de la branche\nSi tout est en ordre, la branche peut √™tre fusionn√©e. Seuls les mainteneurs, qui ont les droits sur master, peuvent fusionner la pull request. En termes d‚Äôhistorique du projet, deux choix sont possibles : - ‚ÄúCreate a merge commit‚Äù : tous les commits r√©alis√©s sur la branche appara√Ætront dans l‚Äôhistorique du projet ; - ‚ÄúSquash and merge‚Äù : les diff√©rents commits r√©alis√©s sur la branche seront rassembl√©s en un commit unique. Cette option est g√©n√©ralement pr√©f√©rable lorsqu‚Äôon utilise des branches de court-terme : elles permettent de garder l‚Äôhistorique plus lisible."
  },
  {
    "objectID": "chapters/git.html#utiliser-les-issues",
    "href": "chapters/git.html#utiliser-les-issues",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Utiliser les issues",
    "text": "Utiliser les issues\nLa mani√®re la plus simple de contribuer √† un projet open-source est d‚Äôouvrir une issue. Sur GitHub, cela se fait sur la page du projet, sous l‚Äôonglet Issue (cf.¬†documentation officielle). Les issues peuvent avoir diff√©rentes nature : - suggestion d‚Äôam√©lioration (sans code) - notification de bug - rapports d‚Äôexp√©rience - etc.\nLes issues sont une mani√®re tr√®s peu couteuse de contributer √† un projet, mais leur importance est capitale, dans la mesure o√π il est impossible pour les d√©veloppeurs d‚Äôun projet de penser en amont √† toutes les utilisations possibles et donc tous les bugs possibles d‚Äôune application."
  },
  {
    "objectID": "chapters/git.html#proposer-une-pull-request",
    "href": "chapters/git.html#proposer-une-pull-request",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Proposer une pull request",
    "text": "Proposer une pull request\nUne autre mani√®re, plus ambitieuse, de contribuer √† l‚Äôopen source est de proposer des pull requests. Concr√®tement, l‚Äôid√©e est de proposer une am√©lioration ou bien de r√©soudre un bug sous forme de code, que les mainteneurs du projet peuvent ensuite d√©cider d‚Äôint√©grer au code existant.\nLa proc√©dure pour proposer une pull request √† un projet sur lequel on n‚Äôa aucun droit est tr√®s similaire √† celle d√©crite ci-dessus dans le cas normal. La principale diff√©rence est que, du fait de l‚Äôabsence de droits, il est impossible de pousser une branche locale sur le r√©pertoire du projet. On va donc devoir cr√©er au pr√©alable un fork, i.e.¬†une copie du projet que l‚Äôon cr√©e dans son espace personnel sur GitHub. C‚Äôest sur cette copie que l‚Äôon va appliquer la proc√©dure d√©crite pr√©c√©demment, en prenant bien soin de travailler sur une branche et non sur master. Une fois les modifications pertinentes effectu√©es sur la branche du fork, GitHub propose de cr√©er une pull request sur le d√©p√¥t original. Cette pull request sera alors visible des mainteneurs du projet, qui pourront l‚Äô√©valuer et d√©cider d‚Äôadopter (ou non) les changements propos√©s."
  },
  {
    "objectID": "chapters/git.html#respecter-les-r√®gles-de-contribution",
    "href": "chapters/git.html#respecter-les-r√®gles-de-contribution",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Respecter les r√®gles de contribution",
    "text": "Respecter les r√®gles de contribution\nVouloir contribuer √† un projet open-source est tr√®s louable, mais ne peut pas pour autant se faire n‚Äôimporte comment. Un projet est constitu√© de personnes, qui ont d√©velopp√© ensemble une mani√®re de travailler, des standards de bonnes pratiques, etc. Pour s‚Äôassurer que sa contribution ne reste pas lettre morte, il est indispensable de s‚Äôimpr√©gner un minimum de la culture du projet.\nPour faciliter les contributions, les projets open-source sp√©cifient bien souvent la mani√®re dont les utilisateurs peuvent contribuer ainsi que le format attendu. En g√©n√©ral, ces r√®gles de contribution sont sp√©cifi√©es dans un fichier CONTRIBUTING.md situ√© √† la racine du projet GitHub, ou a d√©faut dans le README du projet. Il est essentiel de bien lire ce document s‚Äôil existe afin de s‚Äôassurer de proposer des contributions pertinentes."
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours s‚Äôadresse aux praticiens de la data science, entendue ici au sens large comme la combinaison de techniques issues des math√©matiques, de la statistique et de l‚Äôinformatique pour produire de la connaissance utile √† partir de donn√©es. Cela inclut donc tout autant les data scientists et statisticiens travaillant dans le priv√© ou dans des administrations que les chercheurs dont les travaux font intervenir des traitements computationnels √† partir de donn√©es.\nCe cours part du constat que les formations acad√©miques dans ce domaine adoptent souvent une orientation essentiellement technique, visant une compr√©hension fine des mod√®les manipul√©s, mais ne discutent que rarement des probl√®mes pratiques qui forment le quotidien du data scientist dans un contexte professionnel. Ce cours vise √† combler ce manque en proposant des pistes de solution √† diverses questions que peuvent se poser les data scientists lorsqu‚Äôils transitionnent du contexte de la formation initiale √† des projets r√©els :\n\nComment travailler de mani√®re collaborative sur un projet ?\nComment partager du code et s‚Äôassurer que celui-ci va tourner sans erreur sur un autre environnement d‚Äôex√©cution ?\nComment passer d‚Äôun environnement de d√©veloppement1 √† un environnement de production2?\nComment d√©ployer un mod√®le de data science, et rendre celui-ci accessible √† des utilisateurs afin de le valoriser ?\nComment automatiser les diff√©rentes √©tapes de son projet afin de simplifier sa maintenance ?\n\n\n\n\n\n\n\nRessource compl√©mentaire: le missing semester du MIT\n\n\n\nBeaucoup de praticiens ont pris conscience de certains manques dans les cursus de formations en statistiques ou informatique. Certaines ressources tr√®s utiles ont √©t√© regroup√©es dans le missing semester du MIT dont une partie des contenus pr√©sente des th√®mes communs avec ce cours.\n\n\nAfin de proposer des r√©ponses √† ces interrogations, ce cours pr√©sente un ensemble de bonnes pratiques et d‚Äôoutils issus de diff√©rents domaines de l‚Äôinformatique, comme le d√©veloppement logiciel, l‚Äôinfrastructure, l‚Äôadministration de serveurs, le d√©ploiement applicatif, etc.. L‚Äôobjectif n‚Äôest bien entendu pas de d√©velopper une expertise dans chacun de ces domaines, dans la mesure o√π ces comp√©tences font l‚Äôobjet de m√©tiers √† part enti√®re, que sont les d√©veloppeurs, les data architects, les sysadmin, ou encore les data engineers.\nEn revanche, face √† la taille croissante des projets de data science et donc des √©quipes qui les portent, le data scientist tend √† se retrouver √† l‚Äôinterface de ces diff√©rentes professions, avec lesquelles il doit communiquer de mani√®re efficiente pour mener ces projets √† bien. Ce cours vise √† fournir, plus que des connaissances techniques pointues, les √©lements de langage n√©cessaires pour pouvoir jouer ce r√¥le d‚Äôinterface en communiquant √† la fois avec les √©quipes m√©tiers et les √©quipes techniques qui entourent un projet de data science."
  },
  {
    "objectID": "chapters/introduction.html#origine",
    "href": "chapters/introduction.html#origine",
    "title": "Introduction",
    "section": "Origine",
    "text": "Origine\nLa notion de ‚Äúbonnes pratiques‚Äù qui nous int√©resse dans ce cours trouve son origine au sein de la communaut√© des d√©veloppeurs logiciels. Elle constitue une r√©ponse √† plusieurs constats :\n\nle ‚Äúcode est beaucoup plus souvent lu qu‚Äôil n‚Äôest √©crit‚Äù (Guido Van Rossum) ;\nla maintenance d‚Äôun code demande souvent (beaucoup) plus de ressources que son d√©veloppement initial ;\nla personne qui maintient une base de code a de fortes chances de ne pas √™tre celle qui l‚Äôa √©crite.\n\nFace √† ces constats, un ensemble de r√®gles informelles ont √©t√© conventionnellement accept√©es par la communaut√© des d√©veloppeurs comme produisant des logiciels plus fiables, √©volutifs et maintenables dans le temps. Comme toutes conventions de langue, certaines peuvent para√Ætre arbitraires. Ces r√®gles favorisent n√©anmoins la capacit√© √† communiquer du code et r√©duisent le co√ªt pour le faire √©voluer.\nR√©cemment, dans le contexte d‚Äôune √©volution des logiciels vers des applications web vivant dans le cloud, un certain nombre de ces bonnes pratiques ont √©t√© formalis√©es dans un manifeste : la 12 Factor App."
  },
  {
    "objectID": "chapters/introduction.html#pourquoi-sint√©resser-aux-bonnes-pratiques",
    "href": "chapters/introduction.html#pourquoi-sint√©resser-aux-bonnes-pratiques",
    "title": "Introduction",
    "section": "Pourquoi s‚Äôint√©resser aux bonnes pratiques ?",
    "text": "Pourquoi s‚Äôint√©resser aux bonnes pratiques ?\n\nEn quoi est-ce pertinent pour le data scientist, dont le r√¥le n‚Äôest pas de d√©velopper des applications mais de donner du sens aux donn√©es ?\n\nDu fait du d√©veloppement rapide de la data science et cons√©quemment de la croissance de la taille moyenne des projets, l‚Äôactivit√© du data scientist tend √† se rapprocher par certains aspects de celle du d√©veloppeur :\n\nles projets sur lesquels travaille le data scientist sont intenses en code ;\nil doit travailler de mani√®re collaborative au sein de projets de grande envergure ;\nil est de plus en plus amen√© √† travailler √† partir de donn√©es massives, ce qui n√©cessite de travailler sur des infrastructures big data informatiquement complexes ;\nil est amen√© √† interagir avec des profils informatiques pour d√©ployer ses mod√®les et les rendre accessible √† des utilisateurs.\n\nAussi, il fait sens pour le data scientist moderne de s‚Äôint√©resser aux bonnes pratiques en vigueur dans la communaut√© des d√©veloppeurs. Bien entendu, celles-ci doivent √™tre adapt√©es aux sp√©cificit√©s des projets bas√©s sur des donn√©es. Faisons √† pr√©sent un tour d‚Äôhorizon des bonnes pratiques et des outils que nous verrons tout au long ce cours."
  },
  {
    "objectID": "chapters/introduction.html#voir-le-code-comme-un-outil-de-communication",
    "href": "chapters/introduction.html#voir-le-code-comme-un-outil-de-communication",
    "title": "Introduction",
    "section": "Voir le code comme un outil de communication",
    "text": "Voir le code comme un outil de communication\nLa premi√®re bonne pratique √† adopter est de consid√©rer le code comme un outil de communication, et non simplement de mani√®re fonctionnelle. Un code ne sert pas seulement √† r√©aliser une t√¢che donn√©e, il a vocation √† √™tre diffus√©, r√©utilis√©, maintenu, que ce soit dans le contexte d‚Äôune √©quipe dans une organisation ou bien en open-source.\nPour favoriser cette communication du code, des conventions ont √©t√© developp√©es en mati√®re de qualit√© du code et de structuration des projets, qu‚Äôil est utile d‚Äôappliquer dans ses projets. Nous pr√©sentons ces conventions dans les chapitres Qualit√© du Code et Architecture des Projets.\nIl est pour les m√™mes raisons indispensable d‚Äôappliquer les principes du contr√¥le de version, qui permettent une documentation en continu des projets, ce qui accro√Æt fortement leur r√©utilisabilit√© et leur maintenabilit√© dans le temps. Nous proposons donc un chapitre de rappel sur l‚Äôutilisation du logiciel Git dans le chapitre Versionner son code et travailler collaborativement avec Git."
  },
  {
    "objectID": "chapters/introduction.html#travailler-de-mani√®re-collaborative",
    "href": "chapters/introduction.html#travailler-de-mani√®re-collaborative",
    "title": "Introduction",
    "section": "Travailler de mani√®re collaborative",
    "text": "Travailler de mani√®re collaborative\nLe data scientist, quel que soit son contexte de travail, est amen√© √† travailler dans le cadre de projets en √©quipe. Cela implique de d√©finir une organisation du travail ainsi que d‚Äôutiliser des outils permettant de collaborer sur un projet de mani√®re efficace et s√©curis√©e.\nNous pr√©sentons une mani√®re moderne de travailler collaborativement avec Git et GitHub dans le chapitre de rappel Versionner son code et travailler collaborativement avec Git. Les autres chapitres prendront pour acquis cette approche collaborative et la raffineront √† travers l‚Äôapproche DevOps3."
  },
  {
    "objectID": "chapters/introduction.html#maximiser-la-reproductibilit√©",
    "href": "chapters/introduction.html#maximiser-la-reproductibilit√©",
    "title": "Introduction",
    "section": "Maximiser la reproductibilit√©",
    "text": "Maximiser la reproductibilit√©\nLe troisi√®me pilier des bonnes pratiques pr√©sent√©es dans ce cours est la reproductibilit√©.\nUn projet est dit reproductible lorsque, avec le m√™me code et les m√™mes donn√©es, il est possible de reproduire les r√©sultats obtenus. Notons bien que le probl√®me de la reproductibilit√© est diff√©rent de celui de la r√©plicabilit√©. La r√©plicabilit√© est un concept scientifique, qui signifie qu‚Äôun m√™me proc√©d√© exp√©rimental donne des r√©sultats analogues sur des jeux de donn√©es diff√©rents. La reproductibilit√© est un concept technique : elle ne signifie pas que le protocole exp√©rimental est scientifiquement correct, mais qu‚Äôil a √©t√© sp√©cifi√© et diffus√© d‚Äôune mani√®re qui permet √† tous de reproduire les r√©sultats obtenus.\nLa notion de reproductibilit√© est le fil rouge de ce cours : toutes les notions vues dans les diff√©rents chapitres y contribuent. Le fait de produire du code et des projets qui respectent les conventions communautaires, comme le fait d‚Äôutiliser le contr√¥le de version, contribuent √† rendre le code plus lisible et document√©, et donc reproductible.\nIl faut n√©anmoins aller plus loin pour atteindre une v√©ritable reproductibilit√©, et r√©fl√©chir √† la notion d‚Äôenvironnement d‚Äôex√©cution. Un code n‚Äôest pas un objet autonome, il est toujours ex√©cut√© sur un environnement (ordinateur personnel, serveur, etc.), et ces environnements peuvent √™tre tr√®s diff√©rents (syst√®me d‚Äôexploitation, librairies install√©es, contraintes de s√©curit√©, etc.). C‚Äôest pourquoi il faut r√©fl√©chir √† la portabilit√© de son code, i.e.¬†sa capacit√© √† s‚Äôex√©cuter de mani√®re attendue sur diff√©rents environnements, ce qui sera l‚Äôobjet d‚Äôun chapitre √† part enti√®re."
  },
  {
    "objectID": "chapters/introduction.html#faciliter-la-mise-en-production",
    "href": "chapters/introduction.html#faciliter-la-mise-en-production",
    "title": "Introduction",
    "section": "Faciliter la mise en production",
    "text": "Faciliter la mise en production\nPour qu‚Äôun projet de data science cr√©e in fine de la valeur, il faut qu‚Äôil soit d√©ploy√© sous une forme valorisable de sorte √† toucher son public. Cela implique deux choses :\n\ntrouver le format de diffusion adapt√©, i.e.¬†qui valorise au mieux les r√©sultas obtenus aupr√®s des utilisateurs potentiels ;\nfaire transitionner le projet de l‚Äôenvironnement dans lequel il a √©t√© d√©velopp√© vers une infrastructure de production, i.e.¬†permettant un d√©ploiement robuste de l‚Äôoutput du projet afin que celui-ci soit disponible √† la demande.\n\nDans le chapitre D√©ployer et valoriser son projet de data science, nous proposons des pistes permettant de r√©pondre √† ces deux besoins. Nous pr√©sentons un certain nombre de formats standards (API, application, rapport automatis√©, site internet) qui permettent √† un projet de data science d‚Äô√™tre valoris√©, ainsi que les outils modernes qui permettent de les produire.\nNous d√©taillons ensuite les concepts essentiels du d√©ploiement sur une infrastructure de production, et illustrons ces derniers par des exemples de d√©ploiements dans un environnement cloud moderne.\nC‚Äôest en quelque sorte la r√©compense de l‚Äôapplication des bonnes pratiques : d√®s lors que l‚Äôon s‚Äôest donn√© la peine de produire un code et un projet appliquant des standards de qualit√©, que l‚Äôon a bien versionn√© son code, et que l‚Äôon a pris des mesures pour le rendre portable, le d√©ploiement du projet dans un environnement de production s‚Äôen trouve largement facilit√©."
  },
  {
    "objectID": "chapters/introduction.html#chapitres-suppl√©mentaires",
    "href": "chapters/introduction.html#chapitres-suppl√©mentaires",
    "title": "Introduction",
    "section": "Chapitres suppl√©mentaires",
    "text": "Chapitres suppl√©mentaires\nPlusieurs outils pr√©sent√©s tout au long de ce cours, tels que les logiciels Git et Docker, impliquent l‚Äôutilisation du terminal ainsi que des connaissances de base du fonctionnement d‚Äôun syst√®me Linux. Dans le chapitre D√©mystifier le terminal Linux pour gagner en autonomie, nous pr√©sentons les connaissances essentielles des syst√®mes Linux qu‚Äôun data scientist doit poss√©der pour pouvoir √™tre autonome dans ses d√©ploiements et dans l‚Äôapplication des bonnes pratiques de d√©veloppement.\nLa reproductibilit√© √©tant une qu√™te sans fin, nous concluons ce cours par un chapitre nomm√© Des ressources pour aller plus loin dans l‚Äôindustrialisation de son projet. Comme son nom l‚Äôindique, il vise √† pointer vers un certain nombre de ressources qui permettent d‚Äôam√©liorer encore et toujours ses pratiques et de s‚Äôint√©resser √† des sujets qui d√©passent le cadre de ce cours, comme la s√©curit√© ou encore les sp√©cificit√©s li√©es au d√©ploiement et √† la maintenance de mod√®les de machine learning."
  },
  {
    "objectID": "chapters/introduction.html#comment-fixer-le-bon-seuil",
    "href": "chapters/introduction.html#comment-fixer-le-bon-seuil",
    "title": "Introduction",
    "section": "Comment fixer le bon seuil ?",
    "text": "Comment fixer le bon seuil ?\nLa d√©termination du seuil pertinent doit r√©sulter d‚Äôun arbitrage entre diff√©rents crit√®res li√©s au projet :\n\nambitions : le projet est-il amen√© √† √©voluer, prendre de l‚Äôampleur ? Est-il destin√© √† devenir collaboratif, que ce soit dans le cadre d‚Äôune √©quipe en organisation ou bien en open-source ? Les outputs du projet ont-ils vocation √† √™tre diffus√©s au grand public ?\nressources : quels sont les moyens humain du projet ? Pour un projet open-source, existe-t-il une communaut√© potentiel de contributeurs ?\ncontraintes : le projet a-t-il une √©ch√©ance proche ? Des exigences de qualit√© ont-elles √©t√© fix√©es ? Est-il destin√© √† la mise en production ? Existe-t-il des enjeux de s√©curit√© forts ?\n\nIl n‚Äôest donc pas question pour nous de sugg√©rer que tout projet de data science doit respecter toutes les bonnes pratiques pr√©sent√©es dans ce cours."
  },
  {
    "objectID": "chapters/introduction.html#un-socle-minimal-pour-les-projets-de-data-science",
    "href": "chapters/introduction.html#un-socle-minimal-pour-les-projets-de-data-science",
    "title": "Introduction",
    "section": "Un socle minimal pour les projets de data science ?",
    "text": "Un socle minimal pour les projets de data science ?\nCela √©tant dit, nous sommes convaincus qu‚Äôil est important pour tout data scientist de r√©fl√©chir √† ces questions pour am√©liorer ces pratiques au fil du temps.\nEn particulier, nous pensons qu‚Äôil est possible de d√©finir un socle, i.e.¬†un ensemble minimal de bonnes pratiques qui apportent plus d‚Äôavantages qu‚Äôelles ne co√ªtent √† impl√©menter. Notre suggestion pour un tel socle est la suivante :\n\nContr√¥ler la qualit√© de son code en utilisant des outils d√©di√©s (cf.¬†chapitre Qualit√© du Code) ;\nAdopter une structure standardis√©e de projet en utilisant des templates pr√™ts √† l‚Äôemploi (cf.¬†chapitre Architecture des Projets) ;\nutiliser Git pour versionner le code de ses projets, qu‚Äôils soient individuels ou collectifs (cf.¬†chapitre Versionner son code et travailler collaborativement avec Git) ;\ncontr√¥ler les d√©pendances de son projet en d√©veloppant dans des environnements virtuels (cf.¬†chapitre Portabilit√©)."
  },
  {
    "objectID": "chapters/introduction.html#approche-p√©dagogique",
    "href": "chapters/introduction.html#approche-p√©dagogique",
    "title": "Introduction",
    "section": "Approche p√©dagogique",
    "text": "Approche p√©dagogique\nLe parti pris de ce cours est que seule la pratique, et en particulier la confrontation √† des probl√®mes issus de projets r√©els, permet d‚Äôacqu√©rir efficacement des concepts informatiques. Aussi, une large part du cours consistera en l‚Äôapplication des notions √©tudi√©es √† des cas concrets. Chaque chapitre se concluera pas des applications touchant √† des sujets r√©alistes de data science.\nUn exemple fil rouge illustre les progr√®s dans la conception d‚Äôun projet reproductible en appliquant successivement le contenu des chapitres de ce cours.\nPour l‚Äô√©valuation g√©n√©rale du cours, l‚Äôid√©e sera de partir d‚Äôun projet personnel, id√©alement termin√©, et de lui appliquer un maximum de bonnes pratiques pr√©sent√©es dans ce cours."
  },
  {
    "objectID": "chapters/introduction.html#langages",
    "href": "chapters/introduction.html#langages",
    "title": "Introduction",
    "section": "Langages",
    "text": "Langages\nLes principes pr√©sent√©s dans ce cours sont pour la plupart agnostiques du langage de programmation utilis√©.\nCe choix n‚Äôest pas qu‚Äô√©ditorial, c‚Äôest selon nous un aspect fondamental du sujet des bonnes pratiques. Trop souvent, des diff√©rences de langage entre les phases de d√©veloppement (notamment R ou Python) et de mise en production (ex : Java) √©rigent des murs artificiels qui r√©duisent fortement la capacit√© √† valoriser des projets de data science.\nA l‚Äôinverse, plus les diff√©rentes √©quipes qui forment le cycle de vie d‚Äôun projet s‚Äôaccordent pour appliquer le m√™me ensemble de bonnes pratiques, plus ces √©quipes d√©veloppent un langage commun, et plus les d√©ploiements en sont facilit√©s.\nUn exemple parlant est l‚Äôutilisation de la conteneurisation : si le data scientist met √† disposition une image Docker comme output de sa phase de d√©veloppement et que le data engineer s‚Äôoccupe de d√©ployer cette image sur une infrastructure d√©di√©e, le contenu m√™me de l‚Äôapplication en termes de langage importe finalement assez peu. Cet exemple, certes simpliste, illustre malgr√© tout l‚Äôenjeu des bonnes pratiques en mati√®re de communication au sein d‚Äôun projet.\nLes exemples pr√©sent√©s dans ce cours seront pour l‚Äôessentiel en Python. La raison principale est que ce langage, malgr√© ses d√©fauts, est enseign√© dans la majorit√© des cursus de data science mais aussi d‚Äôinformatique. Il peut faciliter la passerelle entre le monde des utilisateurs de donn√©es et celui des d√©veloppeurs informatiques, passerelle indispensable pour favoriser le dialogue entre ces deux profils, n√©cessaires tous deux pour un passage en production. Encore une fois, il est tout √† fait possible d‚Äôappliquer les m√™mes principes avec d‚Äôautres langages, et nous encourageons d‚Äôailleurs les √©tudiants √† s‚Äôessayer √† cet exercice formateur."
  },
  {
    "objectID": "chapters/introduction.html#environnement-dex√©cution",
    "href": "chapters/introduction.html#environnement-dex√©cution",
    "title": "Introduction",
    "section": "Environnement d‚Äôex√©cution",
    "text": "Environnement d‚Äôex√©cution\nA l‚Äôinstar du langage, les principes appliqu√©s dans ce cours sont agnostiques √† l‚Äôinfrastructure utilis√©e pour faire tourner les exemples propos√©s. Il est donc √† la fois possible et souhaitable d‚Äôappliquer les bonnes pratiques aussi bien √† un projet individuel d√©velopp√© sur un ordinateur personnel qu‚Äô√† un projet collaboratif visant √† √™tre d√©ploy√© sur une infrastructure de production d√©di√©e.\nCependant, nous choisissons comme environnement de r√©f√©rence tout au long de ce cours le SSP Cloud, une plateforme de services pour la data science d√©velopp√©e √† l‚ÄôInsee et accessible aux √©l√®ves des √©coles statistiques. Les raisons de ce choix sont multiples :\n\nl‚Äôenvironnement de d√©veloppement est normalis√© : les serveurs du SSP Cloud ont une configuration homog√®ne ‚Äî notamment, ils se basent sur une m√™me distribution Linux (Debian) ‚Äî ce qui garantit la reproductibilit√© des exemples pr√©sent√©s tout au long du cours ;\nvia un cluster Kubernetes sous-jacent, le SSP Cloud met √† disposition une infrastructure robuste permettant le d√©ploiement automatis√© d‚Äôapplications potentiellement intensives en donn√©es, ce qui permet de simuler un v√©ritable environnement de production ;\nle SSP Cloud est construit selon les standards les plus r√©cents des infrastructures data science, et permet donc d‚Äôacqu√©rir les bonnes pratiques de mani√®re organique :\n\nles services sont lanc√©s via des conteneurs, configur√©s par des images Docker. Cela permet de garantir une forte reproductibilit√© des d√©ploiements, au prix d‚Äôune phase de d√©veloppement un peu plus co√ªteuse ;\nle SSP Cloud est bas√© sur une approche dite cloud native : il est construit sur un ensemble modulaire de briques logicielles, qui permettent d‚Äôappliquer une s√©paration nette du code, des donn√©es, de la configuration et de l‚Äôenvironnement d‚Äôex√©cution, principe majeur des bonnes pratiques qui reviendra tout au long de ce cours.\n\n\nPour en savoir plus sur cette plateforme, vous pouvez consulter cette page."
  },
  {
    "objectID": "chapters/introduction.html#ressources-compl√©mentaires",
    "href": "chapters/introduction.html#ressources-compl√©mentaires",
    "title": "Introduction",
    "section": "Ressources compl√©mentaires",
    "text": "Ressources compl√©mentaires\n\nMissing semester du MIT"
  },
  {
    "objectID": "chapters/linux-101.html",
    "href": "chapters/linux-101.html",
    "title": "D√©mystifier le terminal Linux pour gagner en autonomie",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des syst√®mes d‚Äôexploitation. Mais comme il a la r√©putation d‚Äô√™tre aust√®re et complexe, on utilise plut√¥t des interfaces graphiques pour effectuer nos op√©rations informatiques quotidiennes.\nPourtant, avoir des notions quant √† l‚Äôutilisation d‚Äôun terminal est une vraie source d‚Äôautonomie, dans la mesure o√π celui-ci permet de g√©rer bien plus finement les commandes que l‚Äôon r√©alise. Pour le data scientist qui s‚Äôint√©resse aux bonnes pratiques et √† la mise en production, sa ma√Ætrise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont g√©n√©ralement limit√©es par rapport √† l‚Äôutilisation du programme en ligne de commande. C‚Äôest par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de r√©aliser toutes les op√©rations permises par le logiciel ;\nmettre un projet de data science en production n√©cessite d‚Äôutiliser un serveur, qui le rend disponible en permanence √† son public potentiel. Or l√† o√π Windows domine le monde des ordinateurs personnels, une large majorit√© des serveurs et des infrastructures cloud fonctionnent sous Linux.\nplus g√©n√©ralement, une utilisation r√©guli√®re du terminal est source d‚Äôune meilleure compr√©hension du fonctionnement d‚Äôun syst√®me de fichiers et de l‚Äôex√©cution des processus sur un ordinateur. Ces connaissances s‚Äôav√®rent tr√®s utiles dans la pratique quotidienne du data scientist, qui n√©cessite de plus en plus de d√©velopper dans diff√©rents environnements d‚Äôex√©cution.\n\nDans le cadre de ce cours, on s‚Äôint√©ressera donc particuli√®rement au terminal Linux.\n\n\n\nDiff√©rents environnements de travail peuvent √™tre utilis√©s pour apprendre √† se servir d‚Äôun terminal Linux :\n\nle SSP Cloud. Dans la mesure o√π les exemples de mise en production du cours seront illustr√©es sur cet environnement, nous recommandons de l‚Äôutiliser d√®s √† pr√©sent pour se familiariser. Le terminal est accessible √† partir de diff√©rents services (RStudio, Jupyter, etc.), mais nous recommandons d‚Äôutiliser le terminal d‚Äôun service VSCode, dans la mesure o√π se servir d‚Äôun IDE pour organiser notre code est en soi d√©j√† une bonne pratique ;\nKatacoda, un bac √† sable dans un syst√®me Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (√©mulation minimaliste d‚Äôun terminal Linux), qui est install√©e par d√©faut avec Git.\n\n\n\n\nLan√ßons un terminal pour pr√©senter son fonctionnement basique. On prend pour exemple le terminal d‚Äôun service VSCode lanc√© via le SSP Cloud (Application Menu tout en haut √† gauche de VSCode -> Terminal -> New Terminal). Voici √† quoi ressemble le terminal en question.\n\nD√©crivons d‚Äôabord les diff√©rentes inscriptions qui arrivent √† l‚Äôinitialisation : - (base) : cette inscription n‚Äôest pas directement li√©e au terminal, elle provient du fait que l‚Äôon utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en d√©tail dans le chapitre sur la portabilit√© ; - coder@vscode-824991-64744dd6d8-zbgv5 : le nom de l‚Äôutilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que l‚Äôon verra l√† encore dans le chapitre sur la portabilit√© - ~/work : le chemin du r√©pertoire courant, i.e.¬†√† partir duquel va √™tre lanc√©e toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\nPour √©viter la lourdeur des images et permettre de copier/coller facilement les commandes, on repr√©sentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans l‚Äôexemple suivant. Les lignes commen√ßant par un $ sont celles avec lesquelles une commande est lanc√©e, et les lignes sans $ repr√©sentent le r√©sultat d‚Äôune commande. Attention √† ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement √† diff√©rencier celles-ci des r√©sultats.\n$ echo \"une petite illustration\"\nune petite illustration\n\n\n\nLe terme filesystem (syst√®me de fichiers) d√©signe la mani√®re dont sont organis√©s les fichiers au sein d‚Äôun syst√®me d‚Äôexploitation. Cette structure est hi√©rarchique, en forme d‚Äôarbre : - elle part d‚Äôun r√©pertoire racine (le dossier qui contient tous les autres) ; - contient des dossiers ; - les dossiers peuvent contenir √† leur tout des dossiers (sous-dossiers) ou des fichiers.\nInt√©ressons nous √† la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations : - la racine (root) sur Linux s‚Äôappelle /, l√† o√π elle s‚Äôappelle C:\\ par d√©faut sur Windows ; - le r√©pertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un r√¥le essentiellement technique. Il est tout de m√™me utile d‚Äôen d√©crire les principaux : - /bin : contient les binaires, i.e.¬†les programmes ex√©cutables ; - /etc : contient les fichiers de configuration ; - /home : contient l‚Äôensemble des dossiers et fichiers personnels des diff√©rents utilisateurs. Chaque utilisateur a un r√©pertoire dit ‚ÄúHOME‚Äù qui a pour chemin /home/<username> Ce r√©pertoire est souvent repr√©sent√© par le symbole ~. C‚Äô√©tait notamment le cas dans l‚Äôillustration du terminal VSCode ci-dessus, ce qui signifie qu‚Äôon se trouvait formellement dans le r√©pertoire /home/coder/work, coder √©tant l‚Äôutilisateur par d√©faut du service VSCode sur le SSP Cloud.\nChaque dossier ou fichier est repr√©sent√© par un chemin d‚Äôacc√®s, qui correspond simplement √† sa position dans le filesystem. Il existe deux moyens de sp√©cifier un chemin : - en utilisant un chemin absolu, c‚Äôest √† dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconna√Æt donc un chemin absolu par le fait qu‚Äôil commence forc√©ment par /. - en utilisant un chemin relatif, c‚Äôest √† dire en indiquant le chemin du dossier ou fichier relativement au r√©pertoire courant.\nComme tout ce qui touche de pr√®s ou de loin au terminal, la seule mani√®re de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront d‚Äôappliquer ces concepts √† des cas pratiques.\n\n\n\nLe r√¥le d‚Äôun terminal est de lancer des commandes. Ces commandes peuvent √™tre class√©es en trois grandes cat√©gories : - navigation au sein du filesystem - manipulations de fichiers (cr√©er, lire, modifier des dossiers/fichiers) - lancement de programmes\n\n\nLorsque l‚Äôon lance un programme √† partir du terminal, celui-ci a pour r√©f√©rence le r√©pertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si l‚Äôon ex√©cute un script Python en se trouvant dans un certain r√©pertoire, tous les chemins des fichiers utilis√©s dans le script seront relatifs au r√©pertoire courant d‚Äôex√©cution ‚Äî √† moins d‚Äôutiliser uniquement des chemins absolus, ce qui n‚Äôest pas une bonne pratique en termes de reproductibilit√© puisque cela lie votre projet √† la structure de votre filesystem particulier.\nAinsi, la tr√®s grande majorit√© des op√©rations que l‚Äôon est amen√© √† r√©aliser dans un terminal consiste simplement √† se d√©placer au sein du filesystem. Les commandes principales pour naviguer et se rep√©rer dans le filesystem sont pr√©sent√©es dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez p√©nible de manipuler des chemins absolus, qui peuvent facilement √™tre tr√®s longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient √† se d√©placer √† partir du r√©pertoire courant. Pour se faire, voici quelques utilisations tr√®s fr√©quentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter d‚Äôun niveau dans l‚Äôarborescence (dossier parent)\n\n\ncd ~\nrevenir dans le r√©pertoire HOME de l‚Äôutilisateur courant\n\n\n\nLa premi√®re commande est l‚Äôoccasion de revenir sur une convention d‚Äô√©criture importante pour les chemins relatifs : - . repr√©sente le r√©pertoire courant. Ainsi, cd . revient √† changer de r√©pertoire courant‚Ä¶ pour le r√©pertoire courant, ce qui bien s√ªr ne change rien. Mais le . est tr√®s utile pour la copie de fichiers (cf.¬†section suivante) ou encore lorsque l‚Äôon doit passer des param√®tres √† un programme (cf.¬†section Lancement de programmes) ; - .. repr√©sente le r√©pertoire parent du r√©pertoire courant.\nCes diff√©rentes commandes constituent la tr√®s grande majorit√© des usages dans un terminal. Il est essentiel de les pratiquer jusqu‚Äô√† ce qu‚Äôelles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup d‚Äôautres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\nd√©placer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncr√©er (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncr√©er un fichier vide\n\n\n\nDans la mesure o√π il est g√©n√©ralement possible de r√©aliser toutes ces op√©rations √† l‚Äôaide d‚Äôinterfaces graphiques (notamment, l‚Äôexplorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se d√©placer dans le filesystem. Nous vous recommandons malgr√© tout de les pratiquer √©galement, et ce pour plusieurs raisons : - effectuer un maximum d‚Äôop√©rations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ; - en devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ; - lorsque l‚Äôon est amen√© √† manipuler un terminal pour interagir avec un serveur, il n‚Äôy a souvent pas la moindre interface graphique, auquel cas il n‚Äôy a pas d‚Äôautre choix que d‚Äôop√©rer uniquement √† partir du terminal.\n\n\n\nLe r√¥le du terminal est de lancer des programmes. Lancer un programme se fait √† partir d‚Äôun fichier dit ex√©cutable, qui peut √™tre de deux formes : - un binaire, i.e.¬†un programme dont le code n‚Äôest pas lisible par l‚Äôhumain ; - un script, i.e.¬†un fichier texte contenant une s√©rie d‚Äôinstructions √† ex√©cuter. Le langage du terminal Linux est le shell, et les scripts associ√©s ont pour extension .sh.\nDans les deux cas, la syntaxe de lancement d‚Äôune commande est : le nom de l‚Äôex√©cutable, suivi d‚Äô√©ventuels param√®tres, s√©par√©s par des espaces. Par exemple, la commande python monscript.py ex√©cute le binaire python et lui passe comme unique argument le nom d‚Äôun script .py (contenu dans le r√©pertoire courant), qui va donc √™tre ex√©cut√© via Python. De la m√™me mani√®re, toutes les commandes vues pr√©c√©demment pour se d√©placer dans le filesystem ou manipuler des fichiers sont des ex√©cutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier √† copier et le chemin d‚Äôarriv√©e.\nDans les exemples de commandes pr√©c√©dents, les param√®tres √©taient pass√©s en mode positionnel : l‚Äôex√©cutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments n‚Äôest pas toujours fix√© √† l‚Äôavance, du fait de la pr√©sence de param√®tres optionnels. Ainsi, la plupart des ex√©cutables permettent le passage d‚Äôarguments optionnels, qui modifient le comportement de l‚Äôex√©cutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier √† un autre endroit du filesystem, mais peut-on copier un dossier et l‚Äôensemble de son contenu avec ? Nativement non, mais l‚Äôajout d‚Äôun param√®tre le permet : cp -R dossierdepart dossierarrivee permet de copier r√©cursivement le dossier et tout son contenu. Notons que les flags ont tr√®s souvent un √©quivalent en toute lettre, qui s‚Äô√©crit quant √† lui avec deux tirers. Par exemple, la commande pr√©c√©dente peut s‚Äô√©crire de mani√®re √©quivalente cp --recursive dossierdepart dossierarrivee. Il est fr√©quent de voir les deux syntaxes en pratique, parfois m√™me m√©lang√©es au sein d‚Äôune m√™me commande.\n\n\n\n\nComme tout langage de programmation, le langage shell permet d‚Äôassigner et d‚Äôutiliser des variables dans des commandes. Pour afficher le contenu d‚Äôune variable, on utilise la commande echo, qui est l‚Äô√©quivalent de la fonction print en Python ou en R.\n$ MY_VAR=\"toto\"\n$ echo $MY_VAR\ntoto\nQuelques remarques importantes : - la syntaxe pour la cr√©ation de variable est pr√©cise : aucun espace d‚Äôun c√¥t√© comme de l‚Äôautre du = ; - en Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu √©crire MY_VAR=toto pour le m√™me r√©sultat. Par contre, si l‚Äôon veut assigner √† une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message d‚Äôerreur ; - pour acc√©der √† la valeur d‚Äôune variable, on la pr√©fixe d‚Äôun $.\nNotre objectif avec ce tutoriel n‚Äôest pas de savoir coder en shell, on ne va donc pas s‚Äôattarder sur les propri√©t√©s des variables. En revanche, introduire ce concept √©tait n√©cessaire pour en pr√©senter un autre, essentiel quant √† lui dans la pratique quotidienne du data scientist : les variables d‚Äôenvironnement. Pour faire une analogie ‚Äî un peu simpliste ‚Äî avec les langages de programmation, ce sont des sortes de variables ‚Äúglobales‚Äù, dans la mesure o√π elles vont √™tre accessibles √† tous les programmes lanc√©s √† partir d‚Äôun terminal, et vont modifier leur comportement.\nLa liste des variables d‚Äôenvironnement peut √™tre affich√©e √† l‚Äôaide de la commande env. Il y a g√©n√©ralement un grand nombre de variables d‚Äôenvironnement pr√©√©xistantes ; en voici un √©chantillon obtenu √† partir du terminal du service VSCode.\n$ env\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la vari√©t√© des utilisations des variables d‚Äôenvironnements : - la variable $SHELL pr√©cise l‚Äôex√©cutable utilis√© pour lancer le terminal ; - la variable $HOME donne l‚Äôemplacement du r√©pertoire utilisateur. En fait, le symbole ~ que l‚Äôon a rencontr√© plus haut r√©f√©rence cette m√™me variable ; - la variable LANG sp√©cifie la locale, un concept qui permet de d√©finir la langue et l‚Äôencodage utilis√©s par d√©faut par Linux ; - la variable CONDA_PYTHON_EXE existe uniquement parce que l‚Äôon a install√© conda comme syst√®me de gestion de packages Python. C‚Äôest l‚Äôexistance de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous int√©resse.\nUne variable d‚Äôenvironnement essentielle, et que l‚Äôon est fr√©quemment amen√© √† modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concat√©nation de chemins absolus, s√©par√©s par :, qui sp√©cifie les dossiers dans lesquels Linux va chercher les ex√©cutables lorsque l‚Äôon lance une commande, ainsi que l‚Äôordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n$ echo $PATH\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nL‚Äôordre de recherche est de gauche √† droite. C‚Äôest donc parce que le dossier /home/coder/local/bin/conda/bin est situ√© en premier que l‚Äôinterpr√©teur Python qui sera choisi lorsque l‚Äôon lance un script Python est celui issu de Conda, et non celui contenu par d√©faut dans /usr/bin par exemple.\nL‚Äôexistence et la configuration ad√©quate des variables d‚Äôenvironnement est essentielle pour le bon fonctionnement de nombreux outils tr√®s utilis√©s en data science, comme Git ou encore Spark par exemple. Il est donc n√©cessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration d‚Äôun serveur en cas de bug li√© √† une variable d‚Äôenvironnement manquante ou mal configur√©e.\n\n\n\nLa s√©curit√© est un enjeu central en Linux, qui permet une gestion tr√®s fine des permissions sur les diff√©rents fichiers et programmes.\nUne diff√©rence majeure par rapport √† d‚Äôautres syst√®mes d‚Äôexploitation, notamment Windows, est qu‚Äôaucun utilisateur n‚Äôa par d√©faut les droits complets d‚Äôadministrateur (root). Il n‚Äôest donc pas possible nativement d‚Äôacc√©der au parties sensibles du syst√®me, ou bien de lancer certains types de programme. Par exemple, si l‚Äôon essaie de lister les fichiers du dossier /root, on obtient une erreur.\n$ ls /root\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines op√©rations telles que l‚Äôinstallation de binaires ou de packages n√©cessitent cependant des droits administrateurs. Dans ce cas, il est d‚Äôusage d‚Äôutiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de l‚Äôex√©cution de la commande.\n$ sudo ls /root\nLe dossier /root √©tant vide, la commande ls renvoie une cha√Æne de caract√®res vide, mais nous n‚Äôavons plus de probl√®me de permission. Notons qu‚Äôune bonne pratique de s√©curit√©, en particulier dans les scripts shell que l‚Äôon peut √™tre amen√©s √† √©crire ou ex√©cuter, est de limiter l‚Äôutilisation de cette commande aux cas o√π elle s‚Äôav√®re n√©cessaire.\nUne autre subtilit√© concerne justement l‚Äôex√©cution de scripts shell. Par d√©faut, qu‚Äôil soit cr√©√© par l‚Äôutilisateur ou t√©l√©charg√© d‚Äôinternet, un script n‚Äôest pas ex√©cutable.\n$ touch test.sh # Cr√©er le script test.sh (vide)\n$ ./test.sh # Ex√©cuter le script test.sh\nbash: ./test.sh: Permission denied\nC‚Äôest bien entendu une mesure de s√©curit√© pour √©viter l‚Äôex√©cution automatique de scripts potentiellement malveillants. Pour pouvoir ex√©cuter un tel script, il faut attribuer des droits d‚Äôex√©cution au fichier avec la commande chmod. Il devient alors possible d‚Äôex√©cuter le script classiquement.\n$ chmod +x test.sh # Donner des droits d'ex√©cution au script test.sh\n$ ./test.sh # Ex√©cuter le script test.sh\n\n# Le script √©tant vide, il ne se passe rien\n\n\n\nMaintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell pr√©c√©demment √©voqu√©s. A l‚Äôinstar d‚Äôun script Python, un script shell permet d‚Äôautomatiser une s√©rie de commandes lanc√©es dans un terminal. Le but de ce tutoriel n‚Äôest pas de savoir √©crire des scripts shell complexes, travail g√©n√©ralement d√©volu aux les data engineers ou les sysadmin (administrateurs syst√®me), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces comp√©tences sont essentielles lorsque l‚Äôon se pr√©occupe de mise en production. A titre d‚Äôexemple, comme nous le verrons dans le chapitre sur la portabilit√©, il est fr√©quent d‚Äôutiliser un script shell comme entrypoint d‚Äôune image docker, afin de sp√©cifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement √† l‚Äôaide d‚Äôun script simple. Consid√©rons les commandes suivantes, que l‚Äôon met dans un fichier monscript.sh dans le r√©pertoire courant.\n#!/bin/bash\nSECTION=$1\nCHAPTER=$2\nFORMATION_DIR=/home/coder/work/formation\nmkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\ntouch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\nAnalysons la structure de ce script : - la premi√®re ligne est classique, elle se nomme le shebang : elle indique au syst√®me quel interpr√©teur utiliser pour ex√©cuter ce script. Dans notre cas, et de mani√®re g√©n√©rale, on utilise bash (Bourne-Again SHell, l‚Äôimpl√©mentation moderne du shell) ; - les lignes 2 et 3 assignent √† des variables les arguments pass√©s au script dans la commande. Par d√©faut, ceux-ci sont assign√©s √† des variables n o√π n est la position de l‚Äôargument, en commen√ßant √† 1 ; - la ligne 4 assigne un chemin √† une variable - la ligne 5 cr√©e le chemin complet, d√©fini √† partir des variables cr√©√©es pr√©c√©demment. Le param√®tre -p est important : il pr√©cise √† mkdir d‚Äôagir de mani√®re r√©cursive, c‚Äôest √† dire de cr√©er les dossiers interm√©diaires qui n‚Äôexistent pas encore ; - la ligne 6 cr√©e un fichier texte vide dans le dossier cr√©√© avec la commande pr√©c√©dente.\nEx√©cutons maintenant ce script, en prenant soin de lui donner les permission ad√©quates au pr√©alable.\n$ chmod +x monscript.sh\n$ bash monscript.sh section2 chapitre3\n$ ls formation/section1/chapitre2/\ntext.txt\nOp√©ration r√©ussie : le dossier a bien √©t√© cr√©√© et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash tr√®s bien r√©alis√©e.\n\n\n\nUne diff√©rence fondamentale entre Linux et Windows tient √† la mani√®re dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier ex√©cutable en .exe) sur le site du logiciel, et on l‚Äôex√©cute. En Linux, on passe g√©n√©ralement par un gestionnaire de packages qui va chercher les logiciels sur un r√©pertoire centralis√©, √† la mani√®re de pip en Python par exemple.\nPourquoi cette diff√©rence ? Une raison importante est que, contrairement √† Windows, il existe une multitude de distributions diff√©rentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent diff√©remment et peuvent avoir diff√©rentes versions. En utilisant le package manager (gestionnaire de paquets) propre √† la distribution en question, on s‚Äôassure de t√©l√©charger le logiciel adapt√© √† sa distribution. Dans ce cours, on fait le choix d‚Äôutiliser une distribution Debian et son gestionnaire de paquets associ√© apt. Debian est en effet un choix populaire pour les servers de part sa stabilit√© et sa simplicit√©, et sera √©galement famili√®re aux utilisateurs d‚ÄôUbuntu, distribution tr√®s populaire pour les ordinateurs personnels et qui est bas√©e sur Debian.\nL‚Äôutilisation d‚Äôapt est tr√®s simple. La seule difficult√© est de savoir le nom du paquet que l‚Äôon souhaite installer, ce qui n√©cessite en g√©n√©ral d‚Äôutiliser un moteur de recherche. L‚Äôinstallation de paquets est √©galement un cas o√π il faut utiliser sudo, puisque cela implique souvent l‚Äôacc√®s √† des r√©pertoires prot√©g√©s.\n$ sudo apt install tree\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nD√©sinstaller un package est √©galement simple : c‚Äôest l‚Äôop√©ration inverse. Par s√©curit√©, le terminal vous demande si vous √™tes s√ªr de votre choix en vous demandant de tapper la lettre y ou la lettre n.¬†On peut passer automatiquement cette √©tape en ajoutant le param√®tre -y\n$ sudo apt remove -y tree\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant d‚Äôinstaller un package, il est toujours pr√©f√©rable de mettre √† jour la base des packages, pour s‚Äôassurer qu‚Äôon obtiendra bien la derni√®re version.\n$ sudo apt update\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date.\n\n\n\nOn l‚Äôa dit et redit : devenir √† l‚Äôaise avec le terminal Linux est essentiel et demande de la pratique. Il existe n√©anmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa premi√®re est l‚Äôautocompl√©tion. D√®s lors que vous √©crivez une commande contenant un nom d‚Äôex√©cutable, un chemin sur le filesystem, ou autre, n‚Äôh√©sitez pas √† utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majorit√© des cas, cela va vous faire gagner un temps pr√©cieux.\nUne seconde astuce, qui n‚Äôen est pas vraiment une, est de lire la documentation d‚Äôune commande lorsque l‚Äôon n‚Äôest pas s√ªr de sa syntaxe ou des param√®tres admissibles. Via le terminal, la documentation d‚Äôune commande peut √™tre affich√©e en ex√©cutant man suivie de la commande en question, par exemple : man cp. Comme il n‚Äôest pas toujours tr√®s pratique de lire de longs textes dans un petit terminal, on peut √©galement chercher la documentation d‚Äôune commande sur le site man7."
  },
  {
    "objectID": "chapters/portability.html",
    "href": "chapters/portability.html",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "",
    "text": "Dans les chapitres pr√©c√©dents, nous avons vu un ensemble de bonnes pratiques qui permettent de consid√©rablement am√©liorer la qualit√© d‚Äôun projet : rendre le code plus lisible, adopter une structure du projet normalis√©e et √©volutive, et versionner proprement son code sur un d√©p√¥t GitHub.\nUne fois ces bonnes pratiques appliqu√©es √† notre projet, ce dernier appara√Æt largement partageable. Du moins en th√©orie, car la pratique est souvent plus compliqu√©e : il y a fort √† parier que si vous essayez d‚Äôex√©cuter votre projet sur un autre environnement d‚Äôex√©cution (un autre ordinateur, un serveur, etc.), les choses ne se passent pas du tout comme attendu. Cela signifique qu‚Äôen l‚Äô√©tat, le projet n‚Äôest pas portable : il n‚Äôest pas possible, sans modifications co√ªteuses, de l‚Äôex√©cuter dans un environnement diff√©rent de celui dans lequel il a √©t√© d√©velopp√©.\nLa principale raison est qu‚Äôun code ne vit pas dans une bulle isol√©e, il contient en g√©n√©ral de nombreuses adh√©rences, plus ou moins visibles, au langage et √† l‚Äôenvironnement dans lesquels il a √©t√© d√©velopp√© :\n\ndes d√©pendances dans le langage du projet ;\ndes d√©pendances dans d‚Äôautres langages (ex : NumPy est √©crit en C et n√©cessite donc un compilateur C) ;\ndes librairies syst√®mes n√©cessaires pour installer certains packages (par exemple, : les librairies de cartographie dynamique comme Leaflet ou Folium n√©cessitent la librairie syst√®me GDAL), qui ne seront pas les m√™mes selon le syst√®me d‚Äôexploitation utilis√©.\n\nSi le premier probl√®me peut √™tre g√©r√© relativement facilement en adoptant une structure de projet et en sp√©cifiant bien les diff√©rentes d√©pendances utilis√©es, les deux autres n√©cessitent en g√©n√©ral des outils plus avanc√©s.\nCes outils vont nous permettre de normaliser l‚Äôenvironnement afin de produire un projet portable, i.e.¬†ex√©cutable sur une large vari√©t√© d‚Äôenvironnements d‚Äôex√©cution. Cette √©tape est primordiale lorsque l‚Äôon se pr√©occupe de la mise en production d‚Äôun projet, car elle assure une transition relativement indolore entre l‚Äôenvironnement de d√©veloppement et celui de production.\n\nImage emprunt√©e √† https://devrant.com/rants/174386/when-i-say-but-it-works-on-my-machine"
  },
  {
    "objectID": "chapters/portability.html#introduction",
    "href": "chapters/portability.html#introduction",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Introduction",
    "text": "Introduction\nPour illustrer l‚Äôimportance de travailler avec des environnements virtuels, mettons-nous √† la place d‚Äôun.e aspirant data scientist qui commencerait ses premiers projets. Selon toute vraisemblance, il va commencer par installer une distribution de Python ‚Äî souvent, via Anaconda ‚Äî sur son poste et commencer √† d√©velopper, projet apr√®s projet. Dans cette approche, les diff√©rents packages qu‚Äôil va √™tre amen√© √† utiliser vont √™tre install√©s au m√™me endroit. Cela pose plusieurs probl√®mes : - conflits de version : une application A peut d√©pendre de la version 1 d‚Äôun package l√† o√π une application B peut d√©pendre de la version 2 de ce m√™me package. Une seule application peut donc fonctionner dans cette configuration ; - version de Python fixe ‚Äî on ne peut avoir qu‚Äôune seule installation par syst√®me ‚Äî l√† o√π on voudrait pouvoir avoir des versions diff√©rentes selon le projet ; - reproductiblit√© limit√©e : difficile de dire quel projet repose sur tel package, dans la mesure o√π ceux-ci s‚Äôaccumulent en un m√™me endroit au fil des projets ; - portabilit√© limit√©e : cons√©quence du point pr√©c√©dent, il est difficile de fixer dans un fichier les d√©pendances sp√©cifiques √† un projet.\nLes environnements virtuels constituent une solution √† ces diff√©rents probl√®mes."
  },
  {
    "objectID": "chapters/portability.html#fonctionnement",
    "href": "chapters/portability.html#fonctionnement",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLe concept d‚Äôenvironnement virtuel est techniquement tr√®s simple. On peut lui donner la d√©finition suivante pour Python :\n\n‚Äúdossier auto-suffisant qui contient une installation de Python pour une version particuli√®re de Python ainsi que des packages additionnels et qui est isol√© des autres environnements existants.‚Äù\n\nOn peut donc simplement voir les environnements virtuels comme un moyen de faire cohabiter sur un m√™me syst√®me diff√©rentes installations de Python avec chacune leur propre liste de packages install√©s et leurs versions. D√©velopper dans des environnements virtuels vierges √† chaque d√©but de projet est une tr√®s bonne pratique pour accro√Ætre la reproductibilit√© des analyses."
  },
  {
    "objectID": "chapters/portability.html#impl√©mentations",
    "href": "chapters/portability.html#impl√©mentations",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Impl√©mentations",
    "text": "Impl√©mentations\nIl existe diff√©rentes impl√©mentations des environnements virtuels en Python, dont chacune ont leurs sp√©cificit√©s et leur communaut√© d‚Äôutilisateurs :\n\nL‚Äôimpl√©mentation standard en Python est venv.\nDans le domaine de la data science, l‚Äôimpl√©mentation la plus courante est sans doute conda.\n\nEn pratique, ces impl√©mentations sont relativement proches. La diff√©rence majeure est que conda est √† la fois un package manager (comme pip) et un gestionnaire d‚Äôenvironnements virtuels (comme venv).\nPendant longtemps, conda en tant que package manager s‚Äôest av√©r√© tr√®s pratique en data science, dans la mesure o√π il g√©rait non seulement les d√©pendances Python mais aussi dans d‚Äôautres langages ‚Äî comme des d√©pendances C. Par ailleurs, la distribution Anaconda, qui contient √† la fois Python, conda et beaucoup de packages utiles pour la data science, explique √©galement cette popularit√© aupr√®s des data scientists.\nPour toutes ces raisons, nous allons pr√©senter l‚Äôutilisation de conda comme gestionnaire d‚Äôenvironnements virtuels. Les principes pr√©sent√©s restent n√©anmoins valides pour les autres impl√©mentations"
  },
  {
    "objectID": "chapters/portability.html#conda",
    "href": "chapters/portability.html#conda",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Conda",
    "text": "Conda\n\nInstallation\nLes instructions √† suivre pour installer conda sont d√©taill√©es dans la documentation officielle. conda seul √©tant peu utile en pratique, il est g√©n√©ralement install√© dans le cadre de distributions. Les deux plus populaires sont : - Miniconda : une distribution minimaliste contenant conda, Python ainsi qu‚Äôun petit nombre de packages techniques tr√®s utiles ; - Anaconda : une distribution assez volumineuse contenant conda, Python, d‚Äôautres logiciels (R, Spyder, etc.) ainsi qu‚Äôun ensemble de packages utiles pour la data science (SciPy, NumPy, etc.).\nLe choix de la distribution importe assez peu en pratique, dans la mesure o√π nous allons de toute mani√®re utiliser des environnements virtuels vierges pour d√©velopper nos projets.\nL‚Äô√©cosyst√®me Conda\n\n\n\nEn pratique\n\nCr√©er un environnement\nPour commencer √† utiliser conda, commen√ßons par cr√©er un environnement vierge, nomm√© dev, en sp√©cifiant la version de Python que l‚Äôon souhaite installer pour notre projet.\n$ conda create -n dev python=3.9.7\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/coder/local/bin/conda/envs/dev\n\n  added / updated specs:\n    - python=3.9.7\n\n\nThe following packages will be downloaded:\n...\nThe following NEW packages will be INSTALLED:\n...\nProceed ([y]/n)? y\nDownloading and Extracting Packages\n...\nComme indiqu√© dans les logs, Conda a cr√©√© notre environnement et nous indique son emplacement sur le filesystem. En r√©alit√©, l‚Äôenvironnement n‚Äôest jamais vraiment vierge : Conda nous demande ‚Äî et il faut r√©pondre oui en tapant ‚Äúy‚Äù ‚Äî d‚Äôinstaller un certain nombre de packages, qui sont ceux qui viennent avec la distribution Miniconda.\nOn peut v√©rifier que l‚Äôenvironnement a bien √©t√© cr√©√© en listant les environnements install√©s sur le syst√®me.\nconda info --envs\n# conda environments:\n#\nbase                    * /home/coder/local/bin/conda\nbasesspcloud              /home/coder/local/bin/conda/envs/basesspcloud\ndev                       /home/coder/local/bin/conda/envs/dev\n\n\nActiver un environnement\nComme plusieurs environnements peuvent coexister sur un m√™me syst√®me, il faut sp√©cifier √† Conda que l‚Äôon souhaite utiliser cet environnement pour la session courante du terminal.\n$ conda activate dev\nConda nous indique que l‚Äôon travaille √† partir de maintenant dans l‚Äôenvironnement dev en indiquant son nom entre parenth√®ses au d√©but de la ligne de commandes. Autrement dit, dev devient pour un temps notre environnement par d√©faut. Pour s‚Äôen assurer, v√©rifions avec la commande which l‚Äôemplacement de l‚Äôinterpr√©teur Python qui sera utilis√© si on lance une commande du type python mon-script.py.\n(dev) $ which python \n/home/coder/local/bin/conda/envs/dev/bin/python\nOn travaille bien dans l‚Äôenvironnement attendu : l‚Äôinterpr√©teur qui se lance n‚Äôest pas celui du syst√®me global, mais bien celui sp√©cifique √† notre environnement virtuel.\n\n\nLister les packages install√©s\nUne fois l‚Äôenvironnement activ√©, on peut lister les packages install√©s et leur version. Cela confirme qu‚Äôun certain nombre de packages sont install√©s par d√©faut lors de la cr√©ation d‚Äôun environnement virtuel.\n(dev) $ conda list\n# packages in environment at /home/coder/local/bin/conda/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \n_openmp_mutex             4.5                       1_gnu  \nca-certificates           2022.3.29            h06a4308_0  \n...\n\n\nInstaller un package\nLa syntaxe pour installer un package avec Conda est tr√®s similaire √† celle de pip :\nconda install nom_du_package\nLa diff√©rence est que l√† o√π pip install va installer un package √† partir du r√©pertoire PyPI, conda install va chercher le package sur les r√©pertoires maintenus par les d√©veloppeurs de Conda1. Installons par exemple le package phare de machine learning scikit-learn.\n(dev) $ conda install scikit-learn\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/coder/local/bin/conda/envs/dev\n\n  added / updated specs:\n    - scikit-learn\n...\nL√† encore, Conda nous demande d‚Äôinstaller d‚Äôautres packages, qui sont des d√©pendances de scikit-learn. Par exemple, la librairie de calcul scientifique NumPy.\nL‚Äôautre diff√©rence majeure avec pip est que Conda utilise une m√©thode plus avanc√©e ‚Äî et donc √©galement plus co√ªteuse en temps ‚Äî de r√©solution des d√©pendances. En effet, diff√©rents packages peuvent sp√©cifier diff√©rentes versions d‚Äôun m√™me package dont ils d√©pendent tous les deux, ce qui provoque un conflit de version. Conda va par d√©faut appliquer un algorithme qui vise √† g√©rer au mieux ces conflits, l√† o√π pip va choisir une approche plus minimaliste2.\nIl arrive que des packages disponibles sur le r√©pertoire PyPI ne soient pas disponible sur les canaux g√©r√©s par Conda. Dans ce cas, il est possible d‚Äôinstaller un package dans l‚Äôenvironnement via la commande pip install. Il est n√©anmonins toujours pr√©f√©rable de privil√©gier une installation via Conda si disponible.\n\n\nExporter les sp√©cifications de l‚Äôenvironnement\nD√©velopper √† partir d‚Äôun environnement vierge est une bonne pratique de reproductibilit√© : en partant d‚Äôune base minimale, on s‚Äôassure que seuls les packages effectivement n√©cessaires au bon fonctionnement de notre application ont √©t√© install√©s au fur et √† mesure du projet.\nCela rend √©galement notre projet plus portable : on peut exporter les sp√©cifications de l‚Äôenvironnement (version de Python, canaux de t√©l√©chargement des packages, packages install√©s et leurs versions) dans un fichier, appel√© par convention environment.yml.\n(dev) $ conda env export > environment.yml\nCe fichier est mis par convention √† la racine du d√©p√¥t Git du projet. Ainsi, les personnes souhaitant tester l‚Äôapplication peuvent recr√©er le m√™me environnement Conda que celui qui a servi au d√©veloppement via la commande suivante.\n$ conda env create -f environment.yml\n\n\nChanger d‚Äôenvironnement\nPour changer d‚Äôenvironnement, il suffit d‚Äôen activer un autre.\n(dev) $ conda base\n(base) $ \nPour sortir de tout environnement Conda, on utilise la commande conda deactivate :\n(base) $ conda deactivate\n$ \n\n\nSupprimer un environnement\nPour supprimer l‚Äôenvironnement dev, on utilise la commande conda env remove -n dev.\n\n\n\nAide-m√©moire\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\nconda create -n <env_name> python=<python_version>\nCr√©ation d‚Äôun environnement nomm√© <env_name> dont la version de Python est <python_version>\n\n\nconda info --envs\nLister les environnements\n\n\nconda activate <env_name>\nUtiliser l‚Äôenvironnement <env_name> pour la session du terminal\n\n\nconda list\nLister les packages dans l‚Äôenvironnement actif\n\n\nconda install <pkg>\nInstaller le package <pkg> dans l‚Äôenvironnement actif\n\n\nconda env export > environment.yml\nExporter les sp√©cifications de l‚Äôenvironnement dans un fichier environment.yml"
  },
  {
    "objectID": "chapters/portability.html#limites",
    "href": "chapters/portability.html#limites",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Limites",
    "text": "Limites\nD√©velopper dans des environnements virtuels est une bonne pratique, car cela accro√Æt la portabilit√© d‚Äôune application. N√©anmoins, il y a plusieurs limites √† leur utilisation : - les librairies syst√®me n√©cessaires √† l‚Äôinstallation des packages ne sont pas g√©r√©es ; - les environnements virtuels ne permettent pas toujours de g√©rer des projets faisant intervenir diff√©rents langages de programmation ; - devoir installer conda, Python, et les packages n√©cessaires √† chaque changement d‚Äôenvironnement peut √™tre assez long et p√©nible en pratique ; - dans un environnement de production, g√©rer des environnements virtuels diff√©rents pour chaque projet peut s‚Äôav√©rer rapidement complexe pour les administrateurs syst√®me.\nLa technologie des conteneurs permet de r√©pondre √† ces diff√©rents probl√®mes."
  },
  {
    "objectID": "chapters/portability.html#introduction-1",
    "href": "chapters/portability.html#introduction-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Introduction",
    "text": "Introduction\nAvec les environnements virtuels, l‚Äôid√©e √©tait de permettre √† chaque utilisateur potentiel de notre projet d‚Äôinstaller sur son environnement d‚Äôex√©cution les packages n√©cessaires √† la bonne ex√©cution du projet. N√©anmoins, comme on l‚Äôa vu, cette approche ne garantit pas une reproductibilit√© parfaite et a l‚Äôinconv√©nient de n√©cessiter beaucoup de gestion manuelle.\nChangeons de perspective : au lieu de distribuer une recette permettant √† l‚Äôutilisateur de recr√©er l‚Äôenvironnement n√©cessaire sur sa machine, ne pourrait-on pas directement distribuer √† l‚Äôutilisateur une machine contenant l‚Äôenvironnement pr√©-configur√© ? Bien entendu, on ve pas configurer et envoyer des ordinateurs portables √† tous les utilisateurs potentiels d‚Äôun projet. Une autre solution serait de distribuer des machines virtuelles, qui tournent sur un serveur et simulent un v√©ritable ordinateur. Ces machines ont cependant l‚Äôinconv√©nient d‚Äô√™tre assez lourdes, et complexes √† r√©pliquer et distribuer. Pour pallier ces diff√©rentes limites, on va utiliser la technologie des conteneurs.\n\nImage trouv√©e sur reddit"
  },
  {
    "objectID": "chapters/portability.html#fonctionnement-1",
    "href": "chapters/portability.html#fonctionnement-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nComme les machines virtuelles, les conteneurs permettent d‚Äôempaqueter compl√®tement l‚Äôenvironnement (librairies syst√®mes, application, configuration) qui permet de faire tourner l‚Äôapplication. Mais √† l‚Äôinverse d‚Äôune machine virtuelle, le conteneur n‚Äôinclut pas de syst√®me d‚Äôexploitation propre, il utilise celui de la machine h√¥te qui l‚Äôex√©cute. La technologie des conteneurs permet ainsi de garantir une tr√®s forte reproductibilit√© tout en restant suffisamment l√©g√®re pour permettre une distribution et un d√©ploiement simple aux utilisateurs.\nDiff√©rences entre l‚Äôapproche conteneurs (gauche) et l‚Äôapproche machines virtuelles (droite)\n\nSource : docker.com"
  },
  {
    "objectID": "chapters/portability.html#impl√©mentations-1",
    "href": "chapters/portability.html#impl√©mentations-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Impl√©mentations",
    "text": "Impl√©mentations\nComme pour les environnements virtuels, il existe diff√©rentes impl√©mentations de la technologie des conteneurs. En pratique, l‚Äôimpl√©mentation offerte par Docker est devenue largement pr√©dominante, au point qu‚Äôil est devenu courant d‚Äôutiliser de mani√®re interchangeable les termes ‚Äúconteneuriser‚Äù et ‚ÄúDockeriser‚Äù une application. C‚Äôest donc cette impl√©mentation que nous allons √©tudier et utiliser dans ce cours."
  },
  {
    "objectID": "chapters/portability.html#docker",
    "href": "chapters/portability.html#docker",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Docker ",
    "text": "Docker \n\nInstallation\nLes instructions √† suivre pour installer Docker  selon son syst√®me d‚Äôexploiration sont d√©taill√©es dans la documentation officielle. Il existe √©galement des environnements bacs √† sable en ligne comme Play with Docker.\n\n\nPrincipes\nUn conteneur Docker est mis √† disposition sous la forme d‚Äôune image, c‚Äôest √† dire d‚Äôun fichier binaire qui contient l‚Äôenvironnement n√©cessaire √† l‚Äôex√©cution de l‚Äôapplication.\nPour construire (build) l‚Äôimage, on utilise un Dockerfile, un fichier texte qui contient la recette ‚Äî sous forme de commandes Linux ‚Äî de construction de l‚Äôenvironnement. L‚Äôimage va √™tre upload√©e (push) sur un d√©p√¥t (registry), public ou priv√©, depuis lequel les utilisateurs vont pouvoir t√©l√©charger l‚Äôimage (pull). Le moteur Docker permet ensuite de lancer (run) un conteneur, c‚Äôest √† dire une instance vivante de l‚Äôimage.\n{{% box status=‚Äúnote‚Äù title=‚ÄúNote: les commandes Docker‚Äù icon=‚Äúfa fa-comment‚Äù %}} Le r√©pertoire d‚Äôimages publiques le plus connu est DockerHub. Il s‚Äôagit d‚Äôun r√©pertoire o√π n‚Äôimporte qui peut proposer une image Docker, associ√©e ou non √† un projet disponible sur Github  ou Gitlab . Il est possible de mettre √† disposition de mani√®re manuelle des images mais, comme nous le montrerons, il est beaucoup plus pratique d‚Äôutiliser des fonctionalit√©s d‚Äôinteraction automatique entre DockerHub et un d√©p√¥t Git. {{% /box %}}\n\n\nEn pratique\n\nApplication\nAfin de pr√©senter l‚Äôutilisation de Docker en pratique, nous allons pr√©senter les diff√©rentes √©tapes permettant de ‚Äúdockeriser‚Äù une application web minimaliste construite avec le framework Python Flask3.\nLa structure de notre projet est la suivante.\n‚îú‚îÄ‚îÄ myflaskapp\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îú‚îÄ‚îÄ hello-world.py\n‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt\nLe script hello-world.py contient le code d‚Äôune application minimaliste, qui affiche simplement ‚ÄúHello, World!‚Äù sur une page web.\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef hello_world():\n    return \"<p>Hello, World!</p>\"\nPour faire tourner l‚Äôapplication, il nous faut donc √† la fois Python et le package Flask. Ces installations doivent √™tre sp√©cifi√©es dans le Dockerfile (cf.¬†section suivante). L‚Äôinstallation de Flask se fait via un fichier requirements.txt, qui contient juste la ligne suivante :\nFlask==2.1.1\n\n\nLe Dockerfile\nA l√† base de chaque image Docker se trouve un Dockerfile. C‚Äôest un fichier texte qui contient une s√©rie de commandes qui permettent de construire l‚Äôimage. Ces fichiers peuvent √™tre plus ou moins complexes selon l‚Äôapplication que l‚Äôon cherche √† conteneuriser, mais leur structure est assez normalis√©e. Pour s‚Äôen rendre compte, analysons ligne √† ligne le Dockerfile n√©cessaire pour construire une image Docker de notre application Flask.\nFROM ubuntu:20.04\n\nRUN apt-get update -y && \\\n    apt-get install -y python3-pip python3-dev\n    \nWORKDIR /app\n\nCOPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY . /app\n\nENV FLASK_APP=\"hello-world.py\"\nEXPOSE 5000\n\nCMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n\nFROM : sp√©cifie l‚Äôimage de base. Une image Docker h√©rite toujours d‚Äôune image de base. Ici, on choisit l‚Äôimage Ubuntu version 20.04, tout va donc se passer comme si l‚Äôon d√©veloppait sur une machine virtuelle vierge ayant pour syst√®me d‚Äôexploitation Ubuntu 20.044 ;\nRUN : lance une commande Linux. Ici, on met d‚Äôabord √† jour la liste des packages t√©l√©chargeables via apt, puis on installe Python ainsi que des librairies syst√®me n√©cessaires au bon fonctionnement de notre application ;\nWORKDIR : sp√©cifie le r√©pertoire de travail de l‚Äôimage. Ainsi, toutes les commandes suivantes seront ex√©cut√©es depuis ce r√©pertoire ;\nCOPY : copie un fichier local sur l‚Äôimage Docker. Ici, on copie d‚Äôabord le fichier requirements.txt du projet, qui sp√©cifie les d√©pendances Python de notre application, puis on les installe avec une commande RUN. La seconde instruction COPY copie le r√©pertoire du projet sur l‚Äôimage ;\nENV : cr√©e une variable d‚Äôenvironnement qui sera accessible √† l‚Äôapplication dans le conteneur. Ici, on d√©finit une variable d‚Äôenvironnement attendue par Flask, qui sp√©cifie le nom du script permettant de lancer l‚Äôapplication ;\nEXPOSE : informe Docker que le conteneur ‚Äú√©coute‚Äù sur le port 5000, qui est le port par d√©faut utilis√© par le serveur web de Flask ;\nCMD : sp√©cifie la commande que doit ex√©cuter le conteneur lors de son lancement. Il s‚Äôagit d‚Äôune liste, qui contient les diff√©rentes parties de la commande sous forme de cha√Ænes de caract√®res. Ici, on lance Flask, qui sait automatiquement quelle application lancer du fait de la commande ENV sp√©cifi√©e pr√©c√©demment.\n\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint: choix des librairies syst√®me‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}} Avec la premi√®re commande RUN du Dockerfile, nous installons Python mais aussi des librairies syst√®me n√©cessaires au bon fonctionnement de l‚Äôapplication. Mais comment les avons-nous trouv√©es ?\nPar essai et erreur. Lors de l‚Äô√©tape de build que l‚Äôon verra juste apr√®s, le moteur Docker va essayer de construire l‚Äôimage selon les sp√©cifications du Dockerfile, comme s‚Äôil partait d‚Äôun ordinateur vide contenant simplement Ubuntu 20.04. Si des librairies manquent, le processus de build devrait renvoyer une erreur, qui s‚Äôaffichera dans les logs de l‚Äôapplication, affich√©s par d√©faut dans la console. Quand on a de la chance, les logs d√©crivent explicitement les librairies syst√®me manquantes. Mais souvent, les messages d‚Äôerreur ne sont pas tr√®s explicites, et il faut alors les copier dans un moteur de recherche bien connu pour trouver la r√©ponse, souvent sur Stackoverflow. {{% /box %}}\n{{% box status=‚Äúhint‚Äù title=‚ÄúHint: pourquoi COPY ?‚Äù icon=‚Äúfa fa-lightbulb‚Äù %}} La recette pr√©sente dans le Dockerfile peut n√©cessiter l‚Äôutilisation de fichiers appartenant au dossier de travail. Pour que Docker les trouve dans son contexte, il est n√©cessaire d‚Äôintroduire une commande COPY. C‚Äôest un petit peu comme pour la cuisine: pour utiliser un produit dans une recette, il faut le sortir du frigo (fichier local) et le mettre sur la table.\n{{% /box %}}\n{{% box status=‚Äúnote‚Äù title=‚ÄúNote: les commandes Docker‚Äù icon=‚Äúfa fa-comment‚Äù %}} Nous n‚Äôavons vu que les commandes Docker les plus fr√©quentes, il en existe beaucoup d‚Äôautres en pratique. N‚Äôh√©sitez pas √† consulter la documentation officielle pour comprendre leur utilisation. {{% /box %}}\n\n\nConstruction d‚Äôune image Docker\nPour construire une image √† partir d‚Äôun Dockerfile, il suffit d‚Äôutiliser la commande docker build. Il faut ensuite sp√©cifier deux √©l√©ments importnats : - le build context. Il faut indiquer √† Docker le chemin de notre projet, qui doit contenir le Dockerfile. En pratique, il est plus simple de se mettre dans le dossier du projet via la commande cd, puis de passer . comme build context pour indiquer √† Docker de build ‚Äúd‚Äôici‚Äù ; - le tag, c‚Äôest √† dire le nom de l‚Äôimage. Tant que l‚Äôon utilisee Docker en local, le tag importe peu. On verra par la suite que la structure du tag a de l‚Äôimportance lorsque l‚Äôon souhaite exporter ou importer une image Docker √† partir d‚Äôun d√©p√¥t distant.\nRegardons ce qui se passe en pratique lorsque l‚Äôon essaie de construire notre image.\n$ docker build -t myflaskapp .\nSending build context to Docker daemon     47MB\nStep 1/8 : FROM ubuntu:20.04\n ---> 825d55fb6340\nStep 2/8 : RUN apt-get update && apt-get install -y python3-pip python3-dev\n ---> Running in 92b42d579cfa\n...\ndone.\nRemoving intermediate container 92b42d579cfa\n ---> 8826d53e3c01\nStep 3/8 : WORKDIR /app\n ---> Running in 153b32893c23\nRemoving intermediate container 153b32893c23\n ---> 7b4d22021986\nStep 4/8 : COPY requirements.txt /app/requirements.txt\n...\nSuccessfully built 125bd8da70ff\nSuccessfully tagged myflaskapp:latest\nLe moteur Docker essaie de construire notre image s√©quentiellement √† partir des commandes sp√©cifi√©es dans le Dockerfile. S‚Äôil rencontre une erreur, la proc√©dure s‚Äôarr√™te, et il faut alors trouver la source du probl√®me dans les logs et adapter le Dockerfile en cons√©quence. Si tout se passe bien, Docker nous indique que le build a r√©ussi et l‚Äôimage est pr√™te √† √™tre utilis√©e. On peut v√©rifier que l‚Äôimage est bien disponible √† l‚Äôaide de la commande docker images.\n$ docker images\nREPOSITORY                               TAG       IMAGE ID       CREATED          SIZE\nmyflaskapp                               latest    57d2f410a631   2 hours ago      433MB\nInt√©ressons nous un peu plus en d√©tail aux logs de l‚Äô√©tape de build. Entre les √©tapes, Docker affiche des suites de lettres et de chiffres un peu √©sot√©riques, et nous parle de conteneurs interm√©diaires. En fait, il faut voir une image Docker comme un empilement de couches (layers), qui sont elles-m√™mes des images Docker. Quand on h√©rite d‚Äôune image avec l‚Äôinstruction FROM, on sp√©cifie donc √† Docker la couche initiale, sur laquelle il va construire le reste de notre environnement. A chaque √©tape sa nouvelle couche, et √† chaque couche son hash, un identifiant unique fait de lettres et de chiffres.\nCela peut ressembler √† des d√©tails techniques, mais c‚Äôest en fait extr√™mement utile en pratique car cela permet √† Docker de faire du caching. Lorsque l‚Äôon d√©veloppe un Dockerfile, il est fr√©quent de devoir modifier ce dernier de nombreuses fois avant de trouver la bonne recette, et on aimerait bien ne pas avoir √† rebuild l‚Äôenvironnement complet √† chaque fois. Docker g√®re cela tr√®s bien : il cache chacune des couches interm√©diaires. Par exemple, si l‚Äôon modifie la 5√®me commande du Dockerfile, Docker va utiliser le cache pour ne pas avoir √† recalculer les √©tapes pr√©c√©dentes, qui n‚Äôont pas chang√©. Cela s‚Äôappelle l‚Äô‚Äúinvalidation du cache‚Äù : d√®s lors qu‚Äôune √©tape du Dockerfile est modifi√©e, Docker va recalculer toutes les √©tapes suivantes, mais seulement celles-ci. Cons√©quence directe de cette observation : il faut toujours ordonner les √©tapes d‚Äôun Dockerfile de sorte √† ce qui est le plus susceptible d‚Äô√™tre souvent modifi√© soit √† la fin du fichier, et inversement.\nPour illustrer cela, regardons ce qui se passe si l‚Äôon modifie le nom du script qui lance l‚Äôapplication, et donc la valeur de la variable d‚Äôenvironnement FLASK_APP dans le Dockerfile.\n$ docker build . -t myflaskapp\nSending build context to Docker daemon  4.096kB\nStep 1/10 : FROM ubuntu:20.04\n ---> 825d55fb6340\nStep 2/10 : ENV DEBIAN_FRONTEND=noninteractive\n ---> Using cache\n ---> ea1c7c083ac9\nStep 3/10 : RUN apt-get update -y &&     apt-get install -y python3-pip python3-dev\n ---> Using cache\n ---> 078b8ac0e1cb\nStep 4/10 : WORKDIR /app\n ---> Using cache\n ---> cd19632825b3\nStep 5/10 : COPY requirements.txt /app/requirements.txt\n ---> Using cache\n ---> 271cd1686899\nStep 6/10 : RUN pip install -r requirements.txt\n ---> Using cache\n ---> 3ea406fdf383\nStep 7/10 : COPY . /app\n ---> 3ce5bd3a9572\nStep 8/10 : ENV FLASK_APP=\"new.py\"\n ---> Running in b378d16bb605\nRemoving intermediate container b378d16bb605\n ---> e1f50490287b\nStep 9/10 : EXPOSE 5000\n ---> Running in ab53c461d3de\nRemoving intermediate container ab53c461d3de\n ---> 0b86eca40a80\nStep 10/10 : CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n ---> Running in 340eec151a51\nRemoving intermediate container 340eec151a51\n ---> 16d7a5b8db28\nSuccessfully built 16d7a5b8db28\nSuccessfully tagged myflaskapp:latest\nL‚Äô√©tape de build a pris quelques secondes au lieu de plusieurs minutes, et les logs montrent bien l‚Äôutilisation du cache faite par Docker : les √©tapes pr√©c√©dant le changement r√©utilisent les couches cach√©es, mais celle d‚Äôapr√®s sont recalcul√©es.\n\n\nEx√©cuter une image Docker\nL‚Äô√©tape de build a permis de cr√©er une image Docker. Une image doit √™tre vue comme un template : elle permet d‚Äôex√©cuter l‚Äôapplication sur n‚Äôimporte quel environnement d‚Äôex√©cution sur lequel un moteur Docker est install√©. En l‚Äô√©tat, on a donc juste construit, mais rien lanc√© : notre application ne tourne pas encore. Pour cela, il faut cr√©er un conteneur, i.e.¬†une instance vivante de l‚Äôimage qui permet d‚Äôacc√©der √† l‚Äôapplication. Cela se fait via la commande docker run.\n$ docker run -d -p 8000:5000 myflaskapp:latest\n6a2ab0d82d051a3829b182ede7b9152f7b692117d63fa013e7dfe6232f1b9e81\nD√©taillons la syntaxe de cette commande : - docker run tag : lance l‚Äôimage dont on fournit le tag. Le tag est de la forme repository/projet:version. Ici, il n‚Äôy a pas de repository puisque tout est fait en local ; - -d : ‚Äúd√©tache‚Äù le conteneur du terminal qui le lance ; - -p : effectue un mapping entre un port de la machine qui ex√©cute le conteneur, et le conteneur lui-m√™me. Notre conteneur √©coute sur le port 5000, et l‚Äôon veut que notre application soit expos√©e sur le port 8000 de notre machine.\nLorsque l‚Äôon ex√©cute docker run, Docker nous r√©pond simplement un hash qui identifie le conteneur que l‚Äôon a lanc√©. On peut v√©rifier qu‚Äôil tourne bien avec la commande docker ps, qui renvoie toutes les informations associ√©es au conteneur.\n$ docker ps\nCONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS                                   NAMES\n6a2ab0d82d05   myflaskapp   \"flask run --host=0.‚Ä¶\"   7 seconds ago   Up 6 seconds   0.0.0.0:8000->5000/tcp, :::8000->5000/tcp   vigorous_kalam\nLes conteneurs peuvent √™tre utilis√©s pour r√©aliser des t√¢ches tr√®s diff√©rentes. Grossi√®rement, on peut distinguer deux situations : - le conteneur effectue une t√¢che ‚Äúone-shot‚Äù, c‚Äôest √† dire une op√©ration qui a vocation √† s‚Äôeffectuer en un certain temps, suite √† quoi le conteneur peut s‚Äôarr√™ter ; - le conteneur ex√©cute une application. Dans ce cas, on souhaite que le conteneur reste en vie aussi longtemps que l‚Äôon souhaite utiliser l‚Äôapplication en question.\nDans notre cas d‚Äôapplication, on se situe dans la seconde configuration puisque l‚Äôon veut ex√©cuter une application web. Lorsque l‚Äôapplication tourne, elle expose sur le localhost, accessible depuis un navigateur web ‚Äî en l‚Äôoccurence, √† l‚Äôadresse localhost:8000/. Les calculs sont effectu√©s sur un serveur local, et le navigateur sert d‚Äôinterface avec l‚Äôutilisateur ‚Äî comme lorsque vous utilisez un notebook Jupyter par exemple.\nFinalement, on a pu d√©velopper et ex√©cuter une application compl√®te sur notre environnement local, sans avoir eu √† installer quoi que ce soit sur notre machine personnelle, √† part Docker.\n\n\nExporter une image Docker\nJusqu‚Äô√† maintenant, toutes les commandes Docker que nous avons ex√©cut√©es se sont pass√©es en local. Ce mode de fonctionnement peut √™tre int√©ressant pour la phase de d√©veloppement. Mais comme on l‚Äôa vu, un des gros avantages de Docker est la facilit√© de redistribution des images construites, qui peuvent ensuite √™tre utilis√©es par de nombreux utilisateurs pour faire tourner notre application. Pour cela, il nous faut uploader notre image sur un d√©p√¥t distant, √† partir duquel les utilisateurs pourront la t√©l√©charger.\nPlusieurs possibilit√©s existent selon le contexte de travail : une entreprise peut avoir un d√©p√¥t interne par exemple. Si le projet est open-source, on peut utiliser le DockerHub. Le workflow pour uploader une image est le suivant : - cr√©er un compte sur le DockerHub ; - cr√©er un projet (public) sur le DockerHub, qui va h√©berger les images Docker du projet ; - sur un terminal, utiliser docker login pour s‚Äôauthentifier au DockerHub ; - on va modifier le tag que l‚Äôon fournit lors du build pour sp√©cifier le chemin attendu. Dans notre cas : docker build -t compte/projet:version . ; - uploader l‚Äôimage avec docker push compte/projet:version\n$ docker push avouacr/myflaskapp:1.0.0\nThe push refers to repository [docker.io/avouacr/myflaskapp]\n71db96687fe6: Pushed \n624877ac887b: Pushed \nea4ab6b86e70: Pushed \nb5120a5bc48d: Pushed \n5fa484a3c9d8: Pushed \nc5ec52c98b31: Pushed \n1.0.0: digest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9 size: 1574\n\n\nImporter une image Docker\nEn supposant que le d√©p√¥t utilis√© pour uploader l‚Äôimage est public, la proc√©dure que doit suivre un utilisateur pour la t√©l√©charger se r√©sume √† utiliser la commande docker pull compte/projet:version\n$ docker pull avouacr/myflaskapp:1.0.0\n1.0.0: Pulling from avouacr/myflaskapp\ne0b25ef51634: Pull complete \nc0445e4b247e: Pull complete \n48ba4e71d1c2: Pull complete \nffd728caa80a: Pull complete \n906a95f00510: Pull complete \nd7d49b6e17ab: Pull complete \nDigest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9\nStatus: Downloaded newer image for avouacr/myflaskapp:1.0.0\ndocker.io/avouacr/myflaskapp:1.0.0\nDocker t√©l√©charge et extrait chacune des couches qui constituent l‚Äôimage (ce qui peut parfois √™tre long). L‚Äôutilisateur peut alors cr√©er un conteneur √† partir de l‚Äôimage, en utilisant docker run comme illustr√© pr√©c√©demment.\n\n\n\nAide-m√©moire\nVoici une premi√®re aide-m√©moire sur les principales commandes √† int√©grer dans un Dockerfile:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\nFROM <image>:<tag>\nUtiliser comme point de d√©part l‚Äôimage <image> ayant le tag <tag>\n\n\nRUN <instructions>\nUtiliser la suite d‚Äôinstructions <instructions> dans un terminal Linux. Pour passer plusieurs commandes dans un RUN, utiliser &&. Cette suite de commande peut avoir plusieurs lignes, dans ce cas, mettre \\ en fin de ligne\n\n\nCOPY <source> <dest>\nR√©cup√©rer le fichier pr√©sent dans le syst√®me de fichier local √† l‚Äôemplacement <source> pour que les instructions ult√©rieures puissent le trouver √† l‚Äôemplacement <source>\n\n\nADD <source> <dest>\nGlobalement, m√™me r√¥le que COPY\n\n\nENV MY_NAME=\"John Doe\"\nCr√©ation d‚Äôune variable d‚Äôenvironnement (qui devient disponible sous l‚Äôalias $MY_NAME)\n\n\nWORKDIR <path>\nD√©finir le working directory du conteuneur Docker dans le dossier <path>\n\n\nUSER <username>\nCr√©ation d‚Äôun utilisateur non root nomm√© <username>\n\n\nEXPOSE <PORT_ID>\nLorsqu‚Äôelle tournera, l‚Äôapplication sera disponible depuis le port <PORT_ID>\n\n\nCMD [\"executable\",\"param1\",\"param2\"]\nAu lancement de l‚Äôinstance Docker la commande executable (par exemple python3) sera lanc√©e avec les param√®tres additionnels fournis\n\n\n\nUne seconde aide-m√©moire pour les principales commandes Linux est disponible ci-dessous:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\ndocker build . -t <tag>\nConstruire l‚Äôimage Docker √† partir des fichiers dans le r√©pertoire courant (.) en l‚Äôidentifiant avec le tag <tag>\n\n\ndocker run -it <tag>\nLancer l‚Äôinstance docker identifi√©e par <tag>\n\n\ndocker images\nLister les images disponibles sur la machine et quelques unes de leurs propri√©t√©s (tags, volume, etc.)\n\n\ndocker system prune\nFaire un peu de m√©nage dans ses images Docker (bien r√©fl√©chir avant de faire tourner cette commande)"
  },
  {
    "objectID": "chapters/projects-architecture.html",
    "href": "chapters/projects-architecture.html",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "",
    "text": "La structuration d‚Äôun projet permet d‚Äôimm√©diatement identifier les √©l√©ments de code et les √©l√©ments annexes, par exemple les d√©pendances √† g√©rer, la documentation, etc.\nUn certain nombre d‚Äôassistants au d√©veloppement de projets orient√©s donn√©es ont √©merg√© pour gagner en productivit√© et faciliter le lancement d‚Äôun projet (voir ce post tr√®s complet sur les extensions VisualStudio).\nL‚Äôid√©e g√©n√©rale est de privil√©gier une structure de projet bien plus fiable qu‚Äôune suite sans structure de scripts ou un Notebook Jupyter (voir ce post de blog sur ce sujet). L‚ÄôIDE Jupyter n‚Äôest pas le meilleur outil pour le d√©veloppement de projets Python ; pour des projets intensifs en code il vaut mieux se tourner vers des IDE classiques comme VSCode."
  },
  {
    "objectID": "chapters/projects-architecture.html#adopter-une-structure-de-projet",
    "href": "chapters/projects-architecture.html#adopter-une-structure-de-projet",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Adopter une structure de projet",
    "text": "Adopter une structure de projet\nLe principe g√©n√©ral d‚Äôune structure de projet est le suivant :\n\nTous les fichiers n√©cessaires au projet dans un m√™me dossier ;\nLe dossier √† la racine du projet sert de working directory ;\nUtilisation de chemins relatifs plut√¥t qu‚Äôabsolus.\n\nLe principe d‚Äôune structure de projet est d‚Äôadopter une structure arbitraire, mais lisible et coh√©rente.\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ raw\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.csv\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data2.csv\n‚îÇ   ‚îî‚îÄ‚îÄ derived\n‚îÇ       ‚îî‚îÄ‚îÄ partial data.csv\n‚îú‚îÄ‚îÄ src\n|   ‚îú‚îÄ‚îÄ script.py\n‚îÇ   ‚îú‚îÄ‚îÄ script_final.py\n‚îÇ   ‚îî‚îÄ‚îÄ report.qmd\n‚îî‚îÄ‚îÄ output\n    ‚îú‚îÄ‚îÄ fig1.png\n    ‚îú‚îÄ‚îÄ figure 2 (copy).png\n    ‚îú‚îÄ‚îÄ figure10.png\n    ‚îú‚îÄ‚îÄ correlation.png\n    ‚îî‚îÄ‚îÄ report.pdf\nLes output sont stock√©s dans un dossier s√©par√©, de m√™me que les inputs. Id√©alement les inputs ne sont m√™me pas stock√©s avec le code, nous reviendrons sur la distinction entre l‚Äôespace de stockage du code et des donn√©es plus tard.\n\n\n\n\n\n\nNote\n\n\n\nComme Git est un pr√©-requis, tout projet pr√©sente un fichier .gitignore (il est tr√®s important, surtout quand on manipule des donn√©es qui ne devraient pas se retrouver sur Github ou Gitlab).\nUn projet pr√©sente aussi un fichier README.md √† la racine, nous reviendrons dessus.\nUn projet qui utilise l‚Äôint√©gration continue pr√©sentera √©galement :\n\nsi vous utilisez Gitlab, les instructions sont stock√©es dans le fichier gitlab-ci.yml\nsi vous utilisez Github, cela se passe dans le dossier .github/workflows"
  },
  {
    "objectID": "chapters/projects-architecture.html#autodocumenter-le-projet-avec-des-noms-pertinents",
    "href": "chapters/projects-architecture.html#autodocumenter-le-projet-avec-des-noms-pertinents",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Autodocumenter le projet avec des noms pertinents",
    "text": "Autodocumenter le projet avec des noms pertinents\nRien qu‚Äôen changeant le nom des fichiers, on rend la structure du projet tr√®s lisible:\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ raw\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dpe_logement_202103.csv\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dpe_logement_202003.csv\n‚îÇ   ‚îî‚îÄ‚îÄ derived\n‚îÇ       ‚îî‚îÄ‚îÄ dpe_logement_merged_preprocessed.csv\n‚îú‚îÄ‚îÄ src\n|   ‚îú‚îÄ‚îÄ preprocessing.py\n‚îÇ   ‚îú‚îÄ‚îÄ generate_plots.py\n‚îÇ   ‚îî‚îÄ‚îÄ report.qmd\n‚îî‚îÄ‚îÄ output\n    ‚îú‚îÄ‚îÄ histogram_energy_diagnostic.png\n    ‚îú‚îÄ‚îÄ barplot_consumption_pcs.png\n    ‚îú‚îÄ‚îÄ correlation_matrix.png\n    ‚îî‚îÄ‚îÄ report.pdf\nMaintenant, le type de donn√©es en entr√©e de chaine est clair, le lien entre les scripts, les donn√©es interm√©diaires et les output est assez transparent."
  },
  {
    "objectID": "chapters/projects-architecture.html#documenter-son-projet",
    "href": "chapters/projects-architecture.html#documenter-son-projet",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Documenter son projet",
    "text": "Documenter son projet\nLe fichier README.md, situ√© √† la racine du projet, est √† la fois la carte d‚Äôidentit√© et la vitrine du projet. Sur Github et Gitlab, comme il s‚Äôagit de l‚Äô√©l√©ment qui s‚Äôaffiche en accueil, ce fichier fait office de premi√®re impression, instant tr√®s court qui peut √™tre d√©terminant sur la valeur √©valu√©e d‚Äôun projet.\nId√©alement, le README.md contient :\n\nUne pr√©sentation du contexte et des objectifs du projet\nUne description de son fonctionnement\nUn guide de contribution si le projet accepte des retours dans le cadre d‚Äôune d√©marche open-source\n\n\n\n\n\n\n\nNote\n\n\n\nQuelques mod√®les de README.md complets, en R :\n\nutilitR\nDoReMIFaSol"
  },
  {
    "objectID": "chapters/projects-architecture.html#tests-unitaires",
    "href": "chapters/projects-architecture.html#tests-unitaires",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Tests unitaires",
    "text": "Tests unitaires\nLes tests unitaires sont des tests automatis√©s qui v√©rifient le bon fonctionnement d‚Äôune unit√© de code, comme une fonction ou une m√©thode. L‚Äôobjectif est de s‚Äôassurer que chaque unit√© de code fonctionne correctement avant d‚Äô√™tre int√©gr√©e dans le reste du programme.\nLes tests unitaires sont utiles lorsqu‚Äôon travaille sur un code de taille cons√©quente ou lorsqu‚Äôon partage son code √† d‚Äôautres personnes, car ils permettent de s‚Äôassurer que les modifications apport√©es ne cr√©ent pas de nouvelles erreurs.\nEn Python, on peut utiliser le package unittest pour √©crire des tests unitaires. Voici un exemple tir√© de ce site :\n# fichier test_str.py\nimport unittest\n\n\nclass ChaineDeCaractereTest(unittest.TestCase):\n\n    def test_reversed(self):\n        resultat = reversed(\"abcd\")\n        self.assertEqual(\"dcba\", \"\".join(resultat))\n\n    def test_sorted(self):\n        resultat = sorted(\"dbca\")\n        self.assertEqual(['a', 'b', 'c', 'd'], resultat)\n\n    def test_upper(self):\n        resultat = \"hello\".upper()\n        self.assertEqual(\"HELLO\", resultat)\n\n    def test_erreur\n\n\nif __name__ == '__main__':\n    unittest.main()\nPour v√©rifier que les tests fonctionnent, on execute ce script depuis la ligne de commande:\npython3 test_str.py\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n\n\n\n\n\n\nWarning\n\n\n\nSi on √©crit des tests unitaires, il est important de les maintenir !\nPrendre du temps pour √©crire des tests unitaires qui ne sont pas maintenus et donc ne renvoie plus de diagnostics pertinent est du temps perdu."
  },
  {
    "objectID": "chapters/projects-architecture.html#maintenance",
    "href": "chapters/projects-architecture.html#maintenance",
    "title": "Am√©liorer l‚Äôarchitecture de ses projets",
    "section": "Maintenance",
    "text": "Maintenance\nL‚Äôobjectif des conseils de ce cours est de r√©duire le co√ªt de la maintenance √† long terme en adoptant les structures les plus l√©g√®res, automatis√©es et r√©utilisables.\nLes notebooks Jupyter sont tr√®s pratiques pour t√¢tonner et exp√©rimenter. Cependant, ils pr√©sentent un certain nombre d‚Äôinconv√©nients √† long terme qui peuvent rendre impossible √† maintenir le code √©crit avec dans un notebook:\n\ntous les objets (fonctions, classes et donn√©es) sont d√©finis et disponibles dans le m√™me fichier. Le moindre changement √† une fonction n√©cessite de retrouver l‚Äôemplacement dans le code, √©crire et faire tourner √† nouveau une ou plusieurs cellules.\nquand on t√¢tonne, on √©crit du code dans des cellules. Dans un cahier, on utiliserait la marge mais cela n‚Äôexiste pas avec un notebook. On cr√©√© donc de nouvelles cellules, pas n√©cessairement dans l‚Äôordre. Quand il est n√©cessaire de faire tourner √† nouveau le notebook, cela provoque des erreurs difficile √† debugger (il est n√©cessaire de retrouver l‚Äôordre logique du code, ce qui n‚Äôest pas √©vident).\nles notebooks incitent √† faire des copier-coller de cellules et modifier marginalement le code plut√¥t qu‚Äô√† utiliser des fonctions.\nil est quasi-impossible d‚Äôavoir un versioning avec Git des notebooks qui fonctionne. Les notebooks √©tant, en arri√®re plan, de gros fichiers JSON, ils ressemblent plus √† des donn√©es que des codes sources. Git ne parvient pas √† identifier les blocs de code qui ont chang√©\npassage en production des notebooks co√ªteux alors qu‚Äôun script bien fait est beaucoup plus facile √† passer en prod (voir suite cours)\nJupyter manque d‚Äôextensions pour mettre en oeuvre les bonnes pratiques (linters, etc.). VSCode au contraire est tr√®s bien\nRisques de r√©v√©lation de donn√©es confidentielles puisque les outputs des blocs de code, par exemple les head, sont √©crits en dur dans le code source.\n\nGlobalement, les notebooks sont un bon outil pour t√¢tonner ou pour faire communiquer. Mais pour maintenir un projet √† long terme, il vaut mieux privil√©gier les scripts. Les recommandations de ce cours visent √† rendre le plus l√©ger possible la maintenance √† long terme de projets data-science en favorisant la reprise par d‚Äôautres (ou par soi dans le futur)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mise en production de projets data science",
    "section": "",
    "text": "Cours dans le parcours data science en derni√®re ann√©e √† l‚ÄôENSAE construit par Romain Avouac et Lino Galiana.\nParcours en construction"
  }
]