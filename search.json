[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mise en production",
    "section": "",
    "text": "Un parcours de formation construit par Romain Avouac et Lino Galiana pour le cursus d’ingénieurs de la donnée de l’ENSAE.\nLes slides associées au cours sont disponibles à cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "index.html#cours-de-mise-en-production-de-projets-data-science",
    "href": "index.html#cours-de-mise-en-production-de-projets-data-science",
    "title": "Mise en production",
    "section": "",
    "text": "Un parcours de formation construit par Romain Avouac et Lino Galiana pour le cursus d’ingénieurs de la donnée de l’ENSAE.\nLes slides associées au cours sont disponibles à cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "chapters/application.html",
    "href": "chapters/application.html",
    "title": "Application",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran."
  },
  {
    "objectID": "chapters/application.html#objectif",
    "href": "chapters/application.html#objectif",
    "title": "Application",
    "section": "Objectif",
    "text": "Objectif\nL’objectif est d’améliorer le projet de manière incrémentale jusqu’à pouvoir le mettre en production, en le valorisant sous une forme adaptée et en adoptant une méthode de travail fluidifiant les évolutions futures.\nLa Figure 1 montre que notre point de départ initial, à savoir un notebook, mélange tout. Ceci rend très complexe la mise à jour de notre modèle ou l’exploitation de notre modèle sur de nouvelles données, ce qui est pourtant la raison d’être du machine learning qui est pensé pour l’extrapolation. Si on vous demande de valoriser votre modèle sur de nouvelles données, vous risquez de devoir refaire tourner tout votre notebook, avec le risque de ne pas retrouver les mêmes résultats que dans la version précédente.\nLa Figure 2 illustre l’horizon auquel nous aboutirons à la fin de cette application. Nous désynchronisons les étapes d’entraînement et de prédiction, en identifiant mieux les pré-requis de chacune et en adoptant des briques technologiques adaptées à celles-ci. Les noms présents sur cette figure sont encore obscurs, c’est normal, mais ils vous deviendrons familiers si vous adoptez une infrastructure et une méthode de travail à l’état de l’art.\n\n\n\n\n\n\nFigure 1: Illustration de notre point de départ\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration de l’horizon vers lequel on se dirige\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIl est important de bien lire les consignes et d’y aller progressivement. Certaines étapes peuvent être rapides, d’autres plus fastidieuses ; certaines être assez guidées, d’autres vous laisser plus de liberté. Si vous n’effectuez pas une étape, vous risquez de ne pas pouvoir passer à l’étape suivante qui en dépend.\nBien que l’exercice soit applicable sur toute configuration bien faite, nous recommandons de privilégier l’utilisation du SSP Cloud, où tous les outils nécessaires sont pré-installés et pré-configurés. Le service VSCode ne sera en effet que le point d’entrée pour l’utilisation d’outils plus exigeants sur le plan de l’infrastructure: Argo, MLFLow, etc."
  },
  {
    "objectID": "chapters/application.html#ce-que-cette-application-ne-couvre-pas-pour-le-moment",
    "href": "chapters/application.html#ce-que-cette-application-ne-couvre-pas-pour-le-moment",
    "title": "Application",
    "section": "Ce que cette application ne couvre pas (pour le moment)",
    "text": "Ce que cette application ne couvre pas (pour le moment)\nA l’heure actuelle, cette application se concentre sur la mise en oeuvre fiable de l’entraînement de modèles de machine learning. Comme vous pouvez le voir, quand on part d’aussi loin qu’un projet monolithique dans un notebook, c’est un travail conséquent d’en arriver à un pipeline pensé pour la production. Cette application vise à vous sensibiliser au fait qu’avoir la Figure 2 en tête et adopter une organisation de travail et faire des choix techniques adéquats, vous fera économiser des dizaines voire centaines d’heures lorsque votre modèle aura vocation à passer en production.\nA l’heure actuelle, cette application ne se concentre que sur une partie du cycle de vie d’un projet data ; il y a déjà fort à faire. Nous nous concentrons sur l’entraînement et la mise à disposition d’un modèle à des fins opérationnelles. C’est la première partie du cycle de vie d’un modèle. Dans une approche MLOps, il faut également penser la maintenance de ce modèle et les enjeux que représentent l’arrivée continue de nouvelles données, ou le besoin d’en collecter de nouvelles à travers des annotations, sur la qualité prédictive d’un modèle. Toute entreprise qui ne pense pas cet après est vouée à se faire doubler par un nouveau venu. Une prochaine version de cette application permettra certainement d’illustrer certains des enjeux afférants à la vie en production d’un modèle (supervision, annotations…) sur notre cas d’usage.\nIl convient aussi de noter que nous ne faisons que parcourir la surface des sujets que nous évoquons. Ce cours, déjà dense, deviendrait indigeste si nous devions présenter chaque outil dans le détail. Nous laissons donc les curieux approfondir chacun des outils que nous présentons pour découvrir comment en tirer le maximum (et si vous avez l’impression que nous oublions des éléments cruciaux, les issues et pull requests  sont bienvenues)."
  },
  {
    "objectID": "chapters/application.html#comment-gérer-les-checkpoints",
    "href": "chapters/application.html#comment-gérer-les-checkpoints",
    "title": "Application",
    "section": "Comment gérer les checkpoints ?",
    "text": "Comment gérer les checkpoints ?\nPour simplifier la reprise en cours de ce fil rouge, nous proposons un système de checkpoints qui s’appuient sur des tags Git. Ces tags figent le projet tel qu’il est à l’issue d’un exercice donné.\nSi vous faites évoluer votre projet de manière expérimentale mais désirez tout de même utiliser à un moment ces checkpoints, il va falloir faire quelques acrobaties Git. Pour cela, nous mettons à disposition un script qui permet de sauvegarder votre avancée dans un tag donné (au cas où, à un moment, vous vouliez revenir dessus) et écraser la branche main avec le tag en question. Par exemple, si vous désirez reprendre après l’exercice 9, vous devrez faire tourner le code dans cette boite :\n  \n    \n      \n        \n      \n      \n        Checkpoint d'exemple      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli92\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli9\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCelui-ci sauvegarde votre avancée dans un tag nommé dev_before_appli9, le pousse sur votre dépôt Github  puis force votre branche à adopter l’état du tag appli9."
  },
  {
    "objectID": "chapters/application.html#étape-1-sassurer-que-le-script-sexécute-correctement",
    "href": "chapters/application.html#étape-1-sassurer-que-le-script-sexécute-correctement",
    "title": "Application",
    "section": "Étape 1 : s’assurer que le script s’exécute correctement",
    "text": "Étape 1 : s’assurer que le script s’exécute correctement\nOn va partir du fichier notebook.py qui reprend le contenu du notebook2 mais dans un script classique. Le travail de nettoyage en sera facilité.\nLa première étape est simple, mais souvent oubliée : vérifier que le code fonctionne correctement. Pour cela, nous recommandons de faire un aller-retour entre le script ouvert dans VSCode et un terminal pour le lancer.\n\n\n\n\n\n\nApplication 1: corriger les erreurs\n\n\n\n\nOuvrir dans VSCode le script titanic.py ;\nExécuter le script en ligne de commande (python titanic.py)3 pour détecter les erreurs ;\nCorriger les deux erreurs qui empêchent la bonne exécution ;\nVérifier le fonctionnement du script en utilisant la ligne de commande:\n\n\n\nterminal\n\npython titanic.py\n\nLe code devrait afficher des sorties.\n\n\nAide sur les erreurs rencontrées\n\nLa première erreur rencontrée est une alerte FileNotFoundError, la seconde est liée à un package.\n\nIl est maintenant temps de commit les changements effectués avec Git4 :\n\n\nterminal\n\ngit add titanic.py\ngit commit -m \"Corrige l'erreur qui empêchait l'exécution\"\ngit push\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli1      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli1\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-2-utiliser-un-linter-puis-un-formatter",
    "href": "chapters/application.html#étape-2-utiliser-un-linter-puis-un-formatter",
    "title": "Application",
    "section": "Étape 2: utiliser un linter puis un formatter",
    "text": "Étape 2: utiliser un linter puis un formatter\nOn va maintenant améliorer la qualité de notre code en appliquant les standards communautaires. Pour cela, on va utiliser le linter classique PyLint et le formatter Black. Si vous désirez un outil deux en un, il est possible d’utiliser Ruff en complément ou substitut.\nCe nettoyage automatique du code permettra, au passage, de restructurer notre script de manière plus naturelle.\n\n\n\n\n\n\nImportant\n\n\n\nPyLint, Black et Ruff sont des packages Python qui s’utilisent principalement en ligne de commande.\nSi vous avez une erreur qui suggère que votre terminal ne connait pas PyLint, Black, ou Ruff, n’oubliez pas d’exécuter la commande pip install pylint, pip install black ou pip install ruff.\n\n\nLe linter PyLint renvoie alors une série d’irrégularités, en précisant à chaque fois la ligne de l’erreur et le message d’erreur associé (ex : mauvaise identation). Il renvoie finalement une note sur 10, qui estime la qualité du code à l’aune des standards communautaires évoqués dans la partie Qualité du code.\n\n\n\n\n\n\nApplication 2: rendre lisible le script\n\n\n\n\nDiagnostiquer et évaluer la qualité de titanic.py avec PyLint. Regarder la note obtenue.\nUtiliser black titanic.py --diff --color pour observer les changements de forme que va induire l’utilisation du formatter Black. Cette étape n’applique pas les modifications, elle ne fait que vous les montrer.\nAppliquer le formatter Black\nRéutiliser PyLint pour diagnostiquer l’amélioration de la qualité du script et le travail qui reste à faire.\nComme la majorité du travail restant est à consacrer aux imports:\n\nMettre tous les imports ensemble en début de script\nRetirer les imports redondants en s’aidant des diagnostics de votre éditeur\nRéordonner les imports si PyLint vous indique de le faire\nCorriger les dernières fautes formelles suggérées par PyLint\n\nDélimiter des parties dans votre code pour rendre sa structure plus lisible. Si des parties vous semblent être dans le désordre, vous pouvez réordonner le script (mais n’oubliez pas de le tester)\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli2      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli22\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli2\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nLe code est maintenant lisible, il obtient à ce stade une note formelle proche de 10. Mais il n’est pas encore totalement intelligible ou fiable. Il y a notamment quelques redondances de code auxquelles nous allons nous attaquer par la suite. Néanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script: jetons d’API et chemin des fichiers."
  },
  {
    "objectID": "chapters/application.html#étape-3-gestion-des-paramètres",
    "href": "chapters/application.html#étape-3-gestion-des-paramètres",
    "title": "Application",
    "section": "Étape 3: gestion des paramètres",
    "text": "Étape 3: gestion des paramètres\n  \n    \n      \n        \n      \n      \n        Reprendre à partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration proposée dans l'application préliminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, après avoir clôné le dépôt\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli22\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli2\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nL’exécution du code et les résultats obtenus dépendent de certains paramètres définis dans le code. L’étude de résultats alternatifs, en jouant sur des variantes des (hyper)paramètres, est à ce stade compliquée car il est nécessaire de parcourir le code pour trouver ces paramètres. De plus, certains paramètres personnels comme des jetons d’API ou des mots de passe n’ont pas vocation à être présents dans le code.\nIl est plus judicieux de considérer ces paramètres comme des variables d’entrée du script. Cela peut être fait de deux manières:\n\nAvec des arguments optionnels appelés depuis la ligne de commande (Application 3a). Cela peut être pratique pour mettre en oeuvre des tests automatisés mais n’est pas forcément pertinent pour toutes les variables. Nous allons montrer cet usage avec le nombre d’arbres de notre random forest ;\nEn utilisant un fichier de configuration dont les valeurs sont importées dans le script principal (Application 3b).\n\n\n\nUn exemple de définition d’un argument pour l’utilisation en ligne de commande\n\n\n\nprenom.py\n\nimport argparse\nparser = argparse.ArgumentParser(description=\"Qui êtes-vous?\")\nparser.add_argument(\n    \"--prenom\", type=str, default=\"Toto\", help=\"Un prénom à afficher\"\n)\nargs = parser.parse_args()\nprint(args.prenom)\n\nExemples d’utilisations en ligne de commande\n\n\nterminal\n\npython prenom.py\npython prenom.py --prenom \"Zinedine\"\n\n\n\n\n\n\n\n\nApplication 3a: Paramétrisation du script\n\n\n\n\nEn s’inspirant de l’exemple ci-dessus 👆️, créer une variable n_trees qui peut éventuellement être paramétrée en ligne de commande et dont la valeur par défaut est 20 ;\nTester cette paramétrisation en ligne de commande avec la valeur par défaut puis 2, 10 et 50 arbres.\n\n\n\nL’exercice suivant permet de mettre en application le fait de paramétriser un script en utilisant des variables définies dans un fichier YAML.\n\n\n\n\n\n\nApplication 3b: La configuration dans un fichier dédié\n\n\n\n\nInstaller le package python-dotenv que nous allons utiliser pour charger notre jeton d’API à partir d’une variable d’environnement.\nA partir de l’exemple de la documentation, utiliser la fonction load_dotenv pour charger dans Python nos variables d’environnement à partir d’un fichier (vous pouvez le créer mais ne pas le remplir encore avec les valeurs voulues, ce sera fait ensuite)\nCréer la variable et vérifier la sortie de Python en faisant tourner titanic.py en ligne de commande\n\n\n\ntitanic.py\n\njeton_api = os.environ.get(\"JETON_API\", \"\")\n\nif jeton_api.startswith(\"$\"):\n    print(\"API token has been configured properly\")\nelse:\n    print(\"API token has not been configured\")\n\n\nMaintenant introduire la valeur voulue pour le jeton d’API dans le fichier d’environnement lu par dotenv\nS’il n’existe pas déjà, créer un fichier .gitignore (cf. Chapitre Git). Ajouter dans ce fichier .env car il ne faut pas committer ce fichier. Au passage ajouter __pycache__/ au .gitignore5, cela évitera d’avoir à le faire ultérieurement ;\nCréer un fichier README.md où vous indiquez qu’il faut créer un fichier .env pour pouvoir utiliser l’API.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli3      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli32\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli3\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-4-privilégier-la-programmation-fonctionnelle",
    "href": "chapters/application.html#étape-4-privilégier-la-programmation-fonctionnelle",
    "title": "Application",
    "section": "Étape 4 : Privilégier la programmation fonctionnelle",
    "text": "Étape 4 : Privilégier la programmation fonctionnelle\nNous allons mettre en fonctions les parties importantes de l’analyse. Ceci facilitera l’étape ultérieure de modularisation de notre projet. Comme cela est évoqué dans les éléments magistraux de ce cours, l’utilisation de fonctions va rendre notre code plus concis, plus traçable, mieux documenté.\nCet exercice étant chronophage, il n’est pas obligatoire de le réaliser en entier. L’important est de comprendre la démarche et d’adopter fréquemment une approche fonctionnelle6. Pour obtenir une chaine entièrement fonctionnalisée, vous pouvez reprendre le checkpoint.\nPour commencer, cet exercice fait un petit pas de côté pour faire comprendre la manière dont les pipelines scikit sont un outil au service des bonnes pratiques.\n\n\n\n\n\n\nApplication 4 (optionnelle): pourquoi utiliser un pipeline Scikit ?\n\n\n\n\nLe pipeline Scikit d’estimation et d’évaluation vous a été donné tel quel. Regardez, ci-dessous, le code équivalent sans utiliser de pipeline Scikit:\n\n\n\nLe code équivalent sans pipeline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nimport pandas as pd\nimport numpy as np\n\n# Définition des variables\nnumeric_features = [\"Age\", \"Fare\"]\ncategorical_features = [\"Embarked\", \"Sex\"]\n\n# PREPROCESSING ----------------------------\n\n# Handling missing values for numerical features\nnum_imputer = SimpleImputer(strategy=\"median\")\nX_train[numeric_features] = num_imputer.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = num_imputer.transform(X_test[numeric_features])\n\n# Scaling numerical features\nscaler = MinMaxScaler()\nX_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = scaler.transform(X_test[numeric_features])\n\n# Handling missing values for categorical features\ncat_imputer = SimpleImputer(strategy=\"most_frequent\")\nX_train[categorical_features] = cat_imputer.fit_transform(X_train[categorical_features])\nX_test[categorical_features] = cat_imputer.transform(X_test[categorical_features])\n\n# One-hot encoding categorical features\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nX_train_encoded = encoder.fit_transform(X_train[categorical_features])\nX_test_encoded = encoder.transform(X_test[categorical_features])\n\n# Convert encoded features into a DataFrame\nX_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_features), index=X_train.index)\nX_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_features), index=X_test.index)\n\n# Drop original categorical columns and concatenate encoded ones\nX_train = X_train.drop(columns=categorical_features).join(X_train_encoded)\nX_test = X_test.drop(columns=categorical_features).join(X_test_encoded)\n\n# MODEL TRAINING ----------------------------\n\n# Defining the model\nmodel = RandomForestClassifier(n_estimators=n_trees)\n\n# Fitting the model\nmodel.fit(X_train, y_train)\n\n# EVALUATION ----------------------------\n\n# Scoring\nrdmf_score = model.score(X_test, y_test)\nprint(f\"{rdmf_score:.1%} de bonnes réponses sur les données de test pour validation\")\n\n# Confusion matrix\nprint(20 * \"-\")\nprint(\"matrice de confusion\")\nprint(confusion_matrix(y_test, model.predict(X_test)))\n\n\nVoyez-vous l’intérêt de l’approche par pipeline en termes de lisibilité, évolutivité et fiabilité ?\nCréer un notebook qui servira de brouillon. Y introduire le code suivant:\n\n\n\nLe code à copier-coller dans un notebook\n\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nX_train, y_train = train.drop(\"Survived\", axis=\"columns\"), train[\"Survived\"]\nX_test, y_test = test.drop(\"Survived\", axis=\"columns\"), train[\"Survived\"]\n\nMAX_DEPTH = None\nMAX_FEATURES = \"sqrt\"\nn_trees=20\n\nnumeric_features = [\"Age\", \"Fare\"]\ncategorical_features = [\"Embarked\", \"Sex\"]\n\n# Variables numériques\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", MinMaxScaler()),\n    ]\n)\n\n# Variables catégorielles\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder()),\n    ]\n)\n\n# Preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"Preprocessing numerical\", numeric_transformer, numeric_features),\n        (\n            \"Preprocessing categorical\",\n            categorical_transformer,\n            categorical_features,\n        ),\n    ]\n)\n\n# Pipeline\npipe = Pipeline(\n    [\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", RandomForestClassifier(\n            n_estimators=n_trees,\n            max_depth=MAX_DEPTH,\n            max_features=MAX_FEATURES\n        )),\n    ]\n)\n\npipe.fit(X_train, y_train)\n\n\nAfficher ce pipeline dans une cellule de votre notebook. Cela vous aide-t-il mieux à comprendre les différentes étapes du pipeline de modélisation ?\nComment pouvez-vous accéder aux étapes de preprocessing ?\n\n\n\nComment pouvez-vous faire pour appliquer le pipeline de preprocessing des variables numériques (et uniquement celui-ci) à ce DataFrame ?\n\n\n\nLe DataFrame à créer pour appliquer un bout de notre pipeline\n\nimport numpy as np\n\nnew_data = {\n    \"Age\": [22, np.nan, 35, 28, np.nan],\n    \"Fare\": [7.25, 8.05, np.nan, 13.00, 15.50]\n}\n\nnew_data = pd.DataFrame(new_data)\n\n\n\nNormalement ce code ne devrait pas prendre plus d’une demie-douzaine de lignes. Sans pipeline le code équivalent, beaucoup plus verbeux et moins fiable, ressemble à celui-ci\n\n\n\nLe code équivalent, sans pipeline\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Définition des nouvelles données\nnew_data = pd.DataFrame({\n    \"Age\": [25, np.nan, 40, 33, np.nan],\n    \"Fare\": [10.50, 7.85, np.nan, 22.00, 12.75]\n})\n\n# Définition des transformations (même que dans le pipeline)\nnum_imputer = SimpleImputer(strategy=\"median\")\nscaler = MinMaxScaler()\n\n# Apprentissage des transformations sur X_train (assumant que vous l'avez déjà)\nX_train_numeric = X_train[[\"Age\", \"Fare\"]]  # Supposons que X_train existe\nnum_imputer.fit(X_train_numeric)\nscaler.fit(num_imputer.transform(X_train_numeric))\n\n# Transformation des nouvelles données\nnew_data_imputed = num_imputer.transform(new_data)\nnew_data_scaled = scaler.transform(new_data_imputed)\n\n# Création du DataFrame final\nnew_data_preprocessed = pd.DataFrame(\n    new_data_scaled,\n    columns=[\"Age_scaled\", \"Fare_scaled\"]  # Générer des noms de colonnes adaptés\n)\n\n# Affichage du DataFrame\nprint(new_data_preprocessed)\n\n\nImaginons que vous ayez déjà des données préprocessées:\n\n\n\nCréer des données préprocessées\n\nimport numpy as np\nimport pandas as pd\n\nnew_data = pd.DataFrame({\n    \"Age\": [25, np.nan, 40, 33, np.nan],\n    \"Fare\": [10.50, 7.85, np.nan, 22.00, 12.75],\n    \"Embarked\": [\"S\", \"C\", np.nan, \"Q\", \"S\"],\n    \"Sex\": [\"male\", \"female\", \"male\", np.nan, \"female\"]\n})\nnew_y = np.random.randint(0, 2, size=len(new_data))\n\npreprocessed_data = pd.DataFrame(\n    pipe[:-1].transform(new_data),\n    columns = preprocessor_numeric.get_feature_names_out()\n)\npreprocessed_data\n\n\nDéterminer le score en prédiction sur ces données\n\n\n\n\nMaintenant, revenons à notre chaine de production et appliquons des fonctions pour la rendre plus lisible, plus fiable et plus modulaire.\n\n\n\n\n\n\nApplication 4: adoption des standards de programmation fonctionnelle\n\n\n\nCette application peut être chronophage, vous pouvez aller plus ou moins loin dans la fonctionalisation de votre script en fonction du temps dont vous disposez.\n\nCréer une fonction qui intègre les différentes étapes du pipeline (preprocessing et définition du modèle). Cette fonction prend en paramètre le nombre d’arbres (argument obligatoire) et des arguments optionnels supplémentaires (les colonnes sur lesquelles s’appliquent les différentes étapes du pipeline, max_depth et max_features).\nCréer une fonction d’évaluation renvoyant le score obtenu et la matrice de confusion, à l’issue d’une estimation (mais cette estimation est faite en amont de la fonction, pas au sein de celle-ci)\nDéplacer toutes les fonctions ensemble, en début de script. Si besoin, ajouter des paramètres à votre fichier d’environnement pour créer de nouvelles variables comme les chemins des données.\nEn profiter pour supprimer le code zombie qu’on a gardé jusqu’à présent mais qui ne correspond pas vraiment à des opérations utiles à notre chaine de production\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli4      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli42\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli4\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCela ne se remarque pas encore vraiment car nous avons de nombreuses définitions de fonctions mais notre chaine de production est beaucoup plus concise (le script fait environ 150 lignes dont une centaine issues de définitions de fonctions génériques). Cette auto-discipline facilitera grandement les étapes ultérieures. Cela aurait été néanmoins beaucoup moins coûteux en temps d’adopter ces bons gestes de manière plus précoce."
  },
  {
    "objectID": "chapters/application.html#étape-1-modularisation",
    "href": "chapters/application.html#étape-1-modularisation",
    "title": "Application",
    "section": "Étape 1 : modularisation",
    "text": "Étape 1 : modularisation\nNous allons profiter de la modularisation pour adopter une structure applicative pour notre code. Celui-ci n’étant en effet plus lancé que depuis la ligne de commande, on peut considérer qu’on construit une application générique où un script principal (main.py) encapsule des éléments issus d’autres scripts Python.\n\n\n\n\n\n\nApplication 5: modularisation\n\n\n\n\nDéplacer les fonctions dans une série de fichiers dédiés:\n\nbuild_pipeline.py: script avec la définition du pipeline\ntrain_evaluate.py: script avec les fonctions d’évaluation du projet\n\nSpécifier les dépendances (i.e. les packages à importer) dans les modules pour que ceux-ci puissent s’exécuter indépendamment ;\nRenommer titanic.py en main.py pour suivre la convention de nommage des projets Python ;\nImporter les fonctions nécessaires à partir des modules.\nVérifier que tout fonctionne bien en exécutant le script main à partir de la ligne de commande :\n\n\n\nterminal\n\npython main.py\n\n\nOptionnel: profitez en pour mettre un petit coup de formatter à votre projet, si vous ne l’avez pas fait régulièrement.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli5      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli52\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli5\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-2-adopter-une-architecture-standardisée-de-projet",
    "href": "chapters/application.html#étape-2-adopter-une-architecture-standardisée-de-projet",
    "title": "Application",
    "section": "Étape 2 : adopter une architecture standardisée de projet",
    "text": "Étape 2 : adopter une architecture standardisée de projet\nOn dispose maintenant d’une application Python fonctionnelle. Néanmoins, le projet est certes plus fiable mais sa structuration laisse à désirer et il serait difficile de rentrer à nouveau dans le projet dans quelques temps.\n\n\nEtat actuel du projet 🙈\n\n├── .gitignore\n├── .env\n├── data.csv\n├── train.csv\n├── test.csv\n├── README.md\n├── build_pipeline.py\n├── train_evaluate.py\n├── titanic.ipynb\n└── main.py\n\nComme cela est expliqué dans la partie Structure des projets, on va adopter une structure certes arbitraire mais qui va faciliter l’autodocumentation de notre projet. De plus, une telle structure va faciliter des évolutions optionnelles comme la packagisation du projet. Passer d’une structure modulaire bien faite à un package est quasi-immédiat en Python.\nOn va donc modifier l’architecture de notre projet pour la rendre plus standardisée. Pour cela, on va s’inspirer des structures cookiecutter qui génèrent des templates de projet. En l’occurrence notre source d’inspiration sera le template datascience issu d’un effort communautaire.\n\n\n\n\n\n\nNote\n\n\n\nL’idée de cookiecutter est de proposer des templates que l’on utilise pour initialiser un projet, afin de bâtir à l’avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante :\n\n\nterminal\n\npip install cookiecutter\ncookiecutter https://github.com/drivendata/cookiecutter-data-science\n\nIci, on a déjà un projet, on va donc faire les choses dans l’autre sens : on va s’inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.\n\n\nEn s’inspirant du cookiecutter data science on va adopter la structure suivante:\n\n\nStructure recommandée\n\napplication\n├── main.py\n├── .env\n├── README.md\n├── data\n│   ├── raw\n│   │   └── data.csv\n│   └── derived\n│       ├── test.csv\n│       └── train.csv\n├── notebooks\n│   └── titanic.ipynb\n└── src\n    ├── pipeline\n    │   └── build_pipeline.py\n    └── models\n        └── train_evaluate.py\n\n\n\n\n\n\n\nApplication 6: adopter une structure lisible\n\n\n\n\n(optionnel) Analyser et comprendre la structure de projet proposée par le template ;\nModifier l’arborescence du projet selon le modèle ;\nMettre à jour l’import des dépendances, le fichier de configuration et main.py avec les nouveaux chemins ;\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli6      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli62\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli6\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-3-mieux-tracer-notre-chaine-de-production",
    "href": "chapters/application.html#étape-3-mieux-tracer-notre-chaine-de-production",
    "title": "Application",
    "section": "Étape 3: mieux tracer notre chaine de production",
    "text": "Étape 3: mieux tracer notre chaine de production\n\nIndiquer l’environnement minimal de reproductibilité\nLe script main.py nécessite un certain nombre de packages pour être fonctionnel. Chez vous les packages nécessaires sont bien sûr installés mais êtes-vous assuré que c’est le cas chez la personne qui testera votre code ?\nAfin de favoriser la portabilité du projet, il est d’usage de “fixer l’environnement”, c’est-à-dire d’indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version. Nous proposons de créer un fichier requirements.txt minimal, sur lequel nous reviendrons dans la partie consacrée aux environnements reproductibles.\nLe fichier requirements.txt est conventionnellement localisé à la racine du projet. Ici on ne va pas fixer les versions, on raffinera ce fichier ultérieurement.\n\n\n\n\n\n\nApplication 7a: création du requirements.txt\n\n\n\n\nCréer un fichier requirements.txt avec la liste des packages nécessaires\nAjouter une indication dans README.md sur l’installation des packages grâce au fichier requirements.txt\n\n\n\n\n\nTracer notre chaîne\nQuand votre projet passera en production, vous aurez un accès limité à celui-ci. Il est donc important de faire remonter, par le biais du logging des informations critiques sur votre projet qui vous permettront de savoir où il en est (si vous avez accès à la console où il tourne) ou là où il s’est arrêté.\nL’utilisation de print montre rapidement ses limites pour cela. Les informations enregistrées ne persistent pas après la session et sont quelques peu rudimentaires.\nPour faire du logging, la librairie consacrée depuis longtemps en Python est… logging. Il existe aussi une librairie nommée loguru qui est un peu plus simple à configurer (l’instanciation du logger est plus aisée) et plus agréable grâce à ses messages en couleurs qui permettent de visuellement trier les informations.\n\nL’exercice suivant peut être fait avec les deux librairies, cela ne change pas grand chose. Les prochaines applications repartiront de la version utilisant la librairie standard logging.\n\n\n\n\n\n\nApplication 7b: remontée de messages par logging\n\n\n\n\nVersion utilisant loggingVersion utilisant loguru\n\n\n\nAller sur la documentation de la librairie ici et sur ce tutoriel pour trouver des sources d’inspiration sur la configuration et l’utilisation de logging.\nPour afficher les messages dans la console et dans un fichier de log, s’inspirer de cette réponse sur stack overflow.\nTester en ligne de commande votre code et observer le fichier de log\n\n\n\n\nInstaller loguru et l’ajouter au requirements.txt\nEn s’aidant du README du projet sur Github, remplacer nos print par différents types de messages (info, success, etc.).\nTester l’exécution du script en ligne de commande et observer vos sorties\nMettre à jour le logger pour enregistrer dans un fichier de log. Ajouter celui-ci au .gitignore puis tester en ligne de commande votre script. Ouvrir le fichier en question, refaites tourner le script et regardez son évolutoin.\nIl est possible avec loguru de capturer les erreurs des fonctions grâce au système de cache décrit ici. Introduire une erreur dans une des fonctions (par exemple dans create_pipeline) avec un code du type raise ValueError(\"Problème ici\")\n\n\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli7      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli72\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli7\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#stockageS3",
    "href": "chapters/application.html#stockageS3",
    "title": "Application",
    "section": "Étape 4 : stocker les données de manière externe",
    "text": "Étape 4 : stocker les données de manière externe\nPour cette partie, il faut avoir un service VSCode dont les jetons d’authentification à S3 sont valides. Pour cela, si vous êtes sur le SSPCloud, le plus simple est de recréer un nouveau service avec le bouton suivant\n\net remplir l’onglet Git comme ça votre VSCode sera pré à l’emploi (cf. application 0).\nUne fois que vous avez un VSCode fonctionnel, il est possible de reprendre cette application fil rouge depuis le checkpoint précédent.\n  \n    \n      \n        \n      \n      \n        Reprendre à partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration proposée dans l'application préliminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, après avoir clôné le dépôt\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli72\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli7\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nEnfin, il vous suffira d’ouvrir un terminal et faire pip install -r requirements.txt && python main.py pour pouvoir démarrer l’application.\nL’étape précédente nous a permis d’isoler la configuration. Nous avons conceptuellement isolé les données du code lors des applications précédentes. Cependant, nous n’avons pas été au bout du chemin car le stockage des données reste conjoint à celui du code. Nous allons maintenant dissocier ces deux éléments.\n\n\n\n\n\n\nPour en savoir plus sur le système de stockage S3\n\n\n\n\n\nPour mettre en oeuvre cette étape, il peut être utile de comprendre un peu comme fonctionne le SSP Cloud. Vous devrez suivre la documentation du SSP Cloud pour la réaliser. Une aide-mémoire est également disponible dans le cours de 2e année de l’ENSAE Python pour la data science.\n\n\n\n\n\n\n\n\n\nPour en savoir plus sur le format Parquet\n\n\n\n\n\nL’objectif de cette application est de montrer comment utiliser le format Parquet dans une chaîne production ; un objectif somme toute modeste.\nSi vous voulez aller plus loin dans la découverte du format Parquet, vous pouvez consulter cette ressource R très similaire à ce cours (oui elle est faite par les mêmes auteurs…) et essayer de faire les exercices avec votre librairie Python de prédilection (PyArrow ou DuckDB)\n\n\n\n\n\n\n\n\n\nEt si vous utilisez une infrastructure cloud qui n’est pas le SSPCloud ? (une idée saugrenue mais sait-on jamais)\n\n\n\n\n\nLes exemples à venir peuvent très bien être répliqués sur n’importe quel cloud provider qui propose une solution de type S3, qu’il s’agisse d’un cloud provider privé (AWS, GCP, Azure, etc.) ou d’une réinstanciation ad hoc du projet Onyxia, le logiciel derrière le SSPCloud.\nPour un système de stockage S3, il suffit de changer les paramètres de connexion de s3fs (endpoint, region, etc.). Pour les stockages sur GCP, les codes sont presque équivalents, il suffit de remplacer la librairie s3fs par gcfs; ces deux librairies sont en fait des briques d’un standard plus général de gestion de systèmes de fichiers en Python ffspec.\n\n\n\nLe chapitre sur la structure des projets développe l’idée qu’il est recommandé de converger vers un modèle où environnements d’exécution, de stockage du code et des données sont conceptuellement séparés. Ce haut niveau d’exigence est un gain de temps important lors de la mise en production car au cours de cette dernière, le projet est amené à être exécuté sur une infrastructure informatique dédiée qu’il est bon d’anticiper. Schématiquement, nous visons la structure de projet suivante:\n\nA l’heure actuelle, les données sont stockées dans le dépôt. C’est une mauvaise pratique. En premier lieu, Git n’est techniquement pas bien adapté au stockage de données. Ici ce n’est pas très grave car il ne s’agit pas de données volumineuses et ces dernières ne sont pas modifiées au cours de notre chaine de traitement.\nLa raison principale est que les données traitées par les data scientists sont généralement soumises à des clauses de confidentialités (RGPD, secret statistique…). Mettre ces données sous contrôle de version c’est prendre le risque de les divulguer à un public non habilité. Il est donc recommandé de privilégier des outils techniques adaptés au stockage de données.\nL’idéal, dans notre cas, est d’utiliser une solution de stockage externe. On va utiliser pour cela MinIO, la solution de stockage de type S3 offerte par le SSP Cloud. Cela nous permettra de supprimer les données de Github tout en maintenant la reproductibilité de notre projet 7.\nPlus concrètement, nous allons adopter le pipeline suivant pour notre projet:\n\nLe scénario type est que nous avons une source brute, reçue sous forme de CSV, dont on ne peut changer le format. Il aurait été idéal d’avoir un format plus adapté au traitement de données pour ce fichier mais ce n’était pas de notre ressort. Notre chaine va aller chercher ce fichier, travailler dessus jusqu’à valoriser celui-ci sous la forme de notre matrice de confusion. Si on imagine que notre chaine prend un certain temps, il n’est pas inutile d’écrire des données intermédiaires. Pour faire cela, puisque nous avons la main, autant choisir un format adapté, à savoir le format Parquet.\nCette application va se dérouler en trois temps:\n\nUpload de notre source brute (CSV) sur S3\nIllustration de l’usage des librairies cloud native pour lire celle-ci\nPartage public de cette donnée pour la rendre accessible de manière plus simple à nos futures applications.\n\n\n\n\n\n\n\nApplication 8a: ajout de données sur le système de stockage S3\n\n\n\nPour commencer, à partir de la ligne de commande, utiliser l’utilitaire MinIO pour copier les données data/raw/data.csv vers votre bucket personnel. Les données intermédiaires peuvent être laissées en local mais doivent être ajoutées au .gitignore.\n\n\nIndice\n\nStructure à adopter:\n\n\nterminal\n\nBUCKET_PERSONNEL=\"nom_utilisateur_sspcloud\"\nmc cp data/raw/data.csv s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.csv\n\nen modifiant la variable BUCKET_PERSONNEL, l’emplacement de votre bucket personnel\n\nPour se simplifier la vie, dans les prochaines applications, on va utiliser des URL de téléchargement des fichiers (comme si ceux-ci étaient sur n’importe quel espace de stockage) plutôt que d’utiliser une librairie S3 compatible comme boto3 ou s3fs.\nNéanmoins, il est utile de les utiliser une fois pour comprendre la logique. Pour aller plus loin sur ces librairies, vous pouvez consulter cette page du cours de 2A de Python pour la data science.\nPour commencer, on va lister les fichiers se trouvant dans un bucket. En ligne de commande, sur notre poste local, on ferait ls (cf. Linux 101). Cela ne va pas beaucoup différer avec les librairies cloud native:\n\nAvec s3fsAvec mc\n\n\nDans un notebook, copier-coller ce code, le modifier et exécuter:\nimport s3fs\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n2CHEMIN = \"ensae-reproductibilite/data/raw\"\nfs.ls(f\"s3://{MY_BUCKET}/{CHEMIN}\")\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\n\n\nDans un terminal, copier-coller ligne à ligne ce code, le modifier et exécuter:\nimport s3fs\n\n1MY_BUCKET=\"mon_nom_utilisateur_sspcloud\"\n2CHEMIN = \"ensae-reproductibilite/data/raw\"\nmc ls s3/${MY_BUCKET}/${CHEMIN}\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\n\n\n\n\n\nOn va maintenant lire directement une donnée stockée sur S3. Pour illustrer le fait que cela change peu notre code d’être sur un système cloud avec les librairies adaptées, on va lire directement un fichier CSV stocké sur le SSPCloud, sans passer par un fichier en local8.\n\n\n\n\n\n\nApplication 8b: importer une donnée depuis un système de stockage S3\n\n\n\nPour illustrer la cohérence avec un système de fichier local, voici trois solutions pour lire le fichier que vous venez de mettre sur S3. Attention, il faut avoir des jetons de connexion à S3 à jour. Si vous avez cette erreur\n\nA client error (InvalidAccessKeyId) occurred when calling the ListBuckets operation: The AWS Access Key Id you provided does not exist in our records.\n\nc’est que vos identifiants de connexion ne sont plus à jour (pour des raisons de sécurité, ils sont régulièrement renouvelés). Dans ce cas, recréez un service VSCode avec le bouton proposé plus haut.\nDans un notebook, copier-coller et mettre à jour ces deux variables qui seront utilisées dans différents exemples:\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n2CHEMIN_FICHIER = \"ensae-reproductibilite/data/raw/data.csv\"\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\n\nAvec Pandas et s3fsAvec Pyarrow et s3fsAvec DuckDB\n\n\nimport s3fs\nimport pandas as pd\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\nwith fs.open(f\"s3://{MY_BUCKET}/{CHEMIN_FICHIER}\") as f:\n    df = pd.read_csv(f)\n\ndf\n\n\nimport s3fs\nfrom pyarrow import csv\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\nwith fs.open(f\"s3://{MY_BUCKET}/{CHEMIN_FICHIER}\") as f:\n    df = csv.read_csv(f)\n\ndf\n\n\nimport os\nimport duckdb\n\ncon = duckdb.connect(database=\":memory:\")\n\ncon.execute(\n    f\"\"\"\nCREATE SECRET secret (\n    TYPE S3,\n    KEY_ID '{os.environ[\"AWS_ACCESS_KEY_ID\"]}',\n    SECRET '{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}',\n    ENDPOINT 'minio.lab.sspcloud.fr',\n    SESSION_TOKEN '{os.environ[\"AWS_SESSION_TOKEN\"]}',\n    REGION 'us-east-1',\n    URL_STYLE 'path',\n    SCOPE 's3://{MY_BUCKET}/'\n);\n\"\"\"\n)\n\nquery_definition = f\"SELECT * FROM read_csv('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')\"\ndf = con.sql(query_definition)\n\ndf\n\n\n\nPour illustrer le fonctionnement encore plus simple de S3 avec les fichiers Parquet, on propose de copier un Parquet mis à disposition dans un bucket collectiv vers votre bucket personnel:\n1BUCKET_PERSONNEL=\"nom_utilisateur_sspcloud\"\n\n2curl -o rp.parquet \"https://minio.lab.sspcloud.fr/projet-formation/bonnes-pratiques/data/REGION=11/part-0.parquet\"\n\nmc cp rp.parquet s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/example/rp.parquet\n\nrm rp.parquet\n\n1\n\nRemplacer par le nom de votre bucket.\n\n2\n\nTélécharger le fichier Parquet mis à dispositoin\n\n\nPour lire ceux-ci, tester les exemples de code suivants:\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\nCHEMIN_FICHIER = \"ensae-reproductibilite/data/example/rp.parquet\"\n\n1\n\nRemplacer ici par la valeur appropriée\n\n\n\nAvec Pandas et s3fsAvec Pyarrow et s3fsAvec DuckDB\n\n\nimport s3fs\nimport pandas as pd\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\ndf = pd.read_parquet(f\"s3://{MY_BUCKET}/{CHEMIN_FICHIER}\", filesystem=fs)\n\ndf\n\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ns3 = pa.fs.S3FileSystem(endpoint_override =\"https://minio.lab.sspcloud.fr\")\n\ndf = pq.read_table(f\"{MY_BUCKET}/{CHEMIN_FICHIER}\", filesystem=s3)\n\ndf\n\n\nimport os\nimport duckdb\n\ncon = duckdb.connect(database=\":memory:\")\n\ncon.execute(\n    f\"\"\"\nCREATE SECRET secret (\n    TYPE S3,\n    KEY_ID '{os.environ[\"AWS_ACCESS_KEY_ID\"]}',\n    SECRET '{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}',\n    ENDPOINT 'minio.lab.sspcloud.fr',\n    SESSION_TOKEN '{os.environ[\"AWS_SESSION_TOKEN\"]}',\n    REGION 'us-east-1',\n    URL_STYLE 'path',\n    SCOPE 's3://{MY_BUCKET}/'\n);\n\"\"\"\n)\n\nquery_definition = f\"SELECT * FROM read_parquet('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')\"\ndf = con.sql(query_definition)\n\ndf\n\n\n\nPour aller plus loin sur le format Parquet, notamment découvrir comment importer des données partitionnées, vous pouvez traduire en Python les exemples issus de la formation aux bonnes pratiques avec R de l’Insee.\n\n\n\n\n\n\n\n\nApplication 8c: privilégier le format Parquet dans notre chaîne\n\n\n\nDans main.py, remplacer le format csv initialement prévu par un format parquet:\ndata_train_path = os.environ.get(\"train_path\", \"data/derived/train.parquet\")\ndata_test_path = os.environ.get(\"test_path\", \"data/derived/test.parquet\")\nEt modifier l’écriture des données pour utiliser to_parquet plutôt que to_csv pour écrire les fichiers intermédiaires:\n\n\nmain.py\n\npd.concat([X_train, y_train], axis = 1).to_parquet(data_train_path)\npd.concat([X_test, y_test], axis = 1).to_parquet(data_test_path)\n\n\n\n\n\n\n\n\n\nApplication 8d: partage de données sur le système de stockage S3\n\n\n\nPar défaut, le contenu de votre bucket est privé, seul vous y avez accès. Pour pouvoir lire votre donnée, vos applications externes devront utiliser des jetons vous identifiant. Ici, comme nous utilisons une donnée publique, vous pouvez rendre accessible celle-ci à tous en lecture. Dans le jargon S3, cela signifie donner un accès anonyme à votre donnée.\nLe modèle de commande à utiliser dans le terminal est le suivant:\n\n\nterminal\n\n1BUCKET_PERSONNEL=\"nom_utilisateur_sspcloud\"\n\nmc anonymous set download s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/\n\n\n1\n\nRemplacer par le nom de votre bucket.\n\n\nLes URL de téléchargement seront de la forme https://minio.lab.sspcloud.fr/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/data/raw/data.csv\n\nRemplacer la définition de data_path pour utiliser, par défaut, directement l’URL dans l’import. Modifier, si cela est pertinent, aussi votre fichier .env.\n\n1URL_RAW = \"\"\ndata_path = os.environ.get(\"data_path\", URL_RAW)\n\n1\n\nModifier avec URL_RAW un lien de la forme \"https://minio.lab.sspcloud.fr/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.csv\" (ne laissez pas ${BUCKET_PERSONNEL}, remplacez par la vraie valeur!).\n\n\n\nAjouter le dossier data/ au .gitignore ainsi que les fichiers *.parquet\nSupprimer le dossier data de votre projet et faites git rm --cached -r data\nVérifier le bon fonctionnement de votre application.\n\n\n\nMaintenant qu’on a arrangé la structure de notre projet, c’est l’occasion de supprimer le code qui n’est plus nécessaire au bon fonctionnement de notre projet (cela réduit la charge de maintenance9).\nPour vous aider, vous pouvez utiliser vulture de manière itérative pour vous assister dans le nettoyage de votre code.\n\n\nterminal\n\npip install vulture\nvulture .\n\n\n\nExemple de sortie\n\n\n\nterminal\n\nvulture .\n\nsrc/data/import_data.py:3: unused function 'split_and_count' (60% confidence)\nsrc/pipeline/build_pipeline.py:12: unused function 'split_train_test' (60% confidence)\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli8      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli82\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli8\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-1-proposer-des-tests-unitaires-optionnel",
    "href": "chapters/application.html#étape-1-proposer-des-tests-unitaires-optionnel",
    "title": "Application",
    "section": "Étape 1 : proposer des tests unitaires (optionnel)",
    "text": "Étape 1 : proposer des tests unitaires (optionnel)\nNotre code comporte un certain nombre de fonctions génériques. On peut vouloir tester leur usage sur des données standardisées, différentes de celles du Titanic.\nMême si la notion de tests unitaires prend plus de sens dans un package, nous pouvons proposer dans le projet des exemples d’utilisation de la fonction, ceci peut être pédagogique.\nNous allons utiliser unittest pour effectuer des tests unitaires. Cette approche nécessite quelques notions de programmation orientée objet ou une bonne discussion avec ChatGPT.\n\n\n\n\n\n\nApplication 9: test unitaire (optionnel)\n\n\n\nDans le dossier tests/, créer avec l’aide de ChatGPT ou de Copilot un test pour la fonction split_and_count.\n\nEffectuer le test unitaire en ligne de commande avec unittest (python -m unittest tests/test_split.py). Corriger le test unitaire en cas d’erreur.\nSi le temps le permet, proposer des variantes ou d’autres tests.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli9      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli92\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli9\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\n\n\n\n\nNote\n\n\n\nLorsqu’on effectue des tests unitaires, on cherche généralement à tester le plus de lignes possibles de son code. On parle de taux de couverture (coverage rate) pour désigner la statistique mesurant cela.\nCela peut s’effectuer de la manière suivante avec le package coverage:\n\n\nterminal\n\ncoverage run -m unittest tests/test_create_variable_title.py\ncoverage report -m\n\nName                                  Stmts   Miss  Cover   Missing\n-------------------------------------------------------------------\nsrc/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113\ntests/test_create_variable_title.py      21      1    95%   54\n-------------------------------------------------------------------\nTOTAL                                    55     22    60%\nLe taux de couverture est souvent mis en avant par les gros projets comme indicateur de leur qualité. Il existe d’ailleurs des badges Github dédiés."
  },
  {
    "objectID": "chapters/application.html#étape-2-transformer-son-projet-en-package-optionnel",
    "href": "chapters/application.html#étape-2-transformer-son-projet-en-package-optionnel",
    "title": "Application",
    "section": "Étape 2 : transformer son projet en package (optionnel)",
    "text": "Étape 2 : transformer son projet en package (optionnel)\nNotre projet est modulaire, ce qui le rend assez simple à transformer en package, en s’inspirant de la structure du cookiecutter adapté, issu de cet ouvrage.\nOn va créer un package nommé titanicml qui encapsule tout notre code et qui sera appelé par notre script main.py. La structure attendue est la suivante:\n\n\nStructure visée\n\nensae-reproductibilite-application\n├── docs                                    ┐\n│   ├── main.py                             │\n│   └── notebooks                           │ Package documentation and examples\n│       └── titanic.ipynb                   │\n├── configuration                           ┐ Configuration (pas à partager avec Git)\n│   └── config.yaml                         ┘\n├── README.md\n├── pyproject.toml                          ┐\n├── requirements.txt                        │\n├── titanicml                               │\n│   ├── __init__.py                         │ Package source code, metadata\n│   ├── data                                │ and build instructions\n│   │   ├── import_data.py                  │\n│   │   └── test_create_variable_title.py   │\n│   ├── features                            │\n│   │   └── build_features.py               │\n│   └── models                              │\n│       └── train_evaluate.py               ┘\n└── tests                                   ┐\n    └── test_create_variable_title.py       ┘ Package tests\n\n\n\nRappel: structure actuelle\n\nensae-reproductibilite-application\n├── notebooks\n│   └── titanic.ipynb\n├── configuration\n│   └── config.yaml\n├── main.py\n├── README.md\n├── requirements.txt\n└── src\n    ├── data\n    │   ├── import_data.py\n    │   └── test_create_variable_title.py\n    ├── features\n    │   └── build_features.py\n    └── models\n        └── train_evaluate.py\n\nIl existe plusieurs frameworks pour construire un package. Nous allons privilégier Poetry à Setuptools.\n\n\n\n\n\n\nNote\n\n\n\nPour créer la structure minimale d’un package, le plus simple est d’utiliser le cookiecutter adapté, issu de cet ouvrage.\nComme on a déjà une structure très modulaire, on va plutôt recréer cette structure dans notre projet déjà existant. En fait, il ne manque qu’un fichier essentiel, le principal distinguant un projet classique d’un package : pyproject.toml.\n\n\nterminal\n\ncookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git\n\n\n\nDérouler pour voir les choix possibles\n\nauthor_name [Monty Python]: Daffy Duck\npackage_name [mypkg]: titanicml\npackage_short_description []: Impressive Titanic survival analysis\npackage_version [0.1.0]:\npython_version [3.9]:\nSelect open_source_license:\n1 - MIT\n2 - Apache License 2.0\n3 - GNU General Public License v3.0\n4 - Creative Commons Attribution 4.0\n5 - BSD 3-Clause\n6 - Proprietary\n7 - None\nChoose from 1, 2, 3, 4, 5, 6 [1]:\nSelect include_github_actions:\n1 - no\n2 - ci\n3 - ci+cd\nChoose from 1, 2, 3 [1]:\n\n\n\n\n\n\n\n\n\nApplication 10: packagisation (optionnel)\n\n\n\n\nRenommer le dossier titanicml pour respecter la nouvelle arborescence ;\nCréer un fichier pyproject.toml sur cette base ;\n\n#| code-summary: \"pyproject.toml\"\n#| filename: \"pyproject.toml\"\n[tool.poetry]\nname = \"titanicml\"\nversion = \"0.0.1\"\ndescription = \"Awesome Machine Learning project\"\nauthors = [\"Daffy Duck &lt;daffy.duck@fauxmail.fr&gt;\", \"Mickey Mouse\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.pytest.ini_options]\nlog_cli = true\nlog_cli_level = \"WARNING\"\nlog_cli_format = \"%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)\"\nlog_cli_date_format = \"%Y-%m-%d %H:%M:%S\"\n\nCréer le dossier docs et mettre les fichiers indiqués dedans\nDans titanicml/, créer un fichier __init__.py10\n\n#| code-summary: \"__init__.py\"\n#| filename: \"__init__.py\"\nfrom .data.import_data import (\n    split_and_count\n)\nfrom .pipeline.build_pipeline import (\n    split_train_test,\n    create_pipeline\n)\nfrom .models.train_evaluate import (\n    evaluate_model\n)\n__all__ = [\n    \"split_and_count\",\n    \"split_train_test\",\n    \"create_pipeline\",\n    \"evaluate_model\"\n]\n\nInstaller le package en local avec pip install -e .\nModifier le contenu de docs/main.py pour importer les fonctions de notre package titanicml et tester en ligne de commande notre fichier main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli10      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli102\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli10\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#anaconda",
    "href": "chapters/application.html#anaconda",
    "title": "Application",
    "section": "Étape 1 : un environnement pour rendre le projet portable",
    "text": "Étape 1 : un environnement pour rendre le projet portable\nPour qu’un projet soit portable, il doit remplir deux conditions:\n\nNe pas nécessiter de dépendance qui ne soient pas renseignées quelque part ;\nNe pas proposer des dépendances inutiles, qui ne sont pas utilisées dans le cadre du projet.\n\nLe prochain exercice vise à mettre ceci en oeuvre. Comme expliqué dans le chapitre portabilité, le choix du gestionnaire d’environnement est laissé libre. Il est recommandé de privilégier venv si vous découvrez la problématique de la portabilité.\n\nEnvironnement virtuel venvEnvironnement condaEnvironnement virtuel via uv\n\n\nL’approche la plus légère est l’environnement virtuel. Nous avons en fait implicitement déjà commencé à aller vers cette direction en créant un fichier requirements.txt.\n\n\n\n\n\n\nApplication 11a: environnement virtuel venv\n\n\n\n\nExécuter pip freeze en ligne de commande et observer la (très) longue liste de package\nCréer l’environnement virtuel titanic en s’inspirant de la documentation officielle11 ou du chapitre dédié\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin installé\nLe SSPCloud, par défaut, fonctionne sur un environnement conda. Le désactiver en faisant conda deactivate.\nActiver l’environnement et vérifier l’installation de Python maintenant utilisée par votre machine \nVérifier directement depuis la ligne de commande que Python exécute bien une commande12 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la même chose mais avec import pandas as pd\nInstaller les packages à partir du requirements.txt. Tester à nouveau import pandas as pd pour comprendre la différence.\nExécuter pip freeze et comprendre la différence avec la situation précédente.\nVérifier que le script main.py fonctionne bien. Sinon ajouter les packages manquants dans le requirements.txt et reprendre de manière itérative à partir de la question 7.\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier à Git.\n\n\n\nAide pour la question 4\n\nAprès l’activation, vous pouvez vérifier quel python est utilisé de cette manière\n\n\nterminal\n\nwhich python\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11a2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli11a\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\nLes environnements conda sont plus lourds à mettre en oeuvre que les environnements virtuels mais peuvent permettre un contrôle plus formel des dépendances.\n\n\n\n\n\n\nApplication 11b: environnement conda\n\n\n\n\nExécuter conda env export en ligne de commande et observer la (très) longue liste de package\nCréer un environnement titanic avec conda create\nActiver l’environnement et vérifier l’installation de Python maintenant utilisée par votre machine \nVérifier directement depuis la ligne de commande que Python exécute bien une commande13 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la même chose mais avec import pandas as pd\nInstaller les packages qu’on avait listé dans le requirements.txt précédemment. Ne pas faire un pip install -r requirements.txt afin de privilégier conda install\nExécuter à nouveau conda env export et comprendre la différence avec la situation précédente14.\nVérifier que le script main.py fonctionne bien. Sinon installer les packages manquants et reprndre de manière itérative à partir de la question 7.\nQuand main.py fonctionne, faire conda env export &gt; environment.yml pour figer l’environnement de travail.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11b2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli11b\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\nuv est le new kid in the game pour gérer les environnements virtuels avec Python.\n\n\n\n\n\n\nApplication 11c: environnement virtuel venv (via uv)\n\n\n\n\nAprès avoir installé uv, exécuter uv init . et supprimer le fichier hello.py généré. Ouvrir le pyproject.toml et observer sa structure.\nExécuter uv pip freeze en ligne de commande et observer la (très) longue liste de package\nCréer un environnement virtuel titanic par le biais d’uv (documentation) sous le nom titanic\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin installé\nActiver l’environnement et vérifier l’installation de Python maintenant utilisée par votre machine \nVérifier directement depuis la ligne de commande que Python exécute bien une commande15 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la même chose mais avec import pandas as pd. Maintenant, essayer uv run main.py en ligne de commande: comprenez-vous ce qu’il se passe ?\nInstaller de manière itérative les packages à partir d’uv add (documentation) et en testant avec uv run main.py: avez-vous remarqué la vitesse à laquelle cela a été quand vous avez fait uv add pandas ?\nObserver votre pyproject.toml. Regarder le lockfile uv.lock. Générer automatiquement le requirements.txt en faisant pip compile et regarder celui-ci.\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier à Git.\n\n\n\nAide pour la question 5\n\nAprès l’activation, vous pouvez vérifier quel python est utilisé de cette manière\n\n\nterminal\n\nwhich python\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11c      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11c2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli11c\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#shell",
    "href": "chapters/application.html#shell",
    "title": "Application",
    "section": "Étape 2: construire l’environnement de notre application via un script shell",
    "text": "Étape 2: construire l’environnement de notre application via un script shell\nLes environnements virtuels permettent de mieux spécifier les dépendances de notre projet, mais ne permettent pas de garantir une portabilité optimale. Pour cela, il faut recourir à la technologie des conteneurs. L’idée est de construire une machine, en partant d’une base quasi-vierge, qui permette de construire étape par étape l’environnement nécessaire au bon fonctionnement de notre projet. C’est le principe des conteneurs Docker .\nLeur méthode de construction étant un peu difficile à prendre en main au début, nous allons passer par une étape intermédiaire afin de bien comprendre le processus de production.\n\nNous allons d’abord créer un script shell, c’est à dire une suite de commandes Linux permettant de construire l’environnement à partir d’une machine vierge ;\nNous transformerons celui-ci en Dockerfile dans un deuxième temps. C’est l’objet de l’étape suivante.\n\n\nEnvironnement virtuel venvEnvironnement conda\n\n\n\n\n\n\n\n\nApplication 12a : créer un fichier d’installation de A à Z\n\n\n\n\nCréer un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le dépôt\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11a avec git checkout appli11a\nVia l’explorateur de fichiers, créer le fichier install.sh à la racine du projet avec le contenu suivant:\n\n\n\nScript à créer sous le nom install.sh\n\n\n\ninstall.sh\n\n#!/bin/bash\n\n# Install Python\napt-get -y update\napt-get install -y python3-pip python3-venv\n\n# Create empty virtual environment\npython3 -m venv titanic\nsource titanic/bin/activate\n\n# Install project dependencies\npip install -r requirements.txt\n\n\n\nChanger les permissions sur le script pour le rendre exécutable\n\n\n\nterminal\n\nchmod +x install.sh\n\n\nExécuter le script depuis la ligne de commande avec des droits de super-utilisateur (nécessaires pour installer des packages via apt)\n\n\n\nterminal\n\nsudo ./install.sh\n\n\nVérifier que le script main.py fonctionne correctement dans l’environnement virtuel créé\n\n\n\nterminal\n\nsource titanic/bin/activate\npython3 main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli12a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12a2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli12a\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\n\n\n\n\n\n\nApplication 12b : créer un fichier d’installation de A à Z\n\n\n\n\nCréer un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le dépôt\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11b avec git checkout appli11b\nVia l’explorateur de fichiers, créer le fichier install.sh à la racine du projet avec le contenu suivant:\n\n\n\nScript à créer sous le nom install.sh\n\n\n\ninstall.sh\n\napt-get -y update && apt-get -y install wget\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \\\n    rm -f Miniconda3-latest-Linux-x86_64.sh\n\nPATH=\"/miniconda/bin:${PATH}\"\n\n# Create environment\nconda create -n titanic pandas PyYAML scikit-learn -c conda-forge\nconda activate titanic\n\nPATH=\"/miniconda/envs/titanic/bin:${PATH}\"\n\npython main.py\n\n\n\nChanger les permissions sur le script pour le rendre exécutable\n\n\n\nterminal\n\nchmod +x install.sh\n\n\nExécuter le script depuis la ligne de commande avec des droits de super-utilisateur (nécessaires pour installer des packages via apt)\n\n\n\nterminal\n\nsudo ./install.sh\n\n\nVérifier que le script main.py fonctionne correctement dans l’environnement virtuel créé\n\n\n\nterminal\n\nconda activate titanic\npython3 main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli12b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12b2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli12b\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#docker",
    "href": "chapters/application.html#docker",
    "title": "Application",
    "section": "Étape 3: conteneuriser l’application avec Docker",
    "text": "Étape 3: conteneuriser l’application avec Docker\n\n\n\n\n\n\nNote\n\n\n\nCette application nécessite l’accès à une version interactive de Docker. Il n’y a pas beaucoup d’instances en ligne disponibles.\nNous proposons deux solutions:\n\nInstaller Docker sur sa machine ;\nSe rendre sur l’environnement bac à sable Play with Docker\n\nSinon, elle peut être réalisée en essai-erreur par le biais des services d’intégration continue de Github  ou Gitlab . Néanmoins, nous présenterons l’utilisation de ces services plus tard, dans la prochaine partie.\n\n\nMaintenant qu’on sait que ce script préparatoire fonctionne, on va le transformer en Dockerfile pour anticiper la mise en production. Comme la syntaxe Docker est légèrement différente de la syntaxe Linux classique (voir le chapitre portabilité), il va être nécessaire de changer quelques instructions mais ceci sera très léger.\nOn va tester le Dockerfile dans un environnement bac à sable pour ensuite pouvoir plus facilement automatiser la construction de l’image Docker.\n\n\n\n\n\n\nApplication 13: création de l’image Docker\n\n\n\nSe placer dans un environnement avec Docker, par exemple Play with Docker\n\nCréation du Dockerfile\n\nDans le terminal Linux, cloner votre dépôt Github\nRepartir de la dernière version à disposition. Par exemple, si vous avez privilégié l’environnement virtuel venv, ce sera:\n\n\n\nterminal\n\n1git stash\ngit checkout appli12a\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\nCréer via la ligne de commande un fichier texte vierge nommé Dockerfile (la majuscule au début du mot est importante)\n\n\n\nCommande pour créer un Dockerfile vierge depuis la ligne de commande\n\n\n\nterminal\n\ntouch Dockerfile\n\n\n\nOuvrir ce fichier via un éditeur de texte et copier le contenu suivant dedans:\n\n\n\nPremier Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCMD [\"python3\", \"main.py\"]\n\n\n\n\nConstruire (build) l’image\n\nUtiliser docker build pour créer une image avec le tag my-python-app\n\n\n\nterminal\n\ndocker build . -t my-python-app\n\n\nVérifier les images dont vous disposez. Vous devriez avoir un résultat proche de celui-ci :\n\n\n\nterminal\n\ndocker images\n\nREPOSITORY      TAG       IMAGE ID       CREATED              SIZE\nmy-python-app   latest    188957e16594   About a minute ago   879MB\n\n\nTester l’image: découverte du cache\nL’étape de build a fonctionné: une image a été construite.\nMais fait-elle effectivement ce que l’on attend d’elle ?\nPour le savoir, il faut passer à l’étape suivante, l’étape de run.\n\n\nterminal\n\ndocker run -it my-python-app\n\npython3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory\nLe message d’erreur est clair : Docker ne sait pas où trouver le fichier main.py. D’ailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont nécessaires pour faire tourner le code, par exemple le dossier src.\n\nAvant l’étape CMD, copier les fichiers nécessaires sur l’image afin que l’application dispose de tous les éléments nécessaires pour être en mesure de fonctionner.\n\n\n\nNouveau Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY main.py .\nCOPY src ./src\nCMD [\"python3\", \"main.py\"]\n\n\n\nRefaire tourner l’étape de build\nRefaire tourner l’étape de run. A ce stade, la matrice de confusion doit fonctionner 🎉. Vous avez créé votre première application reproductible !\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIci, le cache permet d’économiser beaucoup de temps. Par besoin de refaire tourner toutes les étapes, Docker agit de manière intelligente en faisant tourner uniquement les étapes qui ont changé.\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli13      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli132\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli13\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-1-mise-en-place-de-tests-automatisés",
    "href": "chapters/application.html#étape-1-mise-en-place-de-tests-automatisés",
    "title": "Application",
    "section": "Étape 1: mise en place de tests automatisés",
    "text": "Étape 1: mise en place de tests automatisés\nAvant d’essayer de mettre en oeuvre la création de notre image Docker de manière automatisée, nous allons présenter la logique de l’intégration continue en testant de manière automatisée notre script main.py.\nPour cela, nous allons partir de la structure proposée dans l’action officielle. La documentation associée est ici. Des éléments succincts de présentation de la logique déclarative des actions Github sont disponibles dans le chapitre sur la mise en production. Néanmoins, la meilleure école pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et d’observer les actions Github mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !\n\n\n\n\n\n\nApplication 14: premier script d’intégration continue\n\n\n\nA partir de l’exemple présent dans la documentation officielle de Github , on a déjà une base de départ qui peut être modifiée. Les questions suivantes permettront d’automatiser les tests et le diagnostic qualité de notre code16\n\nCréer un fichier .github/workflows/test.yaml avec le contenu de l’exemple de la documentation\nAvec l’aide de la documentation, introduire une étape d’installation des dépendances. Utiliser le fichier requirements.txt pour installer les dépendances.\nUtiliser pylint pour vérifier la qualité du code. Ajouter l’argument --fail-under=6 pour renvoyer une erreur en cas de note trop basse17\nUtiliser une étape appelant notre application en ligne de commande (python main.py) pour tester que la matrice de confusion s’affiche bien.\nCréer un secret stockant une valeur du JETON_API. Ne le faites pas commencer par un “$” comme ça vous pourrez regarder la log ultérieurement\nAller voir votre test automatisé dans l’onglet Actions de votre dépôt sur Github\n(optionnel): Créer un artefact à partir du fichier de log que vous créez dans main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli14      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli142\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli14\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nMaintenant, nous pouvons observer que l’onglet Actions s’est enrichi. Chaque commit va entraîner une série d’actions automatisées.\nSi l’une des étapes échoue, ou si la note de notre projet est mauvaise, nous aurons une croix rouge (et nous recevrons un mail). On pourra ainsi détecter, en développant son projet, les moments où on dégrade la qualité du script afin de la rétablir immédiatemment."
  },
  {
    "objectID": "chapters/application.html#étape-2-automatisation-de-la-livraison-de-limage-docker",
    "href": "chapters/application.html#étape-2-automatisation-de-la-livraison-de-limage-docker",
    "title": "Application",
    "section": "Étape 2: Automatisation de la livraison de l’image Docker",
    "text": "Étape 2: Automatisation de la livraison de l’image Docker\nMaintenant, nous allons automatiser la mise à disposition de notre image sur DockerHub (le lieu de partage des images Docker). Cela facilitera sa réutilisation mais aussi des valorisations ultérieures.\nLà encore, nous allons utiliser une série d’actions pré-configurées.\nPour que Github puisse s’authentifier auprès de DockerHub, il va falloir d’abord interfacer les deux plateformes. Pour cela, nous allons utiliser un jeton (token) DockerHub que nous allons mettre dans un espace sécurisé associé à votre dépôt Github.\n\n\n\n\n\n\nApplication 15a: configuration\n\n\n\n\nSe rendre sur https://hub.docker.com/ et créer un compte. Il est recommandé d’associer ce compte à votre compte Github.\nCréer un dépôt public application\nAller dans les paramètres de votre compte et cliquer, à gauche, sur Security\nCréer un jeton personnel d’accès, ne fermez pas l’onglet en question, vous ne pouvez voir sa valeur qu’une fois.\nDans le dépôt Github de votre projet, cliquer sur l’onglet Settings et cliquer, à gauche, sur Secrets and variables puis dans le menu déroulant en dessous sur Actions. Sur la page qui s’affiche, aller dans la section Repository secrets\nCréer un jeton DOCKERHUB_TOKEN à partir du jeton que vous aviez créé sur Dockerhub. Valider\nCréer un deuxième secret nommé DOCKERHUB_USERNAME ayant comme valeur le nom d’utilisateur que vous avez créé sur Dockerhub\n\n\n\nEtape optionnelle supplémentaire si on met en production un site web\n\n\nDans le dépôt Github de votre projet, cliquer sur l’onglet Settings et cliquer, à gauche, sur Actions. Donner les droits d’écriture à vos actions sur le dépôt du projet (ce sera nécessaire pour Github Pages)\n\n\n\n\n\nA ce stade, nous avons donné les moyens à Github de s’authentifier avec notre identité sur Dockerhub. Il nous reste à mettre en oeuvre l’action en s’inspirant de la documentation officielle. On ne va modifier que trois éléments dans ce fichier. Effectuer les actions suivantes:\n\n\n\n\n\n\nApplication 15b: automatisation de l’image Docker\n\n\n\n\nEn s’inspirant de ce template, créer le fichier .github/workflows/prod.yml qui va build et push l’image sur le DockerHub. Il va être nécessaire de changer légèrement ce modèle :\n\nRetirer la condition restrictive sur les commits pour lesquels sont lancés cette automatisation. Pour cela, remplacer le contenu de on de sorte à avoir\n\non:\n  push:\n    branches:\n      - main\n      - dev\n\nChanger le tag à la fin pour mettre username/application:latest où username est le nom d’utilisateur sur DockerHub;\nOptionnel: changer le nom de l’action\n\nFaire un commit et un push de ces fichiers\n\nComme on est fier de notre travail, on va afficher ça avec un badge sur le README (partie optionnelle).\n\nSe rendre dans l’onglet Actions et cliquer sur une des actions listées.\nEn haut à droite, cliquer sur ...\nSélectionner Create status badge\nRécupérer le code Markdown proposé\nCopier dans votre README.md le code markdown proposé\n\n\n\nCréer le badge\n\n\n\n\n\nMaintenant, il nous reste à tester notre application dans l’espace bac à sable ou en local, si Docker est installé.\n\n\n\n\n\n\nApplication 15b (partie optionnelle): Tester l’application\n\n\n\n\nSe rendre sur l’environnement bac à sable Play with Docker ou dans votre environnement Docker de prédilection.\nRécupérer et lancer l’image :\n\n\n\nterminal\n\ndocker run -it username/application:latest\n\n🎉 La matrice de confusion doit s’afficher ! Vous avez grandement facilité la réutilisation de votre image.\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli15      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli152\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli15\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-1-développer-une-api-en-local",
    "href": "chapters/application.html#étape-1-développer-une-api-en-local",
    "title": "Application",
    "section": "Étape 1: développer une API en local",
    "text": "Étape 1: développer une API en local\nLe premier livrable devenu classique dans un projet impliquant du machine learning est la mise à disposition d’un modèle par le biais d’une API (voir chapitre sur la mise en production). Le framework FastAPI va permettre de rapidement transformer notre application Python en une API fonctionnelle.\n\n\n\n\n\n\nApplication 16: Mise à disposition sous forme d’API locale\n\n\n\n\nInstaller fastAPI et uvicorn puis les ajouter au requirements.txt\nRenommer le fichier main.py en train.py.\nDans ce script, ajouter une sauvegarde du modèle après l’avoir entraîné, sous le format joblib.\nFaire tourner\n\n\n\nterminal\n\npython train.py\n\npour enregistrer en local votre modèle de production.\n\nModifier les appels à main.py dans votre Dockerfile et vos actions Github sous peine d’essuyer des échecs lors de vos actions Github après le prochain push.\nAjouter model.joblib au .gitignore car Git n’est pas fait pour ce type de fichiers.\n\nNous allons maintenant passer au développement de l’API. Comme découvrir FastAPI n’est pas l’objet de cet enseignement, nous donnons directement le modèle pour créer l’API. Si vous désirez tester de vous-mêmes, vous pouvez créer votre fichier sans vous référer à l’exemple.\n\nCréer le fichier app/api.py permettant d’initialiser l’API:\n\n\n\nFichier app/api.py\n\n\n\napp/api.py\n\n\"\"\"A simple API to expose our trained RandomForest model for Tutanic survival.\"\"\"\nfrom fastapi import FastAPI\nfrom joblib import load\n\nimport pandas as pd\n\nmodel = load('model.joblib')\n\napp = FastAPI(\n    title=\"Prédiction de survie sur le Titanic\",\n    description=\n    \"Application de prédiction de survie sur le Titanic 🚢 &lt;br&gt;Une version par API pour faciliter la réutilisation du modèle 🚀\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\n\n@app.get(\"/\", tags=[\"Welcome\"])\ndef show_welcome_page():\n    \"\"\"\n    Show welcome page with model name and version.\n    \"\"\"\n\n    return {\n        \"Message\": \"API de prédiction de survie sur le Titanic\",\n        \"Model_name\": 'Titanic ML',\n        \"Model_version\": \"0.1\",\n    }\n\n\n@app.get(\"/predict\", tags=[\"Predict\"])\nasync def predict(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\"\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    prediction = \"Survived 🎉\" if int(model.predict(df)) == 1 else \"Dead ⚰️\"\n\n    return prediction\n\n\n\nDéployer l’API en local avec la commande suivante.\n\n\n\nterminal\n\nuvicorn app.api:app\n\n\nObserver l’output dans la console. Notre API est désormais déployée en local, plus précisément sur le localhost, un serveur web local déployé à l’adresse http://127.0.0.1. L’API est déployée sur le port par défaut utilisé par uvicorn, soit le port 8000.\nSans fermer le terminal précédent, ouvrir un nouveau terminal. Tester le bon déploiement de l’API en requêtant son endpoint. Pour cela, on envoie une simple requête GET sur le endpoint via l’utilitaire curl.\n\n\n\nterminal\n\ncurl \"http://127.0.0.1:8000\"\n\n\nSi tout s’est bien passé, on devrait avoir récupéré une réponse (au format JSON) affichant le message d’accueil de notre API. Dans ce cas, on va pouvoir requêter notre modèle via l’API.\nEn vous inspirant du code qui définit le endpoint /predict dans le code de l’API (app/api.py), effectuer sur le même modèle que la requête précédente une requête qui calcule la survie d’une femme de 32 ans qui aurait payé son billet 16 dollars et aurait embarqué au port S.\n\n\n\nSolution\n\n\n\nterminal\n\ncurl \"http://127.0.0.1:8000/predict?sex=female&age=32&fare=16&embarked=S\"\n\n\n\nToujours sans fermer le terminal qui déploie l’API, ouvrir une session Python et tester une requête avec des paramètres différents, avec la librairie requests :\n\n\n\nSolution\n\nimport requests\n\nURL = \"http://127.0.0.1:8000/predict?sex=male&age=25&fare=80&embarked=S\"\nrequests.get(URL).json()\n\n\nUne fois que l’API a été testée, vous pouvez fermer l’application en effectuant CTRL+C depuis le terminal où elle est lancée.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli16      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli162\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli16\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-2-déployer-lapi-de-manière-manuelle",
    "href": "chapters/application.html#étape-2-déployer-lapi-de-manière-manuelle",
    "title": "Application",
    "section": "Étape 2: déployer l’API de manière manuelle",
    "text": "Étape 2: déployer l’API de manière manuelle\nA ce stade, nous avons déployé l’API seulement localement, dans le cadre d’un terminal qui tourne en arrière-plan. C’est une mise en production manuelle, pas franchement pérenne. Ce mode de déploiement est très pratique pour la phase de développement, afin de s’assurer que l’API fonctionne comme attendue. Pour pérenniser la mise en production, on va éliminer l’aspect artisanal de celle-ci.\nIl est temps de passer à l’étape de déploiement, qui permettra à notre API d’être accessible, à tout moment, via une URL sur le web et d’avoir un serveur, en arrière plan, qui effectuera les opérations pour répondre à une requête. Pour se faire, on va utiliser les possibilités offertes par Kubernetes, technologie sur laquelle est basée l’infrastructure SSP Cloud.\n\n\n\n\n\n\nEt si vous n’utilisez pas le SSPCloud ? (une idée saugrenue mais sait-on jamais)\n\n\n\n\n\nLes exemples à venir peuvent très bien être répliqués sur n’importe quel cloud provider qui propose une solution d’ordonnancement type Kubernetes. Il existe également des fournisseurs de services dédiés, généralement associés à une implémentation, par exemple pour Streamlit. Ces services sont pratiques si on n’a pas le choix mais il faut garder à l’esprit qu’ils peuvent constituer un mur de la production car vous ne contrôlez pas l’environnement en question, qui peut se distinguer de votre environnement de développement.\nEt si jamais vous voulez avoir un SSPCloud dans votre entreprise c’est possible: le logiciel Onyxia sur lequel repose cette infrastructure est open source et est, déjà, réimplémenté par de nombreux acteurs. Pour bénéficier d’un accompagnement dans la création d’une telle infrastructure, rdv sur le Slack du projet Onyxia:\n\n\n\n\n\n\n\n\n\n\nApplication 17: Dockeriser l’API (intégration continue)\n\n\n\n\nCréer un script app/run.sh à la racine du projet qui lance le script train.py puis déploie localement l’API. Attention, quand on se place dans le monde des conteneurs et plus généralement des infrastructures cloud, on ne va plus déployer sur le localhost mais sur “l’ensemble des interfaces réseaux”. Lorsqu’on déploie une application web dans un conteneur, on va donc toujours devoir spécifier un host valant 0.0.0.0 (et non plus localhost ou, de manière équivalente, http://127.0.0.1).\n\n\n\nFichier run.sh\n\n\n\napi/run.sh\n\n#/bin/bash\n\npython3 train.py\nuvicorn app.api:app --host \"0.0.0.0\"\n\n\n\nDonner au script api/run.sh des permissions d’exécution : chmod +x api/run.sh\nAjouter COPY app ./app pour avoir les fichiers nécessaires au lancement dans l’API dans l’image\nModifier COPY train.py . pour tenir compte du nouveau nom du fichier\nChanger l’instruction CMD du Dockerfile pour exécuter le script api/run.sh au lancement du conteneur (CMD [\"bash\", \"-c\", \"./app/run.sh\"])\nCommit et push les changements\nUne fois le CI terminé, vérifier que le nouveau tag latest a été pushé sur le DockerHub. Récupérer la nouvelle image dans votre environnement de test de Docker et vérifier que l’API se déploie correctement.\n\n\n\nTester l’image sur le SSP Cloud\n\nLancer dans un terminal la commande suivante pour pull l’application depuis le DockerHub et la déployer en local :\n\n\nterminal\n\nkubectl run -it api-ml --image=votre_compte_docker_hub/application:latest\n\n\n\nSi tout se passe correctement, vous devriez observer dans la console un output similaire au déploiement en local de la partie précédente. Cette fois, l’application est déployée à l’adresse http://0.0.0.0:8000. On ne peut néanmoins pas directement l’exploiter à ce stade : si le conteneur de l’API est déployé, il manque un ensemble de ressources Kubernetes qui permettent de déployer proprement l’API à tout utilisateur. C’est l’objet de l’application suivante !\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli17      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli172\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli17\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nNous avons préparé la mise à disposition de notre API mais à l’heure actuelle elle n’est pas accessible de manière aisée car il est nécessaire de lancer manuellement une image Docker pour pouvoir y accéder. Ce type de travail est la spécialité de Kubernetes que nous allons utiliser pour gérer la mise à disposition de notre API.\n\n\n\n\n\n\nApplication 18: Mettre à disposition l’API (déploiement manuel)\n\n\n\nCette partie nécessite d’avoir à disposition une infrastructure cloud.\n\nCréer un dossier deployment à la racine du projet qui va contenir les fichiers de configuration nécessaires pour déployer sur un cluster Kubernetes\nEn vous inspirant de la documentation, y ajouter un premier fichier deployment.yaml qui va spécifier la configuration du Pod à lancer sur le cluster\n\n\n\nFichier deployment/deployment.yaml\n\n#| filename: \"deployment/deployment.yaml\"\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n        image: votre_compte_docker_hub/application:latest\n        ports:\n        - containerPort: 8000\n\n\nEn vous inspirant de la documentation, y ajouter un second fichier service.yaml qui va créer une ressource Service permettant de donner une identité fixe au Pod précédemment créé au sein du cluster\n\n\n\nFichier deployment/service.yaml\n\n\n\ndeployment/service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: titanic-service\nspec:\n  selector:\n    app: titanic\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8000\n\n\n\nEn vous inspirant de la documentation, y ajouter un troisième fichier ingress.yaml qui va créer une ressource Ingress permettant d’exposer le service via une URL en dehors du cluster\n\n\n\nFichier deployment/ingress.yaml\n\n#| filename: \"deployment/ingress.yaml\"\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: titanic-ingress\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - votre_nom_d_application.lab.sspcloud.fr\n  rules:\n  - host: votre_nom_d_application.lab.sspcloud.fr\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: titanic-service\n            port:\n              number: 80\n\nMettez l’URL auquel vous voulez exposer votre service. Sur le modèle de titanic.lab.sspcloud.fr (mais ne tentez pas celui-là, il est déjà pris 😃)\nMettre cette même URL ici aussi\n\n\n\nAppliquer ces fichiers de configuration sur le cluster : kubectl apply -f deployment/\nVérifier le bon déploiement de l’application (c’est à dire du Pod qui encapsule le conteneur) à l’aide de la commande kubectl get pods\nSi tout a correctement fonctionné, vous devriez pouvoir accéder depuis votre navigateur à l’API à l’URL spécifiée dans le fichier deployment/ingress.yaml. Par exemple https://api-titanic-test.lab.sspcloud.fr/ si vous avez mis celui-ci plus tôt\nExplorer le swagger de votre API à l’adresse https://api-titanic-test.lab.sspcloud.fr/docs. Il s’agit d’une page de documentation standard à la plupart des APIs, bien utiles pour tester des requêtes de manière interactive.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli18      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli182\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli18\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nOn peut remarquer quelques voies d’amélioration de notre approche qui seront ultérieurement traitées:\n\nL’entraînement du modèle est ré-effectué à chaque lancement d’un nouveau conteneur. On relance donc autant de fois un entraînement qu’on déploie de conteneurs pour répondre à nos utilisateurs. Ce sera l’objet de la partie MLOps de fiabiliser et optimiser cette partie du pipeline.\nil est nécessaire de (re)lancer manuellement kubectl apply -f deployment/ à chaque changement de notre code. Autrement dit, lors de cette application, on a amélioré la fiabilité du lancement de notre API mais un lancement manuel est encore indispensable. Comme dans le reste de ce cours, on va essayer d’éviter un geste manuel pouvant être source d’erreur en privilégiant l’automatisation et l’archivage dans des scripts. C’est l’objet de la prochaine étape."
  },
  {
    "objectID": "chapters/application.html#etape-3-automatiser-le-déploiement-déploiement-en-continu",
    "href": "chapters/application.html#etape-3-automatiser-le-déploiement-déploiement-en-continu",
    "title": "Application",
    "section": "Etape 3: automatiser le déploiement (déploiement en continu)",
    "text": "Etape 3: automatiser le déploiement (déploiement en continu)\n\n\n\n\n\n\nClarification sur la branche de travail, les tags et l’image Docker utilisée\n\n\n\n\n\nA partir de maintenant, il est nécessaire de clarifier la branche principale sur laquelle nous travaillons. Toutes les prochaines applications supposeront que vous travaillez depuis la branche main. Si vous avez changé de branche, vous pouvez fusionner celle-ci à main.\nSi vous avez utilisé un tag pour sauter une ou plusieurs étapes, il va être nécessaire de se placer sur une branche car vous êtes en head detached. Si vous avez utilisé les scripts automatisés de checkpoint, cette gymnastique a été faite pour vous.\nLes prochaines applications vont également nécessiter d’utiliser une image Docker. Si vous avez suivi de manière linéaire cette application, votre image Docker devrait exister depuis l’application 15 si vous avez pushé votre dépôt à ce moment là.\nNéanmoins, si vous n’avez pas fait cette application, vous pouvez utiliser le checkpoint de l’application 18 et faire un git push origin main --force (à ne pas reproduire sur vos projets!) qui devrait déclencher les opérations côté Github pour construire et livrer votre image Docker. Cela nécessite quelques opérations de votre côté, notamment la création d’un token Dockerhub à renseigner en secret Github. Pour vous refraîchir la mémoire sur le sujet, vous pouvez retourner consulter l’application 15.\n\n\n\nQu’est-ce qui peut déclencher une évolution nécessitant de mettre à jour l’ensemble de notre processus de production ?\nRegardons à nouveau notre pipeline:\n\nLes inputs de notre pipeline sont donc:\n\nLa configuration. Ici, on peut considérer que notre .env de configuration, les secrets renseignés à Github ou encore le requirements.txt relèvent de cette catégorie ;\nLes données. Nos données sont statiques et n’ont pas vocation à évoluer. Si c’était le cas, il faudrait en tenir compte dans notre automatisation (Note 1). ;\nLe code. C’est l’élément principal qui évolue chez nous. Idéalement, on veut automatiser le processus au maximum en faisant en sorte qu’à chaque mise à jour de notre code (un push sur Github), les étapes ultérieures (production de l’image Docker, etc.) se lancent. Néanmoins, on veut aussi éviter qu’une erreur puisse donner lieu à une mise en production non-fonctionnelle, on va donc maintenir une action manuelle minimale comme garde-fou.\n\n\n\n\n\n\n\nNote 1: Et le versionning des données ?\n\n\n\nIci, nous nous plaçons dans le cas simple où les données brutes reçues sont figées. Ce qui peut changer est la manière dont on constitue nos échantillons train/test. Il sera donc utile de logguer les données en question par le biais de MLFlow. Mais il n’est pas nécessaire de versionner les données brutes.\nSi celles-ci évoluaient, il pourrait être utile de versionner les données, à la manière dont on le fait pour le code. Git n’est pas l’outil approprié pour cela. Parmi les outils populaires de versionning de données, bien intégrés avec S3, il y a, sur le SSPCloud, lakefs.\n\n\nPour automatiser au maximum la mise en production, on va utiliser un nouvel outil : ArgoCD. Ainsi, au lieu de devoir appliquer manuellement la commande kubectl apply à chaque modification des fichiers de déploiement (présents dans le dossier kubernetes/), c’est l’opérateur ArgoCD, déployé sur le cluster, qui va détecter les changements de configuration du déploiement et les appliquer automatiquement.\nC’est l’approche dite GitOps : le dépôt Git du déploiement fait office de source de vérité unique de l’état voulu de l’application, tout changement sur ce dernier doit donc se répercuter immédiatement sur le déploiement effectif.\n\n\n\n\n\n\nApplication 19a: Automatiser la mise à disposition de l’API (déploiement continu)\n\n\n\n\nLancer un service ArgoCD sur le SSPCloud depuis la page Mes services (catalogue Automation). Laisser les configurations par défaut.\nSur GitHub, créer un dépôt application-deployment qui va servir de dépôt GitOps, c’est à dire un dépôt qui spécifie le paramétrage du déploiement de votre application.\nAjouter un dossier deployment à votre dépôt GitOps, dans lequel on mettra les trois fichiers de déploiement qui permettent de déployer notre application sur Kubernetes (deployment.yaml, service.yaml, ingress.yaml).\nA la racine de votre dépôt GitOps, créez un fichier application.yml avec le contenu suivant, en prenant bien soin de modifier les lignes annotées avec des informations pertinentes :\n\n\napplication.yaml\n\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: ensae-mlops\nspec:\n  project: default\n  source:\n1    repoURL: https://github.com/&lt;your_github_username&gt;/application-deployment.git\n2    targetRevision: main\n3    path: deployment\n  destination:\n    server: https://kubernetes.default.svc\n4    namespace: user-&lt;your_sspcloud_username&gt;\n  syncPolicy:\n    automated:\n      selfHeal: true\n\n\n1\n\nL’URL de votre dépôt Github  faisant office de dépôt GitOps.\n\n2\n\nLa branche à partir de laquelle vous déployez.\n\n3\n\nLe nom du dossier contenant vos fichiers de déploiement Kubernetes.\n\n4\n\nVotre namespace Kubernetes. Sur le SSPCloud, cela prend la forme user-${username}.\n\n\nPousser sur Github le dépôt GitOps.\nDans ArgoCD, cliquez sur New App puis Edit as a YAML. Copiez-collez le contenu de application.yml et cliquez sur Create.\nObservez dans l’interface d’ArgoCD le déploiement progressif des ressources nécessaires à votre application sur le cluster. Joli non ?\nVérifiez que votre API est bien déployée en utilisant l’URL définie dans le fichier ingress.yml.\nSupprimer du code applicatif le dossier deployment puisque c’est maintenant votre dépôt de déploiement qui le contrôle.\nIndiquer dans le README.md que le déploiement de votre application (dont vous pouvez mettre l’URL dans le README) est contrôlé par un autre dépôt.\n\n\n\nSi cela a fonctionné, vous devriez maintenant voir votre application dans votre tableau de bord ArgoCD:\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli19a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli19a2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli19a\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nA présent, nous avons tous les outils à notre disposition pour construire un vrai pipeline de CI/CD, automatisé de bout en bout. Il va nous suffire pour cela de mettre à bout les composants :\n\ndans la partie 4 de l’application, nous avons construit un pipeline de CI : on a donc seulement à faire un commit sur le dépôt de l’application pour lancer l’étape de build et de mise à disposition de la nouvelle image sur le DockerHub ;\ndans l’application précédente, nous avons construit un pipeline de CD : ArgoCD suit en permanence l’état du dépôt GitOps, tout commit sur ce dernier lancera donc automatiquement un redéploiement de l’application.\n\nIl y a donc un élément qui fait la liaison entre ces deux pipelines et qui nous sert de garde-fou en cas d’erreur : la version de l’application.\n\n\n\n\n\n\nApplication 19b : Mettre à jour la version en production\n\n\n\nJusqu’à maintenant, on a utilisé le tag latest pour définir la version de notre application. En pratique, lorsqu’on passe de la phase de développement à celle de production, on a plutôt envie de versionner proprement les versions de l’application afin de savoir ce qui est déployé. On va pour cela utiliser les tags avec Git, qui vont se propager au nommage de l’image Docker.\n\nModifier le fichier de CI prod.yml pour assurer la propagation des tags.\n\n\n\nFichier .github/workflows/prod.yml\n\n\n\n.github/workflows/prod.yml\n\nname: Construction image Docker\n\non:\n  push:\n    branches:\n      - main\n      - dev\n    tags:\n      - 'v*.*.*'\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      -\n        name: Docker meta\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n1          images: linogaliana/application\n\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n\n1\n\nModifier ici !\n\n\n\n\nDans le dépôt de l’application, mettre à jour le code dans app/main.py pour changer un élément de l’interface de votre documentation. Par exemple, mettre en gras un titre.\n\n\napp/main.py\n\napp = FastAPI(\n    title=\"Démonstration du modèle de prédiction de survie sur le Titanic\",\n    description=\n    \"&lt;b&gt;Application de prédiction de survie sur le Titanic&lt;/b&gt; 🚢 &lt;br&gt;Une version par API pour faciliter la réutilisation du modèle 🚀\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\nCommit et push les changements.\nTagger le commit effectué précédemment et push le nouveau tag :\n\n\nterminal\n\ngit tag v0.0.1\ngit push --tags\n\nVérifier sur le dépôt GitHub de l’application que ce commit lance bien un pipeline de CI associé au tag v1.0.0. Une fois terminé, vérifier sur le DockerHub que le tag v0.0.1 existe bien parmi les tags disponibles de l’image.\n\nLa partie CI a correctement fonctionné. Intéressons-nous à présent à la partie CD.\n\nSur le dépôt GitOps, mettre à jour la version de l’image à déployer en production dans le fichier deployment/deployment.yaml\n\n\n\nFichier deployment/deployment.yaml\n\n\n\ndeployment/deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n1        image: linogaliana/application:v0.0.1\n        ports:\n        - containerPort: 8000\n\n\n1\n\nRemplacer ici par le dépôt applicatif adéquat\n\n\n\n\nAprès avoir committé et pushé, observer dans ArgoCD le statut de votre application. Normalement, l’opérateur devrait avoir automatiquement identifié le changement, et mettre à jour le déploiement pour en tenir compte.\n\n\n\nVérifier que l’API a bien été mise à jour.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli19b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli19b2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli19b\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#etape-4-construire-un-site-web",
    "href": "chapters/application.html#etape-4-construire-un-site-web",
    "title": "Application",
    "section": "Etape 4: construire un site web",
    "text": "Etape 4: construire un site web\n\n\n\n\n\n\nSi vous prenez ce projet fil rouge en cours de route\n\n\n\n\n\n\n\nterminal\n\ngit checkout appli19\ngit checkout -b dev\ngit push origin dev\n\n\n\n\n\n\n\n\n\nOn va proposer un nouveau livrable pour parler à un public plus large. Pour faire ce site web, on va utiliser Quarto et déployer sur Github Pages.\n\n\n\n\n\n\nApplication 20: Création d’un site web pour valoriser le projet\n\n\n\nquarto create project website mysite\n\nFaire remonter d’un niveau _quarto.yml\nSupprimer about.qmd, déplacer index.qmd vers la racine de notre projet.\nRemplacer le contenu de index.qmd par celui-ci et retirer about.qmd des fichiers à compiler.\nDéplacer styles.css à la racine du projet\nMettre à jour le .gitignore avec les instructions suivantes\n\n/.quarto/\n*.html\n*_files\n_site/\n\nEn ligne de commande, faire quarto preview\nObserver le site web généré en local\n\nEnfin, on va construire et déployer automatiquement ce site web grâce au combo Github Actions et Github Pages:\n\nCréer une branche gh-pages à partir des lignes suivantes\n\n\n\nterminal\n\ngit checkout --orphan gh-pages\ngit reset --hard # make sure all changes are committed before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\n\n\nRevenir à votre branche principale (main normalement)\nCréer un fichier .github/workflows/website.yaml avec le contenu de ce fichier\nModifier le README pour indiquer l’URL de votre site web et de votre API\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli20      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli202\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli20\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#revenir-sur-le-code-dentraînement-du-modèle-pour-faire-de-la-validation-croisée",
    "href": "chapters/application.html#revenir-sur-le-code-dentraînement-du-modèle-pour-faire-de-la-validation-croisée",
    "title": "Application",
    "section": "Revenir sur le code d’entraînement du modèle pour faire de la validation croisée",
    "text": "Revenir sur le code d’entraînement du modèle pour faire de la validation croisée\nPour pouvoir faire ceci, il va falloir changer un tout petit peu notre code applicatif dans sa phase d’entraînement.\n\n\n\n\n\n\nApplication 21 (optionnelle): restructuration de la chaîne\n\n\n\n\nFaire les modifications suivantes pour restructurer notre pipeline afin de mieux distinguer les étapes d’estimation et d’évaluation\n\n\n\nModification de train.py pour faire une grid search\n\n\n\ntrain.py\n\n\"\"\"\nPrediction de la survie d'un individu sur le Titanic\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\nimport argparse\nfrom loguru import logger\nfrom joblib import dump\n\nimport pathlib\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom src.pipeline.build_pipeline import create_pipeline\nfrom src.models.train_evaluate import evaluate_model\n\n\n# ENVIRONMENT CONFIGURATION ---------------------------\n\nlogger.add(\"recording.log\", rotation=\"500 MB\")\nload_dotenv()\n\nparser = argparse.ArgumentParser(description=\"Paramètres du random forest\")\nparser.add_argument(\n    \"--n_trees\", type=int, default=20, help=\"Nombre d'arbres\"\n)\nargs = parser.parse_args()\n\nURL_RAW = \"https://minio.lab.sspcloud.fr/lgaliana/ensae-reproductibilite/data/raw/data.csv\"\n\nn_trees = args.n_trees\njeton_api = os.environ.get(\"JETON_API\", \"\")\ndata_path = os.environ.get(\"data_path\", URL_RAW)\ndata_train_path = os.environ.get(\"train_path\", \"data/derived/train.parquet\")\ndata_test_path = os.environ.get(\"test_path\", \"data/derived/test.parquet\")\nMAX_DEPTH = None\nMAX_FEATURES = \"sqrt\"\n\nif jeton_api.startswith(\"$\"):\n    logger.info(\"API token has been configured properly\")\nelse:\n    logger.warning(\"API token has not been configured\")\n\n\n# IMPORT ET STRUCTURATION DONNEES --------------------------------\n\np = pathlib.Path(\"data/derived/\")\np.mkdir(parents=True, exist_ok=True)\n\nTrainingData = pd.read_csv(data_path)\n\ny = TrainingData[\"Survived\"]\nX = TrainingData.drop(\"Survived\", axis=\"columns\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1\n)\npd.concat([X_train, y_train], axis = 1).to_parquet(data_train_path)\npd.concat([X_test, y_test], axis = 1).to_parquet(data_test_path)\n\n\n# PIPELINE ----------------------------\n\n\n# Create the pipeline\npipe = create_pipeline(\n    n_trees, max_depth=MAX_DEPTH, max_features=MAX_FEATURES\n)\n\nparam_grid = {\n    \"classifier__n_estimators\": [10, 20, 50],\n    \"classifier__max_leaf_nodes\": [5, 10, 50],\n}\n\npipe_cross_validation = GridSearchCV(\n    pipe,\n    param_grid=param_grid,\n    scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n    refit=\"f1\",\n    cv=5,\n    n_jobs=5,\n    verbose=1,\n)\n\npipe_cross_validation.fit(X_train, y_train)\n\npipe = pipe_cross_validation.best_estimator_\n\n\n# ESTIMATION ET EVALUATION ----------------------\n\npipe.fit(X_train, y_train)\n\nwith open(\"model.joblib\", \"wb\") as f:\n    dump(pipe, f)\n\n# Evaluate the model\nscore, matrix = evaluate_model(pipe, X_test, y_test)\n\nlogger.success(f\"{score:.1%} de bonnes réponses sur les données de test pour validation\")\nlogger.debug(20 * \"-\")\nlogger.info(\"Matrice de confusion\")\nlogger.debug(matrix)\n\n\n\nDans le code de l’API (app/api.py), changer la version du modèle mis en oeuvre en “0.2” (dans la fonction show_welcome_page)\nAprès avoir committé cette nouvelle version du code applicatif, tagguer ce dépôt avec le tag v0.0.2\nModifier deployment/deployment.yaml dans le code GitOps pour utiliser ce tag.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli21      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli212\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli21\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#garder-une-trace-des-entraînements-de-notre-modèle-grâce-au-register-de-mlflow",
    "href": "chapters/application.html#garder-une-trace-des-entraînements-de-notre-modèle-grâce-au-register-de-mlflow",
    "title": "Application",
    "section": "Garder une trace des entraînements de notre modèle grâce au register de MLFlow",
    "text": "Garder une trace des entraînements de notre modèle grâce au register de MLFlow\n  \n    \n      \n        \n      \n      \n        Reprendre à partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration proposée dans l'application préliminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, après avoir clôné le dépôt\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli212\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli21\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#enregistrer-nos-premiers-entraînements",
    "href": "chapters/application.html#enregistrer-nos-premiers-entraînements",
    "title": "Application",
    "section": "Enregistrer nos premiers entraînements",
    "text": "Enregistrer nos premiers entraînements\n\n\n\n\n\n\nApplication 22 : archiver nos entraînements avec MLFlow\n\n\n\n\nLancer MLFlow depuis l’onflet Mes services du SSPCloud. Attendre que le service soit bien lancé. Cela créera un service dont l’URL est de la forme https://user-{username}.user.lab.sspcloud.fr. Ce service MLFlow communiquera avec les VSCode que vous ouvrirez ultérieurement à partir de cet URL ainsi qu’avec le système de stockage S318.\nRegarder la page Experiments. Elle ne contient que Default à ce stade, c’est normal.\n\n\nUne fois le service MLFlow fonctionnel, lancer un nouveau VSCode pour bénéficier de la connexion automatique entre les services interactifs du SSPCloud et les services d’automatisation comme MLFlow.\nClôner votre projet, vous situer sur la branche de travail.\nDans la section de passage des paramètres de notre ligne de commande, introduire ce morceau de code:\n\nparser = argparse.ArgumentParser(description=\"Paramètres du random forest\")\nparser.add_argument(\n    \"--n_trees\", type=int, default=20, help=\"Nombre d'arbres\"\n)\nparser.add_argument(\n    \"--experiment_name\", type=str, default=\"titanicml\", help=\"MLFlow experiment name\"\n)\nargs = parser.parse_args()\n\nA la fin du script train.py, ajouter le code suivant\n\n\n\nCode à ajouter\n\n\n\nfin de train.py\n\n# LOGGING IN MLFLOW -----------------\n\nmlflow_server = os.getenv(\"MLFLOW_TRACKING_URI\")\n\nlogger.info(f\"Saving experiment in {mlflow_server}\")\n\nmlflow.set_tracking_uri(mlflow_server)\nmlflow.set_experiment(args.experiment_name)\n\n\ninput_data_mlflow = mlflow.data.from_pandas(\n    TrainingData, source=data_path, name=\"Raw dataset\"\n)\ntraining_data_mlflow = mlflow.data.from_pandas(\n    pd.concat([X_train, y_train], axis=1), source=data_path, name=\"Training data\"\n)\n\n\nwith mlflow.start_run():\n\n    # Log datasets\n    mlflow.log_input(input_data_mlflow, context=\"raw\")\n    mlflow.log_input(training_data_mlflow, context=\"raw\")\n\n    # Log parameters\n    mlflow.log_param(\"n_trees\", n_trees)\n    mlflow.log_param(\"max_depth\", MAX_DEPTH)\n    mlflow.log_param(\"max_features\", MAX_FEATURES)\n\n    # Log best hyperparameters from GridSearchCV\n    best_params = pipe_cross_validation.best_params_\n    for param, value in best_params.items():\n        mlflow.log_param(param, value)\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", score)\n\n    # Log confusion matrix as an artifact\n    matrix_path = \"confusion_matrix.txt\"\n    with open(matrix_path, \"w\") as f:\n        f.write(str(matrix))\n    mlflow.log_artifact(matrix_path)\n\n    # Log model\n    mlflow.sklearn.log_model(pipe, \"model\")\n\n\n\nAjouter mlruns/* dans .gitignore\nTester train.py en ligne de commande\nObserver l’évolution de la page Experiments. Cliquer sur un des run. Observer toutes les métadonnées archivées (hyperparamètres, métriques d’évaluation, requirements.txt dont MLFlow a fait l’inférence, etc.)\nObserver le code proposé par MLFlow pour récupérer le run en question. Tester celui-ci dans un notebook sur le fichier intermédiaire de test au format Parquet\nEn ligne de commande, faites tourner pour une autre valeur de n_trees. Retourner à la liste des runs en cliquant à nouveau sur “titanicml” dans les expérimentations\nDans l’onglet Table, sélectionner plusieurs expérimentations, cliquer sur Columns et ajouter la statistique d’accuracy. Ajuster la taille des colonnes pour la voir et classer les modèles par score décroissants\nCliquer sur Compare après en avoir sélectionné plusieurs. Afficher un scatterplot des performances en fonction du nombre d’estimateurs. Conclure.\nAjouter mlflow au requirements.txt\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli22      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli222\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli22\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCette appplication illustre l’un des premiers apports de MLFlow: on garde une trace de nos expérimentations: le modèle est archivé avec les paramètres et des métriques de performance. On peut donc retrouver de plusieurs manières un modèle qui nous avait tapé dans l’oeil.\nNéanmoins, persistent un certain nombre de voies d’amélioration dans notre pipeline.\n\nOn entraîne le modèle en local, de manière séquentielle, et en lançant nous-mêmes le script train.py.\nPis encore, à l’heure actuelle, cette étape d’estimation n’est pas séparée de la mise à disposition du modèle par le biais de notre API. On archive des modèles mais on les utilise pas ultérieurement.\n\nLes prochaines applications permettront d’améliorer ceci."
  },
  {
    "objectID": "chapters/application.html#consommation-dun-modèle-archivé-sur-mlflow",
    "href": "chapters/application.html#consommation-dun-modèle-archivé-sur-mlflow",
    "title": "Application",
    "section": "Consommation d’un modèle archivé sur MLFlow",
    "text": "Consommation d’un modèle archivé sur MLFlow\nA l’heure actuelle, notre pipeline est linéaire:\n\nCeci nous gêne pour faire évoluer notre modèle: on ne dissocie pas ce qui relève de l’entraînement du modèle de son utilisation. Un pipeline plus cyclique permettra de mieux dissocier l’expérimentation de la production:\n\n\n\n\n\n\n\nApplication 23 : passer en production un modèle avec MLFlow\n\n\n\n\nSi vous avez entraîné plusieurs modèles avec des n_trees différents, utiliser l’interface de MLFlow pour sélectionner le “meilleur”. Cliquer sur le modèle en question et faire l’action “Register Model”. L’enregistrer comme le modèle de “production”\nRendez-vous sur l’onglet Models et observez cet entrepôt de modèles. Cliquez sur le modèle de production. Vous pourrez par ce biais suivre ses différentes versions.\nOuvrir un notebook temporaire et observer le résultat.\n\n\n\nExemple de code à tester\n\nimport mlflow\nimport pandas as pd\n\nmodel_name = \"production\"\nmodel_version = \"latest\"\n\n# Load the model from the Model Registry\nmodel_uri = f\"models:/{model_name}/{model_version}\"\nlogged_model = mlflow.sklearn.load_model(model_uri)\n\n# GENERATE PREDICTION DATA ---------------------\n\ndef create_data(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\",\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    return df\n\n\ndata = pd.concat(\n    [create_data(age=40), create_data(sex=\"male\")]\n)\n\n# PREDICTION ---------------------\n\nlogged_model.predict(pd.DataFrame(data))\n\n\nOn va adapter le code applicatif de notre API pour tenir compte de ce modèle de production.\n\n\n\nVoir le script app/api.py proposé\n\n\"\"\"A simple API to expose our trained RandomForest model for Tutanic survival.\"\"\"\nfrom fastapi import FastAPI\nimport mlflow\n\nimport pandas as pd\n\n# Preload model -------------------\n\nmodel_name = \"production\"\nmodel_version = \"latest\"\n\n# Load the model from the Model Registry\nmodel_uri = f\"models:/{model_name}/{model_version}\"\nmodel = mlflow.sklearn.load_model(model_uri)\n\n# Define app -------------------------\n\n\napp = FastAPI(\n    title=\"Prédiction de survie sur le Titanic\",\n    description=\n    \"Application de prédiction de survie sur le Titanic 🚢 &lt;br&gt;Une version par API pour faciliter la réutilisation du modèle 🚀\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\n\n@app.get(\"/\", tags=[\"Welcome\"])\ndef show_welcome_page():\n    \"\"\"\n    Show welcome page with model name and version.\n    \"\"\"\n\n    return {\n        \"Message\": \"API de prédiction de survie sur le Titanic\",\n        \"Model_name\": 'Titanic ML',\n        \"Model_version\": \"0.3\",\n    }\n\n\n@app.get(\"/predict\", tags=[\"Predict\"])\nasync def predict(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\"\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    prediction = \"Survived 🎉\" if int(model.predict(df)) == 1 else \"Dead ⚰️\"\n\n    return prediction\n\nLes changements principaux de ce code sont:\n\non va chercher le modèle de production\non met à jour la version de notre API pour signaler à nos clients que celle-ci a évolué\n\n\nOn va retirer l’entraînement de la séquence d’opération du api/run.sh. En supprimant la ligne relative à l’entraînement du modèle, vous devriez avoir\n\n#/bin/bash\nuvicorn app.api:app --host \"0.0.0.0\"\nMettons en production cette nouvelle version. Cela implique de faire les gestes suivants:\n\nCommit de ce changement dans main\nPublier un tag v0.0.3 pour le code applicatif\nMettre à jour notre manifeste dans le dépôt GitOps.\n\nEn premier lieu, il faut changer la version de référence pour utiliser le tag v0.0.3.\nDe plus, il faut déclarer la variable d’environnement MLFLOW_TRACKING_URI qui indique à Python l’entrepôt de modèles où aller chercher celui en production. La bonne pratique est de définir ceci hors du code, dans un fichier de configuration donc, ce qui est l’objet de notre manifeste deployment.yaml. On peut donc changer de cette manière ce fichier:\n\n\n\n\nLe modèle deployment.yaml proposé\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: titanic-deployment\nlabels:\n    app: titanic\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: titanic\ntemplate:\n    metadata:\n    labels:\n        app: titanic\n    spec:\n    containers:\n    - name: titanic\n1        image: linogaliana/application:v0.0.3\n        ports:\n        - containerPort: 8000\n        env:\n        - name: MLFLOW_TRACKING_URI\n2            value: https://user-${USERNAME}-mlflow.user.lab.sspcloud.fr\n        resources:\n        limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n\n1\n\nLe tag de notre code applicatif\n\n2\n\nLa variable d’environnement à adapter en fonction de l’adresse du dépôt demodèles utilisé. Remplacer par votre URL MLFlow.\n\n\n\n\nPour s’assurer que l’application fonctionne bien, on peut aller voir les logs de la machine qui fait tourner notre code. Pour ça, faire kubectl get pods et, en supposant que votre service soit nommé titanic dans vos fichiers YAML de configuration, récupérer le nom commençant par titanic-deployment-* et faire kubectl logs titanic-deployment-*\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli23      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli232\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli23\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nA ce stade, nous avons amélioré la fiabilité de notre application car nous utilisons le meilleur modèle. Néanmoins, nos entraînements sont encore manuels. Là encore il y a des gains possibles car cela paraît pénible à la longue de devoir systématiquement relancer des entraînements manuellement pour tester des variations de tel ou tel paramètre. Heureusement, nous allons pouvoir automatiser ceci également."
  },
  {
    "objectID": "chapters/application.html#industrialiser-les-entraînements-de-nos-modèles",
    "href": "chapters/application.html#industrialiser-les-entraînements-de-nos-modèles",
    "title": "Application",
    "section": "Industrialiser les entraînements de nos modèles",
    "text": "Industrialiser les entraînements de nos modèles\nPour industrialiser nos entraînements, nous allons créer des processus parallèles indépendants pour chaque combinaison de nos hyperparamètres.\nCe travail nous amène de l’approche pipeline à mi chemin entre data science et data engineering. Il existe plusieurs outils pour faire ceci, généralement issus de la sphère du data engineering. L’outil le plus complet sur le SSPCloud, bien intégré à l’écosystème Kubernetes, est Argo Workflows19.\nChaque combinaison d’hyperparamètres sera un processus isolé à l’issue duquel sera loggué le résultat dans MLFlow. Ces entraînements auront lieu en parallèle.\nNous allons construire, dans les deux prochaines applications, un pipeline simple prenant cette forme20:\n\n\n\n\n\n\n\n\n\n\n\n(a) Via Argo Workflows\n\n\n\n\n\n\n\n\n\n\n\n(b) Via Github Actions\n\n\n\n\n\n\n\nFigure 3: Pipeline d’entraînement de nos modèles avec deux outils d’automatisation différents\n\n\n\nL’outil permettant une intégration native de notre pipeline dans l’infrastructure cloud (SSPCloud) que nous avons utilisée jusqu’à présent est Argo Workflows. Néanmoins, pour illustrer la modularité de notre chaîne, permise par l’adoption de Docker, nous allons montrer que les serveurs d’intégration continue de Github peuvent très bien servir d’environnement d’exécution, sans rien perdre de ce que nous avons mis en oeuvre précédemment (logging des modèles dans MLFlow, récupération de données depuis S3, etc.)\n\n\n\n\n\n\nApplication 24 : industrialisation des entraînements avec Argo Workflow\n\n\n\nA l’heure actuelle, notre entraînement ne dépend que d’un hyperparamètre fixé à partir de la ligne de commande: n_trees. Nous allons commencer par ajouter un argument à notre chaine de production (code applicatif):\n\nDans train.py, dans la section relative au parsing de nos arguments, ajouter ce bout de code\n\nparser.add_argument(\n    \"--max_features\",\n    type=str, default=\"sqrt\",\n    choices=['sqrt', 'log2'],\n    help=\"Number of features to consider when looking for the best split\"\n)\net remplacer la définition de MAX_FEATURES par l’argument fourni en ligne de commande:\nMAX_FEATURES = args.max_features\n\nFaire un commit, taguer cette version (v0.0.4) et pusher le tag\nMaintenant, dans le dépôt GitOps, créer un fichier argo-workflow/manifest.yaml\n\n\n\nLe modèle proposé\n\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\ngenerateName: titanic-training-workflow-\nnamespace: user-lgaliana\nspec:\nentrypoint: main\nserviceAccountName: workflow\narguments:\n    parameters:\n    # The MLflow tracking server is responsible to log the hyper-parameter and model metrics.\n    - name: mlflow-tracking-uri\n1        value: https://user-${USERNAME}-mlflow.user.lab.sspcloud.fr/\n    - name: mlflow-experiment-name\n2        value: titanicml\n    - name: model-training-conf-list\n        value: |\n        [\n            { \"n_trees\": 10, \"max_features\": \"log2\" },\n            { \"n_trees\": 20, \"max_features\": \"sqrt\" },\n            { \"n_trees\": 20, \"max_features\": \"log2\" },\n            { \"n_trees\": 50, \"max_features\": \"sqrt\" }\n        ]\ntemplates:\n    # Entrypoint DAG template\n    - name: main\n    dag:\n        tasks:\n        # Task 0: Start pipeline\n        - name: start-pipeline\n            template: start-pipeline-wt\n        # Task 1: Train model with given params\n        - name: train-model-with-params\n            dependencies: [ start-pipeline ]\n            template: run-model-training-wt\n            arguments:\n            parameters:\n                - name: max_features\n                value: \"{{item.max_features}}\"\n                - name: n_trees\n                value: \"{{item.n_trees}}\"\n            # Pass the inputs to the task using \"withParam\"\n            withParam: \"{{workflow.parameters.model-training-conf-list}}\"\n    # Now task container templates are defined\n    # Worker template for task 0 : start-pipeline\n    - name: start-pipeline-wt\n    inputs:\n    container:\n        image: busybox\n        command: [ sh, -c ]\n        args: [ \"echo Starting pipeline\" ]\n    # Worker template for task-1 : train model with params\n    - name: run-model-training-wt\n    inputs:\n        parameters:\n        - name: n_trees\n        - name: max_features\n    container:\n3        image: ****/application:v0.0.4\n        imagePullPolicy: Always\n        command: [sh, -c]\n        args: [\n        \"python3 train.py --n_trees={{inputs.parameters.n_trees}} --max_features={{inputs.parameters.max_features}}\"\n        ]\n        env:\n        - name: MLFLOW_TRACKING_URI\n            value: \"{{workflow.parameters.mlflow-tracking-uri}}\"\n        - name: MLFLOW_EXPERIMENT_NAME\n            value: \"{{workflow.parameters.mlflow-experiment-name}}\"\n        - name: AWS_DEFAULT_REGION\n            value: us-east-1\n        - name: AWS_S3_ENDPOINT\n            value: minio.lab.sspcloud.fr\n\n1\n\nChanger pour votre entrepot de modèle\n\n2\n\nLe nom de l’expérimentation MLFLow dont nous allons avoir besoin (on propose de continuer sur titanicml)\n\n3\n\nChanger l’image Docker  ici\n\n\n\n\nObserver l’UI d’Argo Workflow dans vos services ouverts du SSPCloud. Vous devriez retrouver Figure 3 (a) dans celle-ci.\n\n\n\nNous pouvons maintenant passer à la version Github. Celle-ci est optionnelle car elle vient surtout démontrer l’intérêt d’avoir une chaine modulaire et la dissociation que cela permet entre l’environnement d’exécution et les autres environnements nécessaires à notre chaine (notamment le stockage code et le logging).\n\n\n\n\n\n\nApplication 25 (optionnelle) : Github Actions comme ordonnanceur\n\n\n\nPour que Github sache où aller chercher MLFlow et S3 et comment s’y identifier, il va falloir lui donner un certain de variables d’environnement. Il est hors de question de mettre celles-ci dans le code. Heureusement, Github propose la possibilité de renseigner des secrets: nous allons utiliser ceux-ci.\n\nAller dans les paramètres de votre projet GitOps et dans la section Secrets and variables\nVous allez avoir besoin de créer les secrets suivants:\n\nMLFLOW_TRACKING_PASSWORD\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_SESSION_TOKEN\n\n\nLes valeurs à renseigner sont à récupérer à différents endroits:\n\nPour les secrets liés à S3 (AWS_*), ceux-ci sont dans l’espace Mon compte du SSPCloud. Ils ont une durée de validité limitée: si vous devez refaire tourner le code dans quelques jours, il faudra les mettre à jour (ou passer par un compte de service comme indiqué précédemment)\nLe mot de passe de MLFlow est dans le README de votre service, qui s’affiche quand vous cliquez sur le bouton Ouvrir depuis la page Mes services\n\n\nReprendre ce modèle d’action à mettre dans votre dépôt GitOps (.github/workflows/train.yaml par exemple).\n\n\n\nModèle d’action Github\n\nname: Titanic Model Training\n\non:\n  push:\n    branches:\n      - main\n  workflow_dispatch:\n\njobs:\n  start-pipeline:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Start Pipeline\n        run: echo \"Starting pipeline\"\n\n  train-model:\n    needs: start-pipeline\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        model-config:\n          - { n_trees: 10, max_features: \"log2\" }\n          - { n_trees: 20, max_features: \"sqrt\" }\n          - { n_trees: 20, max_features: \"log2\" }\n          - { n_trees: 50, max_features: \"sqrt\" }\n    container:\n1      image: ***/application:v0.0.4\n    env:\n      MLFLOW_TRACKING_URI: \"https://user-lgaliana-mlflow.user.lab.sspcloud.fr/\"\n      MLFLOW_EXPERIMENT_NAME: \"titanicml\"\n      MLFLOW_TRACKING_PASSWORD: \"${{ secrets.MLFLOW_TRACKING_PASSWORD }}\"\n      AWS_DEFAULT_REGION: \"us-east-1\"\n      AWS_S3_ENDPOINT: \"minio.lab.sspcloud.fr\"\n      AWS_ACCESS_KEY_ID: \"${{ secrets.AWS_ACCESS_KEY_ID }}\"\n      AWS_SECRET_ACCESS_KEY: \"${{ secrets.AWS_SECRET_ACCESS_KEY }}\"\n      AWS_SESSION_TOKEN: \"${{ secrets.AWS_SESSION_TOKEN }}\"\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n2            repository: 'ensae-reproductibilite/application'\n            ref: appli24\n\n      - name: Train Model\n        run: |\n          python3 train.py --n_trees=${{ matrix.model-config.n_trees }} --max_features=${{ matrix.model-config.max_features }}\n\n1\n\nMettre votre image ici. Si vous n’en avez pas, vous pouvez mettre linogaliana/application:v0.0.4\n\n2\n\nOn reprend le code applicatif de l’application précédente. Vous pouvez remplacer par votre dépôt et une référence adaptée si vous préférez\n\n\n\n\nPusher et observer l’UI de Github depuis l’onglet Actions. Vous devriez retrouver Figure 3 (b) dans celle-ci."
  },
  {
    "objectID": "chapters/application.html#footnotes",
    "href": "chapters/application.html#footnotes",
    "title": "Application",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIl y a quelques différences entre le VSCode server mis à disposition sur le SSPCloud et la version desktop sur laquelle s’appuient beaucoup de ressources. A quelques extensions prêts (Data Wrangler, Copilot), les différences sont néanmoins minimes.↩︎\nL’export dans un script .py a été fait directement depuis VSCode. Comme cela n’est pas vraiment l’objet du cours, nous passons cette étape et fournissons directement le script expurgé du texte intermédiaire. Mais n’oubliez pas que cette démarche, fréquente quand on a démarré sur un notebook et qu’on désire consolider en faisant la transition vers des scripts, nécessite d’être attentif pour ne pas risquer de faire une erreur.↩︎\nIl est également possible avec VSCode d’exécuter le script ligne à ligne de manière interactive ligne à ligne (MAJ+ENTER). Néanmoins, cela nécessite de s’assurer que le working directory de votre console interactive est le bon. Celle-ci se lance selon les paramètres préconfigurés de VSCode et les votres ne sont peut-être pas les mêmes que les notres. Vous pouvez changer le working directory dans le script en utilisant le package os mais peut-être allez vous découvrir ultérieurement qu’il y a de meilleures pratiques…↩︎\nEssayez de commit vos changements à chaque étape de l’exercice, c’est une bonne habitude à prendre.↩︎\nIl est normal d’avoir des dossiers __pycache__ qui traînent en local : ils se créent automatiquement à l’exécution d’un script en Python. Néanmoins, il ne faut pas associer ces fichiers à Git, voilà pourquoi on les ajoute au .gitignore.↩︎\nNous proposons ici d’adopter le principe de la programmation fonctionnelle. Pour encore fiabiliser un processus, il serait possible d’adopter le paradigme de la programmation orientée objet (POO). Celle-ci est plus rebutante et demande plus de temps au développeur. L’arbitrage coût-avantage est négatif pour notre exemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d’un modèle, il peut être utle de l’adopter car certains frameworks, à commencer par les pipelines scikit, exigeront certaines classes et méthodes si vous désirez brancher des objets ad hoc à ceux-ci.↩︎\nAttention, les données ont été committées au moins une fois. Les supprimer du dépôt ne les efface pas de l’historique. Si cette erreur arrive, le mieux est de supprimer le dépôt en ligne, créer un nouvel historique Git et partir de celui-ci pour des publications ultérieures sur Github. Néanmoins l’idéal serait de ne pas s’exposer à cela. C’est justement l’objet des bonnes pratiques de ce cours: un .gitignore bien construit et une séparation des environnements de stockage du code et des données seront bien plus efficaces pour vous éviter ces problèmes que tout les conseils de vigilance que vous pourrez trouver ailleurs.↩︎\nAlors oui, c’est vrai, s3 se distingue d’un système de fichiers classiques comme on peut le lire dans certains posts énervés sur la question (par exemple sur Reddit). Mais du point de vue de l’utilisateur Python plutôt que de l’architecte cloud, on va avoir assez peu de différence avec un système de fichier local. C’est pour le mieux, cela réduit la difficulté à rentrer dans cette technologie.↩︎\nLorsqu’on développe du code qui finalement ne s’avère plus nécessaire, on a souvent un cas de conscience à le supprimer et on préfère le mettre de côté. Au final, ce syndrôme de Diogène est mauvais pour la pérennité du projet : on se retrouve à devoir maintenir une base de code qui n’est, en pratique, pas utilisée. Ce n’est pas un problème de supprimer un code ; si finalement celui-ci s’avère utile, on peut le retrouver grâce à l’historique Git et les outils de recherche sur Github. Le package vulture est très pratique pour diagnostiquer les morceaux de code inutiles dans un projet.↩︎\nLe fichier __init__.py indique à Python que le dossier est un package. Il permet de proposer certaines configurations lors de l’import du package. Il permet également de contrôler les objets exportés (c’est-à-dire mis à disposition de l’utilisateur) par le package par rapport aux objets internes au package. En le laissant vide, nous allons utiliser ce fichier pour importer l’ensemble des fonctions de nos sous-modules. Ce n’est pas la meilleure pratique mais un contrôle plus fin des objets exportés demanderait un investissement qui ne vaut, ici, pas le coût.↩︎\nSi vous désirez aussi contrôler la version de Python, ce qui peut être important dans une perspective de portabilité, vous pouvez ajouter une option, par exemple -p python3.10. Néanmoins nous n’allons pas nous embarasser de cette nuance pour la suite car nous pourrons contrôler la version de Python plus finement par le biais de Docker.↩︎\nL’option -c passée après la commande python permet d’indiquer à Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu’on va directement lui fournir.↩︎\nL’option -c passée après la commande python permet d’indiquer à Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu’on va directement lui fournir.↩︎\nPour comparer les deux listes, vous pouvez utiliser la fonctionnalité de split du terminal sur VSCode pour comparer les outputs de conda env export en les mettant en face à face.↩︎\nL’option -c passée après la commande python permet d’indiquer à Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu’on va directement lui fournir.↩︎\nIl est tout à fait normal de ne pas parvenir à créer une action fonctionnelle du premier coup. N’hésitez pas à pusher votre code après chaque question pour vérifier que vous parvenez bien à réaliser chaque étape. Sinon vous risquez de devoir corriger bout par bout un fichier plus conséquent.↩︎\nIl existe une approche alternative pour faire des tests réguliers: les hooks Git. Il s’agit de règles qui doivent être satisfaites pour que le fichier puisse être committé. Cela assure que chaque commit remplisse des critères de qualité afin d’éviter le problème de la procrastination.\nLa documentation de pylint offre des explications supplémentaires. Ici, nous allons adopter une approche moins ambitieuse en demandant à notre action de faire ce travail d’évaluation de la qualité de notre code↩︎\nPar conséquent, MLFLow bénéficie de l’injection automatique des tokens pour pouvoir lire/écrire sur S3. Ces jetons ont la même durée avant expiration que ceux de vos services interactifs VSCode. Il faut donc, par défaut, supprimer et rouvrir un service MLFLow régulièrement. La manière d’éviter cela est de créer des service account sur https://minio-console.lab.sspcloud.fr/ et de les renseigner sur la page.↩︎\nIl existe d’autres outils d’ordonnancement de pipelines très utilisés dans l’industrie, notamment Airflow.\nCe dernier est plus utilisé, en pratique, qu’Argo Workflow mais, même s’il est disponible sur le SSPCloud aussi, est moins pensé autour de Kubernetes que l’est Argo.\nPour mieux comprendre la différence entre Argo et Airflow, la philosphie différente de ces deux outils et leurs avantages comparatifs, cette courte vidéo est intéressante:\n\n↩︎\nIl serait bien sûr possible d’aller beaucoup plus loin dans la définition du pipeline.\nPar exemple, il est possible, si le framework utilisé pour la modélisation n’intègre pas la notion de pipeline au niveau de Python de faire ceci au niveau d’Argo. Cela donnerait un pipeline prenant cette forme:\n\nNéanmoins, ici, nous utilisons Scikit qui permet d’intégrer le preprocessing comme une étape de modélisation. Nous n’avons donc pas d’intérêt à définir ceci comme une tâche autonome, raison pour laquelle notre pipeline apparaît plus simple.↩︎"
  }
]