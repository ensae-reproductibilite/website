[
  {
    "objectID": "chapters/application.html",
    "href": "chapters/application.html",
    "title": "Application",
    "section": "",
    "text": "DÃ©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein Ã©cran.\nLâ€™objectif de cette mise en application est dâ€™illustrer les diffÃ©rentes Ã©tapes qui sÃ©parent la phase de dÃ©veloppement dâ€™un projet de celle de la mise en production. Elle permettra de mettre en pratique les diffÃ©rents concepts prÃ©sentÃ©s tout au long du cours.\nCelle-ci est un tutoriel pas Ã  pas pour avoir un projet reproductible et disponible sous plusieurs livrables. Toutes les Ã©tapes ne sont pas indispensables Ã  tous les projets de data science.\nNous nous plaÃ§ons dans une situation initiale correspondant Ã  la fin de la phase de dÃ©veloppement dâ€™un projet de data science. On a un notebook un peu monolithique, qui rÃ©alise les Ã©tapes classiques dâ€™un pipeline de machine learning :\nLâ€™objectif est dâ€™amÃ©liorer le projet de maniÃ¨re incrÃ©mentale jusquâ€™Ã  pouvoir le mettre en production, en le valorisant sous une forme adaptÃ©e."
  },
  {
    "objectID": "chapters/application.html#Ã©tape-1-sassurer-que-le-script-sexÃ©cute-correctement",
    "href": "chapters/application.html#Ã©tape-1-sassurer-que-le-script-sexÃ©cute-correctement",
    "title": "Application",
    "section": "Ã‰tape 1 : sâ€™assurer que le script sâ€™exÃ©cute correctement",
    "text": "Ã‰tape 1 : sâ€™assurer que le script sâ€™exÃ©cute correctement\nOn va partir du fichier notebook.py qui reprend le contenu du notebook1 mais dans un script classique. Le travail de nettoyage en sera facilitÃ©.\nLa premiÃ¨re Ã©tape est simple, mais souvent oubliÃ©e : vÃ©rifier que le code fonctionne correctement. Pour cela, nous recommandons de faire un aller-retour entre le script ouvert dans VSCode et un terminal pour le lancer.\n\n\n\n\n\n\nApplication 1: corriger les erreurs\n\n\n\n\nOuvrir dans VSCode le script titanic.py ;\nExÃ©cuter le script en ligne de commande (python titanic.py)2 pour dÃ©tecter les erreurs ;\nCorriger les deux erreurs qui empÃªchent la bonne exÃ©cution ;\nVÃ©rifier le fonctionnement du script en utilisant la ligne de commande:\n\n\n\nterminal\n\n$ python titanic.py\n\nLe code devrait afficher des sorties.\n\n\nAide sur les erreurs rencontrÃ©es\n\nLa premiÃ¨re erreur rencontrÃ©e est une alerte FileNotFoundError, la seconde est liÃ©e Ã  un package.\n\nIl est maintenant temps de commit les changements effectuÃ©s avec Git3 :\n\n\nterminal\n\n$ git add titanic.py\n$ git commit -m \"Corrige l'erreur qui empÃªchait l'exÃ©cution\"\n$ git push\n\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli1\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#Ã©tape-2-utiliser-un-linter-puis-un-formatter",
    "href": "chapters/application.html#Ã©tape-2-utiliser-un-linter-puis-un-formatter",
    "title": "Application",
    "section": "Ã‰tape 2: utiliser un linter puis un formatter",
    "text": "Ã‰tape 2: utiliser un linter puis un formatter\nOn va maintenant amÃ©liorer la qualitÃ© de notre code en appliquant les standards communautaires. Pour cela, on va utiliser le linter classique PyLint et le formatter Black. Si vous dÃ©sirez un outil deux en un, il est possible dâ€™utiliser Ruff en complÃ©ment ou substitut.\nCe nettoyage automatique du code permettra, au passage, de restructurer notre script de maniÃ¨re plus naturelle.\n\n\n\n\n\n\nImportant\n\n\n\nPyLint et Black sont des packages Python qui sâ€™utilisent principalement en ligne de commande.\nSi vous avez une erreur qui suggÃ¨re que votre terminal ne connait pas PyLint ou Black, nâ€™oubliez pas dâ€™exÃ©cuter la commande pip install pylint ou pip install black.\n\n\nLe linter renvoie alors une sÃ©rie dâ€™irrÃ©gularitÃ©s, en prÃ©cisant Ã  chaque fois la ligne de lâ€™erreur et le message dâ€™erreur associÃ© (ex : mauvaise identation). Il renvoie finalement une note sur 10, qui estime la qualitÃ© du code Ã  lâ€™aune des standards communautaires Ã©voquÃ©s dans la partie QualitÃ© du code.\n\n\n\n\n\n\nApplication 2: rendre lisible le script\n\n\n\n\nDiagnostiquer et Ã©valuer la qualitÃ© de titanic.py avec PyLint. Regarder la note obtenue.\nUtiliser black titanic.py --diff --color pour observer les changements de forme que va induire lâ€™utilisation du formatter Black. Cette Ã©tape nâ€™applique pas les modifications, elle ne fait que vous les montrer.\nAppliquer le formatter Black\nRÃ©utiliser PyLint pour diagnostiquer lâ€™amÃ©lioration de la qualitÃ© du script et le travail qui reste Ã  faire.\nComme la majoritÃ© du travail restant est Ã  consacrer aux imports:\n\nMettre tous les imports ensemble en dÃ©but de script\nRetirer les imports redondants en sâ€™aidant des diagnostics de votre Ã©diteur\nRÃ©ordonner les imports si PyLint vous indique de le faire\nCorriger les derniÃ¨res fautes formelles suggÃ©rÃ©es par PyLint\n\nDÃ©limiter des parties dans votre code pour rendre sa structure plus lisible. Si des parties vous semblent Ãªtre dans le dÃ©sordre, vous pouvez rÃ©ordonner le script (mais nâ€™oubliez pas de le tester)\n\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli2\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\nLe code est maintenant lisible, il obtient Ã  ce stade une note formelle proche de 10. Mais il nâ€™est pas encore totalement intelligible ou fiable. Il y a notamment quelques redondances de code auxquelles nous allons nous attaquer par la suite. NÃ©anmoins, avant cela, occupons-nous de mieux gÃ©rer certains paramÃ¨tres du script: jetons dâ€™API et chemin des fichiers."
  },
  {
    "objectID": "chapters/application.html#Ã©tape-3-gestion-des-paramÃ¨tres",
    "href": "chapters/application.html#Ã©tape-3-gestion-des-paramÃ¨tres",
    "title": "Application",
    "section": "Ã‰tape 3: gestion des paramÃ¨tres",
    "text": "Ã‰tape 3: gestion des paramÃ¨tres\nLâ€™exÃ©cution du code et les rÃ©sultats obtenus dÃ©pendent de certains paramÃ¨tres dÃ©finis dans le code. Lâ€™Ã©tude de rÃ©sultats alternatifs, en jouant sur des variantes des (hyper)paramÃ¨tres, est Ã  ce stade compliquÃ©e car il est nÃ©cessaire de parcourir le code pour trouver ces paramÃ¨tres. De plus, certains paramÃ¨tres personnels comme des jetons dâ€™API ou des mots de passe nâ€™ont pas vocation Ã  Ãªtre prÃ©sents dans le code.\nIl est plus judicieux de considÃ©rer ces paramÃ¨tres comme des variables dâ€™entrÃ©e du script. Cela peut Ãªtre fait de deux maniÃ¨res:\n\nAvec des arguments optionnels appelÃ©s depuis la ligne de commande (Application 3a). Cela peut Ãªtre pratique pour mettre en oeuvre des tests automatisÃ©s mais nâ€™est pas forcÃ©ment pertinent pour toutes les variables. Nous allons montrer cet usage avec le nombre dâ€™arbres de notre random forest ;\nEn utilisant un fichier de configuration dont les valeurs sont importÃ©es dans le script principal (Application 3b).\n\n\n\nUn exemple de dÃ©finition dâ€™un argument pour lâ€™utilisation en ligne de commande\n\n\n\nprenom.py\n\nimport argparse\nparser = argparse.ArgumentParser(description=\"Qui Ãªtes-vous?\")\nparser.add_argument(\n    \"--prenom\", type=str, default=\"Toto\", help=\"Un prÃ©nom Ã  afficher\"\n)\nargs = parser.parse_args()\nprint(args.prenom)\n\nExemples dâ€™utilisations en ligne de commande\n\n\nterminal\n\n$ python prenom.py\n$ python prenom.py --prenom \"Zinedine\"\n\n\n\n\n\n\n\n\nApplication 3a: ParamÃ©trisation du script\n\n\n\n\nEn sâ€™inspirant de lâ€™exemple ci-dessus ğŸ‘†ï¸, crÃ©er une variable n_trees qui peut Ã©ventuellement Ãªtre paramÃ©trÃ©e en ligne de commande et dont la valeur par dÃ©faut est 20 ;\nTester cette paramÃ©trisation en ligne de commande avec la valeur par dÃ©faut puis 2, 10 et 50 arbres.\n\n\n\nLâ€™exercice suivant permet de mettre en application le fait de paramÃ©triser un script en utilisant des variables dÃ©finies dans un fichier YAML.\n\n\n\n\n\n\nApplication 3b: La configuration dans un fichier YAML\n\n\n\nNous allons mettre 4 paramÃ¨tres dans notre YAML. Celui-ci prendra la forme suivante:\n\n\nconfig.yaml\n\njeton_api: ####\ndata_path: ####\n\nAvec #### des valeurs Ã  remplacer.\n\nCrÃ©er Ã  la racine du projet un fichier config.yaml Ã  partir du modÃ¨le ğŸ‘†ï¸ ;\nRepÃ©rer les valeurs dans le code associÃ© et complÃ©ter.\n\nMaintenant, nous allons exploiter ce fichier:\n\nPour Ã©viter dâ€™avoir Ã  le faire plus tard, crÃ©er une fonction import_yaml_config qui prend en argument le chemin dâ€™un fichier YAML et renvoie le contenu de celui-ci en output. Vous pouvez suivre le conseil du chapitre sur la QualitÃ© du code en adoptant le type hinting ;\n\n\n\nIndice si vous ne trouvez pas comment lire un fichier YAML\n\nSi le fichier sâ€™appelle toto.yaml, vous pouvez lâ€™importer de cette maniÃ¨re:\n\nwith open(\"toto.yaml\", \"r\", encoding=\"utf-8\") as stream:\n    dict_config = yaml.safe_load(stream)\n\n\n\nDans la fonction import_yaml_config, crÃ©er une condition logique pour tenir compte du fait que le YAML de configuration peut ne pas exister4 ;\n\n\n\nIndice si vous ne savez comment conditionner la crÃ©ation de la configuration Ã  lâ€™existence du fichier\n\nVoici la ligne qui peut vous aider. Lâ€™idÃ©al est dâ€™insÃ©rer ceci dans import_yaml_config:\n\nCONFIG_PATH = 'config.yaml'\nconfig = {}\nif os.path.exists(CONFIG_PATH):\n    # lecture du fichier\n\n\n\nUtiliser le canevas de code suivant pour crÃ©er les variables adÃ©quates\n\n\nAPI_TOKEN = config.get(\"jeton_api\")\nTRAIN_PATH = config.get(\"train_path\", \"train.csv\")\nTEST_PATH = config.get(\"test_path\", \"test.csv\")\nTEST_FRACTION = config.get(\"test_fraction\", .1)\n\net remplacer dans le code ;\n\nTester en ligne de commande que lâ€™exÃ©cution du fichier est toujours sans erreur et sinon corriger ;\nRefaire un diagnostic avec PyLint et corriger les Ã©ventuels messages ;\nCrÃ©er un fichier .gitignore (cf.Â Chapitre Git). Ajouter dans ce fichier config.yaml car il ne faut pas committer ce fichier. Au passage ajouter __pycache__/ au .gitignore5, cela Ã©vitera dâ€™avoir Ã  le faire ultÃ©rieurement ;\nCrÃ©er un fichier README.md oÃ¹ vous indiquez quâ€™il faut crÃ©er un fichier config.yaml pour pouvoir utiliser lâ€™API.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli3\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#Ã©tape-4-privilÃ©gier-la-programmation-fonctionnelle",
    "href": "chapters/application.html#Ã©tape-4-privilÃ©gier-la-programmation-fonctionnelle",
    "title": "Application",
    "section": "Ã‰tape 4 : PrivilÃ©gier la programmation fonctionnelle",
    "text": "Ã‰tape 4 : PrivilÃ©gier la programmation fonctionnelle\nNous allons mettre en fonctions les parties importantes de lâ€™analyse. Ceci facilitera lâ€™Ã©tape ultÃ©rieure de modularisation de notre projet.\nCet exercice Ã©tant chronophage, il nâ€™est pas obligatoire de le rÃ©aliser en entier. Lâ€™important est de comprendre la dÃ©marche et dâ€™adopter frÃ©quemment une approche fonctionnelle6. Pour obtenir une chaine entiÃ¨rement fonctionnalisÃ©e, vous pouvez reprendre le checkpoint.\n\n\n\n\n\n\nApplication 4: adoption des standards de programmation fonctionnelle\n\n\n\nCette application peut Ãªtre chronophage, vous pouvez aller plus ou moins loin dans la fonctionalisation de votre script en fonction du temps dont vous disposez.\n\nCrÃ©er une fonction gÃ©nÃ©rique pour rÃ©duire la redondance de code dans lâ€™Ã©tape dâ€™exploration des donnÃ©es oÃ¹ on utilise split ;\nCrÃ©er une fonction qui rÃ©alise le split train/test en fonction dâ€™un paramÃ¨tre reprÃ©sentant la proportion de lâ€™Ã©chantillon de test et dâ€™arguments optionnels sur les chemins dâ€™Ã©criture des deux Ã©chantillons en csv.\nCrÃ©er une fonction qui intÃ¨gre les diffÃ©rentes Ã©tapes du pipeline (preprocessing et dÃ©finition du modÃ¨le). Cette fonction prend en paramÃ¨tre le nombre dâ€™arbres (argument obligatoire) et des arguments optionnels supplÃ©mentaires (les colonnes sur lesquelles sâ€™appliquent les diffÃ©rentes Ã©tapes du pipeline, max_depth et max_features).\nCrÃ©er une fonction dâ€™Ã©valuation renvoyant le score obtenu et la matrice de confusion, Ã  lâ€™issue dâ€™une estimation (mais cette estimation est faite en amont de la fonction, pas au sein de celle-ci)\nDÃ©placer toutes les fonctions ensemble, en dÃ©but de script.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLe fait dâ€™appliquer des fonctions a dÃ©jÃ  amÃ©liorÃ© la fiabilitÃ© du processus en rÃ©duisant le nombre dâ€™erreurs de copier-coller. NÃ©anmoins, pour vraiment fiabiliser le processus, il faudrait utiliser un pipeline de transformations de donnÃ©es.\nCeci nâ€™est pas encore au programme du cours mais le sera dans une prochaine version.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli4\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\nCela ne se remarque pas encore vraiment car nous avons de nombreuses dÃ©finitions de fonctions mais notre chaine de production est beaucoup plus concise (le script fait environ 300 lignes dont 250 de dÃ©finitions de fonctions gÃ©nÃ©riques). Cette auto-discipline facilitera grandement les Ã©tapes ultÃ©rieures. Cela aurait Ã©tÃ© nÃ©anmoins beaucoup moins coÃ»teux en temps dâ€™adopter ces bons gestes de maniÃ¨re plus prÃ©coce."
  },
  {
    "objectID": "chapters/application.html#Ã©tape-1-modularisation",
    "href": "chapters/application.html#Ã©tape-1-modularisation",
    "title": "Application",
    "section": "Ã‰tape 1 : modularisation",
    "text": "Ã‰tape 1 : modularisation\nNous allons profiter de la modularisation pour adopter une structure applicative pour notre code. Celui-ci nâ€™Ã©tant en effet plus lancÃ© que depuis la ligne de commande, on peut considÃ©rer quâ€™on construit une application gÃ©nÃ©rique oÃ¹ un script principal (main.py) encapsule des Ã©lÃ©ments issus dâ€™autres scripts Python.\n\n\n\n\n\n\nApplication 5: modularisation\n\n\n\n\nDÃ©placer les fonctions dans une sÃ©rie de fichiers dÃ©diÃ©s:\n\nimport_data.py: fonctions dâ€™import et dâ€™exploration de donnÃ©es\nbuild_features.py: fonctions regroupant la dÃ©finition des Ã©chantillons dâ€™apprentissage et de test ainsi que le pipeline\ntrain_evaluate.py: fonctions dâ€™Ã©valuation du modÃ¨le\n\nSpÃ©cifier les dÃ©pendances (i.e.Â les packages Ã  importer) dans les modules pour que ceux-ci puissent sâ€™exÃ©cuter indÃ©pendamment ;\nRenommer titanic.py en main.py pour suivre la convention de nommage des projets Python ;\nImporter les fonctions nÃ©cessaires Ã  partir des modules.\nVÃ©rifier que tout fonctionne bien en exÃ©cutant le script main Ã  partir de la ligne de commande :\n\n\n\nterminal\n\n$ python main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli5\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#Ã©tape-2-adopter-une-architecture-standardisÃ©e-de-projet",
    "href": "chapters/application.html#Ã©tape-2-adopter-une-architecture-standardisÃ©e-de-projet",
    "title": "Application",
    "section": "Ã‰tape 2 : adopter une architecture standardisÃ©e de projet",
    "text": "Ã‰tape 2 : adopter une architecture standardisÃ©e de projet\nOn dispose maintenant dâ€™une application Python fonctionnelle. NÃ©anmoins, le projet est certes plus fiable mais sa structuration laisse Ã  dÃ©sirer et il serait difficile de rentrer Ã  nouveau dans le projet dans quelques temps.\n\n\nEtat actuel du projet ğŸ™ˆ\n\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ data.csv\nâ”œâ”€â”€ train.csv\nâ”œâ”€â”€ test.csv\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ config.yaml\nâ”œâ”€â”€ import_data.py\nâ”œâ”€â”€ build_features.py\nâ”œâ”€â”€ train_evaluate.py\nâ”œâ”€â”€ titanic.ipynb\nâ””â”€â”€ main.py\n\nComme cela est expliquÃ© dans la partie Structure des projets, on va adopter une structure certes arbitraire mais qui va faciliter lâ€™autodocumentation de notre projet. De plus, une telle structure va faciliter des Ã©volutions optionnelles comme la packagisation du projet. Passer dâ€™une structure modulaire bien faite Ã  un package est quasi-immÃ©diat en Python.\nOn va donc modifier lâ€™architecture de notre projet pour la rendre plus standardisÃ©e. Pour cela, on va sâ€™inspirer des structures cookiecutter qui gÃ©nÃ¨rent des templates de projet. En lâ€™occurrence notre source dâ€™inspiration sera le template datascience issu dâ€™un effort communautaire.\n\n\n\n\n\n\nNote\n\n\n\nLâ€™idÃ©e de cookiecutter est de proposer des templates que lâ€™on utilise pour initialiser un projet, afin de bÃ¢tir Ã  lâ€™avance une structure Ã©volutive. La syntaxe Ã  utiliser dans ce cas est la suivante :\n\n\nterminal\n\n$ pip install cookiecutter\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science\n\nIci, on a dÃ©jÃ  un projet, on va donc faire les choses dans lâ€™autre sens : on va sâ€™inspirer de la structure proposÃ©e afin de rÃ©organiser celle de notre projet selon les standards communautaires.\n\n\nEn sâ€™inspirant du cookiecutter data science on va adopter la structure suivante:\n\n\nStructure recommandÃ©e\n\napplication\nâ”œâ”€â”€ main.py\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ data\nâ”‚   â”œâ”€â”€ raw\nâ”‚   â”‚   â””â”€â”€ data.csv\nâ”‚   â””â”€â”€ derived\nâ”‚       â”œâ”€â”€ test.csv\nâ”‚       â””â”€â”€ train.csv\nâ”œâ”€â”€ configuration\nâ”‚   â””â”€â”€ config.yaml\nâ”œâ”€â”€ notebooks\nâ”‚   â””â”€â”€ titanic.ipynb\nâ””â”€â”€ src\n    â”œâ”€â”€ data\n    â”‚   â””â”€â”€ import_data.py\n    â”œâ”€â”€ pipeline\n    â”‚   â””â”€â”€ build_pipeline.py\n    â””â”€â”€ models\n        â””â”€â”€ train_evaluate.py\n\n\n\n\n\n\n\nApplication 6: adopter une structure lisible\n\n\n\n\n(optionnel) Analyser et comprendre la structure de projet proposÃ©e par le template ;\nModifier lâ€™arborescence du projet selon le modÃ¨le ;\nMettre Ã  jour lâ€™import des dÃ©pendances, le fichier de configuration et main.py avec les nouveaux chemins ;\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli6\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#Ã©tape-3-indiquer-lenvironnement-minimal-de-reproductibilitÃ©",
    "href": "chapters/application.html#Ã©tape-3-indiquer-lenvironnement-minimal-de-reproductibilitÃ©",
    "title": "Application",
    "section": "Ã‰tape 3: indiquer lâ€™environnement minimal de reproductibilitÃ©",
    "text": "Ã‰tape 3: indiquer lâ€™environnement minimal de reproductibilitÃ©\nLe script main.py nÃ©cessite un certain nombre de packages pour Ãªtre fonctionnel. Chez vous les packages nÃ©cessaires sont bien sÃ»r installÃ©s mais Ãªtes-vous assurÃ© que câ€™est le cas chez la personne qui testera votre code ?\nAfin de favoriser la portabilitÃ© du projet, il est dâ€™usage de â€œfixer lâ€™environnementâ€, câ€™est-Ã -dire dâ€™indiquer dans un fichier toutes les dÃ©pendances utilisÃ©es ainsi que leurs version. Nous proposons de crÃ©er un fichier requirements.txt minimal, sur lequel nous reviendrons dans la partie consacrÃ©e aux environnements reproductibles.\nLe fichier requirements.txt est conventionnellement localisÃ© Ã  la racine du projet. Ici on ne va pas fixer les versions, on raffinera ce fichier ultÃ©rieurement.\n\n\n\n\n\n\nApplication 7: crÃ©ation du requirements.txt\n\n\n\n\nCrÃ©er un fichier requirements.txt avec la liste des packages nÃ©cessaires\nAjouter une indication dans README.md sur lâ€™installation des packages grÃ¢ce au fichier requirements.txt\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli7\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#stockageS3",
    "href": "chapters/application.html#stockageS3",
    "title": "Application",
    "section": "Ã‰tape 4 : stocker les donnÃ©es de maniÃ¨re externe",
    "text": "Ã‰tape 4 : stocker les donnÃ©es de maniÃ¨re externe\n\n\n\n\n\n\nPour en savoir plus sur le systÃ¨me de stockage S3\n\n\n\n\n\nPour mettre en oeuvre cette Ã©tape, il peut Ãªtre utile de comprendre un peu comme fonctionne le SSP Cloud. Vous devrez suivre la documentation du SSP Cloud pour la rÃ©aliser. Une aide-mÃ©moire est Ã©galement disponible dans le cours de 2e annÃ©e de lâ€™ENSAE Python pour la data science.\n\n\n\nLe chapitre sur la structure des projets dÃ©veloppe lâ€™idÃ©e quâ€™il est recommandÃ© de converger vers un modÃ¨le oÃ¹ environnements dâ€™exÃ©cution, de stockage du code et des donnÃ©es sont conceptuellement sÃ©parÃ©s. Ce haut niveau dâ€™exigence est un gain de temps important lors de la mise en production car au cours de cette derniÃ¨re, le projet est amenÃ© Ã  Ãªtre exÃ©cutÃ© sur une infrastructure informatique dÃ©diÃ©e quâ€™il est bon dâ€™anticiper.\nA lâ€™heure actuelle, les donnÃ©es sont stockÃ©es dans le dÃ©pÃ´t. Câ€™est une mauvaise pratique. En premier lieu, Git nâ€™est techniquement pas bien adaptÃ© au stockage de donnÃ©es. Ici ce nâ€™est pas trÃ¨s grave car il ne sâ€™agit pas de donnÃ©es volumineuses et ces derniÃ¨res ne sont pas modifiÃ©es au cours de notre chaine de traitement. La raison principale est que les donnÃ©es traitÃ©es par les data scientists sont gÃ©nÃ©ralement soumises Ã  des clauses de confidentialitÃ©s (RGPD, secret statistiqueâ€¦). Mettre ces donnÃ©es sous contrÃ´le de version câ€™est prendre le risque de les divulguer Ã  un public non habilitÃ©. Il est donc recommandÃ© de privilÃ©gier des outils techniques adaptÃ©s au stockage de donnÃ©es.\nLâ€™idÃ©al, dans notre cas, est dâ€™utiliser une solution de stockage externe. On va utiliser pour cela MinIO, la solution de stockage de type S3 offerte par le SSP Cloud. Cela nous permettra de supprimer les donnÃ©es de Github tout en maintenant la reproductibilitÃ© de notre projet 7.\n\n\n\n\n\n\nApplication 8: utilisation dâ€™un systÃ¨me de stockage distant\n\n\n\nA partir de la ligne de commande, utiliser lâ€™utilitaire MinIO pour copier les donnÃ©es data/raw/data.csv vers votre bucket personnel. Les donnÃ©es intermÃ©diaires peuvent Ãªtre laissÃ©es en local mais doivent Ãªtre ajoutÃ©es au .gitignore.\n\n\nIndice\n\nStructure Ã  adopter:\n\n\nterminal\n\n$ mc cp data/raw/data.csv s3/$BUCKET_PERSONNEL/ensae-reproductibilite/data/raw/data.csv\n\nen modifiant $BUCKET_PERSONNEL, lâ€™emplacement de votre bucket personnel\n\nPour se simplifier la vie, on va utiliser des URL de tÃ©lÃ©chargement des fichiers (comme si ceux-ci Ã©taient sur nâ€™importe quel espace de stockage) plutÃ´t que dâ€™utiliser une librairie S3 compatible comme boto3 ou s3fs. Par dÃ©faut, le contenu de votre bucket est privÃ©, seul vous y avez accÃ¨s. NÃ©anmoins, vous pouvez rendre accessible Ã  tous en lecture le contenu de votre bucket en faisant lui donnant des droits anonymes. Pour cela, en ligne de commande, faire:\n\n\nterminal\n\n$ mc anonymous set download s3/$BUCKET_PERSONNEL/ensae-reproductibilite/data/raw/\n\nen modifiant $BUCKET_PERSONNEL. Les URL de tÃ©lÃ©chargement seront de la forme https://minio.lab.sspcloud.fr/$BUCKET_PERSONNEL/ensae-reproductibilite/data/raw/data.csv\n\nModifier configuration/config.yaml pour utiliser directement les URL dans lâ€™import ;\nModifier les valeurs par dÃ©faut dans votre code ;\nAjouter le dossier data/ au .gitignore\nSupprimer le dossier data de votre projet et faites git rm --cached -r data\nVÃ©rifier le bon fonctionnement de votre application.\n\nMaintenant quâ€™on a arrangÃ© la structure de notre projet, câ€™est lâ€™occasion de supprimer le code qui nâ€™est plus nÃ©cessaire au bon fonctionnement de notre projet (cela rÃ©duit la charge de maintenance8).\nPour vous aider, vous pouvez utiliser vulture de maniÃ¨re itÃ©rative pour vous assister dans le nettoyage de votre code.\n\n\nterminal\n\n$ vulture main.py src/\n\n\n\nExemple de sortie\n\n\n\nterminal\n\n$ vulture main.py src/\n\nmain.py:21: unused variable 'jeton_api' (60% confidence)\nmain.py:36: unused variable 'ticket_count' (60% confidence)\nmain.py:37: unused variable 'name_count' (60% confidence)\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli8\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#Ã©tape-1-proposer-des-tests-unitaires-optionnel",
    "href": "chapters/application.html#Ã©tape-1-proposer-des-tests-unitaires-optionnel",
    "title": "Application",
    "section": "Ã‰tape 1 : proposer des tests unitaires (optionnel)",
    "text": "Ã‰tape 1 : proposer des tests unitaires (optionnel)\nNotre code comporte un certain nombre de fonctions gÃ©nÃ©riques. On peut vouloir tester leur usage sur des donnÃ©es standardisÃ©es, diffÃ©rentes de celles du Titanic.\nMÃªme si la notion de tests unitaires prend plus de sens dans un package, nous pouvons proposer dans le projet des exemples dâ€™utilisation de la fonction, ceci peut Ãªtre pÃ©dagogique.\nNous allons utiliser unittest pour effectuer des tests unitaires. Cette approche nÃ©cessite quelques notions de programmation orientÃ©e objet ou une bonne discussion avec ChatGPT.\n\n\n\n\n\n\nApplication 9: test unitaire (optionnel)\n\n\n\nDans le dossier tests/, crÃ©er avec lâ€™aide de ChatGPT ou de Copilot un test pour la fonction split_and_count.\n\nEffectuer le test unitaire en ligne de commande avec unittest (python -m unittest tests/test_split.py). Corriger le test unitaire en cas dâ€™erreur.\nSi le temps le permet, proposer des variantes ou dâ€™autres tests.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli9\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLorsquâ€™on effectue des tests unitaires, on cherche gÃ©nÃ©ralement Ã  tester le plus de lignes possibles de son code. On parle de taux de couverture (coverage rate) pour dÃ©signer la statistique mesurant cela.\nCela peut sâ€™effectuer de la maniÃ¨re suivante avec le package coverage:\n\n\nterminal\n\n$ coverage run -m unittest tests/test_create_variable_title.py\n$ coverage report -m\n\nName                                  Stmts   Miss  Cover   Missing\n-------------------------------------------------------------------\nsrc/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113\ntests/test_create_variable_title.py      21      1    95%   54\n-------------------------------------------------------------------\nTOTAL                                    55     22    60%\nLe taux de couverture est souvent mis en avant par les gros projets comme indicateur de leur qualitÃ©. Il existe dâ€™ailleurs des badges Github dÃ©diÃ©s."
  },
  {
    "objectID": "chapters/application.html#Ã©tape-2-transformer-son-projet-en-package-optionnel",
    "href": "chapters/application.html#Ã©tape-2-transformer-son-projet-en-package-optionnel",
    "title": "Application",
    "section": "Ã‰tape 2 : transformer son projet en package (optionnel)",
    "text": "Ã‰tape 2 : transformer son projet en package (optionnel)\nNotre projet est modulaire, ce qui le rend assez simple Ã  transformer en package, en sâ€™inspirant de la structure du cookiecutter adaptÃ©, issu de cet ouvrage.\nOn va crÃ©er un package nommÃ© titanicml qui encapsule tout notre code et qui sera appelÃ© par notre script main.py. La structure attendue est la suivante:\n\n\nStructure visÃ©e\n\nensae-reproductibilite-application\nâ”œâ”€â”€ docs                                    â” \nâ”‚   â”œâ”€â”€ main.py                             â”‚ \nâ”‚   â””â”€â”€ notebooks                           â”‚ Package documentation and examples\nâ”‚       â””â”€â”€ titanic.ipynb                   â”‚ \nâ”œâ”€â”€ configuration                           â” Configuration (pas Ã  partager avec Git)\nâ”‚   â””â”€â”€ config.yaml                         â”˜ \nâ”œâ”€â”€ README.md                                \nâ”œâ”€â”€ pyproject.toml                          â” \nâ”œâ”€â”€ requirements.txt                        â”‚\nâ”œâ”€â”€ titanicml                               â”‚                \nâ”‚   â”œâ”€â”€ __init__.py                         â”‚ Package source code, metadata\nâ”‚   â”œâ”€â”€ data                                â”‚ and build instructions \nâ”‚   â”‚   â”œâ”€â”€ import_data.py                  â”‚  \nâ”‚   â”‚   â””â”€â”€ test_create_variable_title.py   â”‚   \nâ”‚   â”œâ”€â”€ features                            â”‚\nâ”‚   â”‚   â””â”€â”€ build_features.py               â”‚\nâ”‚   â””â”€â”€ models                              â”‚\nâ”‚       â””â”€â”€ train_evaluate.py               â”˜\nâ””â”€â”€ tests                                   â”\n    â””â”€â”€ test_create_variable_title.py       â”˜ Package tests\n\n\n\nRappel: structure actuelle\n\nensae-reproductibilite-application\nâ”œâ”€â”€ notebooks                                 \nâ”‚   â””â”€â”€ titanic.ipynb                  \nâ”œâ”€â”€ configuration                                 \nâ”‚   â””â”€â”€ config.yaml                  \nâ”œâ”€â”€ main.py                              \nâ”œâ”€â”€ README.md                 \nâ”œâ”€â”€ requirements.txt                      \nâ””â”€â”€ src \n    â”œâ”€â”€ data                                \n    â”‚   â”œâ”€â”€ import_data.py                    \n    â”‚   â””â”€â”€ test_create_variable_title.py      \n    â”œâ”€â”€ features                           \n    â”‚   â””â”€â”€ build_features.py      \n    â””â”€â”€ models                          \n        â””â”€â”€ train_evaluate.py              \n\nIl existe plusieurs frameworks pour construire un package. Nous allons privilÃ©gier Poetry Ã  Setuptools.\n\n\n\n\n\n\nNote\n\n\n\nPour crÃ©er la structure minimale dâ€™un package, le plus simple est dâ€™utiliser le cookiecutter adaptÃ©, issu de cet ouvrage.\nComme on a dÃ©jÃ  une structure trÃ¨s modulaire, on va plutÃ´t recrÃ©er cette structure dans notre projet dÃ©jÃ  existant. En fait, il ne manque quâ€™un fichier essentiel, le principal distinguant un projet classique dâ€™un package : pyproject.toml.\n\n\nterminal\n\n$ cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git\n\n\n\nDÃ©rouler pour voir les choix possibles\n\nauthor_name [Monty Python]: Daffy Duck\npackage_name [mypkg]: titanicml\npackage_short_description []: Impressive Titanic survival analysis\npackage_version [0.1.0]: \npython_version [3.9]: \nSelect open_source_license:\n1 - MIT\n2 - Apache License 2.0\n3 - GNU General Public License v3.0\n4 - Creative Commons Attribution 4.0\n5 - BSD 3-Clause\n6 - Proprietary\n7 - None\nChoose from 1, 2, 3, 4, 5, 6 [1]: \nSelect include_github_actions:\n1 - no\n2 - ci\n3 - ci+cd\nChoose from 1, 2, 3 [1]:\n\n\n\n\n\n\n\n\n\nApplication 10: packagisation (optionnel)\n\n\n\n\nRenommer le dossier titanicml pour respecter la nouvelle arborescence ;\nCrÃ©er un fichier pyproject.toml sur cette base ;\n\n\n\n\npyproject.toml\n\n[tool.poetry]\nname = \"titanicml\"\nversion = \"0.0.1\"\ndescription = \"Awesome Machine Learning project\"\nauthors = [\"Daffy Duck &lt;daffy.duck@fauxmail.fr&gt;\", \"Mickey Mouse\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.pytest.ini_options]\nlog_cli = true\nlog_cli_level = \"WARNING\"\nlog_cli_format = \"%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)\"\nlog_cli_date_format = \"%Y-%m-%d %H:%M:%S\"\n\n\n\nCrÃ©er le dossier docs et mettre les fichiers indiquÃ©s dedans\nDans titanicml/, crÃ©er un fichier __init__.py9\n\n\n\n\n__init__.py\n\nfrom .import_data import (\n    import_data, import_yaml_config\n)\nfrom .build_features import (\n    create_variable_title,\n    fill_na_titanic,\n    label_encoder_titanic,\n    check_has_cabin,\n    ticket_length\n)\nfrom .train_evaluate import random_forest_titanic\n\n__all__ = [\n    \"import_data\", \"import_yaml_config\",\n    \"create_variable_title\",\n    \"fill_na_titanic\",\n    \"label_encoder_titanic\",\n    \"check_has_cabin\",\n    \"ticket_length\",\n    \"random_forest_titanic\"\n]\n\n\n\nInstaller le package en local avec pip install -e .\nModifier le contenu de docs/main.py pour importer les fonctions de notre package titanicml et tester en ligne de commande notre fichier main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli10\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#anaconda",
    "href": "chapters/application.html#anaconda",
    "title": "Application",
    "section": "Ã‰tape 1 : un environnement pour rendre le projet portable",
    "text": "Ã‰tape 1 : un environnement pour rendre le projet portable\nPour quâ€™un projet soit portable, il doit remplir deux conditions:\n\nNe pas nÃ©cessiter de dÃ©pendance qui ne soient pas renseignÃ©es quelque part ;\nNe pas proposer des dÃ©pendances inutiles, qui ne sont pas utilisÃ©es dans le cadre du projet.\n\nLe prochain exercice vise Ã  mettre ceci en oeuvre. Comme expliquÃ© dans le chapitre portabilitÃ©, le choix du gestionnaire dâ€™environnement est laissÃ© libre. Il est recommandÃ© de privilÃ©gier venv si vous dÃ©couvrez la problÃ©matique de la portabilitÃ©.\n\nEnvironnement virtuel venvEnvironnement conda\n\n\nLâ€™approche la plus lÃ©gÃ¨re est lâ€™environnement virtuel. Nous avons en fait implicitement dÃ©jÃ  commencÃ© Ã  aller vers cette direction en crÃ©ant un fichier requirements.txt.\n\n\n\n\n\n\nApplication 11a: environnement virtuel venv\n\n\n\n\nExÃ©cuter pip freeze en ligne de commande et observer la (trÃ¨s) longue liste de package\nCrÃ©er lâ€™environnement virtuel titanic en sâ€™inspirant de la documentation officielle10 ou du chapitre dÃ©diÃ©\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin installÃ©\nActiver lâ€™environnement et vÃ©rifier lâ€™installation de Python maintenant utilisÃ©e par votre machine \nVÃ©rifier directement depuis la ligne de commande que Python exÃ©cute bien une commande11 avec:\n\n\n\nterminal\n\n$ python -c \"print('Hello')\"\n\n\nFaire la mÃªme chose mais avec import pandas as pd\nInstaller les packages Ã  partir du requirements.txt. Tester Ã  nouveau import pandas as pd pour comprendre la diffÃ©rence.\nExÃ©cuter pip freeze et comprendre la diffÃ©rence avec la situation prÃ©cÃ©dente.\nVÃ©rifier que le script main.py fonctionne bien. Sinon ajouter les packages manquants dans le requirements.txt et reprendre de maniÃ¨re itÃ©rative Ã  partir de la question 7.\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier Ã  Git.\n\n\n\nAide pour la question 4\n\nAprÃ¨s lâ€™activation, vous pouvez vÃ©rifier quel python est utilisÃ© de cette maniÃ¨re\n\n\nterminal\n\n(titanic) $ which python\n\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli11a\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\n\n\nLes environnements conda sont plus lourds Ã  mettre en oeuvre que les environnements virtuels mais peuvent permettre un contrÃ´le plus formel des dÃ©pendances.\n\n\n\n\n\n\nApplication 11b: environnement conda\n\n\n\n\nExÃ©cuter conda env export en ligne de commande et observer la (trÃ¨s) longue liste de package\nCrÃ©er un environnement titanic avec conda create\nActiver lâ€™environnement et vÃ©rifier lâ€™installation de Python maintenant utilisÃ©e par votre machine \nVÃ©rifier directement depuis la ligne de commande que Python exÃ©cute bien une commande12 avec:\n\n\n\nterminal\n\n$ python -c \"print('Hello')\"\n\n\nFaire la mÃªme chose mais avec import pandas as pd\nInstaller les packages quâ€™on avait listÃ© dans le requirements.txt prÃ©cÃ©demment. Ne pas faire un pip install -r requirements.txt afin de privilÃ©gier conda install\nExÃ©cuter Ã  nouveau conda env export et comprendre la diffÃ©rence avec la situation prÃ©cÃ©dente13.\nVÃ©rifier que le script main.py fonctionne bien. Sinon installer les packages manquants et reprndre de maniÃ¨re itÃ©rative Ã  partir de la question 7.\nQuand main.py fonctionne, faire conda env export &gt; environment.yml pour figer lâ€™environnement de travail.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli11b\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#shell",
    "href": "chapters/application.html#shell",
    "title": "Application",
    "section": "Ã‰tape 2: construire lâ€™environnement de notre application via un script shell",
    "text": "Ã‰tape 2: construire lâ€™environnement de notre application via un script shell\nLes environnements virtuels permettent de mieux spÃ©cifier les dÃ©pendances de notre projet, mais ne permettent pas de garantir une portabilitÃ© optimale. Pour cela, il faut recourir Ã  la technologie des conteneurs. Lâ€™idÃ©e est de construire une machine, en partant dâ€™une base quasi-vierge, qui permette de construire Ã©tape par Ã©tape lâ€™environnement nÃ©cessaire au bon fonctionnement de notre projet. Câ€™est le principe des conteneurs Docker .\nLeur mÃ©thode de construction Ã©tant un peu difficile Ã  prendre en main au dÃ©but, nous allons passer par une Ã©tape intermÃ©diaire afin de bien comprendre le processus de production.\n\nNous allons dâ€™abord crÃ©er un script shell, câ€™est Ã  dire une suite de commandes Linux permettant de construire lâ€™environnement Ã  partir dâ€™une machine vierge ;\nNous transformerons celui-ci en Dockerfile dans un deuxiÃ¨me temps. Câ€™est lâ€™objet de lâ€™Ã©tape suivante.\n\n\nEnvironnement virtuel venvEnvironnement conda\n\n\n\n\n\n\n\n\nApplication 12a : crÃ©er un fichier dâ€™installation de A Ã  Z\n\n\n\n\nCrÃ©er un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le dÃ©pÃ´t\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11a avec git checkout appli11a\nVia lâ€™explorateur de fichiers, crÃ©er le fichier install.sh Ã  la racine du projet avec le contenu suivant:\n\n\n\nScript Ã  crÃ©er sous le nom install.sh\n\n\n\ninstall.sh\n\n#!/bin/bash\n# Install Python\napt-get -y update\napt-get install -y python3-pip python3-venv\n# Create empty virtual environment\npython3 -m venv titanic\nsource titanic/bin/activate\n# Install project dependencies\npip install -r requirements.txt\n\n\n\nChanger les permissions sur le script pour le rendre exÃ©cutable\n\n\n\nterminal\n\n$ chmod +x install.sh\n\n\nExÃ©cuter le script depuis la ligne de commande avec des droits de super-utilisateur (nÃ©cessaires pour installer des packages via apt)\n\n\n\nterminal\n\n$ sudo ./install.sh\n\n\nVÃ©rifier que le script main.py fonctionne correctement dans lâ€™environnement virtuel crÃ©Ã©\n\n\n\nterminal\n\n$ source titanic/bin/activate\n$ python3 main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli12a\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication 12b : crÃ©er un fichier dâ€™installation de A Ã  Z\n\n\n\n\nCrÃ©er un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le dÃ©pÃ´t\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11b avec git checkout appli11b\nVia lâ€™explorateur de fichiers, crÃ©er le fichier install.sh Ã  la racine du projet avec le contenu suivant:\n\n\n\nScript Ã  crÃ©er sous le nom install.sh\n\n\n\ninstall.sh\n\napt-get -y update && apt-get -y install wget\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \\\n    rm -f Miniconda3-latest-Linux-x86_64.sh\nPATH=\"/miniconda/bin:${PATH}\"\n# Create environment\nconda create -n titanic pandas PyYAML scikit-learn -c conda-forge\nconda activate titanic\nPATH=\"/miniconda/envs/titanic/bin:${PATH}\"\npython main.py\n\n\n\nChanger les permissions sur le script pour le rendre exÃ©cutable\n\n\n\nterminal\n\n$ chmod +x install.sh\n\n\nExÃ©cuter le script depuis la ligne de commande avec des droits de super-utilisateur (nÃ©cessaires pour installer des packages via apt)\n\n\n\nterminal\n\n$ sudo ./install.sh\n\n\nVÃ©rifier que le script main.py fonctionne correctement dans lâ€™environnement virtuel crÃ©Ã©\n\n\n\nterminal\n\n$ conda activate titanic\n$ python3 main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli12b\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#docker",
    "href": "chapters/application.html#docker",
    "title": "Application",
    "section": "Ã‰tape 3: conteneuriser lâ€™application avec Docker",
    "text": "Ã‰tape 3: conteneuriser lâ€™application avec Docker\n\n\n\n\n\n\nNote\n\n\n\nCette application nÃ©cessite lâ€™accÃ¨s Ã  une version interactive de Docker. Il nâ€™y a pas beaucoup dâ€™instances en ligne disponibles.\nNous proposons deux solutions:\n\nInstaller Docker sur sa machine ;\nSe rendre sur lâ€™environnement bac Ã  sable Play with Docker\n\nSinon, elle peut Ãªtre rÃ©alisÃ©e en essai-erreur par le biais des services dâ€™intÃ©gration continue de Github  ou Gitlab . NÃ©anmoins, nous prÃ©senterons lâ€™utilisation de ces services plus tard, dans la prochaine partie.\n\n\nMaintenant quâ€™on sait que ce script prÃ©paratoire fonctionne, on va le transformer en Dockerfile pour anticiper la mise en production. Comme la syntaxe Docker est lÃ©gÃ¨rement diffÃ©rente de la syntaxe Linux classique (voir le chapitre portabilitÃ©), il va Ãªtre nÃ©cessaire de changer quelques instructions mais ceci sera trÃ¨s lÃ©ger.\nOn va tester le Dockerfile dans un environnement bac Ã  sable pour ensuite pouvoir plus facilement automatiser la construction de lâ€™image Docker.\n\n\n\n\n\n\nApplication 13: crÃ©ation de lâ€™image Docker\n\n\n\nSe placer dans un environnement avec Docker, par exemple Play with Docker\n\nCrÃ©ation du Dockerfile\n\nDans le terminal Linux, cloner votre dÃ©pÃ´t Github\nRepartir de la derniÃ¨re version Ã  disposition. Par exemple, si vous avez privilÃ©giÃ© lâ€™environnement virtuel venv, ce sera:\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli12a\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\nCrÃ©er via la ligne de commande un fichier texte vierge nommÃ© Dockerfile (la majuscule au dÃ©but du mot est importante)\n\n\n\nCommande pour crÃ©er un Dockerfile vierge depuis la ligne de commande\n\n\n\nterminal\n\n$ touch Dockerfile\n\n\n\nOuvrir ce fichier via un Ã©diteur de texte et copier le contenu suivant dedans:\n\n\n\nPremier Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\nWORKDIR ${HOME}/titanic\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCMD [\"python3\", \"main.py\"]\n\n\n\n\nConstruire (build) lâ€™image\n\nUtiliser docker build pour crÃ©er une image avec le tag my-python-app\n\n\n\nterminal\n\n$ docker build . -t my-python-app\n\n\nVÃ©rifier les images dont vous disposez. Vous devriez avoir un rÃ©sultat proche de celui-ci :\n\n\n\nterminal\n\n$ docker images\n\nREPOSITORY      TAG       IMAGE ID       CREATED              SIZE\nmy-python-app   latest    188957e16594   About a minute ago   879MB\n\n\nTester lâ€™image: dÃ©couverte du cache\nLâ€™Ã©tape de build a fonctionnÃ©: une image a Ã©tÃ© construite.\nMais fait-elle effectivement ce que lâ€™on attend dâ€™elle ?\nPour le savoir, il faut passer Ã  lâ€™Ã©tape suivante, lâ€™Ã©tape de run.\n\n\nterminal\n\n$ docker run -it my-python-app\n\npython3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory\nLe message dâ€™erreur est clair : Docker ne sait pas oÃ¹ trouver le fichier main.py. Dâ€™ailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont nÃ©cessaires pour faire tourner le code, par exemple le dossier src.\n\nAvant lâ€™Ã©tape CMD, copier les fichiers nÃ©cessaires sur lâ€™image afin que lâ€™application dispose de tous les Ã©lÃ©ments nÃ©cessaires pour Ãªtre en mesure de fonctionner.\n\n\n\nNouveau Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\nWORKDIR ${HOME}/titanic\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY main.py .\nCOPY src ./src\nCMD [\"python3\", \"main.py\"]\n\n\n\nRefaire tourner lâ€™Ã©tape de build\nRefaire tourner lâ€™Ã©tape de run. A ce stade, la matrice de confusion doit fonctionner ğŸ‰. Vous avez crÃ©Ã© votre premiÃ¨re application reproductible !\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIci, le cache permet dâ€™Ã©conomiser beaucoup de temps. Par besoin de refaire tourner toutes les Ã©tapes, Docker agit de maniÃ¨re intelligente en faisant tourner uniquement les Ã©tapes qui ont changÃ©.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli13\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#Ã©tape-1-mise-en-place-de-tests-automatisÃ©s",
    "href": "chapters/application.html#Ã©tape-1-mise-en-place-de-tests-automatisÃ©s",
    "title": "Application",
    "section": "Ã‰tape 1: mise en place de tests automatisÃ©s",
    "text": "Ã‰tape 1: mise en place de tests automatisÃ©s\nAvant dâ€™essayer de mettre en oeuvre la crÃ©ation de notre image Docker de maniÃ¨re automatisÃ©e, nous allons prÃ©senter la logique de lâ€™intÃ©gration continue en testant de maniÃ¨re automatisÃ©e notre script main.py.\nPour cela, nous allons partir de la structure proposÃ©e dans lâ€™action officielle. La documentation associÃ©e est ici. Des Ã©lÃ©ments succincts de prÃ©sentation de la logique dÃ©clarative des actions Github sont disponibles dans le chapitre sur la mise en production. NÃ©anmoins, la meilleure Ã©cole pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et dâ€™observer les actions Github mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !\n\n\n\n\n\n\nApplication 14: premier script dâ€™intÃ©gration continue\n\n\n\nA partir de lâ€™exemple prÃ©sent dans la documentation officielle de Github , on a dÃ©jÃ  une base de dÃ©part qui peut Ãªtre modifiÃ©e. Les questions suivantes permettront dâ€™automatiser les tests et le diagnostic qualitÃ© de notre code14\n\nCrÃ©er un fichier .github/workflows/test.yaml avec le contenu de lâ€™exemple de la documentation\nAvec lâ€™aide de la documentation, introduire une Ã©tape dâ€™installation des dÃ©pendances. Utiliser le fichier requirements.txt pour installer les dÃ©pendances.\nUtiliser pylint pour vÃ©rifier la qualitÃ© du code. Ajouter lâ€™argument --fail-under=6 pour renvoyer une erreur en cas de note trop basse15\nUtiliser une Ã©tape appelant notre application en ligne de commande (python main.py) pour tester que la matrice de confusion sâ€™affiche bien.\nAller voir votre test automatisÃ© dans lâ€™onglet Actions de votre dÃ©pÃ´t sur Github\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli14\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\nMaintenant, nous pouvons observer que lâ€™onglet Actions sâ€™est enrichi. Chaque commit va entraÃ®ner une sÃ©rie dâ€™actions automatisÃ©es.\nSi lâ€™une des Ã©tapes Ã©choue, ou si la note de notre projet est mauvaise, nous aurons une croix rouge (et nous recevrons un mail). On pourra ainsi dÃ©tecter, en dÃ©veloppant son projet, les moments oÃ¹ on dÃ©grade la qualitÃ© du script afin de la rÃ©tablir immÃ©diatemment."
  },
  {
    "objectID": "chapters/application.html#Ã©tape-2-automatisation-de-la-livraison-de-limage-docker",
    "href": "chapters/application.html#Ã©tape-2-automatisation-de-la-livraison-de-limage-docker",
    "title": "Application",
    "section": "Ã‰tape 2: Automatisation de la livraison de lâ€™image Docker",
    "text": "Ã‰tape 2: Automatisation de la livraison de lâ€™image Docker\nMaintenant, nous allons automatiser la mise Ã  disposition de notre image sur DockerHub (le lieu de partage des images Docker). Cela facilitera sa rÃ©utilisation mais aussi des valorisations ultÃ©rieures.\nLÃ  encore, nous allons utiliser une sÃ©rie dâ€™actions prÃ©-configurÃ©es.\nPour que Github puisse sâ€™authentifier auprÃ¨s de DockerHub, il va falloir dâ€™abord interfacer les deux plateformes. Pour cela, nous allons utiliser un jeton (token) DockerHub que nous allons mettre dans un espace sÃ©curisÃ© associÃ© Ã  votre dÃ©pÃ´t Github.\n\n\n\n\n\n\nApplication 15a: configuration\n\n\n\n\nSe rendre sur https://hub.docker.com/ et crÃ©er un compte. Il est recommandÃ© dâ€™associer ce compte Ã  votre compte Github.\nCrÃ©er un dÃ©pÃ´t public application-correction\nAller dans les paramÃ¨tres de votre compte et cliquer, Ã  gauche, sur Security\nCrÃ©er un jeton personnel dâ€™accÃ¨s, ne fermez pas lâ€™onglet en question, vous ne pouvez voir sa valeur quâ€™une fois.\nDans le dÃ©pÃ´t Github de votre projet, cliquer sur lâ€™onglet Settings et cliquer, Ã  gauche, sur Secrets and variables puis dans le menu dÃ©roulant en dessous sur Actions. Sur la page qui sâ€™affiche, aller dans la section Repository secrets\nCrÃ©er un jeton DOCKERHUB_TOKEN Ã  partir du jeton que vous aviez crÃ©Ã© sur Dockerhub. Valider\nCrÃ©er un deuxiÃ¨me secret nommÃ© DOCKERHUB_USERNAME ayant comme valeur le nom dâ€™utilisateur que vous avez crÃ©Ã© sur Dockerhub\n\n\n\nEtape optionnelle supplÃ©mentaire si on met en production un site web\n\n\nDans le dÃ©pÃ´t Github de votre projet, cliquer sur lâ€™onglet Settings et cliquer, Ã  gauche, sur Actions. Donner les droits dâ€™Ã©criture Ã  vos actions sur le dÃ©pÃ´t du projet (ce sera nÃ©cessaire pour Github Pages)\n\n\n\n\n\nA ce stade, nous avons donnÃ© les moyens Ã  Github de sâ€™authentifier avec notre identitÃ© sur Dockerhub. Il nous reste Ã  mettre en oeuvre lâ€™action en sâ€™inspirant de la documentation officielle. On ne va modifier que trois Ã©lÃ©ments dans ce fichier. Effectuer les actions suivantes:\n\n\n\n\n\n\nApplication 15b: automatisation de lâ€™image Docker\n\n\n\n\nEn sâ€™inspirant de ce template, crÃ©er le fichier .github/workflows/prod.yml qui va build et push lâ€™image sur le DockerHub. Il va Ãªtre nÃ©cessaire de changer lÃ©gÃ¨rement ce modÃ¨le :\n\nRetirer la condition restrictive sur les commits pour lesquels sont lancÃ©s cette automatisation. Pour cela, remplacer le contenu de on de sorte Ã  avoir on: push:   branches:     - main     - dev\nChanger le tag Ã  la fin pour mettre username/application-correction:latest oÃ¹ username est le nom dâ€™utilisateur sur DockerHub;\nOptionnel: changer le nom de lâ€™action\n\nFaire un commit et un push de ces fichiers\n\nComme on est fier de notre travail, on va afficher Ã§a avec un badge sur le README (partie optionnelle).\n\nSe rendre dans lâ€™onglet Actions et cliquer sur une des actions listÃ©es.\nEn haut Ã  droite, cliquer sur ...\nSÃ©lectionner Create status badge\nRÃ©cupÃ©rer le code Markdown proposÃ©\nCopier dans votre README.md le code markdown proposÃ©\n\n\n\nCrÃ©er le badge\n\n\n\n\n\nMaintenant, il nous reste Ã  tester notre application dans lâ€™espace bac Ã  sable ou en local, si Docker est installÃ©.\n\n\n\n\n\n\nApplication 15b (partie optionnelle): Tester lâ€™application\n\n\n\n\nSe rendre sur lâ€™environnement bac Ã  sable Play with Docker ou dans votre environnement Docker de prÃ©dilection.\nRÃ©cupÃ©rer et lancer lâ€™image :\n\n\n\nterminal\n\n$ docker run -it username/application-correction:latest\n\nğŸ‰ La matrice de confusion doit sâ€™afficher ! Vous avez grandement facilitÃ© la rÃ©utilisation de votre image.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli15\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#Ã©tape-1-dÃ©velopper-une-api-en-local",
    "href": "chapters/application.html#Ã©tape-1-dÃ©velopper-une-api-en-local",
    "title": "Application",
    "section": "Ã‰tape 1: dÃ©velopper une API en local",
    "text": "Ã‰tape 1: dÃ©velopper une API en local\nLe premier livrable devenu classique dans un projet impliquant du machine learning est la mise Ã  disposition dâ€™un modÃ¨le par le biais dâ€™une API (voir chapitre sur la mise en production). Le framework FastAPI va permettre de rapidement transformer notre application Python en une API fonctionnelle.\n\n\n\n\n\n\nSi vous prenez ce projet fil rouge en cours de route\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication 16: Mise Ã  disposition sous forme dâ€™API locale\n\n\n\n\nInstaller fastAPI et uvicorn puis les ajouter au requirements.txt\nRenommer le fichier main.py en train.py. Dans ce script, ajouter une sauvegarde du modÃ¨le aprÃ¨s lâ€™avoir entraÃ®nÃ©, sous le format joblib.\nFaire tourner\n\n\n\nterminal\n\n$ python train.py\n\npour enregistrer en local votre modÃ¨le de production.\n\nModifier les appels Ã  main.py dans votre Dockerfile et vos actions Github sous peine dâ€™essuyer des Ã©checs lors de vos actions Github aprÃ¨s le prochain push.\nAjouter model.joblib au .gitignore car Git nâ€™est pas fait pour ce type de fichiers.\n\nNous allons maintenant passer au dÃ©veloppement de lâ€™API. Comme dÃ©couvrir FastAPI nâ€™est pas lâ€™objet de cet enseignement, nous donnons directement le modÃ¨le pour crÃ©er lâ€™API. Si vous dÃ©sirez tester de vous-mÃªmes, vous pouvez crÃ©er votre fichier sans vous rÃ©fÃ©rer Ã  lâ€™exemple\n\nCrÃ©er le fichier api.py permettant dâ€™initialiser lâ€™API:\n\n\n\nFichier api.py\n\n\n\nsrc/models/train_evaluation.py\n\n\"\"\"A simple API to expose our trained RandomForest model for Tutanic survival.\"\"\"\nfrom fastapi import FastAPI\nfrom joblib import load\n\nimport pandas as pd\n\nmodel = load('model.joblib')\n\napp = FastAPI(\n    title=\"PrÃ©diction de survie sur le Titanic\",\n    description=\n    \"Application de prÃ©diction de survie sur le Titanic ğŸš¢ &lt;br&gt;Une version par API pour faciliter la rÃ©utilisation du modÃ¨le ğŸš€\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\n\n@app.get(\"/\", tags=[\"Welcome\"])\ndef show_welcome_page():\n    \"\"\"\n    Show welcome page with model name and version.\n    \"\"\"\n\n    return {\n        \"Message\": \"API de prÃ©diction de survie sur le Titanic\",\n        \"Model_name\": 'Titanic ML',\n        \"Model_version\": \"0.1\",\n    }\n\n\n@app.get(\"/predict\", tags=[\"Predict\"])\nasync def predict(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\"\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    prediction = \"Survived ğŸ‰\" if int(model.predict(df)) == 1 else \"Dead âš°ï¸\"\n\n    return prediction\n\n\n\nDÃ©ployer en local lâ€™API avec la commande\n\n\n\nterminal\n\n$ uvicorn api:app --reload --host \"0.0.0.0\" --port 5000\n\n\nA partir du README du service, se rendre sur lâ€™URL de dÃ©ploiement, ajouter /docs/ Ã  celui-ci et observer la documentation de lâ€™API\nSe servir de la documentation pour tester les requÃªtes /predict\nRÃ©cupÃ©rer lâ€™URL dâ€™une des requÃªtes proposÃ©es. La tester dans le navigateur et depuis Python avec requests :\n\nimport request\nrequests.get(url).json()\n\nUne fois que vous avez testÃ©, vous pouvez tuer lâ€™application en faisant CTRL+C. Retester votre bout de code Python et comprendre lâ€™origine du problÃ¨me.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli17\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#Ã©tape-2-dÃ©ployer-lapi-de-maniÃ¨re-manuelle",
    "href": "chapters/application.html#Ã©tape-2-dÃ©ployer-lapi-de-maniÃ¨re-manuelle",
    "title": "Application",
    "section": "Ã‰tape 2: dÃ©ployer lâ€™API de maniÃ¨re manuelle",
    "text": "Ã‰tape 2: dÃ©ployer lâ€™API de maniÃ¨re manuelle\n\n\n\n\n\n\nSi vous prenez ce projet fil rouge en cours de route\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli16\n\n\n\n\n\n\n\n\n\nA ce stade, nous avons dÃ©ployÃ© lâ€™API seulement localement, dans le cadre dâ€™un terminal qui tourne en arriÃ¨re-plan. Câ€™est une mise en production manuelle, pas franchement pÃ©renne. Ce mode de dÃ©ploiement est trÃ¨s pratique pour la phase de dÃ©veloppement, afin de sâ€™assurer que lâ€™API fonctionne comme attendu. Pour pÃ©renniser la mise en production, on va Ã©liminer lâ€™aspect artisanal de celle-ci.\nIl est temps de passer Ã  lâ€™Ã©tape de dÃ©ploiement, qui permettra Ã  notre API dâ€™Ãªtre accessible via une URL sur le web et dâ€™avoir un serveur, en arriÃ¨re plan, qui effectuera les opÃ©rations pour rÃ©pondre Ã  une requÃªte. Pour se faire, on va utiliser les possibilitÃ©s offertes par Kubernetes, sur lequel est basÃ© le SSP Cloud.\n\n\n\n\n\n\nApplication 17: Dockeriser lâ€™API (intÃ©gration continue)\n\n\n\n\nPour rendre la structure du projet plus lisible, dÃ©placer api.py -&gt; api/main.py\nCrÃ©er un script api/run.sh Ã  la racine du projet qui lance le script train.py puis dÃ©ploie localement lâ€™API\n\n\n\nFichier run.sh\n\n\n\napi/run.sh\n\n#/bin/bash\npython3 train.py\nuvicorn api.main:app --reload --host \"0.0.0.0\" --port 5000\n\n\n\nDonner au script api/run.sh des permissions dâ€™exÃ©cution : chmod +x api/run.sh\nAjouter COPY api ./api pour avoir les fichiers nÃ©cessaires au lancement dans lâ€™API dans lâ€™image\nChanger lâ€™instruction CMD du Dockerfile pour exÃ©cuter le script api/run.sh au lancement du conteneur (CMD [\"bash\", \"-c\", \"./api/run.sh\"])\nMettre Ã  jour votre requirements.txt pour tenir compte des nouveaux packages utilisÃ©s\nCommit et push les changements\nUne fois le CI terminÃ©, rÃ©cupÃ©rer la nouvelle image dans votre environnement de test de Docker et vÃ©rifier que lâ€™API se dÃ©ploie correctement\n\n\n\nNous avons prÃ©parÃ© la mise Ã  disposition de notre API mais Ã  lâ€™heure actuelle elle nâ€™est pas disponible de maniÃ¨re aisÃ©e car il est nÃ©cessaire de lancer manuellement une image Docker pour pouvoir y accÃ©der. Ce type de travail est la spÃ©cialitÃ© de Kubernetes que nous allons utiliser pour gÃ©rer la mise Ã  disposition de notre API.\n\n\n\n\n\n\nApplication 18b: Mettre Ã  disposition lâ€™API (dÃ©ploiement manuel)\n\n\n\nCette partie nÃ©cessite dâ€™avoir Ã  disposition une infrastructure cloud.\n\nCrÃ©er un dossier deployment Ã  la racine du projet qui va contenir les fichiers de configuration nÃ©cessaires pour dÃ©ployer sur un cluster Kubernetes\nEn vous inspirant de la documentation, y ajouter un premier fichier deployment.yaml qui va spÃ©cifier la configuration du Pod Ã  lancer sur le cluster\n\n\n\nFichier deployment/deployment.yaml\n\n\n\ndeployment/deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n        image: linogaliana/application-correction:latest\n        ports:\n        - containerPort: 5000\n\n\n\nEn vous inspirant de la documentation, y ajouter un second fichier service.yaml qui va crÃ©er une ressource Service permettant de donner une identitÃ© fixe au Pod prÃ©cÃ©demment crÃ©Ã© au sein du cluster\n\n\n\nFichier deployment/service.yaml\n\n\n\ndeployment/service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: titanic-service\nspec:\n  selector:\n    app: titanic\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 5000\n\n\n\nEn vous inspirant de la documentation, y ajouter un troisiÃ¨me fichier ingress.yaml qui va crÃ©er une ressource Ingress permettant dâ€™exposer le service via une URL en dehors du cluster\n\n\n\nFichier deployment/ingress.yaml\n\n\n\n\ndeployment/ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: titanic-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n1    - # METTRE URL ICI\n  rules:\n2  - host: # METTRE URL ICI\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: titanic-service\n            port:\n              number: 80\n\n\n\n1\n\nMettez lâ€™URL auquel vous voulez exposer votre service. Sur le modÃ¨le de titanic.kub.sspcloud.fr (mais ne tentez pas celui-lÃ , il est dÃ©jÃ  pris ğŸ˜ƒ)\n\n2\n\nMettre ici aussi\n\n\n\n\n\n\nAppliquer ces fichiers de configuration sur le cluster : kubectl apply -f deployment/\nSi tout a correctement fonctionnÃ©, vous devriez pouvoir accÃ©der depuis votre navigateur Ã  lâ€™API Ã  lâ€™URL spÃ©cifiÃ©e dans le fichier deployment/ingress.yaml. Par exemple https://toto.kub.sspcloud.fr/ si vous avez mis celui-ci plus tÃ´t (et https://toto.kub.sspcloud.fr/docs pour la documentation).\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli18\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGÃ©rer le CORS\n\n\n\nNotre API est accessible sans problÃ¨me depuis Python ou notre navigateur.\nEn revanche, si on dÃ©sire utiliser JavaScript pour crÃ©er une application interactive il est indispensable de mettre les lignes un peu obscure sur le CORS dans le fichier ingress.yaml.\nComme câ€™est un point technique qui ne concerne pas les compÃ©tences liÃ©es Ã  ce cours, nous donnons directement les mises Ã  jour nÃ©cessaires du projet.\nCeci consiste principalement Ã  ajouter la ligne suivante au fichier ingress.yaml :\nnginx.ingress.kubernetes.io/enable-cors: \"true\"\n\n\nOn peut remarquer quelques voies dâ€™amÃ©lioration de notre approche qui seront ultÃ©rieurement traitÃ©es:\n\nLâ€™entraÃ®nement du modÃ¨le est rÃ©-effectuÃ© Ã  chaque lancement dâ€™un nouveau conteneur. On relance donc autant de fois un entraÃ®nement quâ€™on dÃ©ploie de conteneurs pour rÃ©pondre Ã  nos utilisateurs. Ce sera lâ€™objet de la partie MLOps de fiabiliser et optimiser cette partie du pipeline.\nil est nÃ©cessaire de (re)lancer manuellement kubectl apply -f deployment/ Ã  chaque changement de notre code. Autrement dit, lors de cette application, on a amÃ©liorÃ© la fiabilitÃ© du lancement de notre API mais un lancement manuel est encore indispensable. Comme dans le reste de ce cours, on va essayer dâ€™Ã©viter un geste manuel pouvant Ãªtre source dâ€™erreur en privilÃ©giant lâ€™automatisation et lâ€™archivage dans des scripts. Câ€™est lâ€™objet de la prochaine Ã©tape."
  },
  {
    "objectID": "chapters/application.html#etape-3-automatiser-le-dÃ©ploiement-dÃ©ploiement-en-continu",
    "href": "chapters/application.html#etape-3-automatiser-le-dÃ©ploiement-dÃ©ploiement-en-continu",
    "title": "Application",
    "section": "Etape 3: automatiser le dÃ©ploiement (dÃ©ploiement en continu)",
    "text": "Etape 3: automatiser le dÃ©ploiement (dÃ©ploiement en continu)\n\n\n\n\n\n\nClarification sur la branche de travail\n\n\n\nA partir de maintenant, il est nÃ©cessaire de clarifier la branche principale sur laquelle nous travaillons. De maniÃ¨re traditionnelle, on utilise la branche main. NÃ©anmoins, pour Ãªtre cohÃ©rent avec les instructions du dÃ©but, qui Ã©taient de crÃ©er une branche dev, tous les exemples ultÃ©rieures partiront de cette hypothÃ¨se.\nSi vous avez fait les applications les unes aprÃ¨s les autres, et que vous vous situez toujours sur dev, vous pouvez passer aux applications suivantes. Si vous avez changÃ© de branche, vous pouvez continuer mais en tenir compte dans les exemples ultÃ©rieurs.\nSi vous avez utilisÃ© un tag pour sauter une ou plusieurs Ã©tapes, il va Ãªtre nÃ©cessaire de se placer sur une branche car vous Ãªtes en head detached. Pour cela, aprÃ¨s avoir committÃ© les fichiers que vous dÃ©sirez garder\n\n\n\nterminal\n\n1$ git branch -D dev\n2$ git push origin -d dev\n3$ git checkout -b dev\n4$ git push --set-upstream origin dev\n\n\n\n1\n\nSupprime la branche dev locale (si elle existe).\n\n2\n\nSupprime la branche dev remote (si elle existe).\n\n3\n\nCrÃ©e une nouvelle branche dev locale et on se place sur cette branche.\n\n4\n\nPousse la branche dev et active la synchronisation entre la branche locale et la branche remote.\n\n\n\n\n\n\nQuâ€™est-ce qui peut dÃ©clencher une Ã©volution nÃ©cessitant de mettre Ã  jour lâ€™ensemble de notre processus de production ?\nRegardons Ã  nouveau notre pipeline:\n\nLes inputs de notre pipeline sont donc:\n\nLa configuration. Ici, on peut considÃ©rer que notre YAML de configuration relÃ¨ve de cette catÃ©gorie ;\nLes donnÃ©es. Nos donnÃ©es sont statiques et nâ€™ont pas vocation Ã  Ã©voluer. Si câ€™Ã©tait le cas, il faudrait en tenir compte dans notre automatisation. ;\nLe code. Câ€™est lâ€™Ã©lÃ©ment principal qui Ã©volue chez nous. IdÃ©alement, on veut automatiser le processus au maximum en faisant en sorte quâ€™Ã  chaque mise Ã  jour de notre code (un push sur Github), les Ã©tapes ultÃ©rieures (production de lâ€™image Docker, etc.) se lancent. NÃ©anmoins, on veut aussi Ã©viter quâ€™une erreur puisse donner lieu Ã  une mise en production non-fonctionnelle, on va donc maintenir une action manuelle minimale comme garde-fou.\n\nPour automatiser au maximum la mise en production, on va utiliser un nouvel outil : ArgoCD. Ainsi, au lieu de devoir appliquer manuellement la commande kubectl apply Ã  chaque modification des fichiers de dÃ©ploiement (prÃ©sents dans le dossier kubernetes/), câ€™est lâ€™opÃ©rateur ArgoCD, dÃ©ployÃ© sur le cluster, qui va dÃ©tecter les changements de configuration du dÃ©ploiement et les appliquer automatiquement. Câ€™est lâ€™approche dite GitOps : le dÃ©pÃ´t Git du dÃ©ploiement fait office de source de vÃ©ritÃ© unique de lâ€™Ã©tat voulu de lâ€™application, tout changement sur ce dernier doit donc se rÃ©percuter immÃ©diatement sur le dÃ©ploiement effectif.\n\n\n\n\n\n\nApplication 19a: Automatiser la mise Ã  disposition de lâ€™API (dÃ©ploiement continu)\n\n\n\n\nLancer un service ArgoCD sur le SSPCloud depuis la page Mes services (catalogue Automation). Laisser les configurations par dÃ©faut.\nSur GitHub, crÃ©er un dÃ©pÃ´t application-deployment qui va servir de dÃ©pÃ´t GitOps, câ€™est Ã  dire un dÃ©pÃ´t qui spÃ©cifie le paramÃ©trage du dÃ©ploiement de votre application.\nAjouter un dossier deployment Ã  votre dÃ©pÃ´t GitOps, dans lequel on mettra les trois fichiers de dÃ©ploiement qui permettent de dÃ©ployer notre application sur Kubernetes (deployment.yaml, service.yaml, ingress.yaml).\nA la racine de votre dÃ©pÃ´t GitOps, crÃ©ez un fichier application.yml avec le contenu suivant, en prenant bien soin de modifier les lignes surlignÃ©es avec les informations pertinentes :\n\n\n\n\napplication.yaml\n\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: ensae-mlops\nspec:\n  project: default\n  source:\n1    repoURL: https://github.com/&lt;your_github_username&gt;/application-deployment.git\n2    targetRevision: main\n3    path: deployment\n  destination:\n    server: https://kubernetes.default.svc\n4    namespace: user-&lt;your_sspcloud_username&gt;\n  syncPolicy:\n    automated:\n      selfHeal: true\n\n\n\n1\n\nLâ€™URL de votre dÃ©pÃ´t Github \n\n2\n\nLa branche Ã  partir de laquelle vous dÃ©ployez\n\n3\n\nLe nom du dossier contenant vos fichiers de dÃ©ploiement Kubernetes\n\n4\n\nVotre namespace Kubernetes. Sur le SSPCloud, cela prend la forme user-${username}\n\n\n\n\n\nDans ArgoCD, cliquez sur New App puis Edit as a YAML. Copiez-collez le contenu de application.yml et cliquez sur Create.\nObservez dans lâ€™interface dâ€™ArgoCD le dÃ©ploiement progressif des ressources nÃ©cessaires Ã  votre application sur le cluster. Joli non ?\nVÃ©rifiez que votre API est bien dÃ©ployÃ©e en utilisant lâ€™URL dÃ©finie dans le fichier ingress.yml.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli19a\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\nA prÃ©sent, nous avons tous les outils Ã  notre disposition pour construire un vrai pipeline de CI/CD, automatisÃ© de bout en bout. Il va nous suffire pour cela de mettre Ã  bout les composants :\n\ndans la partie 4 de lâ€™application, nous avons construit un pipeline de CI : on a donc seulement Ã  faire un commit sur le dÃ©pÃ´t de lâ€™application pour lancer lâ€™Ã©tape de build et de mise Ã  disposition de la nouvelle image sur le DockerHub ;\ndans lâ€™application prÃ©cÃ©dente, nous avons construit un pipeline de CD : ArgoCD suit en permanence lâ€™Ã©tat du dÃ©pÃ´t GitOps, tout commit sur ce dernier lancera donc automatiquement un redÃ©ploiement de lâ€™application.\n\nIl y a donc un Ã©lÃ©ment qui fait la liaison entre ces deux pipelines et qui nous sert de garde-fou en cas dâ€™erreur : la version de lâ€™application.\n\n\n\n\n\n\nApplication 19b : Mettre Ã  jour la version en production\n\n\n\nJusquâ€™Ã  maintenant, on a utilisÃ© le tag latest pour dÃ©finir la version de notre application. En pratique, lorsquâ€™on passe de la phase de dÃ©veloppement Ã  celle de production, on a plutÃ´t envie de versionner proprement les versions de lâ€™application afin de savoir ce qui est dÃ©ployÃ©. On va pour cela utiliser les tags avec Git, qui vont se propager au nommage de lâ€™image Docker.\n\nModifier le fichier de CI prod.yml pour assurer la propagation des tags.\n\n\n\nFichier .github/workflows/prod.yml\n\n\n\n.github/workflows/prod.yml\n\nname: Construction image Docker\n\non: \n  push:\n    branches:\n      - main\n      - dev\n    tags:\n      - 'v*.*.*'\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      -\n        name: Docker meta\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n          images: linogaliana/application-correction\n\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n\n\nDans le dÃ©pÃ´t de lâ€™application, mettre Ã  jour le code dans api/main.py pour changer un Ã©lÃ©ment de lâ€™interface de votre documentation. Par exemple, mettre en gras un titre.\n\napp = FastAPI(\n    title=\"PrÃ©diction de survie sur le Titanic\",\n    description=\n    \"&lt;b&gt;Application de prÃ©diction de survie sur le Titanic&lt;/b&gt; ğŸš¢ &lt;br&gt;Une version par API pour faciliter la rÃ©utilisation du modÃ¨le ğŸš€\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\nCommit et push les changements.\nTagger le commit effectuÃ© prÃ©cÃ©demment et push le nouveau tag :\n\n\n\nterminal\n\n$ git tag v1.0.0\n$ git push --tags\n\n\nVÃ©rifier sur le dÃ©pÃ´t GitHub de lâ€™application que ce commit lance bien un pipeline de CI associÃ© au tag v1.0.0. Une fois terminÃ©, vÃ©rifier sur le DockerHub que le tag v1.0.0 existe bien parmi les tags disponibles de lâ€™image.\n\nLa partie CI a correctement fonctionnÃ©. IntÃ©ressons-nous Ã  prÃ©sent Ã  la partie CD.\n\nSur le dÃ©pÃ´t GitOps de lâ€™application, mettre Ã  jour la version de lâ€™image Ã  dÃ©ployer en production dans le fichier deployment/deployment.yaml\n\n\n\nFichier deployment/deployment.yaml\n\n\n\n\ndeployment/deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n        image: linogaliana/application-correction:v1.0.0\n        ports:\n        - containerPort: 5000\n\n\n\nRemplacer username par la valeur adÃ©quate\n\n\n\nAprÃ¨s avoir committÃ© et pushÃ©, observer dans ArgoCD le statut de votre application. Normalement, lâ€™opÃ©rateur devrait avoir automatiquement identifiÃ© le changement, et mettre Ã  jour le dÃ©ploiement pour en tenir compte.\n\n\n\nVÃ©rifier que lâ€™API a bien Ã©tÃ© mise Ã  jour.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli19b\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#etape-4-construire-un-site-web",
    "href": "chapters/application.html#etape-4-construire-un-site-web",
    "title": "Application",
    "section": "Etape 4: construire un site web",
    "text": "Etape 4: construire un site web\n\n\n\n\n\n\nSi vous prenez ce projet fil rouge en cours de route\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli19\n$ git checkout -b dev\n$ git push origin dev\n\n\n\n\n\n\n\n\n\nOn va proposer un nouveau livrable pour parler Ã  un public plus large. Pour faire ce site web, on va utiliser Quarto et dÃ©ployer sur Github Pages.\n\n\n\n\n\n\nApplication 20: CrÃ©ation dâ€™un site web pour valoriser le projet\n\n\n\n\n\nterminal\n\n$ quarto create project website mysite\n\n\nFaire remonter dâ€™un niveau _quarto.yml\nSupprimer about.qmd, dÃ©placer index.qmd vers la racine de notre projet.\nRemplacer le contenu de index.qmd par celui-ci et retirer about.qmd des fichiers Ã  compiler.\nDÃ©placer styles.css Ã  la racine du projet\nMettre Ã  jour le .gitignore avec les instructions suivantes\n\n/.quarto/\n*.html\n*_files\n_site/\n\nEn ligne de commande, faire quarto preview (ajouter les arguments --port 5000 --host 0.0.0.0 si vous passez par le SSPCloud)\nObserver le site web gÃ©nÃ©rÃ© en local\n\nEnfin, on va construire et dÃ©ployer automatiquement ce site web grÃ¢ce au combo Github Actions et Github Pages:\n\nCrÃ©er une branche gh-pages Ã  partir du contenu de cette page\nRevenir Ã  votre branche\nCrÃ©er un fichier .github/workflows/website.yaml avec le contenu de ce fichier\nModifier le README pour indiquer lâ€™URL de votre site web et de votre API\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli20\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#restructurer-le-pipeline-pour-fluidifier-la-mise-en-production",
    "href": "chapters/application.html#restructurer-le-pipeline-pour-fluidifier-la-mise-en-production",
    "title": "Application",
    "section": "Restructurer le pipeline pour fluidifier la mise en production",
    "text": "Restructurer le pipeline pour fluidifier la mise en production\n\n\n\n\n\n\nApplication 21 (optionnelle): restructuration de la chaÃ®ne\n\n\n\n\nFaire les modifications suivantes pour restructurer notre pipeline afin de mieux distinguer les Ã©tapes dâ€™estimation et dâ€™Ã©valuation\n\n\n\nModification de src/models/train_evaluation.py Ã  effectuer\n\n\n\nsrc/models/train_evaluation.py\n\ndef build_pipeline(\n    n_trees: int = 20,\n    numeric_features=[\"Age\", \"Fare\"],\n    categorical_features=[\"Title\", \"Embarked\", \"Sex\"]):\n    \"\"\"Random forest model for Titanic survival\n\n    Args:\n        n_trees (int, optional): _description_. Defaults to 20.\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n\n    numeric_transformer = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"median\")),\n            (\"scaler\", MinMaxScaler()),\n        ]\n    )\n\n    categorical_transformer = Pipeline(\n        steps=[\n            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"onehot\", OneHotEncoder()),\n        ]\n    )\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"Preprocessing numerical\", numeric_transformer, numeric_features),\n            (\n                \"Preprocessing categorical\",\n                categorical_transformer,\n                categorical_features,\n            ),\n        ]\n    )\n\n    pipe = Pipeline(\n        [\n            (\"preprocessor\", preprocessor),\n            (\"classifier\", RandomForestClassifier(n_estimators=n_trees)),\n        ]\n    )\n\n    return pipe\n\n\n\n\nModification de src/features/build_features.py pour enchaÃ®ner les Ã©tapes de feature engineering\n\n\n\nsrc/features/build_features.py\n\ndef feature_engineering(data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Applying our feature engineering pipeline\n\n    Args:\n        data (pd.DataFrame): Initial dataframe\n\n    Returns:\n        pd.DataFrame: Dataframe with feature engineering being handled\n    \"\"\"\n    data_training = create_variable_title(data)\n    data_training = check_has_cabin(data_training)\n    data_training = ticket_length(data_training)\n    return data_training\n\n\n\n\nModification de train.py pour faire une grid search\n\n\n\ntrain.py\n\n\"\"\"\nPrediction de la survie d'un individu sur le Titanic\n\"\"\"\n\n# GESTION ENVIRONNEMENT --------------------------------\n\nfrom pathlib import Path\nimport argparse\nfrom joblib import dump\nfrom sklearn.model_selection import GridSearchCV\n\nimport src.data.import_data as imp\nimport src.features.build_features as bf\nimport src.models.train_evaluate as te\n\n# PARAMETRES -------------------------------\n\n# ParamÃ¨tres ligne de commande\nparser = argparse.ArgumentParser(description=\"ParamÃ¨tres du random forest\")\nparser.add_argument(\"--n_trees\", type=int, default=20, help=\"Nombre d'arbres\")\nparser.add_argument(\"--appli\", type=str, default=\"appli21\", help=\"Application number\")\nargs = parser.parse_args()\n\n# ParamÃ¨tres YAML\nconfig = imp.import_yaml_config(\"configuration/config.yaml\")\nbase_url = (\n    \"https://minio.lab.sspcloud.fr/projet-formation/ensae-reproductibilite/data/raw\"\n)\nAPI_TOKEN = config.get(\"jeton_api\")\nLOCATION_TRAIN = config.get(\"train_path\", f\"{base_url}/train.csv\")\nLOCATION_TEST = config.get(\"test_path\", f\"{base_url}/test.csv\")\nTEST_FRACTION = config.get(\"test_fraction\", 0.1)\nN_TREES = args.n_trees\nAPPLI_ID = args.appli\nEXPERIMENT_NAME = \"titanicml\"\n\n# FEATURE ENGINEERING --------------------------------\n\ntitanic_raw = imp.import_data(LOCATION_TRAIN)\n\n# Create a 'Title' variable\ntitanic_intermediate = bf.feature_engineering(titanic_raw)\n\n\ntrain, test = te.split_train_test_titanic(\n    titanic_intermediate, fraction_test=TEST_FRACTION\n)\nX_train, y_train = train.drop(\"Survived\", axis=\"columns\"), train[\"Survived\"]\nX_test, y_test = test.drop(\"Survived\", axis=\"columns\"), test[\"Survived\"]\n\n\ndef log_local_data(data, filename):\n    data.to_csv(f\"data/intermediate/{filename}.csv\", index=False)\n\n\noutput_dir = Path(\"data/intermediate\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nlog_local_data(X_train, \"X_train\")\nlog_local_data(X_test, \"X_test\")\nlog_local_data(y_train, \"y_train\")\nlog_local_data(y_test, \"y_test\")\n\n\n# MODELISATION: RANDOM FOREST ----------------------------\n\npipe = te.build_pipeline(n_trees=N_TREES, categorical_features=[\"Embarked\", \"Sex\"])\n\nparam_grid = {\n    \"classifier__n_estimators\": [10, 20, 50],\n    \"classifier__max_leaf_nodes\": [5, 10, 50],\n}\n\n\npipe_cross_validation = GridSearchCV(\n    pipe,\n    param_grid=param_grid,\n    scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n    refit=\"f1\",\n    cv=5,\n    n_jobs=5,\n    verbose=1,\n)\n\n\npipe_cross_validation.fit(X_train, y_train)\npipe = pipe_cross_validation.best_estimator_\n\ndump(pipe, \"model.joblib\")\n\n\n\n\nFichier eval.py pour Ã©valuer la meilleure validation croisÃ©e\n\n\n\neval.py\n\nimport pandas as pd\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom joblib import load\n\n# EVALUATE ----------------------------\n\nloaded_model = load(\"model.joblib\")\n\n# Predict on a Pandas DataFrame.\nX_test = pd.read_csv(\"data/intermediate/X_test.csv\")\ny_test = pd.read_csv(\"data/intermediate/y_test.csv\")\ny_test_predict = loaded_model.predict(X_test)\n\n# EVALUATE ----------------------------\n\nmatrix = confusion_matrix(y_test, y_test_predict)\n\nprint(\"Accuracy:\")\nprint(f\"{accuracy_score(y_test, y_test_predict):.0%}\")\nprint(\"Matrice de confusion:\")\nprint(matrix)\n\n\n\n\n\nModification de api/main.py Ã  effectuer\n\n\n\napi/main.py\n\n\"\"\"A simple API to expose our trained RandomForest model for Tutanic survival.\"\"\"\nimport requests\nfrom fastapi import FastAPI\nfrom joblib import load\n\nimport pandas as pd\n\n# GET PRODUCTION MODEL -------------\n\nusername_sspcloud = \"lgaliana\"\nurl = f\"https://minio.lab.sspcloud.fr/{username_sspcloud}/ensae-reproductibilite/model/model.joblib\"\nlocal_filename = \"model.joblib\"\n\nwith open(local_filename, mode = \"wb\") as file:\n    file.write(requests.get(url).content)\n\n\nmodel = load(local_filename)\n\n\n# USE PRODUCTION MODEL IN APP ----------\n\napp = FastAPI(\n    title=\"PrÃ©diction de survie sur le Titanic\",\n    description=\n    \"&lt;b&gt;Application de prÃ©diction de survie sur le Titanic&lt;/b&gt; ğŸš¢ &lt;br&gt;Une version par API pour faciliter la rÃ©utilisation du modÃ¨le ğŸš€\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\n\n@app.get(\"/\", tags=[\"Welcome\"])\ndef show_welcome_page():\n    \"\"\"\n    Show welcome page with model name and version.\n    \"\"\"\n\n    return {\n        \"Message\": \"API de prÃ©diction de survie sur le Titanic\",\n        \"Model_name\": 'Titanic ML',\n        \"Model_version\": \"0.1\",\n    }\n\n\n@app.get(\"/predict\", tags=[\"Predict\"])\nasync def predict(\n    pclass: int = 3,\n    sex: str = \"female\",\n    age: float = 29.0,\n    sib_sp: int = 1,\n    parch: int = 1,\n    fare: float = 16.5,\n    embarked: str = \"S\",\n    has_cabin: int = 1,\n    ticket_len: int = 7\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Pclass\": [pclass],\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"SibSp\": [sib_sp],\n            \"parch\": [parch],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n            \"hasCabin\": [has_cabin],\n            \"Ticket_Len\": [ticket_len] \n        }\n    )\n\n    prediction = \"Survived ğŸ‰\" if int(model.predict(df)) == 1 else \"Dead âš°ï¸\"\n\n    return prediction\n\n\n\nTester votre script train.py et uploader le meilleur modÃ¨le sur S3 de la maniÃ¨re suivante:\n\n\n\n\nterminal\n\n1mc cp model.joblib s3/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/model/model.joblib\n2mc anonymous set download s3/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/\n\n\n\n1\n\nUploader sur S3\n\n2\n\nOuvrir les droits en lecture de ce fichier pour simplifier la rÃ©cupÃ©ration17\n\n\n\n\noÃ¹ &lt;BUCKET_PERSONNEL&gt; est Ã  remplacer par votre nom dâ€™utilisateur sur le SSPCloud\n\nTester en local api/main.py pour vÃ©rifier le caractÃ¨re fonctionnel de lâ€™API. Les deux modifications principales de celle-ci sont les suivantes:\n\nOn nâ€™utilise plus la variable Title puisque celle-ci a Ã©tÃ© retirÃ©e des donnÃ©es en entrÃ©e du modÃ¨le\nOn rÃ©cupÃ¨re la version de â€œproductionâ€ sur le systÃ¨me de stockage S3, on ne fait plus lâ€™entraÃ®nement Ã  chaque initialisation dâ€™un conteneur\n\nModifier deployment/deployment.yaml et .github/workflows/prod.yaml pour dÃ©finir et utiliser le tag v0.0.3 de lâ€™image Docker\nRetirer la ligne python train.py du fichier run.sh\nDans index.qmd, remplacer la ligne suivante:\n\n```{ojs}\nprediction = `https://titanic.kub.sspcloud.fr/predict?pclass=${class_boat}&sex=${gender}&age=${age}&sib_sp=1&parch=1&fare=16.5&embarked=S&has_cabin=1&ticket_len=7`\n```\npar celle-ci\n```{ojs}\nprediction = `https://titanic.kub.sspcloud.fr/predict?pclass=${class_boat}&sex=${gender}&age=${age}&sib_sp=1&parch=1&fare=16.5&embarked=S&has_cabin=1&ticket_len=7`\n```\nFaire un commit et un push des modifications\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli21\n\n\n1\n\nPour annuler les modifications depuis le dernier commit"
  },
  {
    "objectID": "chapters/application.html#garder-une-trace-des-entraÃ®nements-de-notre-modÃ¨le-grÃ¢ce-au-register-de-mlflow",
    "href": "chapters/application.html#garder-une-trace-des-entraÃ®nements-de-notre-modÃ¨le-grÃ¢ce-au-register-de-mlflow",
    "title": "Application",
    "section": "Garder une trace des entraÃ®nements de notre modÃ¨le grÃ¢ce au register de MLFlow",
    "text": "Garder une trace des entraÃ®nements de notre modÃ¨le grÃ¢ce au register de MLFlow\n\n\n\n\n\n\nSi vous prenez ce projet fil rouge en cours de route\n\n\n\n\n\n\n\nterminal\n\n$ git checkout appli21\n$ git checkout -b dev\n$ git push origin dev\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplication 22 : archiver nos entraÃ®nements avec MLFlow\n\n\n\n\nLancer MLFlow depuis lâ€™onflet Mes services du SSPCloud. Attendre que le service soit bien lancÃ©. Cela crÃ©era un service dont lâ€™URL est de la forme https://user-{username}-{pod_id}.user.lab.sspcloud.fr, oÃ¹ pod_id est un identifiant alÃ©atoire. Ce service MLFlow communiquera avec les VSCode que vous ouvrirez ultÃ©rieurement Ã  partir de cet URL ainsi quâ€™avec le systÃ¨me de stockage S318.\nRegarder la page Experiments. Elle est vide Ã  ce stade, câ€™est normal\n\n\nUne fois le service MLFlow fonctionnel, lancer un nouveau VSCode pour bÃ©nÃ©ficier de la configuration automatisÃ©e\nClÃ´ner votre projet, vous situer sur la branche de travail (nous supposerons quâ€™il sâ€™agit de dev).\nDepuis un terminal Python, lancer les commandes suivantes:\n\nimport mlflow\nmlflow_experiment_name = \"titanicml\"\nmlflow.set_experiment(experiment_name=mlflow_experiment_name)\nRetourner sur lâ€™UI et observer la diffÃ©rence, Ã  gauche.\n\nCrÃ©er un fichier src/models/log.py\n\n\n\nContenu du fichier src/models/log.py\n\n\n\nsrc/models/log.py\n\nimport mlflow\nimport os\n\n\ndef log_gsvc_to_mlflow(\n    gscv, mlflow_experiment_name, application_number: str = \"appli21\"\n):\n    \"\"\"Log a scikit-learn trained GridSearchCV object as an MLflow experiment.\"\"\"\n    # Set up MLFlow context\n    mlflow.set_experiment(experiment_name=mlflow_experiment_name)\n\n    for run_idx in range(len(gscv.cv_results_[\"params\"])):\n        # For each hyperparameter combination we trained the model with, we log a run in MLflow\n        run_name = f\"run {run_idx}\"\n        with mlflow.start_run(run_name=run_name):\n            # Log hyperparameters\n            params = gscv.cv_results_[\"params\"][run_idx]\n            for param in params:\n                mlflow.log_param(param, params[param])\n\n            # Log fit metrics\n            scores = [\n                score\n                for score in gscv.cv_results_\n                if \"mean_test\" in score or \"std_test\" in score\n            ]\n            for score in scores:\n                mlflow.log_metric(score, gscv.cv_results_[score][run_idx])\n\n            # Log model as an artifact\n            mlflow.sklearn.log_model(gscv, \"gscv_model\")\n            # Log training data URL\n            mlflow.log_param(\"appli\", application_number)\n\n\n\nModifier le fichier train.py pour ajouter la ligne\n\nmlog.log_gsvc_to_mlflow(pipe_cross_validation, EXPERIMENT_NAME, APPLI_ID)\navec\nimport src.models.log as mlog\n\nFaire tourner avec le paramÃ¨tre --appli appli22:\n\n\n\nterminal\n\n$ python train.py --appli appli22\n\n\nObserver lâ€™Ã©volution de la page Experiments. Cliquer sur un des run. Observer toutes les mÃ©tadonnÃ©es archivÃ©es (hyperparamÃ¨tres, mÃ©triques dâ€™Ã©valuation, requirements.txt dont MLFlow a fait lâ€™infÃ©rence, etc.)\nObserver le code proposÃ© par MLFlow pour rÃ©cupÃ©rer le run en question. Modifier le fichier eval.py Ã  partir de cet exemple et du modÃ¨le suivant pour utiliser un des modÃ¨les archivÃ©s dans MLFlow\nRetourner Ã  la liste des runs en cliquant Ã  nouveau sur â€œtitanicmlâ€ dans les expÃ©rimentations\nDans lâ€™onglet Table, sÃ©lectionner plusieurs expÃ©rimentations, cliquer sur Columns et ajouter mean_test_f1. Ajuster la taille des colonnes pour la voir et classer les modÃ¨les par score dÃ©croissants\nCliquer sur Compare aprÃ¨s en avoir sÃ©lectionnÃ© plusieurs. Afficher un scatterplot des performances en fonction du nombre dâ€™estimateurs. Conclure.\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli22\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\nCette appplication illustre lâ€™un des premiers apports de MLFlow: on garde une trace de nos expÃ©rimentations et on peut dÃ©jÃ  mieux comprendre la maniÃ¨re dont certains paramÃ¨tres de notre modÃ¨le peuvent influencer la qualitÃ© de nos prÃ©dictions.\nNÃ©anmoins, persistent un certain nombre de voies dâ€™amÃ©lioration:\n\nOn entraÃ®ne le modÃ¨le en local, de maniÃ¨re sÃ©quentielle, et en lanÃ§ant nous-mÃªmes le script train.py\nOn nâ€™archive pas les jeux de donnÃ©es associÃ©s Ã  ces modÃ¨les (les jeux dâ€™entraÃ®nement et de test). On doit alors le faire manuellement si on dÃ©sire Ã©valuer les performances ex post, ce qui est pÃ©nible.\nOn rÃ©cupÃ¨re manuellement les modÃ¨les ce qui nâ€™est pas trÃ¨s pÃ©renne.\nNotre API nâ€™utilise pas encore lâ€™un des modÃ¨les archivÃ© sur MLFlow.\n\nLes prochaines applications permettront dâ€™amÃ©liorer ceci.\n\nMise en production dâ€™un modÃ¨le\n\n\n\n\n\n\nApplication 23a : rÃ©utiliser un modÃ¨le archivÃ© sur MLFlow\n\n\n\n\nA partir du tableau de performance prÃ©cÃ©dent, choisir le modÃ¨le avec le F1 score maximal. AccÃ©der Ã  celui-ci.\n\nCrÃ©er un script dans mlflow/predict.py pour illustrer lâ€™utilisation dâ€™un modÃ¨le depuis MLFlow. Nous allons progressivement lâ€™amÃ©liorer.\n\nCopier-coller le contenu ci-dessous afin de se simplifier la crÃ©ation de donnÃ©es en entrÃ©e de notre code\n\n\n\nmlflow/predict.py\n\nimport pandas as pd\n\ndef create_data(\n    pclass: int = 3,\n    sex: str = \"female\",\n    age: float = 29.0,\n    sib_sp: int = 1,\n    parch: int = 1,\n    fare: float = 16.5,\n    embarked: str = \"S\",\n    has_cabin: int = 1,\n    ticket_len: int = 7\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Pclass\": [pclass],\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"SibSp\": [sib_sp],\n            \"parch\": [parch],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n            \"hasCabin\": [has_cabin],\n            \"Ticket_Len\": [ticket_len] \n        }\n    )\n\n    return df\n\ndata = pd.concat([\n    create_data(),\n    create_data(sex=\"male\")\n])\n\n\nCliquer sur votre meilleur modÃ¨le et introduire dans mlflow/predict.py le morceau de code suggÃ©rÃ© par MLFlow, du type de celui-ci:\n\n\nimport mlflow\n1logged_model = #A CHANGER\n\n# Load model as a PyFuncModel.\nloaded_model = mlflow.pyfunc.load_model(logged_model)\n\n# Predict on a Pandas DataFrame.\nimport pandas as pd\nloaded_model.predict(pd.DataFrame(data))\n\n\n1\n\nHash du modÃ¨le\n\n\n\n\nLancer depuis la ligne de commande ce script et observer lâ€™application obtenue.\n\n\nA ce stade, nous avons amÃ©liorÃ© la fiabilitÃ© de notre modÃ¨le car nous utilisons le meilleur. NÃ©anmoins, celui-ci nâ€™est pas forcÃ©ment pratique Ã  rÃ©cupÃ©rer car nous utilisons un hash qui certes identifie de maniÃ¨re unique notre modÃ¨le mais prÃ©sente lâ€™inconvÃ©nient dâ€™Ãªtre peu intelligible. Nous allons passer de lâ€™expÃ©rimentation Ã  la mise en production en sÃ©lectionnant explicitement notre meilleur modÃ¨le.\n\n\n\n\n\n\nApplication 23b : passer en production un modÃ¨le\n\n\n\n\nDans la page du modÃ¨le en question sur MLFlow, cliquer sur Register model et le nommer titanic.\nAller dans lâ€™onglet Models et observer le changement par rapport Ã  prÃ©cÃ©demment.\nMettre Ã  jour le code dans mlflow/predict.py pour utiliser la version en production :\n\n{.python filename = \"mlflow/predict.py\"} model_name = \"titanic\" model_version = 1 loaded_model = mlflow.pyfunc.load_model(     model_uri=f\"models:/{model_name}/{model_version}\" )\n\nTester cette application. Si celle-ci fonctionne, modifier la rÃ©cupÃ©ration du modÃ¨le dans votre script dâ€™API.\nTester en local cette API mise Ã  jour\n\n\n\nterminal\n\nuvicorn api.main:app --reload --host \"0.0.0.0\" --port 5000\n\n\nAjouter mlflow au requirements.txt\nMettre Ã  jour les fichiers .github/worflows/prod.yaml et kubernetes/deployment.yaml pour produire et utiliser le tag v0.0.5\n\n\n\n\n.github/worflows/prod.yaml\n\nname: Construction image Docker\n\non:\n  push:\n    branches:\n      - main\n      - dev\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          push: true\n1          tags: linogaliana/application-correction:v0.0.7\n\n\n\n1\n\nModifier lâ€™image ici\n\n\n\n\n\n# Creating MLflow deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanicml\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanicml\n  template:\n    metadata:\n      labels:\n        app: titanicml\n    spec:\n      containers:\n        - name: api\n1          image: linogaliana/application-correction:v0.0.7\n          imagePullPolicy: Always\n          env:\n            - name: MLFLOW_TRACKING_URI\n2              value: https://user-{USERNAME}-mlflow.user.lab.sspcloud.fr\n            - name: MLFLOW_MODEL_NAME\n              value: titanic\n            - name: MLFLOW_MODEL_VERSION\n              value: \"1\"\n          resources:\n            limits:\n              memory: \"2Gi\"\n              cpu: \"1000m\"\n\n\n1\n\nModifier lâ€™image Docker\n\n2\n\nModifier lâ€™URL de MLFlow\n\n\n\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\n\n\nterminal\n\n1$ git stash\n$ git checkout appli23\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\n\n\n\n\n\n\n\n\n\nIndustrialiser les entraÃ®nements de nos modÃ¨les\nPour industrialiser nos entraÃ®nements, nous allons crÃ©er des processus parallÃ¨les indÃ©pendants pour chaque combinaison de nos hyperparamÃ¨tres. Pour cela, lâ€™outil pratique sur le SSPCloud est Argo workflows. Chaque combinaison dâ€™hyperparamÃ¨tres sera un processus isolÃ© Ã  lâ€™issue duquel sera logguÃ© le rÃ©sultat dans MLFlow. Ces entraÃ®nements auront lieu en parallÃ¨le.\n\n\nLancer un service Argo Workflows\nDans mlflow/training.yaml\n\n\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: titanic-training-workflow-\nspec:\n  entrypoint: main\n  arguments:\n    parameters:\n      # The MLflow tracking server is responsible to log the hyper-parameter and model metrics.\n      - name: mlflow-tracking-uri\n1        value: https://user-lgaliana-argo-workflows.user.lab.sspcloud.fr\n      - name: mlflow-experiment-name\n2        value: titanicml\n      - name: model-training-conf-list\n        value: |\n          [\n            { \"dim\": 25, \"lr\": 0.1 },\n            { \"dim\": 100, \"lr\": 0.2 },\n            { \"dim\": 150, \"lr\": 0.3 }\n          ]\n  templates:\n    # Entrypoint DAG template\n    - name: main\n      dag:\n        tasks:\n          # Task 0: Start pipeline\n          - name: start-pipeline\n            template: start-pipeline-wt\n          # Task 1: Train model with given params\n          - name: train-model-with-params\n            dependencies: [ start-pipeline ]\n            template: run-model-training-wt\n            arguments:\n              parameters:\n                - name: dim\n                  value: \"{{item.dim}}\"\n                - name: lr\n                  value: \"{{item.lr}}\"\n            # Pass the inputs to the task using \"withParam\"\n            withParam: \"{{workflow.parameters.model-training-conf-list}}\"\n\n    # Now task container templates are defined\n    # Worker template for task 0 : start-pipeline\n    - name: start-pipeline-wt\n      inputs:\n      container:\n        image: busybox\n        command: [ sh, -c ]\n        args: [ \"echo Starting pipeline\" ]\n\n    # Worker template for task-1 : train model with params\n    - name: run-model-training-wt\n      inputs:\n        parameters:\n          - name: dim\n          - name: lr\n      container:\n        image: inseefrlab/formation-mlops:main\n        imagePullPolicy: Always\n        command: [sh, -c]\n        args: [\"mlflow run .\n                --env-manager=local\n                -P remote_server_uri=$MLFLOW_TRACKING_URI\n                -P experiment_name=$MLFLOW_EXPERIMENT_NAME\n                -P dim={{inputs.parameters.dim}}\n                -P lr={{inputs.parameters.lr}}\"]\n        env:\n          - name: MLFLOW_TRACKING_URI\n            value: \"{{workflow.parameters.mlflow-tracking-uri}}\"\n          - name: MLFLOW_EXPERIMENT_NAME\n            value: \"{{workflow.parameters.mlflow-experiment-name}}\"\n\n\n1\n\nChanger\n\n2\n\ntitanicml\n\n\n\n\nmax_depth\nmax_features â€œsqrtâ€, â€œlog2â€"
  },
  {
    "objectID": "chapters/application.html#pour-aller-plus-loin",
    "href": "chapters/application.html#pour-aller-plus-loin",
    "title": "Application",
    "section": "Pour aller plus loin",
    "text": "Pour aller plus loin\nCrÃ©er un service label studio pour Ã©valuer la qualitÃ© du modÃ¨le"
  },
  {
    "objectID": "chapters/application.html#footnotes",
    "href": "chapters/application.html#footnotes",
    "title": "Application",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLâ€™export dans un script .py a Ã©tÃ© fait directement depuis VSCode. Comme cela nâ€™est pas vraiment lâ€™objet du cours, nous passons cette Ã©tape et fournissons directement le script expurgÃ© du texte intermÃ©diaire. Mais nâ€™oubliez pas que cette dÃ©marche, frÃ©quente quand on a dÃ©marrÃ© sur un notebook et quâ€™on dÃ©sire consolider en faisant la transition vers des scripts, nÃ©cessite dâ€™Ãªtre attentif pour ne pas risquer de faire une erreur.â†©ï¸\nIl est Ã©galement possible avec VSCode dâ€™exÃ©cuter le script ligne Ã  ligne de maniÃ¨re interactive ligne Ã  ligne (MAJ+ENTER). NÃ©anmoins, cela nÃ©cessite de sâ€™assurer que le working directory de votre console interactive est le bon. Celle-ci se lance selon les paramÃ¨tres prÃ©configurÃ©s de VSCode et les votres ne sont peut-Ãªtre pas les mÃªmes que les notres. Vous pouvez changer le working directory dans le script en utilisant le package os mais peut-Ãªtre allez vous dÃ©couvrir ultÃ©rieurement quâ€™il y a de meilleures pratiquesâ€¦â†©ï¸\nEssayez de commit vos changements Ã  chaque Ã©tape de lâ€™exercice, câ€™est une bonne habitude Ã  prendre.â†©ï¸\nIci, le jeton dâ€™API nâ€™est pas indispensable pour que le code fonctionne. Afin dâ€™Ã©viter une erreur non nÃ©cessaire lorsquâ€™on automatisera le processus, on peut crÃ©er une condition qui vÃ©rifie la prÃ©sence ou non de ce fichier. Le script reste donc reproductible mÃªme pour un utilisateur nâ€™ayant pas le fichier secrets.yaml.â†©ï¸\nIl est normal dâ€™avoir des dossiers __pycache__ qui traÃ®nent en local : ils se crÃ©ent automatiquement Ã  lâ€™exÃ©cution dâ€™un script en Python. NÃ©anmoins, il ne faut pas associer ces fichiers Ã  Git, voilÃ  pourquoi on les ajoute au .gitignore.â†©ï¸\nNous proposons ici dâ€™adopter le principe de la programmation fonctionnelle. Pour encore fiabiliser un processus, il serait possible dâ€™adopter le paradigme de la programmation orientÃ©e objet (POO). Celle-ci est plus rebutante et demande plus de temps au dÃ©veloppeur. Lâ€™arbitrage coÃ»t-avantage est nÃ©gatif pour notre exemple, nous proposons donc de nous en passer. NÃ©anmoins, pour une mise en production rÃ©elle dâ€™un modÃ¨le, il est recommandÃ© de lâ€™adopter. Câ€™est dâ€™ailleurs obligatoire avec des pipelines scikit.â†©ï¸\nAttention, les donnÃ©es ont Ã©tÃ© committÃ©es au moins une fois. Les supprimer du dÃ©pÃ´t ne les efface pas de lâ€™historique. Si cette erreur arrive, le mieux est de supprimer le dÃ©pÃ´t en ligne, crÃ©er un nouvel historique Git et partir de celui-ci pour des publications ultÃ©rieures sur Github. NÃ©anmoins lâ€™idÃ©al serait de ne pas sâ€™exposer Ã  cela. Câ€™est justement lâ€™objet des bonnes pratiques de ce cours: un .gitignore bien construit et une sÃ©paration des environnements de stockage du code et des donnÃ©es seront bien plus efficaces pour vous Ã©viter ces problÃ¨mes que tout les conseils de vigilance que vous pourrez trouver ailleurs.â†©ï¸\nLorsquâ€™on dÃ©veloppe du code qui finalement ne sâ€™avÃ¨re plus nÃ©cessaire, on a souvent un cas de conscience Ã  le supprimer et on prÃ©fÃ¨re le mettre de cÃ´tÃ©. Au final, ce syndrÃ´me de DiogÃ¨ne est mauvais pour la pÃ©rennitÃ© du projet : on se retrouve Ã  devoir maintenir une base de code qui nâ€™est, en pratique, pas utilisÃ©e. Ce nâ€™est pas un problÃ¨me de supprimer un code ; si finalement celui-ci sâ€™avÃ¨re utile, on peut le retrouver grÃ¢ce Ã  lâ€™historique Git et les outils de recherche sur Github. Le package vulture est trÃ¨s pratique pour diagnostiquer les morceaux de code inutiles dans un projet.â†©ï¸\nLe fichier __init__.py indique Ã  Python que le dossier est un package. Il permet de proposer certaines configurations lors de lâ€™import du package. Il permet Ã©galement de contrÃ´ler les objets exportÃ©s (câ€™est-Ã -dire mis Ã  disposition de lâ€™utilisateur) par le package par rapport aux objets internes au package. En le laissant vide, nous allons utiliser ce fichier pour importer lâ€™ensemble des fonctions de nos sous-modules. Ce nâ€™est pas la meilleure pratique mais un contrÃ´le plus fin des objets exportÃ©s demanderait un investissement qui ne vaut, ici, pas le coÃ»t.â†©ï¸\nSi vous dÃ©sirez aussi contrÃ´ler la version de Python, ce qui peut Ãªtre important dans une perspective de portabilitÃ©, vous pouvez ajouter une option, par exemple -p python3.10. NÃ©anmoins nous nâ€™allons pas nous embarasser de cette nuance pour la suite car nous pourrons contrÃ´ler la version de Python plus finement par le biais de Docker.â†©ï¸\nLâ€™option -c passÃ©e aprÃ¨s la commande python permet dâ€™indiquer Ã  Python que la commande ne se trouve pas dans un fichier mais sera dans le texte quâ€™on va directement lui fournir.â†©ï¸\nLâ€™option -c passÃ©e aprÃ¨s la commande python permet dâ€™indiquer Ã  Python que la commande ne se trouve pas dans un fichier mais sera dans le texte quâ€™on va directement lui fournir.â†©ï¸\nPour comparer les deux listes, vous pouvez utiliser la fonctionnalitÃ© de split du terminal sur VSCode pour comparer les outputs de conda env export en les mettant en face Ã  face.â†©ï¸\nIl est tout Ã  fait normal de ne pas parvenir Ã  crÃ©er une action fonctionnelle du premier coup. Nâ€™hÃ©sitez pas Ã  pusher votre code aprÃ¨s chaque question pour vÃ©rifier que vous parvenez bien Ã  rÃ©aliser chaque Ã©tape. Sinon vous risquez de devoir corriger bout par bout un fichier plus consÃ©quent.â†©ï¸\nIl existe une approche alternative pour faire des tests rÃ©guliers: les hooks Git. Il sâ€™agit de rÃ¨gles qui doivent Ãªtre satisfaites pour que le fichier puisse Ãªtre committÃ©. Cela assure que chaque commit remplisse des critÃ¨res de qualitÃ© afin dâ€™Ã©viter le problÃ¨me de la procrastination.\nLa documentation de pylint offre des explications supplÃ©mentaires. Ici, nous allons adopter une approche moins ambitieuse en demandant Ã  notre action de faire ce travail dâ€™Ã©valuation de la qualitÃ© de notre codeâ†©ï¸\nVous nâ€™Ãªtes pas obligÃ©s pour lâ€™Ã©valuation de mettre en oeuvre les jalons de plusieurs parcours. NÃ©anmoins, vous dÃ©couvrirez que chaque nouveau pas en avant est moins coÃ»teux que le prÃ©cÃ©dent si vous avez mis en oeuvre les rÃ©flexes des bonnes pratiques.â†©ï¸\nCe nâ€™est pas indispensable si vous avez une maniÃ¨re cohÃ©rente de gÃ©rer vos jetons dâ€™accÃ¨s aux donnÃ©es dans votre API, par exemple par le biais de service account. NÃ©anmoins, pour se faciliter la tÃ¢che, on ne va pas se poser de question sur les droits dâ€™accÃ¨s au modÃ¨le.â†©ï¸\nPar consÃ©quent, MLFLow bÃ©nÃ©ficie de lâ€™injection automatique des tokens pour pouvoir lire/Ã©crire sur S3. Ces jetons ont la mÃªme durÃ©e avant expiration que ceux de vos services interactifs VSCode. Il faut donc supprimer et rouvrir un service MLFLow rÃ©guliÃ¨rement. La maniÃ¨re dâ€™Ã©viter cela est de crÃ©er des service account sur https://minio-console.lab.sspcloud.fr/ et de les renseigner sur la page.â†©ï¸"
  },
  {
    "objectID": "chapters/code-quality.html",
    "href": "chapters/code-quality.html",
    "title": "QualitÃ© du code",
    "section": "",
    "text": "DÃ©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein Ã©cran.\nCe chapitre constitue une introduction Ã  la question de la qualitÃ© du code, premier niveau dans lâ€™Ã©chelle des bonnes pratiques. Celui-ci prÃ©sente les enjeux de la qualitÃ© du code, les principes gÃ©nÃ©raux pour amÃ©liorer celui-ci et quelques outils ou gestes faciles Ã  mettre en Å“uvre pour amÃ©liorer la qualitÃ© du code. Ceux-ci sont approfondis dans lâ€™application fil rouge."
  },
  {
    "objectID": "chapters/code-quality.html#lenjeu-dun-code-lisible-et-maintenable",
    "href": "chapters/code-quality.html#lenjeu-dun-code-lisible-et-maintenable",
    "title": "QualitÃ© du code",
    "section": "Lâ€™enjeu dâ€™un code lisible et maintenable",
    "text": "Lâ€™enjeu dâ€™un code lisible et maintenable\n\nâ€œThe code is read much more often than it is written.â€\nGuido Van Rossum1\n\nLorsquâ€™on sâ€™initie Ã  la pratique de la data science, il est assez naturel de voir le code dâ€™une maniÃ¨re trÃ¨s fonctionnelle : je veux rÃ©aliser une tÃ¢che donnÃ©e â€” par exemple un algorithme de classification â€” et je vais donc assembler dans un notebook des bouts de code, souvent trouvÃ©s sur internet, jusquâ€™Ã  obtenir un projet qui rÃ©alise la tÃ¢che voulue. La structure du projet importe assez peu, tant quâ€™elle permet dâ€™importer correctement les donnÃ©es nÃ©cessaires Ã  la tÃ¢che en question.\nSi cette approche flexible et minimaliste fonctionne trÃ¨s bien lors de la phase dâ€™apprentissage, il est malgrÃ© tout indispensable de sâ€™en dÃ©tacher progressivement Ã  mesure que lâ€™on progresse et que lâ€™on est amenÃ© Ã  rÃ©aliser des projets plus professionnels ou bien Ã  intÃ©grer des projets collaboratifs. Autrement, on risque de produire un code complexe Ã  reprendre et Ã  faire Ã©voluer, ce qui pourrait conduire inÃ©vitablement Ã  son abandon.\nEn particulier, il est important de proposer, parmi les multiples maniÃ¨res de rÃ©soudre un problÃ¨me informatique, une solution qui soit intelligible par dâ€™autres personnes parlant le mÃªme langage. Le code est en effet lu bien plus souvent quâ€™il nâ€™est Ã©crit, câ€™est donc avant tout un outil de communication. De mÃªme, la maintenance dâ€™un code demande gÃ©nÃ©ralement beaucoup plus de moyens que sa phase de dÃ©veloppement initial. Il est donc important de penser en amont la qualitÃ© de son code et la structure de son projet de sorte Ã  le rendre maintenable dans le temps.\nAfin de faciliter la communication et rÃ©duire la douleur dâ€™avoir Ã  faire Ã©voluer un code obscur, des tentatives plus ou moins institutionnalisÃ©es de dÃ©finir des conventions ont Ã©mergÃ©. Ces conventions dÃ©pendent naturellement du langage utilisÃ©, mais les principes sous-jacents sâ€™appliquent de maniÃ¨re universelle Ã  tout projet basÃ© sur du code."
  },
  {
    "objectID": "chapters/code-quality.html#de-limportance-de-suivre-les-conventions",
    "href": "chapters/code-quality.html#de-limportance-de-suivre-les-conventions",
    "title": "QualitÃ© du code",
    "section": "De lâ€™importance de suivre les conventions",
    "text": "De lâ€™importance de suivre les conventions\nPython est un langage trÃ¨s lisible. Avec un peu dâ€™effort sur le nom des objets, sur la gestion des dÃ©pendances et sur la structure du programme, on peut trÃ¨s bien comprendre un script sans avoir besoin de lâ€™exÃ©cuter. Câ€™est lâ€™une des principales forces du langage Python qui permet ainsi une acquisition rapide des bases et facilite lâ€™appropriation dâ€™un script.\nLa communautÃ© Python a abouti Ã  un certain nombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard dans lâ€™Ã©cosystÃ¨me Python. Les deux normes les plus connues sont :\n\nla norme PEP8 qui dÃ©finit un certain nombre de conventions relatives au code ;\nla norme PEP257 consacrÃ©e Ã  la documentation (docstrings).\n\nCes conventions vont au-delÃ  de la syntaxe. Un certain nombre de standards dâ€™organisation dâ€™un projet ont Ã©mergÃ©, qui seront abordÃ©es dans le prochain chapitre.\n\n\n\n\n\n\nComparaison avec \n\n\n\n\n\nDans lâ€™univers , la formalisation a Ã©tÃ© moins organisÃ©e. Ce langage est plus permissif que Python sur certains aspects2. NÃ©anmoins, des standards ont Ã©mergÃ© rÃ©cemment, Ã  travers un certain nombre de style guides dont les plus connus sont le tidyverse style guide et le google style guide, MLR style guideâ€¦\nPour aller plus loin sur :\n\nLa formation Insee aux bonnes pratiques avec Git et  dont le parcours est trÃ¨s proche de celui de ce cours ;\nDes Ã©lÃ©ments complÃ©mentaires dans la documentation collaborative utilitR ;\nCe post qui pointe vers un certain nombre de ressources sur le sujet.\n\n\n\n\nCes conventions sont arbitraires, dans une certaine mesure. Il est tout Ã  fait possible de trouver certaines conventions moins esthÃ©tiques que dâ€™autres.\nCes conventions ne sont pas non plus immuables : les langages et leurs usages Ã©voluent, ce qui nÃ©cessite de mettre Ã  jour les conventions. Cependant, adopter dans la mesure du possible certains des rÃ©flexes prÃ©conisÃ©s par ces conventions devrait amÃ©liorer la capacitÃ© Ã  Ãªtre compris par la communautÃ©, augmenter les chances de bÃ©nÃ©ficier dâ€™apport de celle-ci pour adapter le code, mais aussi rÃ©duire la difficultÃ© Ã  faire Ã©voluer un code.\nIl existe beaucoup de philosophies diffÃ©rentes sur le style de codage et, en fait, le plus important est la cohÃ©rence : si on choisit une convention, par exemple snake case (toto_a_la_plage) plutÃ´t que camel case (totoALaPlage), le mieux est de sâ€™y tenir."
  },
  {
    "objectID": "chapters/code-quality.html#un-bon-ide-un-premier-pas-vers-la-qualitÃ©",
    "href": "chapters/code-quality.html#un-bon-ide-un-premier-pas-vers-la-qualitÃ©",
    "title": "QualitÃ© du code",
    "section": "Un bon IDE, un premier pas vers la qualitÃ©",
    "text": "Un bon IDE, un premier pas vers la qualitÃ©\nSans les outils automatisÃ©s de mise en forme du code, lâ€™adoption des bonnes pratiques serait coÃ»teuse en temps et donc difficile Ã  mettre en Å“uvre au quotidien. Ces outils, que ce soit par le biais de diagnostics ou de mise aux normes automatisÃ©e du code rendent de prÃ©cieux services. Adopter les standards minimaux de qualitÃ© est plus ou moins instantanÃ© et Ã©conomise un temps prÃ©cieux dans la vie dâ€™un projet de data science. Câ€™est un prÃ©alable indispensable Ã  la mise en production, sur laquelle nous reviendrons ultÃ©rieurement.\nLe premier pas vers les bonnes pratiques est dâ€™adopter un environnement de dÃ©veloppement adaptÃ©. VSCode est un trÃ¨s bon environnement comme nous le dÃ©couvrirons dans la partie pratique. Il propose tous les outils dâ€™autocomplÃ©tion et de diagnostics usuels (contrairement Ã  Jupyter) et propose une grande gamme dâ€™extensions pour enrichir les fonctionnalitÃ©s de lâ€™IDE de maniÃ¨re contributive :\n\n\n\nExemple de diagnostics et dâ€™actions proposÃ©s par VSCode\n\n\nNÃ©anmoins, les outils de dÃ©tection de code au niveau des IDE ne suffisent pas. En effet, ils nÃ©cessitent une composante manuelle qui peut Ãªtre chronophage et ainsi pÃ©nible Ã  appliquer rÃ©guliÃ¨rement. Heureusement, il existe des outils automatisÃ©s de diagnostics et de mise en forme.\n\nLes outils automatisÃ©s pour le diagnostic et la mise en forme du code\nPython Ã©tant lâ€™outil de travail principal de milliers de data-scientists, un certain nombre dâ€™outils ont vu le jour pour rÃ©duire le temps nÃ©cessaire pour crÃ©er un projet ou disposer dâ€™un code fonctionnel. Ces outils permettent un gros gain de productivitÃ©, rÃ©duisent le temps passÃ© Ã  effectuer des tÃ¢ches rÃ©barbatives et amÃ©liorent la qualitÃ© dâ€™un projet en offrant des diagnostics, voire des correctifs Ã  des codes perfectibles.\nLes deux principaux types dâ€™outils sont les suivants :\n\nLinter : programme qui vÃ©rifie que le code est formellement conforme Ã  un certain guidestyle\n\nsignale des problÃ¨mes formels, sans corriger\n\nFormatter : programme qui reformate un code pour le rendre conforme Ã  un certain guidestyle\n\nmodifie directement le code\n\n\n\n\n\n\n\n\nExemples\n\n\n\n\n\n\nExemples dâ€™erreurs repÃ©rÃ©es par un linter :\n\nlignes de code trop longues ou mal indentÃ©es, parenthÃ¨ses non Ã©quilibrÃ©es, noms de fonctions mal construitsâ€¦\n\nExemples dâ€™erreurs non repÃ©rÃ©es par un linter :\n\nfonctions mal utilisÃ©es, arguments mal spÃ©cifiÃ©s, structure du code incohÃ©rente, code insuffisamment documentÃ©â€¦\n\n\n\n\n\n\n\nLes linters pour comprendre les mauvaises pratiques appliquÃ©es\nLes linters sont des outils qui permettent dâ€™Ã©valuer la qualitÃ© du code et son risque de provoquer une erreur (explicite ou silencieuse).\nVoici quelques exemples de problÃ¨mes que peuvent rencontrer les linters:\n\nles variables sont utilisÃ©es mais nâ€™existent pas (erreur)\nles variables inutilisÃ©es (inutiles)\nla mauvaise organisation du code (risque dâ€™erreur)\nle non-respect des bonnes pratiques dâ€™Ã©criture de code\nles erreurs de syntaxe (par exemple les coquilles)\n\nLa plupart des logiciels de dÃ©veloppement embarquent des fonctionnalitÃ©s de diagnostic (voire de suggestion de correctif). Il faut parfois les paramÃ©trer dans les options (ils sont dÃ©sactivÃ©s pour ne pas effrayer lâ€™utilisateur avec des croix rouges partout). NÃ©anmoins, si on nâ€™a pas appliquÃ© les correctifs au fil de lâ€™eau la masse des modifications Ã  mettre en Å“uvre peut Ãªtre effrayante.\nEn Python, les deux principaux linters sont PyLint et Flake8. Dans les exercices, nous proposons dâ€™utiliser PyLint qui est pratique et pÃ©dagogique. Celui-ci sâ€™utilise en ligne de commande, de la maniÃ¨re suivante :\n$ pip install pylint\n$ pylint monscript.py #pour un fichier\n$ pylint src #pour tous les fichiers du dossier src\n\n\n\n\n\n\nTip\n\n\n\n\n\nLâ€™un des intÃ©rÃªts dâ€™utiliser PyLint est quâ€™on obtient une note, ce qui est assez instructif. Nous lâ€™utiliserons dans lâ€™application fil rouge pour comprendre la maniÃ¨re dont chaque Ã©tape amÃ©liore la qualitÃ© du code.\nIl est possible de mettre en Å“uvre des pre commit hooks qui empÃªchent un commit nâ€™ayant pas une note minimale.\n\n\n\n\n\nLes formatters pour nettoyer en masse ses scripts\nLe formatter modifie directement le code. On peut faire un parallÃ¨le avec le correcteur orthographique. Cet outil peut donc induire un changement substantiel du script afin de le rendre plus lisible.\nLe formater le plus utilisÃ©\nest Black. RÃ©cemment, Ruff, qui est Ã  la fois un linter et un formatter a Ã©mergÃ© pour intÃ©grer Ã  Black des diagnostics supplÃ©mentaires, issus dâ€™autres packages.\n\n\n\n\n\n\nNote\n\n\n\n\n\nPour signaler sur Github la qualitÃ© dâ€™un projet utilisant Black, il est possible dâ€™ajouter un badge dans le README:\n\n\n\n\nIl est assez instructif de regarder le code modifiÃ© par les outils pour comprendre et corriger certains problÃ¨mes dans sa maniÃ¨re de dÃ©velopper. Par exemple, Ã  la lecture de ce chapitre, vous allez certainement retenir en particulier certaines rÃ¨gles qui tranchent avec vos pratiques actuelles. Vous pouvez alors essayer dâ€™appliquer ces nouvelles rÃ¨gles pendant un certain temps puis, lorsque celles-ci seront devenues naturelles, revenir Ã  ce guide et appliquer le processus Ã  nouveau. En procÃ©dant ainsi de maniÃ¨re incrÃ©mentale, vous amÃ©liorerez progressivement la qualitÃ© de vos projets sans avoir lâ€™impression de passer trop de temps sur des micro-dÃ©tails, au dÃ©triment des objectifs globaux du projet."
  },
  {
    "objectID": "chapters/code-quality.html#le-partage-une-dÃ©marche-favorable-Ã -la-qualitÃ©-du-code",
    "href": "chapters/code-quality.html#le-partage-une-dÃ©marche-favorable-Ã -la-qualitÃ©-du-code",
    "title": "QualitÃ© du code",
    "section": "Le partage, une dÃ©marche favorable Ã  la qualitÃ© du code",
    "text": "Le partage, une dÃ©marche favorable Ã  la qualitÃ© du code\n\nLâ€™opensource comme moyen pour amÃ©liorer la qualitÃ©\nEn ouvrant son code sur des forges opensource (cf.Â chapitre Git), il est possible de recevoir des suggestions voire, des contributions de rÃ©-utilisateurs du code. Cependant, les vertus de lâ€™ouverture vont au-delÃ . En effet, lâ€™ouverture se traduit gÃ©nÃ©ralement par des codes de meilleure qualitÃ©, mieux documentÃ©s pour pouvoir Ãªtre rÃ©utilisÃ©s ou ayant simplement bÃ©nÃ©ficiÃ© dâ€™une attention accrue sur la qualitÃ© pour ne pas paraÃ®tre ridicule. MÃªme en lâ€™absence de retour de (rÃ©)utilisateurs du code, le partage de code amÃ©liore la qualitÃ© des projets.\n\n\nLa revue de code\nLa revue de code sâ€™inspire de la mÃ©thode du peer reviewing du monde acadÃ©mique pour amÃ©liorer la qualitÃ© du code Python. Dans une revue de code, le code Ã©crit par une personne est relu et Ã©valuÃ© par un ou plusieurs autres dÃ©veloppeurs afin dâ€™identifier les erreurs et les amÃ©liorations possibles. Cette pratique permet de dÃ©tecter les erreurs avant quâ€™elles ne deviennent des problÃ¨mes majeurs, dâ€™assurer une cohÃ©rence dans le code, de garantir le respect des bonnes pratiques mais aussi dâ€™amÃ©liorer la qualitÃ© du code en identifiant les parties du code qui peuvent Ãªtre simplifiÃ©es, optimisÃ©es ou refactorisÃ©es pour en amÃ©liorer la lisibilitÃ© et la maintenabilitÃ©.\nUn autre avantage de cette approche est quâ€™elle permet le partage de connaissances entre des personnes expÃ©rimentÃ©es et des personnes plus dÃ©butantes ce qui permet Ã  ces derniÃ¨res de monter en compÃ©tence. Github  et Gitlab  proposent des fonctionnalitÃ©s trÃ¨s pratiques pour la revue de code : discussions, suggestions de modificationsâ€¦"
  },
  {
    "objectID": "chapters/code-quality.html#objectifs",
    "href": "chapters/code-quality.html#objectifs",
    "title": "QualitÃ© du code",
    "section": "Objectifs",
    "text": "Objectifs\n\nFavoriser la concision pour rÃ©duire le risque dâ€™erreur et rendre la dÃ©marche plus claire ;\nAmÃ©liorer la lisibilitÃ© ce qui est indispensable pour rendre la dÃ©marche intelligible par dâ€™autres mais aussi pour soi, lorsquâ€™on reprend un code Ã©crit il y a quelques temps ;\nLimiter la redondance ce qui permet de simplifier un code (paradigme du donâ€™t repeat yourself) ;\nLimite les risques dâ€™erreurs liÃ©es aux copier/coller"
  },
  {
    "objectID": "chapters/code-quality.html#avantages-des-fonctions",
    "href": "chapters/code-quality.html#avantages-des-fonctions",
    "title": "QualitÃ© du code",
    "section": "Avantages des fonctions",
    "text": "Avantages des fonctions\nLes fonctions ont de nombreux avantages par rapport Ã  de longs scripts :\n\nLimite les risques dâ€™erreurs liÃ©s aux copier/coller\nRend le code plus lisible et plus compact\nUn seul endroit du code Ã  modifier lorsquâ€™on souhaite modifier le traitement\nFacilite la rÃ©utilisation et la documentation du code !\n\n\n\n\n\n\n\nRÃ¨gle dâ€™or\n\n\n\nIl faut utiliser une fonction dÃ¨s quâ€™on utilise une mÃªme portion de code plus de deux fois (donâ€™t repeat yourself (DRY))\n\n\n\n\n\n\n\n\nRÃ¨gles pour Ã©crire des fonctions pertinentes\n\n\n\n\nUne tÃ¢che = une fonction\nUne tÃ¢che complexe = un enchaÃ®nement de fonctions rÃ©alisant chacune une tÃ¢che simple\nLimiter lâ€™utilisation de variables globales\n\n\n\nEn ce qui concerne lâ€™installation des packages, nous allons voir dans les parties Structure de code et PortabilitÃ© quâ€™il ne faut pas gÃ©rer ceci dans le script mais dans un Ã©lÃ©ment Ã  part, relatif Ã  lâ€™environnement dâ€™exÃ©cution du projet3. De mÃªme, ces parties prÃ©senteront des conseils pratiques sur la gestion des jetons dâ€™accÃ¨s Ã  des API ou bases de donnÃ©es qui ne doivent jamais Ãªtre inscrites dans un code.\nLes scripts trop longs ne sont pas une bonne pratique. Il est prÃ©fÃ©rable de diviser lâ€™ensemble des scripts exÃ©cutant une chaÃ®ne de production en â€œmonadesâ€, câ€™est-Ã -dire en petites unitÃ©s cohÃ©rentes. Les fonctions sont un outil privilÃ©giÃ© pour cela (en plus de limiter la redondance, et dâ€™Ãªtre un outil privilÃ©giÃ© pour documenter un code).\n\n\n\n\n\n\nExemple: privilÃ©gier les list comprehensions\n\n\n\n\n\nEn Python, il est recommandÃ© de privilÃ©gier les list comprehensions Ã  lâ€™utilisation de boucles for indentÃ©es. Ces derniÃ¨res sont en gÃ©nÃ©ral moins efficaces et surtout impliquent un nombre important de ligne de codes lÃ  oÃ¹ les comprÃ©hensions de listes sont beaucoup plus concises\nliste_nombres = range(10)\n\n# trÃ¨s mauvais\ny = []\nfor x in liste_nombres:\n    if x % 2 == 0:\n        y.append(x*x)\n\n# mieux\ny = [x*x for x in liste_nombres if x % 2 == 0]\n\n\n\n\nConseils pour la programmation\nDans le monde de la programmation en Python, il existe deux paradigmes diffÃ©rents :\n\nLa programmation fonctionnelle est une approche qui construit un code en enchaÃ®nant des fonctions, câ€™est-Ã -dire des opÃ©rations plus ou moins standardisÃ©es ;\nLa programmation orientÃ©e objet (POO) consiste Ã  construire son code en dÃ©finissant des objets dâ€™une certaine classe ayant des attributs (les caractÃ©ristiques intrinsÃ¨ques de lâ€™objet) et sur lequel on effectue des opÃ©rations ad hoc par le biais de mÃ©thodes qui encapsulent des opÃ©rations propres Ã  chaque classe.\n\n\n\nExemple de comparaison des deux paradigmes\n\nMerci ChatGPT pour lâ€™exemple :\n\nclass AverageCalculator:\n    def __init__(self, numbers):\n        self.numbers = numbers\n\n    def calculate_average(self):\n        return sum(self.numbers) / len(self.numbers)\n\n# Utilisation\ncalculator = AverageCalculator([1, 2, 3, 4, 5])\nprint(\"Moyenne (POO):\", calculator.calculate_average())\n\ndef calculate_average(numbers):\n    return sum(numbers) / len(numbers)\n\n# Utilisation\nnumbers = [1, 2, 3, 4, 5]\nprint(\"Moyenne (PF):\", calculate_average(numbers))\n\nMoyenne (POO): 3.0\nMoyenne (PF): 3.0\n\n\n\nLa programmation fonctionnelle est plus intuitive que la POO et permet souvent de dÃ©velopper du code plus rapidement. La POO est une approche plus formaliste. Celle-ci est intÃ©ressante lorsquâ€™une fonction doit sâ€™adapter au type dâ€™objet en entrÃ©e (par exemple aller chercher des poids diffÃ©rents selon le type de modÃ¨le Pytorch). Cela Ã©vite les codes spaghetti ğŸ inutilement complexes qui sont impossibles Ã  dÃ©bugger.\nNÃ©anmoins, il convient dâ€™Ãªtre pragmatique. La programmation orientÃ©e objet peut Ãªtre plus complexe Ã  mettre en Å“uvre que la programmation fonctionnelle. Dans de nombreuses situations, cette derniÃ¨re, si elle est bien faite, suffit largement. Il est utile lorsquâ€™on dÃ©veloppe dans le cadre dâ€™un projet important dâ€™adopter une approche dite de programmation dÃ©fensive. Il sâ€™agit dâ€™un principe de prÃ©caution dans le paradigme de la programmation fonctionnelle qui vise Ã  limiter les situations imprÃ©vues en Ã©tant capable de gÃ©rer, par exemple, un argument dâ€™une fonction inattendu ou un objet Ã  la structure diffÃ©rente de celle pour lequel le code a Ã©tÃ© pensÃ©.\n\n\n\n\n\n\nLe code spaghetti\n\n\n\nLe code spaghetti est un style dâ€™Ã©criture qui favorise lâ€™apparition du syndrome du plat de spaghettis : un code impossible Ã  dÃ©mÃªler parce quâ€™il fait un usage excessif de conditions, dâ€™exceptions en tous sens, de gestion des Ã©vÃ©nements complexes. Il devient quasi impossible de savoir quelles ont Ã©tÃ© les conditions Ã  lâ€™origine de telle ou telle erreur sans exÃ©cuter ligne Ã  ligne (et celles-ci sont excessivement nombreuses du fait de mauvaises pratiques de programmation) le programme.\nEn fait, la programmation spaghetti qualifie tout ce qui ne permet pas de dÃ©terminer le qui, le quoi et le comment. Le code est donc plus long Ã  mettre Ã  jour car cela nÃ©cessite de remonter un Ã  un le fil des renvois.\n\n\n\n\n\n\n\n\nUn exemple progressif pour comprendre\n\n\n\n\n\nğŸ’¡ Supposons quâ€™on dispose dâ€™une table de donnÃ©es qui utilise le code âˆ’99 pour reprÃ©senter les valeurs manquantes. On dÃ©sire remplacer lâ€™ensemble des âˆ’99 par des NA.\nVoici un code Python qui permet de se placer dans ce cas qui, malheureusement, arrive frÃ©quemment.\n# On fixe la racine pour Ãªtre sÃ»r de tous avoir le mÃªme dataset\nnp.random.seed(1234)\n\n# On crÃ©Ã© un dataframe\na = np.random.randint(1, 10, size = (5,6))\ndf = np.insert(\n    a,\n    np.random.choice(len(a), size=6),\n    -99,\n)\ndf = pd.DataFrame(df.reshape((6,6)), columns=[chr(x) for x in range(97, 103)])\nUn premier jet de code pourrait prendre la forme suivante :\n# Dupliquer les donnÃ©es\ndf2 = df.copy()\n# Remplacer les -99 par des NA\ndf2.loc[df2['a'] == -99,'a'] = np.nan\ndf2.loc[df2['b'] == -99,'b'] = np.nan\ndf2.loc[df2['c'] == -99,'c'] = np.nan\ndf2.loc[df2['d'] == -99,'d'] = np.nan\ndf2.loc[df2['e'] == -98,'e'] = np.nan\ndf2.loc[df2['f'] == -99,'e'] = np.nan\nQuelles sont les choses qui vous dÃ©rangent dans le code ci-dessus ?\n\n\nIndice ğŸ’¡ Regardez prÃ©cisÃ©ment le code et le DataFrame, notamment les colonnes e et g.\n\nIl y a deux erreurs, difficiles Ã  dÃ©tecter:\n\ndf2.loc[df2['e'] == -98,'e'] = np.nan: une erreur de copier-coller sur la valeur de lâ€™erreur ;\ndf2.loc[df2['f'] == -99,'e'] = np.nan: une erreur de copier-coller sur les colonnes en question\n\n\nOn peut noter au moins deux trois :\n\nLe code est long et rÃ©pÃ©titif, ce qui nuit Ã  sa lisibilitÃ© ;\nLe code est trÃ¨s dÃ©pendant de la structure des donnÃ©es (nom et nombre de colonnes) et doit Ãªtre adaptÃ© dÃ¨s que celle-ci Ã©volue ;\nOn a introduit des erreurs humaines dans le code, difficiles Ã  dÃ©tecter.\n\nOn voit dans la premiÃ¨re version de notre code quâ€™il y a une structure commune Ã  toutes nos lignes de la forme .[. == -99] = np.nan. Cette structure va servir de base Ã  notre fonction, en vue de gÃ©nÃ©raliser le traitement que nous voulons faire.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2['a'] = fix_missing(df['a'])\ndf2['b'] = fix_missing(df['b'])\ndf2['c'] = fix_missing(df['c'])\ndf2['d'] = fix_missing(df['d'])\ndf2['e'] = fix_missing(df['e'])\ndf2['f'] = fix_missing(df['f'])\nCette seconde version du code est meilleure que la premiÃ¨re version, car on a rÃ©glÃ© le problÃ¨me dâ€™erreur humaine (il nâ€™est plus possible de taper -98 au lieu de -99).\n\n\nMais voyez-vous le problÃ¨me qui persiste ?\n\nLe code reste long et rÃ©pÃ©titif, et nâ€™Ã©limine pas encore toute possibilitÃ© dâ€™erreur, car il est toujours possible de se tromper dans le nom des variables.\n\nLa prochaine Ã©tape consiste Ã  Ã©liminer ce risque dâ€™erreur en combinant deux fonctions (ce quâ€™on appelle la combinaison de fonctions).\nLa premiÃ¨re fonction fix_missing() sert Ã  rÃ©gler le problÃ¨me sur un vecteur. La seconde gÃ©nÃ©ralisera ce procÃ©dÃ© Ã  toutes les colonnes. Comme Pandas permet une approche vectorielle, il est frÃ©quent de construire des fonctions sur des vecteurs et les appliquer ensuite Ã  plusieurs colonnes.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2 = df2.apply(fix_missing)\nCette troisiÃ¨me version du code a plusieurs avantages sur les deux autres versions :\n\nElle est plus concise et plus lisible ;\nSi on a un changement de code pour les valeurs manquantes, il suffit de le mettre Ã  un seul endroit ;\nElle fonctionne quel que soit le nombre de colonnes et le nom des colonnes ;\nOn ne peut pas traiter une colonne diffÃ©remment des autres par erreur.\n\nDe plus, le code est facilement gÃ©nÃ©ralisable.\nPar exemple, Ã  partir de la mÃªme structure, Ã©crire le code qui permet de ne traiter que les colonnes a,b et e ne demande pas beaucoup dâ€™Ã©nergie.\ndf2 = df.copy()\ndf2[['a','b','e']] = df2[['a','b','e']].apply(fix_missing)\n\n\n\nUn certain nombre de conseils sont prÃ©sents dans le Hitchhikerâ€™s Guide to Python qui vise Ã  faire connaÃ®tre les prÃ©ceptes du â€œZen of Pythonâ€ (PEP 20). Ce post de blog illustre quelques uns de ces principes avec des exemples.\n\n\n\n\n\n\nLe Zen de Python\n\n\n\n\n\nLe â€œZen de Pythonâ€ est une collection de principes pour la programmation en Python, Ã©crite par Tim Peters en 2004 sous la forme dâ€™aphorismes. Ceux-ci mettent en lumiÃ¨re la philosophie de conception du langage Python.\nVous pouvez retrouver ces conseils dans Python en tapant le code suivant:\n\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "chapters/code-quality.html#footnotes",
    "href": "chapters/code-quality.html#footnotes",
    "title": "QualitÃ© du code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGuido Van Rossum est le crÃ©ateur de , câ€™est donc quelquâ€™un quâ€™il est pertinent dâ€™Ã©couter.â†©ï¸\nPar exemple, en , il est possible dâ€™utiliser &lt;- ou = pour lâ€™assignation, on ne recontre pas dâ€™erreur en cas de mauvaise indentationâ€¦â†©ï¸\nNous prÃ©senterons les deux approches principales en Python, leurs points commun et les points par lesquels ils diffÃ¨rent : les environnements virtuels (gÃ©rÃ©s par un fichier requirements.txt) et les environnements conda (gÃ©rÃ©s par un fichier environment.yml)â†©ï¸"
  },
  {
    "objectID": "chapters/galerie/2024/resultAthle.html",
    "href": "chapters/galerie/2024/resultAthle.html",
    "title": "ResultAthle",
    "section": "",
    "text": "ResultAthle est un projet visant Ã  rendre les outils statistiques dâ€™analyse de performance plus accessibles au niveau amateur en athlÃ©tisme. Il aborde les dÃ©fis de la collecte de rÃ©sultats et le manque de statistiques descriptives accessibles pour les clubs."
  },
  {
    "objectID": "chapters/galerie/2024/model.html",
    "href": "chapters/galerie/2024/model.html",
    "title": "ModÃ¨le de carte",
    "section": "",
    "text": "Une description en quelques mots du projet"
  },
  {
    "objectID": "chapters/mlops.html",
    "href": "chapters/mlops.html",
    "title": "Introduction aux enjeux du MLOps",
    "section": "",
    "text": "Dans les chapitres prÃ©cÃ©dents, nous avons vu quâ€™une majoritÃ© des projets data-driven restaient au stade de lâ€™expÃ©rimentation, et quâ€™une des raisons pour expliquer ce phÃ©nomÃ¨ne Ã©tait lâ€™existence de frictions empÃªchant lâ€™amÃ©lioration continue des projets. Dans le cadre des projets basÃ©s sur des modÃ¨les de machine learning, cette problÃ©matique devient encore plus cruciale : en supplÃ©ment des enjeux sur le cycle de vie de la donnÃ©e intervient la dimension supplÃ©mentaire du cycle de vie des modÃ¨les. Parmi les principaux enjeux, une question souvent Ã©ludÃ©e dans les enseignements ou les nombreuses ressources en ligne sur le machine learning est la problÃ©matique des rÃ©-entraÃ®nements pÃ©riodiques, guidÃ©s par lâ€™utilisation faite des modÃ¨les et les retours des utilisateurs, afin de maintenir Ã  jour la base de connaissance des modÃ¨les et ainsi garantir leur pouvoir prÃ©dictif. Ce sujet du rÃ©-entraÃ®nement des modÃ¨les rend les aller-retours entre les phases dâ€™expÃ©rimentation et de production nÃ©cessairement frÃ©quents. Pour faciliter la mise en place de pipelines favorisant ces boucles de rÃ©troaction, une nouvelle approche a Ã©mergÃ© : le MLOps, qui vise lÃ  encore Ã  mobiliser les concepts et outils issus de lâ€™approche DevOps tout en les adaptant au contexte et aux spÃ©cificitÃ©s des projets de machine learning."
  },
  {
    "objectID": "chapters/mlops.html#du-devops-au-mlops",
    "href": "chapters/mlops.html#du-devops-au-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Du DevOps au MLOps",
    "text": "Du DevOps au MLOps\nLâ€™approche MLOps sâ€™est construite sur les bases de lâ€™approche DevOps. En cela, on peut considÃ©rer quâ€™il sâ€™agit simplement dâ€™une extension de lâ€™approche DevOps, dÃ©veloppÃ©e pour rÃ©pondre aux dÃ©fis spÃ©cifiques liÃ©s Ã  la gestion du cycle de vie des modÃ¨les de machine learning. Le MLOps intÃ¨gre les principes de collaboration et dâ€™automatisation propres au DevOps, mais prend Ã©galement en compte tous les aspects liÃ©s aux donnÃ©es et aux modÃ¨les de machine learning.\n\n\n\n\n\n\n\nA mettre en regard Ã  la boucle du DevOps\n\n\n\nLe MLOps implique lâ€™automatisation des tÃ¢ches telles que la gestion des donnÃ©es, le suivi des versions des modÃ¨les, leurs dÃ©ploiements, ainsi que lâ€™Ã©valuation continue de la performance des modÃ¨les en production. De la mÃªme maniÃ¨re que le DevOps, le MLOps met lâ€™accent sur la collaboration Ã©troite entre les Ã©quipes de dÃ©veloppement et dâ€™administration systÃ¨me dâ€™une part, ainsi que les Ã©quipes de data science dâ€™autre part. Cette collaboration est clÃ© pour garantir une communication efficace tout au long du cycle de vie du modÃ¨le de machine learning et fludifier le passage entre les Ã©tapes dâ€™expÃ©rimentation et de passage en production."
  },
  {
    "objectID": "chapters/mlops.html#principes-du-mlops",
    "href": "chapters/mlops.html#principes-du-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Principes du MLOps",
    "text": "Principes du MLOps\nPuisque le MLOps est ainsi une extension des principes du DevOps aux enjeux du machine learning, les principes gÃ©nÃ©raux sont les mÃªmes que ceux Ã©voquÃ©s prÃ©cÃ©demment mais ceux-ci sâ€™adaptent Ã  la problÃ©matique de la gestion du cycle de vie dâ€™un modÃ¨le:\n\nla reproductibilitÃ© : les rÃ©sultats de chaque expÃ©rimentation, fructueuse comme infructueuse, doivent pouvoir Ãªtre reproduits sans coÃ»t. Cela implique dâ€™abord une certaine rigueur dans la gestion des packages, la gestion des environnements, la gestion des librairies systÃ¨me, le contrÃ´le de version du code, etc.\nle contrÃ´le de version: au-delÃ  du simple suivi des versions du code, pour reproduire de maniÃ¨re identique les rÃ©sultats dâ€™un code câ€™est lâ€™ensemble des inputs et paramÃ¨tres influenÃ§ant lâ€™entraÃ®nement dâ€™un modÃ¨le (donnÃ©es dâ€™entraÃ®nement, hyper-paramÃ¨tres, etc.) qui doivent Ãªtre versionnÃ©es avec le modÃ¨le ;\nlâ€™automatisation : afin de favoriser les boucles rÃ©troactives dâ€™amÃ©lioration continue, le cycle de vie du modÃ¨le (tests, build, validation, dÃ©ploiement) doit Ãªtre automatisÃ© au maximum. Les outils issus de lâ€™approche DevOps, en particulier lâ€™intÃ©gration et dÃ©ploiement continus (CI/CD), doivent Ãªtre mobilisÃ©s ;\nla collaboration : valoriser une culture de travail collaborative autour des projets de ML, dans laquelle la communication au sein des Ã©quipes doit permettre de rÃ©duire le travail en silos et bÃ©nÃ©ficier des expertises des diffÃ©rents mÃ©tiers parti prenantes dâ€™un modÃ¨le (analystes, data engineers, devs..). Sur le plan technique, les outils MLOps utilisÃ©s doivent favoriser le travail collaboratif sur les donnÃ©es, le modÃ¨le et le code utilisÃ©s par le projet ;\nlâ€™amÃ©lioration continue : une fois dÃ©ployÃ©, il est essentiel de sâ€™assurer que le modÃ¨le fonctionne bien comme attendu en Ã©valuant ses performances sur des donnÃ©es rÃ©elles Ã  lâ€™aide dâ€™outils de monitoring en continu. Dans le cas dâ€™une dÃ©gradation des performances dans le temps, un rÃ©-entraÃ®nement pÃ©riodique ou un entraÃ®nement en continu du modÃ¨le doivent Ãªtre envisagÃ©s.\n\nPour plus de dÃ©tails, voir Kreuzberger, KÃ¼hl, and Hirschl (2023)."
  },
  {
    "objectID": "chapters/mlops.html#entraÃ®nements-des-modÃ¨les",
    "href": "chapters/mlops.html#entraÃ®nements-des-modÃ¨les",
    "title": "Introduction aux enjeux du MLOps",
    "section": "1ï¸âƒ£ EntraÃ®nements des modÃ¨les",
    "text": "1ï¸âƒ£ EntraÃ®nements des modÃ¨les\nLa premiÃ¨re Ã©tape dâ€™un projet de machine learning correspond Ã  tout ce que lâ€™on effectue jusquâ€™Ã  lâ€™entraÃ®nement des premiers modÃ¨les. Cette Ã©tape est un processus itÃ©ratif et fastidieux qui ne suit pas un dÃ©veloppement linÃ©aire : les mÃ©thodes de rÃ©cupÃ©ration des donnÃ©es peuvent Ãªtre changeantes, le preprocessing peut varier, de mÃªme que la sÃ©lection des features pour le modÃ¨le (feature engineering), et les algorithmes testÃ©s peuvent Ãªtre nombreuxâ€¦ On est donc aux antipodes des hypothÃ¨ses habituelles de stabilitÃ© nÃ©cessaires Ã  lâ€™entraÃ®nement et la validitÃ© externe dans les enseignements de machine learning.\nGarder une trace de tous les essais effectuÃ©s apparaÃ®t indispensable afin de savoir ce qui a fonctionnÃ© ou non. Le besoin dâ€™archiver ne concerne pas que les mÃ©triques de performances associÃ©es Ã  un jeu de paramÃ¨tres. Ceux-ci ne sont quâ€™une partie des ingrÃ©dients nÃ©cessaires pour aboutir Ã  une estimation. Lâ€™ensemble des inputs dâ€™un processus de production (code, donnÃ©es, configuration logicielle, etc.) est Ã©galement Ã  conserver pour Ãªtre en mesure de rÃ©pliquer une expÃ©rimentation.\n\n\n\n\n\n\nLe tracking server de MLFlow, un environnement idÃ©al pour archiver des expÃ©rimentations\n\n\n\nLa phase exploratoire est rendue trÃ¨s simple grÃ¢ce au Tracking Server de MLFlow. Comme cela sera expliquÃ© ultÃ©rieurement, lors de lâ€™exÃ©cution dâ€™un run, MLflow enregistre tout un tas de mÃ©tadonnÃ©es qui permettent de retrouver toutes les informations relatives Ã  ce run : la date, le hash du commit, les paramÃ¨tres du modÃ¨le, le dataset utilisÃ©, les mÃ©triques spÃ©cifiÃ©es, etc. Cela permet non seulement de comparer les diffÃ©rents essais rÃ©alisÃ©s, mais aussi dâ€™Ãªtre capable de reproduire un run passÃ©.\n\n\nDe maniÃ¨re gÃ©nÃ©rale, cette phase exploratoire est rÃ©alisÃ©e par le data scientist ou le ML engineer dans des notebooks. Ces notebooks sont en effet parfaitement adaptÃ©s pour cette Ã©tape puisquâ€™ils permettent une grande flexibilitÃ© et sont particuliÃ¨rement commodes pour effectuer des tests. En revanche, lorsque lâ€™on souhaite aller plus loin et que lâ€™on vise une mise en production de son projet, les notebooks ne sont plus adaptÃ©s, et cela pour diverses raisons :\n\nla collaboration est grandement limitÃ©e Ã  cause dâ€™une compatibilitÃ© trÃ¨s faible avec les outils de contrÃ´le de version standard (notamment Git).\nlâ€™automatisation de pipeline est beaucoup plus compliquÃ©e et peu lisible. Il existe certes des packages qui permettent dâ€™automatiser des pipelines de notebooks comme Elyra par exemple, mais ce nâ€™est clairement pas lâ€™approche que nous vous recommandons car les scripts sont beaucoup moins usine Ã  gaz.\nLes workflows sont souvent moins clairs, mal organisÃ©s (toutes les fonctions dÃ©finies dans le mÃªme fichier affectant la lisibilitÃ© du code par exemple) voire peu reproductibles car les cellules sont rarement ordonnÃ©es de sorte Ã  exÃ©cuter le code de maniÃ¨re linÃ©aire.\nLes notebooks offrent gÃ©nÃ©ralement une modularitÃ© insuffisante lorsque lâ€™on veut travailler avec des composants de machine learning complexes.\n\nToutes ces raisons nous amÃ¨nent Ã  vous conseiller de rÃ©duire au maximum votre utilisation de notebooks et de restreindre leur utilisation Ã  la phase exploratoire ou Ã  la diffusion de rÃ©sultats/rapports. Passer le plus tÃ´t possible Ã  des scripts .py vous permettra de rÃ©duire le coÃ»t de la mise en production. Pour reprendre ce qui a dÃ©jÃ  Ã©tÃ© Ã©voquÃ© dans le chapitre Architecture des projets, nous vous invitons Ã  favoriser une structure modulaire de sorte Ã  pouvoir industrialiser votre projet.\nUne autre spÃ©cificitÃ© pouvant impacter la mise en production concerne la maniÃ¨re dont lâ€™entraÃ®nement est rÃ©alisÃ©. Il existe pour cela 2 Ã©coles qui ont chacune leurs avantages et dÃ©savantages : le batch training et lâ€™online training.\n\nBatch training\nLe batch training est la maniÃ¨re usuelle dâ€™entraÃ®ner un modÃ¨le de machine learning. Cette mÃ©thode consiste Ã  entraÃ®ner son modÃ¨le sur un jeu de donnÃ©es fixe dâ€™une seule traite. Le modÃ¨le est entraÃ®nÃ© sur lâ€™intÃ©gralitÃ© des donnÃ©es disponibles et les prÃ©dictions sont rÃ©alisÃ©es sur de nouvelles donnÃ©es. Cela signifie que le modÃ¨le nâ€™est pas mis Ã  jour une fois quâ€™il est entraÃ®nÃ©, et quâ€™il est nÃ©cessaire de le rÃ©-entraÃ®ner si lâ€™on souhaite ajuster ses poids. Cette mÃ©thode est relativement simple Ã  mettre en Å“uvre : il suffit dâ€™entraÃ®ner le modÃ¨le une seule fois, de le dÃ©ployer, puis de le rÃ©-entraÃ®ner ultÃ©rieurement en cas de besoin. Cependant, cette simplicitÃ© comporte des inconvÃ©nients : le modÃ¨le reste statique, nÃ©cessitant un rÃ©-entraÃ®nement frÃ©quent pour intÃ©grer de nouvelles donnÃ©es. Par exemple, dans le cas de la dÃ©tection de spams, si un nouveau type de spam apparaÃ®t, le modÃ¨le entraÃ®nÃ© en batch ne sera pas capable de le dÃ©tecter sans un rÃ©-entraÃ®nement complet. De plus, cette mÃ©thode peut rapidement exiger une grande quantitÃ© de mÃ©moire en fonction de la taille du jeu de donnÃ©es, ce qui peut poser des contraintes sur lâ€™infrastructure et prolonger considÃ©rablement le temps dâ€™entraÃ®nement.\n\n\nOnline training\nLâ€™online training se prÃ©sente comme lâ€™antithÃ¨se du batch training, car il se dÃ©roule de maniÃ¨re incrÃ©mentale. Dans ce mode dâ€™entraÃ®nement, de petits lots de donnÃ©es sont envoyÃ©s sÃ©quentiellement Ã  lâ€™algorithme, ce qui permet Ã  celui-ci de mettre Ã  jour ses poids Ã  chaque nouvelle donnÃ©e reÃ§ue. Cette approche permet au modÃ¨le de dÃ©tecter efficacement les variations dans les donnÃ©es en temps rÃ©el. Il est toutefois crucial de bien ajuster le learning rate afin dâ€™Ã©viter que le modÃ¨le oublie les informations apprises sur les donnÃ©es prÃ©cÃ©dentes. Lâ€™un des principaux avantages de cette mÃ©thode est sa capacitÃ© Ã  permettre un entraÃ®nement continu mÃªme lorsque le modÃ¨le est en production, ce qui se traduit par une rÃ©duction des coÃ»ts computationnels. De plus, lâ€™online training est particuliÃ¨rement adaptÃ© aux situations oÃ¹ les donnÃ©es dâ€™entrÃ©e Ã©voluent frÃ©quemment, comme dans le cas des prÃ©dictions de cours de bourse. Cependant, sa mise en Å“uvre dans un contexte de production est bien plus complexe que celle du batch training, et les frameworks traditionnels de machine learning tels que Scikit-learn, PyTorch, TensorFlow et Keras ne sont pas compatibles avec cette approche.\n\n\nDistribuer lâ€™optimisation des hyperparamÃ¨tres\nUne autre spÃ©cificitÃ© des modÃ¨les de machine learning rÃ©side dans le nombre important dâ€™hyperparamÃ¨tres Ã  optimiser, lesquels peuvent sensiblement impacter les performances du modÃ¨le. Lâ€™approche standard pour rÃ©aliser cette optimisation est ce quâ€™on appelle un Grid Search. Il sâ€™agit simplement de lister toutes les combinaisons dâ€™hyperparamÃ¨tres Ã  tester et dâ€™entraÃ®ner successivement des modÃ¨les avec ces combinaisons prÃ©dÃ©finies. Il nâ€™est pas difficile de comprendre que cette technique est trÃ¨s coÃ»teuse en temps de calcul lorsque le nombre dâ€™hyperparamÃ¨tres Ã  optimiser et leurs modalitÃ©s Ã  tester sont Ã©levÃ©s. Cependant, cette optimisation est indispensable pour entraÃ®ner le meilleur modÃ¨le pour notre tÃ¢che, et si sâ€™inspirer de la littÃ©rature est crucial pour limiter le domaine dâ€™optimisation de nos hyperparamÃ¨tres, rÃ©aliser un Grid Search est une Ã©tape incontournable.\nAinsi, pour sâ€™inscrire dans lâ€™approche du MLOps, une bonne mÃ©thode est dâ€™automatiser cette optimisation des hyperparamÃ¨tres en la distribuant sur un cluster lorsquâ€™on dispose de lâ€™infrastructure adÃ©quate. Lâ€™idÃ©e est de crÃ©er des processus indÃ©pendants, chacun liÃ©s Ã  une combinaison de nos hyperparamÃ¨tres, et dâ€™entraÃ®ner notre modÃ¨le sur ceux-ci puis dâ€™enregister les informations Ã  archiver dans un environnement adÃ©quat, par exemple dans MLFlow.\nIl existe un moteur de workflow populaire pour orchestrer des tÃ¢ches parallÃ¨les sur un cluster Kubernetes : Argo Workflow. Le principe est de dÃ©finir un workflow dans lequel chaque Ã©tape correspond Ã  un conteneur contenant uniquement ce qui est strictement nÃ©cessaire Ã  lâ€™exÃ©cution de cette Ã©tape. Ainsi, on sâ€™approche de la perfection en ce qui concerne la reproductibilitÃ©, car on maÃ®trise totalement les installations nÃ©cessaires Ã  lâ€™exÃ©cution de notre entraÃ®nement. Un workflow Ã  plusieurs Ã©tapes peut ainsi Ãªtre modÃ©lisÃ© comme un graphe acyclique orientÃ©, et lâ€™exemple ci-dessous reprÃ©sente un cas dâ€™optimisation dâ€™hyperparamÃ¨tres :\n\n\n\nWorkflow dâ€™optimisation dâ€™hyperparamÃ¨tres en parallÃ¨le\n\n\nCette approche permet dâ€™exÃ©cuter facilement en parallÃ¨le des tÃ¢ches intensives en calcul de maniÃ¨re totalement reproductible. Ã‰videmment, lâ€™utilisation de tels workflows ne se limite pas Ã  lâ€™optimisation dâ€™hyperparamÃ¨tres mais peut Ã©galement Ãªtre utilisÃ©e pour le preprocessing de donnÃ©es, la crÃ©ation de pipelines dâ€™ETL, etc. Dâ€™ailleurs, Ã  lâ€™origine, ces outils ont Ã©tÃ© pensÃ© pour ces tÃ¢ches et permettent ainsi de dÃ©finir un processus de donnÃ©es comme un ensemble de transformations sous la forme de diagramme acyclique dirigÃ© (DAG)."
  },
  {
    "objectID": "chapters/mlops.html#servir-un-modÃ¨le-ml-Ã -des-utilisateurs",
    "href": "chapters/mlops.html#servir-un-modÃ¨le-ml-Ã -des-utilisateurs",
    "title": "Introduction aux enjeux du MLOps",
    "section": "2ï¸âƒ£ Servir un modÃ¨le ML Ã  des utilisateurs",
    "text": "2ï¸âƒ£ Servir un modÃ¨le ML Ã  des utilisateurs\nUne partie trÃ¨s importante, parfois nÃ©gligÃ©e, des projets de machine learning est la mise Ã  disposition des modÃ¨les entraÃ®nÃ©s Ã  dâ€™autres utilisateurs. Puisque vous avez parfaitement suivi les diffÃ©rents chapitres de ce cours, votre projet est en thÃ©orie totalement reproductible. Une maniÃ¨re triviale de transmettre le modÃ¨le que vous avez sÃ©lectionnÃ© serait de partager votre code et toutes les informations nÃ©cessaires pour quâ€™une personne tierce rÃ©-entraÃ®ne votre modÃ¨le de son cÃ´tÃ©. Ã‰videmment, ce procÃ©dÃ© nâ€™est pas optimal, car il suppose que tous les utilisateurs disposent des ressources/infrastructures/connaissances nÃ©cessaires pour rÃ©aliser lâ€™entraÃ®nement.\nLâ€™objectif est donc de mettre Ã  disposition votre modÃ¨le de maniÃ¨re simple et efficace. Pour cela, plusieurs possibilitÃ©s sâ€™offrent Ã  vous en fonction de votre projet, et il est important de se poser quelques questions prÃ©alables :\n\nQuel format est le plus pertinent pour mettre Ã  disposition des utilisateurs ?\nLes prÃ©dictions du modÃ¨le doivent-elles Ãªtre rÃ©alisÃ©es par lots (batch) ou en temps rÃ©el (online) ?\nQuelle infrastructure utiliser pour dÃ©ployer notre modÃ¨le de machine learning ?\n\nDans le cadre de ce cours, nous avons choisi dâ€™utiliser une API REST pour mettre Ã  disposition un modÃ¨le de machine learning. Cela nous semble Ãªtre la mÃ©thode la plus adaptÃ©e dans une grande majoritÃ© des cas, car elle rÃ©pond Ã  plusieurs critÃ¨res :\n\nSimplicitÃ© : les API REST permettent de crÃ©er une porte dâ€™entrÃ©e qui peut cacher la complexitÃ© sous-jacente du modÃ¨le, facilitant ainsi sa mise Ã  disposition.\nStandardisation : lâ€™un des principaux avantages des API REST est quâ€™elles reposent sur le standard HTTP. Cela signifie quâ€™elles sont agnostiques au langage de programmation utilisÃ© et que les requÃªtes peuvent Ãªtre rÃ©alisÃ©es en XML, JSON, HTML, etc.\nModularitÃ© : le client et le serveur sont indÃ©pendants. En dâ€™autres termes, le stockage des donnÃ©es, lâ€™interface utilisateur ou encore la gestion du modÃ¨le sont complÃ¨tement sÃ©parÃ©s de la mise Ã  disposition (le serveur).\nPassage Ã  lâ€™Ã©chelle : la sÃ©paration entre le serveur et le client permet aux API REST dâ€™Ãªtre trÃ¨s flexibles et facilite le passage Ã  lâ€™Ã©chelle (scalability). Elles peuvent ainsi sâ€™adapter Ã  la charge de requÃªtes concurrentes.\n\nLâ€™exposition dâ€™un modÃ¨le de machine learning peut Ãªtre rÃ©sumÃ©e par le schÃ©ma suivant :\n\n\n\nExposer un modÃ¨le de ML via une API\n\n\nComme le montre le schÃ©ma, lâ€™API est exÃ©cutÃ©e dans un conteneur afin de garantir un environnement totalement autonome et isolÃ©. Seules les dÃ©pendances nÃ©cessaires Ã  lâ€™exÃ©cution du modÃ¨le et au fonctionnement de lâ€™API ne sont intÃ©grÃ©es Ã  ce conteneur. Travailler avec des images docker lÃ©gÃ¨res prÃ©sente plusieurs avantages. Tout dâ€™abord, crÃ©er une image ne contenant que le strict nÃ©cessaire au fonctionnement de votre application permet justement de savoir ce qui est absolument indispensable et ce qui est superflu. De plus, plus votre image est lÃ©gÃ¨re, plus son temps de tÃ©lÃ©chargement depuis votre Hub dâ€™images (e.g.Â Dockerhub) sera rapide Ã  chaque crÃ©ation de conteneur de votre application. Les conteneurs ont lâ€™avantage dâ€™Ãªtre totalement portables et offrent la possibilitÃ© de mettre Ã  lâ€™Ã©chelle votre application de maniÃ¨re simple et efficace. Par exemple, si lâ€™on imagine que vous avez dÃ©ployÃ© votre modÃ¨le et que vous souhaitez le requÃªter un grand nombre de fois dans un laps de temps court, il est alors prÃ©fÃ©rable de crÃ©er plusieurs instances de votre application pour que les calculs puissent Ãªtre effectuÃ©s en parallÃ¨le. Lâ€™avantage de procÃ©der de cette maniÃ¨re est quâ€™une fois quâ€™on a crÃ©Ã© lâ€™image sous-jacente Ã  notre application, il est ensuite trÃ¨s simple de crÃ©er une multitude de conteneurs (replicas) toutes basÃ©es sur lâ€™image en question.\nPour tout ce qui concerne le dÃ©ploiement de votre application, vous pouvez vous rÃ©fÃ©rer au chapitre Mise en production. Techniquement, il nâ€™y a aucune difficultÃ© supplÃ©mentaire lorsque lâ€™on veut avoir une approche MLOps lors de cette Ã©tape. Lâ€™unique subtilitÃ© Ã  avoir en tÃªte est que lâ€™on souhaite maintenant faire communiquer notre application avec MLflow. En effet, chaque dÃ©ploiement est basÃ© sur une version particuliÃ¨re du modÃ¨le et il est nÃ©cessaire de renseigner quelques informations afin de rÃ©cupÃ©rer le bon modÃ¨le au sein de notre entrepÃ´t de modÃ¨le. Comme pour tout dÃ©ploiement sous Kubernetes, il faut tout dâ€™abord crÃ©er les 3 fichiers YAML : deployment.yaml, service.yaml, ingress.yaml. Ensuite, comme vous pouvez le voir sur le schÃ©ma, notre API doit pouvoir Ãªtre reliÃ©e Ã  MLflow qui lui-mÃªme a besoin dâ€™Ãªtre connectÃ© Ã  un espace de stockage (ici s3/MinIO) qui contient lâ€™entrepÃ´t des modÃ¨les. Pour cela, dans le fichier deployment.yaml, on rajoute simplement quelques variables dâ€™environnement qui nous permettent de crÃ©er de lien Ã  savoir :\n\nMLFLOW_S3_ENDPOINT_URL : Lâ€™URL de lâ€™endpoint S3 utilisÃ© par MLflow pour stocker les donnÃ©es (et modÃ¨les)\nMLFLOW_TRACKING_URI : Lâ€™URI du serveur de suivi MLflow, qui spÃ©cifie oÃ¹ les informations concernant les modÃ¨les sont stockÃ©es.\nAWS_ACCESS_KEY_ID : Lâ€™identifiant dâ€™accÃ¨s utilisÃ© pour authentifier lâ€™accÃ¨s aux services de stockage s3.\nAWS_SECRET_ACCESS_KEY : La clÃ© secrÃ¨te utilisÃ©e pour authentifier lâ€™accÃ¨s aux services de stockage s3.\nAWS_DEFAULT_REGION : Identifie la rÃ©gion S3 pour laquelle vous souhaitez envoyer les demandes aux serveurs. \n\nPour faciliter le dÃ©ploiement continu (voir chapitre Mise en production), il est conseillÃ© de rajouter des variables dâ€™environnement spÃ©cifiant la version du modÃ¨le Ã  dÃ©ployer ainsi que le nom du modÃ¨le Ã  dÃ©ployer. En effet, en spÃ©cifiant ces valeurs dans le fichier deployment.yaml, cela va permettre de dÃ©clencher un nouveau dÃ©ploiement dÃ¨s lors que lâ€™on modifiera ces valeurs.\nIl est bon de noter que MLflow permet Ã©galement de dÃ©ployer directement un modÃ¨le MLflow. Vous pouvez aller regarder la documentation si cela vous intÃ©resse. Cette option est relativement rÃ©cente et pas encore tout Ã  fait mature mais se base sur les mÃªmes technologies que celles prÃ©sentÃ©es dans ce cours (Kubernetes, S3, etc.). Câ€™est pour cela que nous avons prÃ©fÃ©rÃ© dÃ©tailler le dÃ©veloppement de notre propre API en utilisant le framework FastAPI, qui est devenu le standard pour le dÃ©veloppement dâ€™API en Python.\n\nDÃ©ployer sur Kubernetes (plutot dans chap mise en prod ?) \nBatch vs real-time prediction"
  },
  {
    "objectID": "chapters/mlops.html#observabilitÃ©-en-temps-rÃ©el-dun-modÃ¨le-de-ml",
    "href": "chapters/mlops.html#observabilitÃ©-en-temps-rÃ©el-dun-modÃ¨le-de-ml",
    "title": "Introduction aux enjeux du MLOps",
    "section": "3ï¸âƒ£ ObservabilitÃ© en temps rÃ©el dâ€™un modÃ¨le de ML",
    "text": "3ï¸âƒ£ ObservabilitÃ© en temps rÃ©el dâ€™un modÃ¨le de ML\nUne fois la modÃ©lisation rÃ©alisÃ©e, le modÃ¨le entraÃ®nÃ©, optimisÃ© et mis Ã  disposition des utilisateurs grÃ¢ce Ã  un dÃ©ploiement sur un serveur, on peut considÃ©rer que le travail est fini. Du point de vue du data-scientist stricto sensu, cela peut Ãªtre le cas, puisque lâ€™on considÃ¨re souvent que le domaine du data-scientist sâ€™arrÃªte Ã  la sÃ©lection du modÃ¨le Ã  dÃ©ployer, le dÃ©ploiement Ã©tant rÃ©alisÃ© par ce quâ€™on appelle les data-engineers. Pourtant, une fois dÃ©ployÃ© dans un environnement de production, le modÃ¨le nâ€™a pas rÃ©alisÃ© lâ€™intÃ©gralitÃ© de son cycle de vie. En production, le cycle de vie dâ€™un modÃ¨le de machine learning suivant lâ€™approche MLOps peut Ãªtre schÃ©matisÃ© de la maniÃ¨re suivante :\n\n\n\n\n\n\nFigureÂ 1: Source : martinfowler.com\n\n\n\nOn retrouve les diffÃ©rentes composantes du MLOps avec les donnÃ©es (DataOps), les modÃ¨les (ModelOps) et le code (DevOps). Ces composantes rendent le cycle de vie dâ€™un modÃ¨le de machine learning complexe impliquant plusieurs parties prenantes autour du projet. En rÃ¨gle gÃ©nÃ©rale, on observe trois parties prenantes principales :\n\nData-scientists/Data-engineers\nIT/DevOps\nÃ‰quipes mÃ©tiers\n\nQuelques fois, les data-scientists peuvent Ãªtre intÃ©grÃ©s aux Ã©quipes mÃ©tiers et les data-engineers aux Ã©quipes IT. Cela peut simplifier les Ã©changes entre les deux Ã©quipes, mais cela peut Ã©galement entraÃ®ner un travail en silos et cloisonner les deux Ã©quipes aux expertises, attentes et vocabulaires trÃ¨s diffÃ©rents. Or, la communication est primordiale pour permettre une bonne gestion du cycle de vie du modÃ¨le de machine learning et notamment pour surveiller le modÃ¨le dans son environnement de production.\nIl est extrÃªmement important de surveiller comment le modÃ¨le se comporte une fois dÃ©ployÃ© pour sâ€™assurer que les rÃ©sultats renvoyÃ©s sont conformes aux attentes. Cela permet dâ€™anticiper des changements dans les donnÃ©es, une baisse des performances ou encore dâ€™amÃ©liorer le modÃ¨le de maniÃ¨re continue. Il est Ã©galement nÃ©cessaire que notre modÃ¨le soit toujours accessible, que notre application soit bien dimensionnÃ©e, etc. Câ€™est pour cela que la surveillance (monitoring) dâ€™un modÃ¨le de machine learning est un enjeu capital dans lâ€™approche MLOps.\nLe terme surveillance peut renvoyer Ã  plusieurs dÃ©finitions en fonction de lâ€™Ã©quipe dans laquelle lâ€™on se situe. Pour une personne travaillant dans lâ€™Ã©quipe informatique, surveiller une application signifie vÃ©rifier sa validitÃ© technique. Elle va donc sâ€™assurer que la latence nâ€™est pas trop Ã©levÃ©e, que la mÃ©moire est suffisante ou encore que le stockage sur le disque est bien proportionnÃ©. Pour un data-scientist ou une personne travaillant dans lâ€™Ã©quipe mÃ©tier, ce qui va lâ€™intÃ©resser est la surveillance du modÃ¨le dâ€™un point de vue mÃ©thodologique. Malheureusement, il nâ€™est pas souvent Ã©vident que contrÃ´ler la performance en temps rÃ©el dâ€™un modÃ¨le de machine learning. Il est rare que lâ€™on connaisse la vraie valeur au moment de la prÃ©diction du modÃ¨le (sinon on ne sâ€™embÃªterait pas Ã  construire un modÃ¨le !) et on ne peut pas vraiment savoir sâ€™il sâ€™est trompÃ© ou non. Il est donc commun dâ€™utiliser des proxies pour anticiper une potentielle dÃ©gradation de la performance de notre modÃ¨le. On distingue gÃ©nÃ©ralement 2 principaux types de dÃ©gradation dâ€™un modÃ¨le de machine learning : le data drift et le concept drift.\n\n\n\nSource : whylabs.ai\n\n\n\nData drift\nOn parle de data drift lorsque lâ€™on observe un changement de distribution dans les donnÃ©es utilisÃ©es en entrÃ©e du modÃ¨le. En dâ€™autres termes, il y a data drift lorsque les donnÃ©es utilisÃ©es lors de lâ€™entraÃ®nement sont sensiblement diffÃ©rentes des donnÃ©es utilisÃ©es lors de lâ€™infÃ©rence en production. Imaginons que vous souhaitez repÃ©rer des habitations Ã  partir dâ€™images satellites. Vous entraÃ®nez votre modÃ¨le sur des donnÃ©es datant par exemple de fÃ©vrier 2022, et une fois en production vous essayer de repÃ©rer les habitations tous les mois suivants. Vous constatez finalement durant lâ€™Ã©tÃ© que votre modÃ¨le nâ€™est plus du tout aussi performant puisque les images satellites de juillet diffÃ¨rent fortement de celle de fÃ©vrier. La distribution des donnÃ©es dâ€™entraÃ®nement nâ€™est plus proche de celle dâ€™infÃ©rence, \\(P_{train}(X) \\neq P_{inference}(X)\\). Les data drifts apparaissent dÃ¨s lors que les propriÃ©tÃ©s statistiques des donnÃ©es changent et cela peut venir de plusieurs facteurs en fonction de votre modÃ¨le : changements de comportement, dynamique de marchÃ©, nouvelles rÃ©glementations politiques, problÃ¨me de qualitÃ© des donnÃ©es, etc. Il nâ€™est pas si simple de dÃ©tecter rapidement des data drifts, cela suppose de surveiller de maniÃ¨re continue la distribution des donnÃ©es en entrÃ©e et en sortie de votre modÃ¨le sur un certain laps de temps et dâ€™identifier quand celles-ci diffÃ¨rent significativement de la distribution des donnÃ©es dâ€™entraÃ®nement. Pour obtenir une idÃ©e visuelle, on peut crÃ©er des reprÃ©sentations graphiques comme des histogrammes pour comparer les distributions Ã  plusieurs pÃ©riodes dans le temps, voire des boÃ®tes Ã  moustaches. On peut aussi calculer des mÃ©triques, qui seront plus simples dâ€™utilisation si lâ€™on souhaite automatiser un systÃ¨me dâ€™alerte, comme des distances entre distributions (distance de Bhattacharyya, divergence de Kullback-Leibler, distance de Hellinger) ou effectuer des tests statistiques (Test de Kolmogorov-Smirnov, Test du Ï‡Â²). Pour rÃ©sumer, la dÃ©tection dâ€™un data drift peut sâ€™effectuer en plusieurs Ã©tapes :\n\nDÃ©finition dâ€™une rÃ©fÃ©rence : on dÃ©finit la distribution de rÃ©fÃ©rence (e.g.Â celle utilisÃ©e lors de lâ€™entraÃ®nement).\nDÃ©finition de seuils : on dÃ©termine en dessous de quelles valeurs de nos mÃ©triques cela peut Ãªtre considÃ©rÃ© comme un data drift.\nSurveillance continue : soit en temps rÃ©el, soit de maniÃ¨re pÃ©riodique (relativement courte), on compare nos distributions et on calcule les mÃ©triques dÃ©finies prÃ©alablement.\nAlerte et correction : on met en place un systÃ¨me dâ€™alerte automatique dÃ¨s lors que nos mÃ©triques indiquent la prÃ©sence dâ€™un data drift, puis on agit en consÃ©quence (rÃ©-entraÃ®nement sur de nouvelles donnÃ©es, ajustement des paramÃ¨tres du modÃ¨le, etc.).\n\n\n\nConcept drift\nOn parle de concept drift lorsque lâ€™on observe un changement dans la relation statistique entre les features (\\(X\\)) et la variable Ã  prÃ©dire (\\(Y\\)) au cours du temps. En termes mathÃ©matiques, on considÃ¨re quâ€™il y a un concept drift dÃ¨s lors que \\(P_{train}(Y|X) \\neq P_{inference}(Y|X)\\) alors mÃªme que \\(P_{train}(X) = P_{inference}(X)\\). Cela peut avoir un impact important sur les performances du modÃ¨le si la relation diffÃ¨re fortement. Par exemple, un modÃ¨le de prÃ©diction de la demande de masques chirurgicaux entraÃ®nÃ© sur des donnÃ©es avant la pandÃ©mie de COVID-19 deviendra totalement inadÃ©quat pour effectuer des prÃ©dictions lors de cette pandÃ©mie, car il y a eu un changement dans la relation entre la demande de masques chirurgicaux et les features utilisÃ©es pour prÃ©dire cette demande. Dans le cas dâ€™un concept drift, on sera plus tentÃ© de surveiller des mÃ©triques de performance pour repÃ©rer une potentielle anomalie. Dans le cas oÃ¹ lâ€™on possÃ¨de un jeu de test gold standard, alors on sera en capacitÃ© de calculer de nombreuses mÃ©triques usuelles de machine learning (Ã  savoir lâ€™accuracy, la precision, le recall ou le F1-score pour des problÃ¨mes de classification, et toutes les mÃ©triques dâ€™erreurs - MSE, RMSE, MAE, â€¦ - pour les problÃ¨mes de rÃ©gression) et repÃ©rer une baisse tendancielle ou brutale des performances. Dans le cas oÃ¹ lâ€™on nâ€™a pas de jeu de test gold standard, on sâ€™attachera Ã  dÃ©terminer des proxys qui peuvent Ãªtre liÃ©s Ã  des mÃ©triques de performance ou alors utiliser des algorithmes de dÃ©tection de changement dans le flux de donnÃ©es (Drift Detection Method, Early Drift Detection Method, Adaptive Windowing)."
  },
  {
    "objectID": "chapters/mlops.html#rÃ©-entraÃ®nement-dun-modÃ¨le-ml",
    "href": "chapters/mlops.html#rÃ©-entraÃ®nement-dun-modÃ¨le-ml",
    "title": "Introduction aux enjeux du MLOps",
    "section": "4ï¸âƒ£ RÃ©-entraÃ®nement dâ€™un modÃ¨le ML",
    "text": "4ï¸âƒ£ RÃ©-entraÃ®nement dâ€™un modÃ¨le ML\nDÃ¨s lors que lâ€™on a constatÃ© une baisse de la performance de notre modÃ¨le grÃ¢ce Ã  notre surveillance fine, il faut ensuite pallier au problÃ¨me et redÃ©ployer un modÃ¨le avec des performances satisfaisantes. On est donc Ã  la fin du cycle de vie de notre modÃ¨le, ce qui va nous reconduire au dÃ©but du cycle pour un nouveau modÃ¨le comme lâ€™illustre la figure FigureÂ 1. Le rÃ©-entraÃ®nement est partie intÃ©grante dâ€™un projet de machine learning dÃ¨s lors que celui-ci est mis en production. Il existe plusieurs mÃ©thodes pour rÃ©-entraÃ®ner de la plus basique Ã  la plus MLOps-compatible.\nLa mÃ©thode classique est de rÃ©aliser un nouvel entraÃ®nement from scratch en ajoutant les nouvelles donnÃ©es Ã  notre disposition dans le jeu dâ€™entraÃ®nement. Cela permet au modÃ¨le de connaÃ®tre les derniÃ¨res relations entre les features et la variable Ã  prÃ©dire. Cependant, rÃ©-entraÃ®ner un modÃ¨le peut Ãªtre particuliÃ¨rement coÃ»teux lorsque lâ€™on travaille sur de gros modÃ¨les dont les ressources computationnelles nÃ©cessaires sont importantes. Il est aussi possible de fine-tuner un modÃ¨le dÃ©jÃ  prÃ©-entraÃ®nÃ©. Dans ce cas-lÃ , on nâ€™a pas besoin de repartir de zÃ©ro, on repart des poids optimisÃ©s lors du premier entraÃ®nement et on les rÃ©-optimise en utilisant les nouvelles donnÃ©es Ã  notre disposition. Cette mÃ©thode est naturellement beaucoup moins longue Ã  rÃ©aliser et est moins coÃ»teuse, notamment lorsque la quantitÃ© de nouvelles donnÃ©es est faible par rapport Ã  la quantitÃ© des donnÃ©es utilisÃ©es lors du premier entraÃ®nement.\nLâ€™approche MLOps consiste Ã  automatiser ce rÃ©-entraÃ®nement, quâ€™on appelle Ã©galement entraÃ®nement continu, de sorte Ã  obtenir un cycle de vie totalement automatisÃ©. En effet, le rÃ©-entraÃ®nement est fondamental pour sâ€™assurer que le modÃ¨le de machine learning est constamment en train de fournir des prÃ©dictions cohÃ©rentes, tout en minimisant les interventions manuelles. Lâ€™objectif est donc de crÃ©er un processus qui lance de nouveaux entraÃ®nements de maniÃ¨re automatique en prenant en compte les derniÃ¨res informations disponibles. Les entraÃ®nements peuvent Ãªtre dÃ©clenchÃ©s soit de maniÃ¨re pÃ©riodique (tous les lundis Ã  2h du matin), dÃ¨s lors quâ€™une alerte a Ã©tÃ© dÃ©clenchÃ©e dans notre systÃ¨me de monitoring, ou bien dÃ¨s quâ€™on a une quantitÃ© de nouvelles donnÃ©es suffisante pour rÃ©aliser un online training par exemple.\nLâ€™utilisation dâ€™outils dâ€™orchestration de workflow comme Argo Workflow ou Airflow est donc indispensable pour rÃ©aliser cette automatisation de maniÃ¨re pertinente."
  },
  {
    "objectID": "chapters/mlops.html#dÃ©fis-organisationnels-du-mlops",
    "href": "chapters/mlops.html#dÃ©fis-organisationnels-du-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "5ï¸âƒ£ DÃ©fis organisationnels du MLOps",
    "text": "5ï¸âƒ£ DÃ©fis organisationnels du MLOps\nOutre les spÃ©cificitÃ©s techniques prÃ©cÃ©demment explicitÃ©es, le MLOps prÃ©sente Ã©galement plusieurs dÃ©fis en termes organisationnels et managÃ©riaux. En effet, dans la plupart des organisations, les Ã©quipes data transverses ou intÃ©grÃ©es dans diffÃ©rents dÃ©partements mÃ©tier sont relativement jeunes et peuvent manquer de ressources qualifiÃ©es pour gÃ©rer le dÃ©ploiement et le maintien en condition opÃ©rationnelle de systÃ¨mes ML complexes. Ces Ã©quipes se composent principalement de data scientists qui se concentrent sur le dÃ©veloppement des modÃ¨les de machine learning, mais nâ€™ont pas les compÃ©tences nÃ©cessaires pour gÃ©rer le dÃ©ploiement et la maintenance dâ€™applications complÃ¨tes.\nDe plus, les Ã©quipes data Ã©voluent encore trop souvent en silo, sans communiquer avec les diffÃ©rentes Ã©quipes techniques avec lesquelles elles devraient interagir pour mettre en production leurs modÃ¨les. Or ces Ã©quipes techniques, souvent composÃ©es dâ€™informaticiens/dÃ©veloppeurs, ne connaissent pas forcÃ©ment les spÃ©cificitÃ©s des modÃ¨les de machine learning, accentuant dâ€™autant plus la nÃ©cessitÃ© dâ€™une communication continue entre ces Ã©quipes.\nUne autre difficultÃ© pouvant intervenir lors du dÃ©ploiement est la diffÃ©rence dâ€™environnements utilisÃ©s ainsi que les diffÃ©rents langages connus entre les deux Ã©quipes. Il nâ€™est pas rare que les data-scientists dÃ©veloppent des modÃ¨les en Python tandis que les Ã©quipes informatiques gÃ¨rent leur serveur de production dans un langage diffÃ©rent, comme Java par exemple.\nAinsi, lâ€™approche MLOps engendre aussi des dÃ©fis managÃ©riaux qui impliquent de faire converger les compÃ©tences entre les Ã©quipes afin de fluidifier la mise en production de modÃ¨les de machine learning.\n\n\n\nGouvernance dâ€™un projet de machine learning"
  },
  {
    "objectID": "chapters/mlops.html#pourquoi-mlflow",
    "href": "chapters/mlops.html#pourquoi-mlflow",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Pourquoi MLflow ?",
    "text": "Pourquoi MLflow ?\nIl existe aujourdâ€™hui de nombreux outils pour orchestrer des tÃ¢ches et des pipelines de donnÃ©es. Parmi les plus populaires (selon leur â­ GitHub), on peut citer Airflow, Luigi, MLflow, Argo Workflow, Prefect ou encore Kubeflow, BentoMLâ€¦ Il est difficile dâ€™affirmer sâ€™il y en a un meilleur quâ€™un autre ; en rÃ©alitÃ©, votre choix dÃ©pend surtout de votre infrastructure informatique et de votre projet. En lâ€™occurrence ici, nous avons fait le choix dâ€™utiliser MLflow pour sa simplicitÃ© dâ€™utilisation grÃ¢ce Ã  une interface web bien faite, parce quâ€™il intÃ¨gre lâ€™ensemble du cycle de vie dâ€™un modÃ¨le et Ã©galement parce quâ€™il sâ€™intÃ¨gre trÃ¨s bien avec Kubernetes. De plus, il est prÃ©sent dans le catalogue du SSP Cloud, ce qui simplifie grandement son installation. Afin dâ€™intÃ©grer les dimensions dâ€™intÃ©gration et de dÃ©ploiement continus, nous utiliserons Ã©galement Argo CD et Argo Workflow dans la boucle. Ceux-ci sont privilÃ©giÃ©s par rapport Ã  Airflow car ils sont optimisÃ©s pour les clusters Kubernetes qui reprÃ©sentent aujourdâ€™hui la norme des cloud en ligne ou on premise.\n\n\n\nVue dâ€™ensemble de MLFlow. Source: https://dzlab.github.io\n\n\nMLflow est une plateforme qui permet dâ€™optimiser le dÃ©veloppement du cycle de vie dâ€™un modÃ¨le de machine learning. Elle permet de suivre en dÃ©tail les diffÃ©rentes expÃ©rimentations, de packager son code pour garantir la reproductibilitÃ©, et de servir un modÃ¨le Ã  des utilisateurs. MLFlow possÃ¨de Ã©galement une API qui permet dâ€™Ãªtre compatible avec la majoritÃ© des librairies de machine learning (PyTorch, Scikit-learn, XGBoost, etc.) mais Ã©galement diffÃ©rents langages (Python, R et Java)."
  },
  {
    "objectID": "chapters/mlops.html#les-projets-mlflow",
    "href": "chapters/mlops.html#les-projets-mlflow",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Les projets MLflow",
    "text": "Les projets MLflow\nMLflow propose un format pour packager son projet de data science afin de favoriser la rÃ©utilisation et la reproductibilitÃ© du code. Ce format sâ€™appelle tout simplement MLflow Project. ConcrÃ¨tement, un MLflow project nâ€™est rien dâ€™autre quâ€™un rÃ©pertoire contenant le code et les ressources nÃ©cessaires (donnÃ©es, fichiers de configurationâ€¦) pour lâ€™exÃ©cution de votre projet. Il est rÃ©sumÃ© par un fichier MLproject qui liste les diffÃ©rentes commandes pour exÃ©cuter une pipeline ainsi que les dÃ©pendances nÃ©cessaires. En gÃ©nÃ©ral, un projet MLflow a la structure suivante :\nProjet_ML/\nâ”œâ”€â”€ artifacts/\nâ”‚   â”œâ”€â”€ model.bin\nâ”‚   â””â”€â”€ train_text.txt\nâ”œâ”€â”€ code/\nâ”‚   â”œâ”€â”€ main.py\nâ”‚   â””â”€â”€ preprocessing.py\nâ”œâ”€â”€ MLmodel\nâ”œâ”€â”€ conda.yaml\nâ”œâ”€â”€ python_env.yaml\nâ”œâ”€â”€ python_model.pkl\nâ””â”€â”€ requirements.txt\nEn plus de packager son projet, MLflow permet Ã©galement de packager son modÃ¨le, quel que soit la librairie de machine learning sous-jacente utilisÃ©e (parmi celles compatibles avec MLflow, câ€™est-Ã -dire toutes les librairies que vous utilisez !). Ainsi, deux modÃ¨les entraÃ®nÃ©s avec des librairies diffÃ©rentes, disons PyTorch et Keras, peuvent Ãªtre dÃ©ployÃ©s et requÃªtÃ©s de la mÃªme maniÃ¨re grÃ¢ce Ã  cette surcouche ajoutÃ©e par MLflow.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIl est Ã©galement possible de packager son propre modÃ¨le personnalisÃ© ! Pour cela vous pouvez suivre le tutoriel prÃ©sent dans la documentation.\n\n\nAutrement dit, un projet MLFlow archive lâ€™ensemble des Ã©lÃ©ments nÃ©cessaires pour reproduire un entraÃ®nement donnÃ© dâ€™un modÃ¨le ou pour rÃ©utiliser celui-ci Ã  tout moment."
  },
  {
    "objectID": "chapters/mlops.html#le-serveur-de-suivi-tracking-server",
    "href": "chapters/mlops.html#le-serveur-de-suivi-tracking-server",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Le serveur de suivi (tracking server)",
    "text": "Le serveur de suivi (tracking server)\nLe tracking server est le lieu oÃ¹ sont archivÃ©s lâ€™ensemble des entraÃ®nements dâ€™un modÃ¨le. Attention, il ne sâ€™agit pas du serveur sur lequel les modÃ¨les sont entraÃ®nÃ©s mais de celui oÃ¹ les entraÃ®nements sont archivÃ©s aprÃ¨s avoir eu lieu. Au-delÃ  de stocker seulement les poids dâ€™un modÃ¨le, câ€™est lâ€™ensemble de lâ€™environnement nÃ©cessaire qui peut Ãªtre retrouvÃ© dans ce serveur.\n\n\n\n\n\nTechniquement, cela prend la forme dâ€™une API et dâ€™une interface utilisateur pour enregistrer les paramÃ¨tres, les versions du code, les mÃ©triques ou encore les artefacts associÃ©s Ã  un entraÃ®nement.\n\n\n\nSource: Databricks\n\n\nEn arriÃ¨re plan, MLFlow va enregistrer tout ceci dans un bucket S3. NÃ©anmoins, lâ€™utilisateur nâ€™aura pas Ã  se soucier de cela puisque câ€™est MLFLow qui fera lâ€™interface entre lâ€™utilisateur et le systÃ¨me de stockage. Avec son API, MLFLow fournit mÃªme une maniÃ¨re simplifiÃ©e de rÃ©cupÃ©rer ces objets archivÃ©s, par exemple avec un code prenant la forme\nimport mlflow\nmodel = mlflow.pyfunc.load_model(model_uri=\"runs:/d16076a3ec534311817565e6527539c\")\nLe tracking server est trÃ¨s utile pour comparer les diffÃ©rentes expÃ©rimentations que vous avez effectuÃ©es, pour les stocker et Ã©galement pour Ãªtre capable de les reproduire. En effet, chaque run sauvegarde la source des donnÃ©es utilisÃ©es, mais Ã©galement le commit sur lequel le run est basÃ©.\nA la maniÃ¨re de Git qui permet dâ€™identifier chaque moment de lâ€™histoire dâ€™un projet Ã  partir dâ€™un identifiant unique, MLFlow permet de rÃ©cupÃ©rer chaque entraÃ®nement dâ€™un modÃ¨le Ã  partir dâ€™un SHA. NÃ©anmoins, en pratique, certains modÃ¨les ont un statut Ã  part, notamment ceux en production."
  },
  {
    "objectID": "chapters/mlops.html#lentrepÃ´t-de-modÃ¨les-model-registry",
    "href": "chapters/mlops.html#lentrepÃ´t-de-modÃ¨les-model-registry",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Lâ€™entrepÃ´t de modÃ¨les (model registry)",
    "text": "Lâ€™entrepÃ´t de modÃ¨les (model registry)\nUne fois que lâ€™on a effectuÃ© diffÃ©rentes expÃ©rimentations et pu sÃ©lectionner les modÃ¨les qui nous satisfont, il est temps de passer Ã  lâ€™Ã©tape suivante du cycle de vie dâ€™un modÃ¨le. En effet, le modÃ¨le choisi doit ensuite pouvoir passer dans un environnement de production ou de prÃ©-production. Or, connaÃ®tre lâ€™Ã©tat dâ€™un modÃ¨le dans son cycle de vie nÃ©cessite une organisation trÃ¨s rigoureuse et nâ€™est pas si aisÃ©. MLflow a dÃ©veloppÃ© une fonctionnalitÃ© qui permet justement de simplifier cette gestion des versions des modÃ¨les grÃ¢ce Ã  son Model Registry. Cet entrepÃ´t permet dâ€™ajouter des tags et des alias Ã  nos modÃ¨les pour dÃ©finir leur position dans leur cycle de vie et ainsi pouvoir les rÃ©cupÃ©rer de maniÃ¨re efficace.\nDe maniÃ¨re gÃ©nÃ©rale, un modÃ¨le de machine learning passe par 4 stades quâ€™il est nÃ©cessaire de connaÃ®tre en tout temps :\n\nExpÃ©rimental\nEn Ã©valuation\nEn production\nArchivÃ©"
  },
  {
    "objectID": "chapters/mlops.html#mlflow-en-rÃ©sumÃ©",
    "href": "chapters/mlops.html#mlflow-en-rÃ©sumÃ©",
    "title": "Introduction aux enjeux du MLOps",
    "section": "MLflow en rÃ©sumÃ©",
    "text": "MLflow en rÃ©sumÃ©\nMLflow est donc un projet open-source qui fournit une plateforme pour suivre le cycle de vie dâ€™un modÃ¨le de machine learning de bout en bout. Ce nâ€™est pas le seul outil disponible et il nâ€™est peut-Ãªtre pas le plus adaptÃ© Ã  certains de vos projets prÃ©cis. En revanche, il prÃ©sente selon nous plusieurs avantages, en premier lieu sa prise en main trÃ¨s simple et sa capacitÃ© Ã  rÃ©pondre aux besoins de lâ€™approche MLOps. Il faut garder Ã  lâ€™esprit que cet environnement est encore trÃ¨s rÃ©cent et que de nouveaux projets open-source Ã©mergent chaque jour, donc il est nÃ©cessaire de rester Ã  jour sur les derniÃ¨res Ã©volutions.\nPour rÃ©sumer, MLFlow permet :\n\nde simplifier le suivi de lâ€™entraÃ®nement des modÃ¨les de machine learning grÃ¢ce Ã  son API et Ã  son tracking server\ndâ€™intÃ©grer les principaux frameworks de machine learning de maniÃ¨re simple\ndâ€™intÃ©grer son propre framework si besoin\nde standardiser son script dâ€™entraÃ®nement et donc de pouvoir lâ€™industrialiser, pour rÃ©aliser un fine-tuning des hyperparamÃ¨tres, par exemple\nde packager ses modÃ¨les, de sorte Ã  pouvoir les requÃªter de maniÃ¨re simple et harmonisÃ©e entre les diffÃ©rents frameworks\nde stocker ses modÃ¨les de maniÃ¨re pertinente en leur affectant des tags et en favorisant le suivi de leur cycle de vie"
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "DÃ©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein Ã©cran."
  },
  {
    "objectID": "chapters/introduction.html#origine",
    "href": "chapters/introduction.html#origine",
    "title": "Introduction",
    "section": "Origine",
    "text": "Origine\nLa notion de â€œbonnes pratiquesâ€ qui nous intÃ©resse dans ce cours trouve son origine au sein de la communautÃ© des dÃ©veloppeurs logiciels. Elle constitue une rÃ©ponse Ã  plusieurs constats :\n\nle â€œcode est beaucoup plus souvent lu quâ€™il nâ€™est Ã©critâ€ (Guido Van Rossum) ;\nla maintenance dâ€™un code demande souvent (beaucoup) plus de ressources que son dÃ©veloppement initial ;\nla personne qui maintient une base de code a de fortes chances de ne pas Ãªtre celle qui lâ€™a Ã©crite.\n\nFace Ã  ces constats, un ensemble de rÃ¨gles informelles ont Ã©tÃ© conventionnellement acceptÃ©es par la communautÃ© des dÃ©veloppeurs comme produisant des logiciels plus fiables, Ã©volutifs et maintenables dans le temps. Comme toutes conventions de langue, certaines peuvent paraÃ®tre arbitraires. Ces rÃ¨gles favorisent nÃ©anmoins la capacitÃ© Ã  communiquer du code, un aspect communautaire qui peut paraÃ®tre secondaire au premier abord mais qui est pourtant le principe ayant fait le succÃ¨s dâ€™un langage open source en favorisant le partage dâ€™expÃ©rience et dâ€™assistance.\n\n\n\n\n\n\nLa 12 Factor App\n\n\n\n\n\nRÃ©cemment, dans le contexte dâ€™une Ã©volution des logiciels vers des applications web vivant dans le cloud, un certain nombre de ces bonnes pratiques ont Ã©tÃ© formalisÃ©es dans un manifeste : la 12 Factor App. Le dÃ©veloppement du cloud, câ€™est-Ã -dire dâ€™infrastructures standardisÃ©es hors des systÃ¨mes dâ€™information des dÃ©tenteurs de donnÃ©es, rend les besoins de bonnes pratiques plus prÃ©gnant."
  },
  {
    "objectID": "chapters/introduction.html#pourquoi-sintÃ©resser-aux-bonnes-pratiques",
    "href": "chapters/introduction.html#pourquoi-sintÃ©resser-aux-bonnes-pratiques",
    "title": "Introduction",
    "section": "Pourquoi sâ€™intÃ©resser aux bonnes pratiques ?",
    "text": "Pourquoi sâ€™intÃ©resser aux bonnes pratiques ?\n\nEn quoi est-ce pertinent pour le data scientist, dont le rÃ´le nâ€™est pas de dÃ©velopper des applications mais de donner du sens aux donnÃ©es ?\n\nDu fait du dÃ©veloppement rapide de la data science et consÃ©quemment de la croissance de la taille moyenne des projets, lâ€™activitÃ© du data scientist tend Ã  se rapprocher par certains aspects de celle du dÃ©veloppeur :\n\nles projets sur lesquels travaille le data scientist sont intenses en code ;\nil doit travailler de maniÃ¨re collaborative au sein de projets de grande envergure ;\nil est de plus en plus amenÃ© Ã  travailler Ã  partir de donnÃ©es massives, ce qui nÃ©cessite de travailler sur des infrastructures big data informatiquement complexes ;\nil est amenÃ© Ã  interagir avec des profils informatiques pour dÃ©ployer ses modÃ¨les et les rendre accessibles Ã  des utilisateurs.\n\nAussi, il fait sens pour le data scientist moderne de sâ€™intÃ©resser aux bonnes pratiques en vigueur dans la communautÃ© des dÃ©veloppeurs. Bien entendu, celles-ci doivent Ãªtre adaptÃ©es aux spÃ©cificitÃ©s des projets basÃ©s sur des donnÃ©es. Lâ€™effet bÃ©nÃ©fique de ces bonnes pratiques est que les projets les adoptant auront un coÃ»t bien plus minimal pour Ã©voluer, ce qui les rend plus compÃ©titif dans un Ã©cosystÃ¨me mouvant comme lâ€™est la data science oÃ¹ les donnÃ©es, les outils et les attentes des utilisateurs sont en changements continuels."
  },
  {
    "objectID": "chapters/introduction.html#un-continuum-de-bonnes-pratiques",
    "href": "chapters/introduction.html#un-continuum-de-bonnes-pratiques",
    "title": "Introduction",
    "section": "Un continuum de bonnes pratiques",
    "text": "Un continuum de bonnes pratiques\nLa notion de bonnes pratiques ne doit pas Ãªtre vue de maniÃ¨re binaire : il nâ€™y a pas dâ€™un cÃ´tÃ© les projets qui les appliquent et de lâ€™autre ceux qui ne les appliquent pas. Les bonnes pratiques ont un coÃ»t, quâ€™il ne faut pas nÃ©gliger â€” mÃªme si leur application Ã©vite aussi des coÃ»ts futurs, notamment en terme de maintenance. Il faut donc plutÃ´t voir les bonnes pratiques comme un spectre, sur lequel on vient positionner son projet en fonction de diffÃ©rents critÃ¨res, notamment du coÃ»t-avantage Ã  avancer dâ€™un niveau dans le spectre de la reproductibilitÃ©.\nLa dÃ©termination du seuil pertinent doit rÃ©sulter dâ€™un arbitrage entre diffÃ©rents critÃ¨res liÃ©s au projet :\n\nambitions : le projet est-il amenÃ© Ã  Ã©voluer, prendre de lâ€™ampleur ? Est-il destinÃ© Ã  devenir collaboratif, que ce soit dans le cadre dâ€™une Ã©quipe en organisation ou bien en open-source ? Les outputs du projet ont-ils vocation Ã  Ãªtre diffusÃ©s au grand public ?\nressources : quels sont les moyens humain du projet ? Pour un projet open-source, existe-t-il une communautÃ© potentielle de contributeurs ?\ncontraintes : le projet a-t-il une Ã©chÃ©ance proche ? Des exigences de qualitÃ© ont-elles Ã©tÃ© fixÃ©es ? Est-il destinÃ© Ã  la mise en production ? Existe-t-il des enjeux de sÃ©curitÃ© forts ?\npublic cible: Ã  quels profils sâ€™adresse les diffÃ©rentes valorisations de donnÃ©es de ce projet ? Quel est leur niveau de technicitÃ© et le temps quâ€™elles vont consacrer Ã  suivre votre projet ?\n\nIl nâ€™est donc pas question pour nous de suggÃ©rer que tout projet de data science doit respecter toutes les bonnes pratiques prÃ©sentÃ©es dans ce cours. Cela Ã©tant dit, nous sommes convaincus quâ€™il est important pour tout data scientist de rÃ©flÃ©chir Ã  ces questions pour amÃ©liorer ces pratiques au fil du temps.\nEn particulier, nous pensons quâ€™il est possible de dÃ©finir un socle, i.e.Â un ensemble minimal de bonnes pratiques qui apportent plus dâ€™avantages quâ€™elles ne coÃ»tent Ã  implÃ©menter. Notre suggestion pour un tel socle est la suivante :\n\nContrÃ´ler la qualitÃ© de son code en utilisant des outils dÃ©diÃ©s (cf.Â chapitre QualitÃ© du Code) ;\nAdopter une structure standardisÃ©e de projet en utilisant des templates prÃªts Ã  lâ€™emploi (cf.Â chapitre Architecture des Projets) ;\nUtiliser Git pour versionner le code de ses projets, quâ€™ils impliquent dâ€™autres dÃ©veloppeurs ou seulement vous (cf.Â chapitre Versionner son code et travailler collaborativement avec Git) ;\ncontrÃ´ler les dÃ©pendances de son projet en dÃ©veloppant dans des environnements virtuels (cf.Â chapitre PortabilitÃ©).\n\nLes Ã©tapes suivantes dans lâ€™Ã©chelle de la reproductibilitÃ© dÃ©pendront de lâ€™arbitrage coÃ»t-avantage. Lâ€™adoption du socle minimal de reproductibilitÃ© facilitera Ã©normÃ©ment lâ€™avancÃ©e ultÃ©rieure dans lâ€™ambition dâ€™un projet.\nFaisons Ã  prÃ©sent un tour dâ€™horizon des principes dÃ©fendus dans ce cours et de la progression logique de celui-ci."
  },
  {
    "objectID": "chapters/introduction.html#le-code-est-un-outil-de-communication",
    "href": "chapters/introduction.html#le-code-est-un-outil-de-communication",
    "title": "Introduction",
    "section": "Le code est un outil de communication",
    "text": "Le code est un outil de communication\nLa premiÃ¨re bonne pratique Ã  adopter est de considÃ©rer le code comme un outil de communication, et non simplement de maniÃ¨re fonctionnelle. Un code ne sert pas seulement Ã  rÃ©aliser une tÃ¢che donnÃ©e, il a vocation Ã  Ãªtre diffusÃ©, rÃ©utilisÃ©, maintenu, que ce soit dans le contexte dâ€™une Ã©quipe dans une organisation ou bien en open-source.\nPour favoriser cette communication du code, des conventions ont Ã©tÃ© developpÃ©es en matiÃ¨re de qualitÃ© du code et de structuration des projets, quâ€™il est utile dâ€™appliquer dans ses projets. Nous prÃ©sentons ces conventions dans les chapitres QualitÃ© du Code et Architecture des Projets.\nIl est pour les mÃªmes raisons indispensable dâ€™appliquer les principes du contrÃ´le de version, qui permettent une documentation en continu des projets, ce qui accroÃ®t fortement leur rÃ©utilisabilitÃ© et leur maintenabilitÃ© dans le temps. Nous proposons donc un chapitre de rappel sur lâ€™utilisation du logiciel Git dans le chapitre Versionner son code et travailler collaborativement avec Git."
  },
  {
    "objectID": "chapters/introduction.html#travailler-de-maniÃ¨re-collaborative",
    "href": "chapters/introduction.html#travailler-de-maniÃ¨re-collaborative",
    "title": "Introduction",
    "section": "Travailler de maniÃ¨re collaborative",
    "text": "Travailler de maniÃ¨re collaborative\nLe data scientist, quel que soit son contexte de travail, est amenÃ© Ã  travailler dans le cadre de projets en Ã©quipe. Cela implique de dÃ©finir une organisation du travail ainsi que dâ€™utiliser des outils permettant de collaborer sur un projet de maniÃ¨re efficace et sÃ©curisÃ©e.\nNous prÃ©sentons une maniÃ¨re moderne de travailler collaborativement avec Git et GitHub dans le chapitre de rappel Versionner son code et travailler collaborativement avec Git. Les autres chapitres prendront pour acquis cette approche collaborative et la raffineront Ã  travers lâ€™approche DevOps4."
  },
  {
    "objectID": "chapters/introduction.html#maximiser-la-reproductibilitÃ©",
    "href": "chapters/introduction.html#maximiser-la-reproductibilitÃ©",
    "title": "Introduction",
    "section": "Maximiser la reproductibilitÃ©",
    "text": "Maximiser la reproductibilitÃ©\nLe troisiÃ¨me pilier des bonnes pratiques prÃ©sentÃ©es dans ce cours est la reproductibilitÃ©.\nUn projet est dit reproductible lorsque, avec le mÃªme code et les mÃªmes donnÃ©es, il est possible de reproduire les rÃ©sultats obtenus. Notons bien que le problÃ¨me de la reproductibilitÃ© est diffÃ©rent de celui de la rÃ©plicabilitÃ©. La rÃ©plicabilitÃ© est un concept scientifique, qui signifie quâ€™un mÃªme procÃ©dÃ© expÃ©rimental donne des rÃ©sultats analogues sur des jeux de donnÃ©es diffÃ©rents. La reproductibilitÃ© est un concept technique : elle ne signifie pas que le protocole expÃ©rimental est scientifiquement correct, mais quâ€™il a Ã©tÃ© spÃ©cifiÃ© et diffusÃ© dâ€™une maniÃ¨re qui permet Ã  tous de reproduire les rÃ©sultats obtenus.\nLa notion de reproductibilitÃ© est le fil rouge de ce cours : toutes les notions vues dans les diffÃ©rents chapitres y contribuent. Le fait de produire du code et des projets qui respectent les conventions communautaires, comme le fait dâ€™utiliser le contrÃ´le de version, contribuent Ã  rendre le code plus lisible et documentÃ©, et donc reproductible.\nIl faut nÃ©anmoins aller plus loin pour atteindre une vÃ©ritable reproductibilitÃ©, et rÃ©flÃ©chir Ã  la notion dâ€™environnement dâ€™exÃ©cution. Un code nâ€™est pas un objet autonome, il est toujours exÃ©cutÃ© sur un environnement (ordinateur personnel, serveur, etc.), et ces environnements peuvent Ãªtre trÃ¨s diffÃ©rents (systÃ¨me dâ€™exploitation, librairies installÃ©es, contraintes de sÃ©curitÃ©, etc.). Câ€™est pourquoi il faut rÃ©flÃ©chir Ã  la portabilitÃ© de son code, i.e.Â sa capacitÃ© Ã  sâ€™exÃ©cuter de maniÃ¨re attendue sur diffÃ©rents environnements, ce qui sera lâ€™objet dâ€™un chapitre Ã  part entiÃ¨re."
  },
  {
    "objectID": "chapters/introduction.html#faciliter-la-mise-en-production",
    "href": "chapters/introduction.html#faciliter-la-mise-en-production",
    "title": "Introduction",
    "section": "Faciliter la mise en production",
    "text": "Faciliter la mise en production\nPour quâ€™un projet de data science crÃ©e in fine de la valeur, il faut quâ€™il soit dÃ©ployÃ© sous une forme valorisable de sorte Ã  toucher son public. Cela implique deux choses :\n\ntrouver le format de diffusion adaptÃ©, i.e.Â qui valorise au mieux les rÃ©sultas obtenus auprÃ¨s des utilisateurs potentiels ;\nfaire transitionner le projet de lâ€™environnement dans lequel il a Ã©tÃ© dÃ©veloppÃ© vers une infrastructure de production, i.e.Â permettant un dÃ©ploiement robuste de lâ€™output du projet afin que celui-ci soit disponible Ã  la demande.\n\nDans le chapitre DÃ©ployer et valoriser son projet de data science, nous proposons des pistes permettant de rÃ©pondre Ã  ces deux besoins. Nous prÃ©sentons un certain nombre de formats standards (API, application, rapport automatisÃ©, site internet) qui permettent Ã  un projet de data science dâ€™Ãªtre valorisÃ©, ainsi que les outils modernes qui permettent de les produire.\nNous dÃ©taillons ensuite les concepts essentiels du dÃ©ploiement sur une infrastructure de production, et illustrons ces derniers par des exemples de dÃ©ploiements dans un environnement cloud moderne.\nCâ€™est en quelque sorte la rÃ©compense de lâ€™application des bonnes pratiques : dÃ¨s lors que lâ€™on sâ€™est donnÃ© la peine de produire un code et un projet appliquant des standards de qualitÃ©, que lâ€™on a bien versionnÃ© son code, et que lâ€™on a pris des mesures pour le rendre portable, le dÃ©ploiement du projet dans un environnement de production sâ€™en trouve largement facilitÃ©."
  },
  {
    "objectID": "chapters/introduction.html#une-ouverture-Ã -lindustrialisation-de-la-production",
    "href": "chapters/introduction.html#une-ouverture-Ã -lindustrialisation-de-la-production",
    "title": "Introduction",
    "section": "Une ouverture Ã  lâ€™industrialisation de la production",
    "text": "Une ouverture Ã  lâ€™industrialisation de la production\nEn simplifiant la structure dâ€™un projet, on facilite sa production en sÃ©rie. Dans le domaine de la data science, cela prendra par exemple la forme dâ€™une industrialisation des entraÃ®nements dâ€™un modÃ¨le permettant de choisir le â€œmeilleurâ€ dans un ensemble beaucoup plus complet de modÃ¨les que ne le permettrait une approche artisanale.\nNÃ©anmoins, tout modÃ¨le apprend du passÃ© et avoir un bon modÃ¨le aujourdâ€™hui nâ€™assure en rien que ce dernier sera pertinent demain, lorsquâ€™on aura rÃ©ellement besoin de celui-ci. Pour intÃ©grer cette dimension mouvante inhÃ©rante Ã  tout projet de data science, nous aurons lâ€™occasion de prÃ©senter quelques principes du MLOps. Ce terme, qui est certes un buzz-word mais qui rassemble un ensemble pertinent de pratiques pour les data scientists, sera prÃ©sentÃ© dans le chapitre consacrÃ©."
  },
  {
    "objectID": "chapters/introduction.html#chapitres-supplÃ©mentaires",
    "href": "chapters/introduction.html#chapitres-supplÃ©mentaires",
    "title": "Introduction",
    "section": "Chapitres supplÃ©mentaires",
    "text": "Chapitres supplÃ©mentaires\nPlusieurs outils prÃ©sentÃ©s tout au long de ce cours, tels que les logiciels Git et Docker, impliquent lâ€™utilisation du terminal ainsi que des connaissances de base du fonctionnement dâ€™un systÃ¨me Linux. Dans le chapitre DÃ©mystifier le terminal Linux pour gagner en autonomie, nous prÃ©sentons les connaissances essentielles des systÃ¨mes Linux quâ€™un data scientist doit possÃ©der pour pouvoir Ãªtre autonome dans ses dÃ©ploiements et dans lâ€™application des bonnes pratiques de dÃ©veloppement."
  },
  {
    "objectID": "chapters/introduction.html#approche-pÃ©dagogique",
    "href": "chapters/introduction.html#approche-pÃ©dagogique",
    "title": "Introduction",
    "section": "Approche pÃ©dagogique",
    "text": "Approche pÃ©dagogique\nLe parti pris de ce cours est que seule la pratique, et en particulier la confrontation Ã  des problÃ¨mes issus de projets rÃ©els, permet dâ€™acquÃ©rir efficacement des concepts informatiques. Aussi, une large part du cours consistera en lâ€™application des notions Ã©tudiÃ©es Ã  des cas concrets. Chaque chapitre se concluera par des applications touchant Ã  des sujets rÃ©alistes de data science.\nUn exemple fil rouge illustre les progrÃ¨s dans la conception dâ€™un projet reproductible en appliquant successivement le contenu des chapitres de ce cours.\nPour lâ€™Ã©valuation gÃ©nÃ©rale du cours, lâ€™idÃ©e sera de partir dâ€™un projet personnel, idÃ©alement terminÃ©, et de lui appliquer un maximum de bonnes pratiques prÃ©sentÃ©es dans ce cours."
  },
  {
    "objectID": "chapters/introduction.html#langages",
    "href": "chapters/introduction.html#langages",
    "title": "Introduction",
    "section": "Langages",
    "text": "Langages\nLes principes prÃ©sentÃ©s dans ce cours sont pour la plupart agnostiques du langage de programmation utilisÃ©.\nCe choix nâ€™est pas quâ€™Ã©ditorial, câ€™est selon nous un aspect fondamental du sujet des bonnes pratiques. Trop souvent, des diffÃ©rences de langage entre les phases de dÃ©veloppement (notamment R ou Python) et de mise en production (ex : Java) Ã©rigent des murs artificiels qui rÃ©duisent fortement la capacitÃ© Ã  valoriser des projets de data science.\nA lâ€™inverse, plus les diffÃ©rentes Ã©quipes qui forment le cycle de vie dâ€™un projet sâ€™accordent pour appliquer le mÃªme ensemble de bonnes pratiques, plus ces Ã©quipes dÃ©veloppent un langage commun, et plus les dÃ©ploiements en sont facilitÃ©s.\nUn exemple parlant est lâ€™utilisation de la conteneurisation : si le data scientist met Ã  disposition une image Docker comme output de sa phase de dÃ©veloppement et que le data engineer sâ€™occupe de dÃ©ployer cette image sur une infrastructure dÃ©diÃ©e, le contenu mÃªme de lâ€™application en termes de langage importe finalement assez peu. Cet exemple, certes simpliste, illustre malgrÃ© tout lâ€™enjeu des bonnes pratiques en matiÃ¨re de communication au sein dâ€™un projet.\nLes exemples prÃ©sentÃ©s dans ce cours seront pour lâ€™essentiel en Python. La raison principale est que ce langage, malgrÃ© ses dÃ©fauts, est enseignÃ© dans la majoritÃ© des cursus de data science mais aussi dâ€™informatique. Il peut faciliter la passerelle entre le monde des utilisateurs de donnÃ©es et celui des dÃ©veloppeurs informatiques, passerelle indispensable pour favoriser le dialogue entre ces deux profils, nÃ©cessaires tous deux pour un passage en production. Encore une fois, il est tout Ã  fait possible dâ€™appliquer les mÃªmes principes avec dâ€™autres langages, et nous encourageons dâ€™ailleurs les Ã©tudiants Ã  sâ€™essayer Ã  cet exercice formateur."
  },
  {
    "objectID": "chapters/introduction.html#environnement-dexÃ©cution",
    "href": "chapters/introduction.html#environnement-dexÃ©cution",
    "title": "Introduction",
    "section": "Environnement dâ€™exÃ©cution",
    "text": "Environnement dâ€™exÃ©cution\nA lâ€™instar du langage, les principes appliquÃ©s dans ce cours sont agnostiques Ã  lâ€™infrastructure utilisÃ©e pour faire tourner les exemples proposÃ©s. Il est donc Ã  la fois possible et souhaitable dâ€™appliquer les bonnes pratiques aussi bien Ã  un projet individuel dÃ©veloppÃ© sur un ordinateur personnel quâ€™Ã  un projet collaboratif visant Ã  Ãªtre dÃ©ployÃ© sur une infrastructure de production dÃ©diÃ©e.\nCependant, nous choisissons comme environnement de rÃ©fÃ©rence tout au long de ce cours le SSP Cloud, une plateforme de services pour la data science dÃ©veloppÃ©e Ã  lâ€™Insee et accessible aux Ã©lÃ¨ves des Ã©coles statistiques. Les raisons de ce choix sont multiples :\n\nlâ€™environnement de dÃ©veloppement est normalisÃ© : les serveurs du SSP Cloud ont une configuration homogÃ¨ne â€” notamment, ils se basent sur une mÃªme distribution Linux (Debian) â€” ce qui garantit la reproductibilitÃ© des exemples prÃ©sentÃ©s tout au long du cours ;\nvia un cluster Kubernetes sous-jacent, le SSP Cloud met Ã  disposition une infrastructure robuste permettant le dÃ©ploiement automatisÃ© dâ€™applications potentiellement intensives en donnÃ©es, ce qui permet de simuler un vÃ©ritable environnement de production ;\nle SSP Cloud est construit selon les standards les plus rÃ©cents des infrastructures data science, et permet donc dâ€™acquÃ©rir les bonnes pratiques de maniÃ¨re organique :\n\nles services sont lancÃ©s via des conteneurs, configurÃ©s par des images Docker. Cela permet de garantir une forte reproductibilitÃ© des dÃ©ploiements, au prix dâ€™une phase de dÃ©veloppement un peu plus coÃ»teuse ;\nle SSP Cloud est basÃ© sur une approche dite cloud native : il est construit sur un ensemble modulaire de briques logicielles, qui permettent dâ€™appliquer une sÃ©paration nette du code, des donnÃ©es, de la configuration et de lâ€™environnement dâ€™exÃ©cution, principe majeur des bonnes pratiques qui reviendra tout au long de ce cours.\n\n\nPour en savoir plus sur cette plateforme, vous pouvez consulter cette page."
  },
  {
    "objectID": "chapters/introduction.html#ressources-complÃ©mentaires",
    "href": "chapters/introduction.html#ressources-complÃ©mentaires",
    "title": "Introduction",
    "section": "Ressources complÃ©mentaires",
    "text": "Ressources complÃ©mentaires\n\nMissing semester du MIT"
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCelui que vous connaissez certainement le mieux est le Notebook Jupyter. TrÃ¨s pratique pour produire du code exploratoire ou pour transmettre un code avec de la documentation, nous aurons lâ€™occasion de dÃ©couvrir ses limites dans le cadre dâ€™un projet collaboratif ou un projet Ã  grande Ã©chelle.â†©ï¸\nNous aurons lâ€™occasion ultÃ©rieurement de dÃ©finir de maniÃ¨re formelle cette notion centrale. En attendant, on peut entendre ce concept comme un environnement disponible en continu afin de mettre Ã  disposition une valorisation de donnÃ©es. Cela prend souvent la forme dâ€™un serveur de production ou dâ€™un cluster informatique qui doit Ãªtre disponible en continu.â†©ï¸\nLâ€™aspect trÃ¨s intriquÃ© des notions de bonnes pratiques, de reproductibilitÃ© et de mise en production nous a dâ€™ailleurs longtemps fait hÃ©siter sur le nom Ã  donner Ã  ce cours. Parmi la shortlist des noms possibles, nous avions â€œBonnes pratiques en data scienceâ€ ou â€œBonnes pratiques pour la reproductibilitÃ© en data science. NÃ©anmoins, les bonnes pratiques restent un moyen lÃ  oÃ¹ la mise en production est la finalitÃ©, nous avons ainsi privilÃ©giÃ© le fait de mettre en avant cette derniÃ¨re notion.â†©ï¸\nDÃ©marche consistant Ã  automatiser et intÃ©grer la conception et la production des livrables avant la phase de mise en production. Comme les bonnes pratiques, cette approche issue Ã  lâ€™origine du monde du dÃ©veloppement logiciel est devenue incontournable pour les data scientists.â†©ï¸"
  },
  {
    "objectID": "chapters/galerie.html",
    "href": "chapters/galerie.html",
    "title": "Galerie dâ€™exemples",
    "section": "",
    "text": "Une galerie dâ€™exemple de projets Ã  venir\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            ModÃ¨le de carte\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            PrimePredict\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            ResultAthle\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "chapters/deployment.html",
    "href": "chapters/deployment.html",
    "title": "DÃ©ploiement",
    "section": "",
    "text": "DÃ©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein Ã©cran.\n\n\n\n\n\n\n\nPage en construction."
  },
  {
    "objectID": "chapters/big-data.html",
    "href": "chapters/big-data.html",
    "title": "Traitement des donnÃ©es volumineuses",
    "section": "",
    "text": "DÃ©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein Ã©cran.\n\n\n\n\n\n\n\nPage en construction."
  },
  {
    "objectID": "chapters/evaluation.html",
    "href": "chapters/evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "Lâ€™objectif gÃ©nÃ©ral de lâ€™Ã©valuation de ce cours est de mettre en pratique les notions Ã©tudiÃ©es (bonnes pratiques de dÃ©veloppement et mise en production) de maniÃ¨re appliquÃ©e et rÃ©aliste, i.e.Â Ã  travers un projet basÃ© sur une problÃ©matique â€œmÃ©tierâ€ et des donnÃ©es rÃ©elles. Pour cela, lâ€™Ã©valuation sera en deux parties :\n\nPar groupe de 3 : un projet Ã  choisir parmi les 3 parcours (MLOps, app interactive / dashboard, publication reproductible + site web). IdÃ©alement, on choisira un projet rÃ©el, effectuÃ© par exemple dans le cadre dâ€™un cours prÃ©cÃ©dent et qui gÃ©nÃ¨re un output propice Ã  une mise en production.\nSeul : effectuer une revue de code dâ€™un autre projet. CompÃ©tence essentielle et souvent attendue dâ€™un data scientist, la revue de code sera lâ€™occasion de bien intÃ©grer les bonnes pratiques de dÃ©veloppement (cf.Â checklist ci-dessous) et de faire un retour bienveillant sur un autre projet que celui de son groupe.\n\n\n\n\n\n\n\nAvertissement\n\n\n\nCe projet doit mobiliser des donnÃ©es publiquement accessibles. La rÃ©cupÃ©ration et structuration de ces donnÃ©es peut faire partie des enjeux du projet mais celles-ci ne doivent pas provenir dâ€™un projet antÃ©rieur de votre scolaritÃ© pour lequel le partage de donnÃ©es nâ€™est pas possible."
  },
  {
    "objectID": "chapters/evaluation.html#modalitÃ©s",
    "href": "chapters/evaluation.html#modalitÃ©s",
    "title": "Evaluation",
    "section": "",
    "text": "Lâ€™objectif gÃ©nÃ©ral de lâ€™Ã©valuation de ce cours est de mettre en pratique les notions Ã©tudiÃ©es (bonnes pratiques de dÃ©veloppement et mise en production) de maniÃ¨re appliquÃ©e et rÃ©aliste, i.e.Â Ã  travers un projet basÃ© sur une problÃ©matique â€œmÃ©tierâ€ et des donnÃ©es rÃ©elles. Pour cela, lâ€™Ã©valuation sera en deux parties :\n\nPar groupe de 3 : un projet Ã  choisir parmi les 3 parcours (MLOps, app interactive / dashboard, publication reproductible + site web). IdÃ©alement, on choisira un projet rÃ©el, effectuÃ© par exemple dans le cadre dâ€™un cours prÃ©cÃ©dent et qui gÃ©nÃ¨re un output propice Ã  une mise en production.\nSeul : effectuer une revue de code dâ€™un autre projet. CompÃ©tence essentielle et souvent attendue dâ€™un data scientist, la revue de code sera lâ€™occasion de bien intÃ©grer les bonnes pratiques de dÃ©veloppement (cf.Â checklist ci-dessous) et de faire un retour bienveillant sur un autre projet que celui de son groupe.\n\n\n\n\n\n\n\nAvertissement\n\n\n\nCe projet doit mobiliser des donnÃ©es publiquement accessibles. La rÃ©cupÃ©ration et structuration de ces donnÃ©es peut faire partie des enjeux du projet mais celles-ci ne doivent pas provenir dâ€™un projet antÃ©rieur de votre scolaritÃ© pour lequel le partage de donnÃ©es nâ€™est pas possible."
  },
  {
    "objectID": "chapters/evaluation.html#checklist-des-bonnes-pratiques-de-dÃ©veloppement",
    "href": "chapters/evaluation.html#checklist-des-bonnes-pratiques-de-dÃ©veloppement",
    "title": "Evaluation",
    "section": "Checklist des bonnes pratiques de dÃ©veloppement",
    "text": "Checklist des bonnes pratiques de dÃ©veloppement\nLes bonnes pratiques de dÃ©veloppement ci-dessous sont les indispensables de ce cours. Elles doivent Ãªtre Ã  la fois appliquÃ©es dans les projets de groupe, et Ã  la base de la revue de code individuelle.\n\nUtilisation de Git\n\nPrÃ©sence dâ€™un fichier .gitignore adaptÃ© au langage et avec des rÃ¨gles additionnelles pour respecter les bonnes pratiques de versioning\nTravail collaboratif : utilisation des branches et des pull requests\n\nPrÃ©sence dâ€™un fichier README prÃ©sentant le projet : contexte, objectif, comment lâ€™utiliser ?\nPrÃ©sence dâ€™un fichier LICENSE dÃ©clarant la licence (open-source) dâ€™exploitation du projet.\nVersioning des packages : prÃ©sence dâ€™un fichier requirements.txt ou dâ€™un fichier dâ€™environnement environment.yml pour conda\nQualitÃ© du code\n\nRespect des standards communautaires : utiliser un linter et/ou un formatter\nModularitÃ© : un script principal qui appelle des modules\n\nStructure des projets\n\nRespect des standards communautaires (cookiecutter)\nModularitÃ© du projet selon le modÃ¨le Ã©voquÃ© dans le cours:\n\nCode sur GitHub\nDonnÃ©es sur S3\nFichiers de configuration (secrets, etc.) Ã  part\n\n\n\n\n\n\nProposition de modularitÃ© du projet illustrÃ©e pour un projet mixte MLOps et dashboard"
  },
  {
    "objectID": "chapters/evaluation.html#projets",
    "href": "chapters/evaluation.html#projets",
    "title": "Evaluation",
    "section": "Projets",
    "text": "Projets\nVoici trois â€œparcoursâ€ possibles afin de mettre en application les concepts et techniques du cours dans le cadre de projets appliquÃ©s. Des projets qui sortiraient de ces parcours-types sont tout Ã  fait possibles et apprÃ©ciÃ©s, il suffit dâ€™en discuter avec les auteurs du cours.\n\nParcours MLOps\n\n\n\n\n\n\nObjectif\n\n\n\nA partir dâ€™un projet existant ou dâ€™un projet type contest Kaggle, dÃ©velopper un modÃ¨le de ML rÃ©pondant Ã  une problÃ©matique mÃ©tier, puis la dÃ©ployer sur une infrastructure de production conformÃ©ment aux principes du MLOps.\n\n\nÃ‰tapes :\n\nRespecter la checklist des bonnes pratiques de dÃ©veloppement ;\nDÃ©velopper un modÃ¨le de ML qui rÃ©pond Ã  un besoin mÃ©tier ;\nEntraÃ®ner le modÃ¨le via validation croisÃ©e, avec une procÃ©dure de fine-tuning des hyperparamÃ¨tres ;\nFormaliser le processus de fine-tuning de maniÃ¨re reproductible via MLFlow ;\nConstruire une API avec Fastapi pour exposer le meilleur modÃ¨le ;\nCrÃ©er une image Docker pour mettre Ã  disposition lâ€™API ;\nDÃ©ployer lâ€™API sur le SSP Cloud ;\nIndustrialiser le dÃ©ploiement en mode GitOps avec ArgoCD\nGÃ©rer le monitoring de lâ€™application : logs, dashboard de suivi des performances, etc.\n\n\n\nParcours dashboard / application interactive\n\n\n\n\n\n\nObjectif\n\n\n\nA partir dâ€™un projet existant ou dâ€™un projet que vous construirez, dÃ©velopper une application interactive ou un dashboard statique rÃ©pondant Ã  une problÃ©matique mÃ©tier, puis dÃ©ployer sur une infrastructure de production.\n\n\nÃ‰tapes :\n\nRespecter la checklist des bonnes pratiques de dÃ©veloppement\nDÃ©velopper une application interactive Streamlit ou un dashboard statique avec Quarto rÃ©pondant Ã  une problÃ©matique mÃ©tier\nCrÃ©er une image Docker permettant dâ€™exposer lâ€™application en local\nDÃ©ployer lâ€™application sur le SSP Cloud (application interactive) ou sur Github Pages (site statique)\nCustomiser le thÃ¨me, le CSS etc. pour mettre en valeur au maximum les rÃ©sultats de la publication et les messages principaux\nAutomatiser lâ€™ingestion des donnÃ©es en entrÃ©e pour que le site web se mette Ã  jour rÃ©guliÃ¨rement\nIndustrialiser le dÃ©ploiement en mode GitOps avec ArgoCD\nGÃ©rer le monitoring de lâ€™application : logs, mÃ©triques de suivi des performances, etc.\n\n\n\nParcours big data\n\n\n\n\n\n\nObjectif\n\n\n\nLâ€™objectif de ce parcours est de construire un pipeline type ETL (Extract/Transform/Load) prenant en entrÃ©e une source de donnÃ©es massives afin de les mettre Ã  disposition dans un systÃ¨me de base de donnÃ©es optimisÃ© pour lâ€™analyse. Ce parcours est intÃ©ressant pour les Ã©tudiant.e.s souhaitant un projet avec une coloration data engineering plus marquÃ©e.\n\n\nÃ‰tapes :\n\nRespecter la checklist des bonnes pratiques de dÃ©veloppement\nExtract : identifier une ou plusieurs sources de donnÃ©es massives ouvertes (idÃ©es : 1, 2), et rÃ©aliser lâ€™ingestion de ces donnÃ©es sur le service de stockage S3 du SSP Cloud (documentation)\nTransform : en utilisant une technologie big data adoptÃ© Ã  la volumÃ©trie des donnÃ©es en entrÃ©e (donnÃ©es massives : Spark, donnÃ©es volumineuses : Arrow / DuckDB, toutes disponibles sur le SSP Cloud), effectuer des opÃ©rations sur les donnÃ©es brutes (filtrages, agrÃ©gations, etc.) afin dâ€™en extraire des sous-ensembles de donnÃ©es pertinents pour rÃ©pondre Ã  une problÃ©matique mÃ©tier\nLoad : charger les tables construites Ã  lâ€™Ã©tape prÃ©cÃ©dente dans un systÃ¨me de base de donnÃ©es relationnelle (ex : PostgreSQL, disponible dans le catalogue du SSP Cloud)\nIntÃ©grer lâ€™ensemble des Ã©tapes dans un pipeline de donnÃ©es avec un orchestrateur de traitements (ex : Argo Workflows, disponible dans le catalogue du SSP Cloud) afin dâ€™automatiser leur exÃ©cution\nConstruire un dashboard minimaliste (par exemple, avec Superset, disponible dans le catalogue du SSP Cloud) afin de valoriser les donnÃ©es produites\n\n\n\nParcours publication reproductible\n\n\n\n\n\n\nObjectif\n\n\n\nA partir dâ€™un projet existant ou dâ€™un projet que vous construirez, rÃ©diger un rapport reproductible Ã  partir de donnÃ©es afin de rÃ©pondre Ã  une problÃ©matique mÃ©tier, puis le mettre Ã  disposition Ã  travers un site web automatiquement gÃ©nÃ©rÃ© et publiÃ©.\n\n\nÃ‰tapes :\n\nRespecter la checklist des bonnes pratiques de dÃ©veloppement\nRÃ©diger un rapport reproductible avec Quarto qui fasse intervenir des donnÃ©es, du code, de la visualisation de donnÃ©es, du texte, etc.\nExposer le rapport sous la forme dâ€™un site web via GitHub Actions\nCustomiser le thÃ¨me, le CSS etc. pour mettre en valeur au maximum les rÃ©sultats de la publication et les messages principaux\nAutomatiser lâ€™ingestion des donnÃ©es en entrÃ©e pour que le site web se mette Ã  jour rÃ©guliÃ¨rement\nMettre en place des tests automatisÃ©s de vÃ©rification des standards de qualitÃ© du code (linter), de dÃ©tection de fautes dâ€™orthographes/de grammaire, etc.\nGÃ©nÃ©rer des slides au format quarto-revealjs afin de prÃ©senter les principaux rÃ©sultats de la publication, et les exposer comme une page du site"
  },
  {
    "objectID": "chapters/evaluation.html#revue-de-code",
    "href": "chapters/evaluation.html#revue-de-code",
    "title": "Evaluation",
    "section": "Revue de code",
    "text": "Revue de code\nSur le projet dâ€™un groupe diffÃ©rent du sien (attribuÃ© alÃ©atoirement au cours du semestre) :\n\nouvrir une pull request de revue de code via un fork (cf.Â chapitre sur Git pour la procÃ©dure)\ndonner une apprÃ©ciation gÃ©nÃ©rale de la conformitÃ© du projet Ã  la checklist des bonnes pratiques de dÃ©veloppement\nsuggÃ©rer des pistes dâ€™amÃ©lioration du projet\n\nChaque groupe, ayant reÃ§u des revues de code de son projet, pourra prendre en compte ces pistes dâ€™amÃ©liorations dans la mesure du temps disponible, par le biais dâ€™une autre pull request qui devra rÃ©fÃ©rencer celle de la revue de code. Cette derniÃ¨re partie ne sera cependant pas strictement attendue, elle sera valorisÃ©e en bonus dans la notation finale."
  },
  {
    "objectID": "chapters/git.html",
    "href": "chapters/git.html",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "",
    "text": "DÃ©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein Ã©cran."
  },
  {
    "objectID": "chapters/git.html#pourquoi-faire",
    "href": "chapters/git.html#pourquoi-faire",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi faire ?",
    "text": "Pourquoi faire ?\nLe dÃ©veloppement rapide de la data science au cours de ces derniÃ¨res annÃ©es sâ€™est accompagnÃ©e dâ€™une complexification substantielle des projets. Par ailleurs, les projets sont de plus en plus collaboratifs, que ce soit dans le cadre dâ€™Ã©quipes dans un contexte professionnel ou bien pour des contributions Ã  des projets open-source. Naturellement, ces Ã©volutions doivent nous amener Ã  modifier nos maniÃ¨res de travailler pour gÃ©rer cette complexitÃ© croissante et continuer Ã  produire de la valeur Ã  partir des projets de data science.\nPourtant, tout data scientist sâ€™est parfois demandÃ© :\n\nquelle Ã©tait la bonne version dâ€™un programme\nqui Ã©tait lâ€™auteur dâ€™un bout de code en particulier\nsi un changement Ã©tait important ou juste un essai\ncomment fusionner des programmes\netc.\n\nEt il nâ€™est pas rare de perdre le fil des versions de son projet lorsque lâ€™on garde trace de celles-ci de faÃ§on manuelle.\nExemple de contrÃ´le de version fait â€œÃ  la mainâ€\n\nPourtant, il existe un outil informatique puissant afin de rÃ©pondre Ã  tous ces besoins : la gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer lâ€™historique des modifications dâ€™un ensemble de fichiers\nrevenir Ã  des versions prÃ©cÃ©dentes dâ€™un ou plusieurs fichiers\nrechercher les modifications qui ont pu crÃ©er des erreurs\ntravailler simultanÃ©ment sur un mÃªme fichier sans risque de perte\npartager ses modifications et rÃ©cupÃ©rer celles des autres\nproposer des modifications, les discuter, sans pour autant modifier la derniÃ¨re version existante\nidentifier les auteurs et la date des modifications\n\nEn outre, ces outils fonctionnent avec tous les langages informatiques car ils reposent sur la comparaison des lignes et des caractÃ¨res des programmes, indÃ©pendamment du langage. En bref, câ€™est la bonne maniÃ¨re pour partager des codes et travailler Ã  plusieurs sur un projet de data science. En rÃ©alitÃ©, il ne serait pas exagÃ©rÃ© de dire que lâ€™utilisation du contrÃ´le de version est la bonne pratique la plus fondamentale de tout projet faisant intervenir du code, et quâ€™elle conditionne largement toutes les autres."
  },
  {
    "objectID": "chapters/git.html#pourquoi-git",
    "href": "chapters/git.html#pourquoi-git",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi Git  ?",
    "text": "Pourquoi Git  ?\nPlusieurs logiciels de contrÃ´le de version existent sur le marchÃ©. En principe, le logiciel Git, dÃ©veloppÃ© initialement pour fournir une solution dÃ©centralisÃ©e et open-source dans le cadre du dÃ©veloppement du noyau Linux, est devenu largement hÃ©gÃ©monique. Aussi, toutes les application de ce cours sâ€™effectueront Ã  lâ€™aide du logiciel Git."
  },
  {
    "objectID": "chapters/git.html#pourquoi-github",
    "href": "chapters/git.html#pourquoi-github",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi GitHub  ?",
    "text": "Pourquoi GitHub  ?\nTravailler de maniÃ¨re collaborative avec Git implique de synchroniser son rÃ©pertoire local avec une copie distante, situÃ©e sur un serveur hÃ©bergeant des projets Git. Ce serveur peut Ãªtre un serveur interne Ã  une organisation, ou bien Ãªtre fourni par un hÃ©bergeur externe. Les deux alternatives les plus populaires en la matiÃ¨re sont GitHub et GitLab. Dans ce cours, nous utiliserons GitHub, qui est devenu au fil des annÃ©es la rÃ©fÃ©rence pour lâ€™hÃ©bergement des projets open-source. En pratique, les deux services sont relativement semblables, et tous les concepts prÃ©sentÃ©s se retrouvent sous une forme similaire sur les deux plateformes."
  },
  {
    "objectID": "chapters/git.html#principes-et-commandes-usuelles",
    "href": "chapters/git.html#principes-et-commandes-usuelles",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Principes et commandes usuelles",
    "text": "Principes et commandes usuelles\nLe graphique suivant illustre les principes fondamentaux de Git.\nGit tout-en-un (Source) \nLorsquâ€™on utilise Git, il est important de bien distinguer ce qui se passe en local (sur son poste, sur le serveur sur lequel on travailleâ€¦) de ce qui se passe en remote, i.e.Â en intÃ©ragissant avec un serveur distant. Comme le montre le graphique, lâ€™essentiel du contrÃ´le de version se passe en rÃ©alitÃ© en local.\nEn thÃ©orie, sur un projet individuel, il est mÃªme possible de rÃ©aliser lâ€™ensemble du contrÃ´le de version en mode hors-ligne. Pour cela, il suffit dâ€™indiquer Ã  Git le projet (dossier) que lâ€™on souhaite versionner en utilisant la commande git init. Cette commande a pour effet de crÃ©er un dossier .git Ã  la racine du projet, dans lequel Git va stocker tout lâ€™historique du projet (commits, branches, etc.) et permettre de naviguer entre les versions. A cause du . qui prÃ©fixe son nom, ce dossier est gÃ©nÃ©ralement cachÃ© par dÃ©faut, ce qui nâ€™est pas problÃ©matique dans la mesure oÃ¹ il nâ€™y a jamais besoin de le parcourir ou de le modifier Ã  la main en pratique. Retenez simplement que câ€™est la prÃ©sence de ce dossier .git qui fait quâ€™un dossier est considÃ©rÃ© comme un projet Git, et donc que vous pouvez utilisez les commandes usuelles de Git dans ce dossier Ã  lâ€™aide dâ€™un terminal : - git status : affiche les modifications du projet par rapport Ã  la version prÃ©cÃ©dente ; - git add chemin_du_fichier : ajoute un fichier nouveau ou modifiÃ© Ã  la zone de staging de Git en vue dâ€™un commit ; - git add -A : ajoute tous les fichiers nouveaux ou modifiÃ©s Ã  la zone de staging ; - git commit -m \"message de commit\" : crÃ©e un commit, i.e.Â une photographie des modifications (ajouts, modifications, suppressions) apportÃ©es au projet depuis la derniÃ¨re version, et lui assigne un message dÃ©crivant ces changements. Les commits sont lâ€™unitÃ© de base de lâ€™historique du projet construit par Git.\nEn pratique, travailler uniquement en local nâ€™est pas trÃ¨s intÃ©ressant. Pour pouvoir travailler de maniÃ¨re collaborative, on va vouloir synchroniser les diffÃ©rentes copies locales du projet Ã  un rÃ©pertoire centralisÃ©, qui maintient de fait la â€œsource de vÃ©ritÃ©â€ (single source of truth). MÃªme sur un projet individuel, il fait sens de synchroniser son rÃ©pertoire local Ã  une copie distante pour assurer lâ€™intÃ©gritÃ© du code de son projet en cas de problÃ¨me matÃ©riel.\nEn gÃ©nÃ©ral, on va donc initialiser le projet dans lâ€™autre sens : - crÃ©er un nouveau projet sur GitHub - gÃ©nÃ©rer un jeton dâ€™accÃ¨s (personal access token) - cloner le projet en local via la mÃ©thode HTTPS : git clone https://github.com/&lt;username&gt;/&lt;project_name&gt;.git\nLe projet clonÃ© est un projet Git â€” il contient le dossier .git â€” synchronisÃ© par dÃ©faut avec le rÃ©pertoire distant. On peut le vÃ©rifier avec la commande remote de Git :\n\n\nterminal\n\n$ git remote -v\n\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (fetch)\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (push)\nLe projet local est bien liÃ© au rÃ©pertoire distant sur GitHub, auquel Git donne par dÃ©faut le nom origin. Ce lien permet dâ€™utiliser les commandes de synchronisation usuelles : - git pull : rÃ©cupÃ©rer les changements (fetch) sur le remote et les appliquer au projet local - git push : envoyer les changements locaux sur le remote"
  },
  {
    "objectID": "chapters/git.html#implÃ©mentations",
    "href": "chapters/git.html#implÃ©mentations",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "ImplÃ©mentations",
    "text": "ImplÃ©mentations\nGit est un logiciel, qui peut Ãªtre tÃ©lÃ©chargÃ© sur le site officiel pour diffÃ©rents systÃ¨mes dâ€™exploitation. Il existe cependant diffÃ©rentes maniÃ¨res dâ€™utiliser Git : - le client en ligne de commande : câ€™est lâ€™implÃ©mentation standard, et donc la plus complÃ¨te. Câ€™est celle quâ€™on utilisera dans ce cours. Le client Git est installÃ© par dÃ©faut sur les diffÃ©rents services du SSP Cloud (VSCode, RStudio, Jupyter, etc.) et peut donc Ãªtre utilisÃ© via nâ€™importe quel terminal. La documentation du SSP Cloud dÃ©taille la procÃ©dure ; - des interfaces graphiques : elles facilitent la prise en main de Git via des guides visuels, mais ne permettent pas de rÃ©aliser toutes les opÃ©rations permises par Git - lâ€™interface native de RStudio pour les utilisateurs de R : trÃ¨s complÃ¨te et stable. La formation au travail collaboratif avec Git et RStudio prÃ©sente son utilisation de maniÃ¨re dÃ©taillÃ©e ; - le plugin Jupyter-git pour les utilisateurs de Python : elle implÃ©mente les principales features de Git, mais sâ€™avÃ¨re assez instable Ã  lâ€™usage."
  },
  {
    "objectID": "chapters/git.html#bonnes-pratiques",
    "href": "chapters/git.html#bonnes-pratiques",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\nLe contrÃ´le de version est une bonne pratique de dÃ©veloppement en soiâ€¦ mais son utilisation admet elle mÃªme des bonnes pratiques qui, lorsquâ€™elles sont appliquÃ©es, permettent dâ€™en tirer le plus grand profit.\n\nQue versionne-t-on ?\n\nUniquement des fichiers texte\nA chaque commit, Git calcule les diffÃ©rences entre les versions successives du projet, afin de ne pas avoir Ã  stocker une image complÃ¨te de ce dernier Ã  chaque fois. Câ€™est ce qui permet aux projets Git de rester trÃ¨s lÃ©gers par dÃ©faut, et donc aux diffÃ©rentes opÃ©rations impliquant le remote (clone, push, pull..) dâ€™Ãªtre trÃ¨s rapides.\nLa contrepartie de cette lÃ©gÃ¨retÃ© est une contrainte sur les types dâ€™objets que lâ€™on doit versionner. Les diffÃ©rences sont calculables uniquement sur des fichiers de type texte : codes source, fichiers texte, fichiers de configuration non-sensiblesâ€¦ Voici donc une liste non-exhaustive des extensions de fichier que lâ€™on retrouve frÃ©quemment dans un dÃ©pÃ´t Git dâ€™un projet R ou Python : .py, .R, .Rmd, .txt, .json, .xml, .yaml, .toml, et bien dâ€™autres.\nEn revanche tous les fichiers binaires â€” pour faire simple, tous les fichiers qui ne peuvent pas Ãªtre ouverts dans un Ã©diteur de texte basique sans produire une suite inintelligible de caractÃ¨res â€” nâ€™ont gÃ©nÃ©ralement pas destination Ã  se retrouver sur un dÃ©pÃ´t Git. Du fait de leur formatage (binaire), Git ne peut pas calculer les diffÃ©rences entre versions pour ces fichiers et câ€™est donc le fichier entier qui est sauvegardÃ© dans lâ€™historique Ã  chaque changement, ce qui peut trÃ¨s rapidement faire croÃ®tre la taille du dÃ©pÃ´t. Pour Ã©viter de versionner ces fichiers par erreur, on va les ajouter au fichier .gitignore (cf.Â supra).\n\n\nPas de donnÃ©es\nComme expliquÃ© en introduction, le fil rouge de ce cours sur les bonnes pratiques est lâ€™importance de bien sÃ©parer code, donnÃ©es et environnement dâ€™exÃ©cution afin de favoriser la reproductibilitÃ© des projets de data science. Ce principe doit sâ€™appliquer Ã©galement Ã  lâ€™usage du contrÃ´le de version, et ce pour diffÃ©rentes raisons.\nA priori, inclure ces donnÃ©es dans un dÃ©pÃ´t Git peut sembler une bonne idÃ©e en termes de reproductibilitÃ©. En machine learning par exemple, on est souvent amenÃ© Ã  rÃ©aliser de nombreuses expÃ©rimentations Ã  partir dâ€™un mÃªme modÃ¨le appliquÃ© Ã  diffÃ©rentes transformations des donnÃ©es initiales, transformations que lâ€™on pourrait versionner. En pratique, il est gÃ©nÃ©ralement prÃ©fÃ©rable de versionner le code qui permet de gÃ©nÃ©rer ces transformations et donc les expÃ©rimentations associÃ©es, dans la mesure oÃ¹ le suivi des versions des datasets peut sâ€™avÃ©rer rapidement complexe. Pour de plus gros projets, des alternatives spÃ©cifiques existent : câ€™est le champ du MLOps, domaine en constante expansion qui vise Ã  rendre les pipelines de machine learning plus reproductibles.\nEnfin, la structure mÃªme de Git nâ€™est techniquement pas faite pour le stockage de donnÃ©es. Si des petits datasets dans un format texte ne poseront pas de problÃ¨me, des donnÃ©es volumineuses (Ã  partir de plusieurs Mo) vont faire croÃ®tre la taille du dÃ©pÃ´t et donc ralentir significativement les opÃ©rations de synchronisation avec le remote.\n\n\nPas dâ€™informations locales\nLÃ  encore en vertu du principe de sÃ©paration donnÃ©es / code/ environnement, les donnÃ©es locales, i.e.Â spÃ©cifiques Ã  lâ€™environnement de travail sur lequel le code a Ã©tÃ© exÃ©cutÃ©, nâ€™ont pas vocation Ã  Ãªtre versionnÃ©es. Par exemple, des fichiers de configuration spÃ©cifiques Ã  un poste de travail, des chemins dâ€™accÃ¨s spÃ©cifiques Ã  un ordinateur donnÃ©, etc. Cela demande une plus grande rigueur lors de la construction du projet, mais garantit par lÃ  mÃªme une meilleure reproductiblitÃ© pour les futurs utilisateurs du projet.\n\n\nPas dâ€™outputs\nLes outputs dâ€™un projet (graphiques, publications, modÃ¨le entraÃ®nÃ©â€¦) nâ€™ont pas vocation Ã  Ãªtre versionnÃ©, en vertu des diffÃ©rents arguments prÃ©sentÃ©s ci-dessus : - il ne sâ€™agit gÃ©nÃ©ralement pas de fichiers de type texte ; - le code source du projet doit dans tous les cas permettre des les regÃ©nÃ©rer Ã  lâ€™identique.\n\n\nUtiliser un .gitignore\nOn a listÃ© prÃ©cÃ©demment un large Ã©ventail de fichiers qui nâ€™ont, par nature, pas vocation Ã  Ãªtre versionnÃ©. Bien entendu, faire attention Ã  ne pas ajouter ces diffÃ©rents fichiers au moment de chaque git add serait assez pÃ©nible. Git simplifie largement cette procÃ©dure en nous donnant la possibilitÃ© de remplir un fichier .gitignore, situÃ© Ã  la racine du projet, qui spÃ©cifie lâ€™ensemble des fichiers et types de fichiers que lâ€™on ne souhaite pas versionner dans le cadre du projet courant.\nDe maniÃ¨re gÃ©nÃ©rale, il y a pour chaque langage des fichiers que lâ€™on ne souhaitera jamais versionner. Pour en tenir compte, une premiÃ¨re bonne pratique est de choisir le .gitignore associÃ© au langage du projet lors de la crÃ©ation du dÃ©pÃ´t sur GitHub. Ce faisant, le projet est initialitÃ© avec un gitignore dÃ©jÃ  existant et prÃ©-rempli de chemins et de types de fichiers qui ne sont pas Ã  versionner. Regardons un extrait du gitignore Python pour comprendre sa structure et son fonctionnement.\npip-log.txt\n__pycache__/\n*.log\nChaque ligne du gitignore spÃ©cifie un Ã©lÃ©ment Ã  ignorer du contrÃ´le de version, Ã©lÃ©ment qui peut Ãªtre un ficher/dossier ou bien une rÃ¨gle concernant un ensemble de fichiers/dossiers. Sauf si spÃ©cifiÃ© explicitement, les chemins sont relatifs Ã  la racine du projet. Lâ€™extrait du gitignore Python illustre les diffÃ©rentes possibilitÃ©s :\n\nligne 1 : ignore le fichier pip-log.txt ;\nligne 2 : ignore le dossier __pycache__/ ;\nligne 3 : ignore tous les fichiers dont lâ€™extension est .log.\n\nDe nombreuses autres possiblitÃ©s existent, et sont dÃ©taillÃ©es par exemple dans la documentation de Git.\n\n\n\nMessages des commits\nLe commit est lâ€™unitÃ© de temps de Git, et donc fondamentalement ce qui permet de remonter dans lâ€™historique dâ€™un projet. Afin de pouvoir bÃ©nÃ©ficier Ã  plein de cet avantage de Git, il est capital dâ€™accompagner ses commits de messages pertinents, en se plaÃ§ant dans la perspective que lâ€™on peut Ãªtre amenÃ© plusieurs semaines ou mois plus tard Ã  vouloir retrouver du code dans lâ€™historique de son projet. Les quelques secondes prises Ã  chaque commit pour rÃ©flÃ©chir Ã  une description pertinente du bloc de modifications que lâ€™on apporte au projet peuvent donc faire gagner un temps prÃ©cieux Ã  la longue.\nDe nombreuses conventions existent pour rÃ©diger des messages de commit pertinents. Nous rappelons ici les plus importantes :\n\ncontenu : le message doit dÃ©tailler le pourquoi plutÃ´t que le comment des modifications. Par exemple, plutÃ´t que â€œAjoute le fichier test.pyâ€, on prÃ©fÃ©rera Ã©crire â€œAjout dâ€™une sÃ©rie de tests unitairesâ€ ;\nstyle : le message doit Ãªtre Ã  lâ€™impÃ©ratif et former une phrase (sans point Ã  la fin) ;\nlongueur : le message du commit doit Ãªtre court (&lt; 72 caractÃ¨res). Sâ€™il nâ€™est pas possible de trouver un message de cette taille qui rÃ©sume le commit, câ€™est gÃ©nÃ©ralement un signe que le commit regroupe trop de changements (cf.Â point suivant). Le fait de devoir mettre des + ou des & / et dans un message de commit pour sÃ©parer les changements est Ã©galement un bon indicateur dâ€™un commit trop gros.\n\n\n\nFrÃ©quence des commits\nDe maniÃ¨re gÃ©nÃ©rale, il est conseillÃ© de rÃ©aliser des commits rÃ©guliers lorsque lâ€™on travaille sur un projet. Une rÃ¨gle simple que lâ€™on peut par exemple appliquer est la suivante : dÃ¨s lors quâ€™un ensemble de modifications forment un tout cohÃ©rent et peuvent Ãªtre rÃ©sumÃ©es par un message simple, il est temps dâ€™en faire un commit. Cette approche a de nombreux avantages :\n\nsi lâ€™on fait suivre chaque commit dâ€™un push â€” ce qui est conseillÃ© en pratique â€” on sâ€™assure de disposer rÃ©guliÃ¨reemnt dâ€™une copie de ses travaux, ce qui limite le risque de perte ;\nil est plus facile de revenir en arriÃ¨re en cas dâ€™erreur si les commits portent sur des changements ciblÃ©s et cohÃ©rents ;\nle processus de review dâ€™une pull request est facilitÃ©, car les diffÃ©rents blocs de modification sont plus clairement sÃ©parÃ©s ;\ndans une approche dâ€™intÃ©gration continue â€” concept que lâ€™on verra en dÃ©tail dans le chapitre sur la mise en production â€” faire des commits et des PR rÃ©guliÃ¨rement permet de dÃ©ployer de maniÃ¨re continue les changements en production, et donc dâ€™obtenir les feedbacks des utilisateurs et dâ€™adapter plus rapidement si besoin."
  },
  {
    "objectID": "chapters/git.html#branches",
    "href": "chapters/git.html#branches",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Branches",
    "text": "Branches\n\nConcept\nLa possibilitÃ© de crÃ©er des branches est lâ€™une des fonctionnalitÃ©s majeures de Git. La crÃ©ation dâ€™une branche au sein dâ€™un projet permet de diverger de la ligne principale de dÃ©veloppement (gÃ©nÃ©ralement appelÃ©e master, terme tendant Ã  disparaÃ®tre au profit de celui de main) sans impacter cette ligne. Cela permet de sÃ©parer le nouveau dÃ©veloppement et de faire cohabiter plusieurs versions, pouvant Ã©voluer sÃ©parÃ©ment et pouvant Ãªtre facilement rassemblÃ©es si nÃ©cessaire.\nPour comprendre comment fonctionnent les branches, il nous faut revenir un peu plus en dÃ©tail sur la maniÃ¨re dont Git stocke lâ€™historique du projet. Comme nous lâ€™avons vu, lâ€™unitÃ© temporelle de Git est le commit, qui correspond Ã  une photographie Ã  un instant donnÃ© de lâ€™Ã©tat du projet (snapshot). Chaque commit est uniquement identifiÃ© par un hash, une longue suite de caractÃ¨res. La commande git log, qui liste les diffÃ©rents commits dâ€™un projet, permet dâ€™afficher ce hash ainsi que diverses mÃ©tadonnÃ©es (auteur, date, message) associÃ©es au commit.\n\n\nterminal\n\n$ git log\n\ncommit e58b004d3b68bdf28093fe6ad6036b5d13216e55 (HEAD -&gt; master, origin/master, origin/HEAD)\nAuthor: Lino Galiana &lt;xxx@xxx.fr&gt;\nDate:   Tue Mar 22 14:34:04 2022 +0100\n\n    ajoute code Ã©quivalent python\n\n...\nUne branche est simplement un pointeur vers un commit. Dans lâ€™exemple prÃ©cÃ©dent, on a imprimÃ© les informations du dernier commit en date. La branche principale (master) pointe vers ce commit. Si lâ€™on faisait un nouveau commit, le pointeur se dÃ©calerait et la branche master pointerait Ã  prÃ©sent sur le nouveau commit.\n\n\nBranches locales\nDans ce contexte, crÃ©er une nouvelle branche (en local) revient simplement Ã  crÃ©er un nouveau pointeur vers un commit donnÃ©. Supposons que lâ€™on crÃ©e une branche testing Ã  partir du dernier commit.\n\n\nterminal\n\n1$ git branch testing\n2$ git branch\n\n\n1\n\nCrÃ©e une nouvelle branche\n\n2\n\nListe les branches existantes\n\n\n1* master\n2  testing\n\n1\n\nLa branche sur laquelle on se situe\n\n2\n\nLa nouvelle branche crÃ©Ã©e\n\n\nLa figure suivante illustre lâ€™effet de cette crÃ©ation sur lâ€™historique Git.\n\nDÃ©sormais, deux branches (master et testing) pointent vers le mÃªme commit. Si lâ€™on effectue Ã  prÃ©sent des commits sur la branche testing, on va diverger de la branche principale, ce qui permet de dÃ©velopper une nouvelle fonctionnalitÃ© sans risquer dâ€™impacter master.\nPour savoir sur quelle branche on se situe Ã  instant donnÃ© â€” et donc sur quelle branche on va commiter â€” Git utilise un pointeur spÃ©cial, appelÃ© HEAD, qui pointe vers la branche courante. On comprend Ã  prÃ©sent mieux la signification de HEAD -&gt; master dans lâ€™output de la commande git log vu prÃ©cÃ©demment. Cet Ã©lÃ©ment spÃ©cifie la situation locale actuelle et signifie : on se situe actuellement sur la branche master, qui pointe sur le commit e58b004. Pour changer de branche, i.e.Â dÃ©placer le HEAD, on utilise la commande git checkout. Par exemple, pour passer de la branche master sur laquelle on est par dÃ©faut Ã  la branche testing :\n\n\nterminal\n\n$ git checkout testing  # Changement de branche\n\nSwitched to branch 'testing'\nOn se situe dÃ©sormais sur la branche testing, sur laquelle on peut laisser libre cours Ã  sa crÃ©ativitÃ© sans risquer dâ€™impacer la branche principale du projet. Mais que se passe-t-il si, pendant que lâ€™on dÃ©veloppe sur testing, un autre membre du projet commit sur master ? On dit que les historiques ont divergÃ©. La figure suivante illustre Ã  quoi ressemble Ã  prÃ©sent lâ€™historique du projet (et suppose que lâ€™on est repassÃ© sur master).\n\nCette divergence nâ€™est pas problÃ©matique en soi : il est normal que les diffÃ©rentes parties et expÃ©rimentations dâ€™un projet avancent Ã  diffÃ©rents rythmes. La difficultÃ© est de savoir comment rÃ©concillier les diffÃ©rents changements si lâ€™on dÃ©cide que la branche testing doit Ãªtre intÃ©grÃ©e dans master. Deux situations peuvent survenir : - les modifications opÃ©rÃ©es en parallÃ¨le sur les deux branches ne concernent pas les mÃªmes fichiers ou les mÃªmes parties des fichiers. Dans ce cas, Git est capable de fusionner (merge) les changements automatiquement et tout se passe sans encombre ; - dans le cas contraire, survient un merge conflict : les branches ont divergÃ© de telle sorte quâ€™il nâ€™est pas possible pour Git de fusionner les changements automatiquement. Il faut alors rÃ©soudre les conflits manuellement.\nLa rÃ©solution des conflits est une Ã©tape souvent douloureuse lors de lâ€™apprentissage de Git. Aussi, nous conseillons dans la mesure du possible de ne pas fusionner des branches manuellement en local avec Git â€” câ€™est dâ€™ailleurs pour cette raison que nous nâ€™avons pas dÃ©taillÃ© les commandes pour le faire. Dans les sections suivantes, nous verrons comment une bonne organisation prÃ©alable du travail en Ã©quipe, combinÃ©e aux outils collaboratifs fournis par GitHub, permet de rendre le processus de fusion des branches largement indolore.\n\n\nBranches remote\nRappellons que toutes les opÃ©rations que nous avons effectuÃ©es sur les branches dans cette section se sont passÃ©s en local, le rÃ©pertoire distant est restÃ© totalement inchangÃ©. Pour pouvoir collaborer sur une branche ou bien en faire une pull request (cf.Â supra), il faut pousser la branche sur le rÃ©pertoire distant. La commande est simple : git push origin &lt;branche&gt;.\n\n\nterminal\n\n$ git push origin testing\n\nCounting objects: 24, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (15/15), done.\nWriting objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done.\nTotal 24 (delta 2), reused 0 (delta 0)\nTo https://github.com/linogaliana/ensae-reproductibilite-website\n * [new branch]      testing -&gt; testing"
  },
  {
    "objectID": "chapters/git.html#workflow-collaboratif",
    "href": "chapters/git.html#workflow-collaboratif",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Workflow collaboratif",
    "text": "Workflow collaboratif\nComme on lâ€™a vu prÃ©cÃ©demment, si le modÃ¨le des branches de Git semble idÃ©al pour gÃ©rer le travail collaboratif et asynchrone, il peut Ã©galement sâ€™avÃ©rer rapidement complexe Ã  manipuler en lâ€™absence dâ€™une bonne organisation du travail en Ã©quipe. De nombreux modÃ¨les (â€œworkflowsâ€) existent en la matiÃ¨re, avec des complexitÃ©s plus ou moins grandes selon la nature du projet. Nous conseillons dâ€™adopter dans la plupart des cas un modÃ¨le trÃ¨s simple : le GitHub Flow.\nLe GitHub Flow est une mÃ©thode dâ€™organisation minimaliste du travail collaboratif, qui est progressivement devenue la norme dans les projets open-source. Elle est rÃ©sumÃ©e par la figure suivante, dont nous dÃ©taillons par la suite les diffÃ©rentes Ã©tapes.\n\n\nDÃ©finition des rÃ´les des contributeurs\nDans tout projet collaboratif, une premiÃ¨re Ã©tape essentielle est de bien dÃ©limiter les rÃ´les des diffÃ©rents contributeurs. Les diffÃ©rents participants au projet ont en effet gÃ©nÃ©ralement des rÃ´les diffÃ©rents dans lâ€™organisation, des niveaux diffÃ©rents de pratique de Git, etc. Il est important de reflÃ©ter ces diffÃ©rents rÃ´les dans lâ€™organisation du travail collaboratif.\nSur les diffÃ©rents hÃ©bergeurs de projets Git, cela prend la forme de rÃ´les que lâ€™on attribue aux diffÃ©rents membres du porjet. Les mainteneurs sont les seuls Ã  pouvoir Ã©crire directement sur master. Les contributeurs sont quant Ã  eux tenus de dÃ©velopper sur des branches. Cela permet de protÃ©ger la branche principale, qui doit rester une version propre et jamais compromise du projet.\nNotons que la possibilitÃ© de donner des rÃ´les sur les projets GitHub nâ€™est possible que dans le cadre dâ€™organisations (payantes), donc dans un contexte professionnel ou de projets open-source dâ€™une certaine ampleur. Pour des petits projets, il est nÃ©cessaire de sâ€™astreindre Ã  une certaine rigueur individuelle pour respecter cette organisation.\n\n\nDÃ©veloppement sur des branches de court-terme\nLes contributeurs dÃ©veloppent uniquement sur des branches. Il est dâ€™usage de crÃ©er une branche par fonctionnalitÃ©, en lui donnant un nom reflÃ©tant la fonctionnalitÃ© en cours de dÃ©veloppement (ex : ajout-tests-unitaires). Les diffÃ©rents contributeurs Ã  la fonctionnalitÃ© en cours de dÃ©veloppement font des commits sur la branche, en prenant bien soin de pull rÃ©guliÃ¨rement les Ã©ventuels changements pour ne pas risquer de conflits de version. Pour la mÃªme raison, il est prÃ©fÃ©rable de faire des branches dites de court-terme, câ€™est Ã  dire propres Ã  une petite fonctionnalitÃ©, quite Ã  diviser une fonctionnalitÃ© en sÃ©ries dâ€™implÃ©mentations. Cela permet de limiter les Ã©ventuels conflits Ã  gÃ©rer lors de la fusion finale de la branche avec master.\n\n\nPull Request\nUne fois la sÃ©rie de modifications terminÃ©e, vient le temps de rassembler les diffÃ©rents travaux, par lâ€™intermÃ©diaire de la fusion entre la branche et master. Il faut alors â€œdemanderâ€ de fusionner (pull request) sur GitHub. Cela ouvre une page liÃ©e Ã  la pull request, qui rappelle les diffÃ©rents changements apportÃ©s et leurs auteurs, et permet dâ€™entamer une discussion Ã  propos de ces changements.\n\n\nProcessus de review\nLes diffÃ©rents membres du projet peuvent donc analyser et commenter les changements, poser des questions, suggÃ©rer des modifications, apporter dâ€™autres contributions, etc. Il est par exemple possible de mentionner un membre de lâ€™Ã©quipe par lâ€™intermÃ©diaire de @personne. Il est Ã©galement possible de procÃ©der Ã  une code review, par exemple par un dÃ©veloppeur plus expÃ©rimentÃ©.\n\n\nRÃ©solution des Ã©ventuels conflits\nEn adoptant cette maniÃ¨re de travailler, master ne sera modifiÃ©e que via des pull requests. Il ne devrait donc jamais y avoir le moindre conflit Ã  rÃ©gler sur master, les seuls conflits possibles se passent sur les branches. Par exemple, dans le cas oÃ¹ une autre pull request aurait Ã©tÃ© fusionnÃ©e sur master depuis lâ€™ouverture de la pull request en question.\nDans le cas dâ€™un conflit Ã  gÃ©rer, le conflit doit Ãªtre rÃ©solu dans la branche et pas dans master. Voici la marche Ã  suivre :\n\nappliquez le conseil de survie : faites une copie de sauvegarde de votre clone\ndans votre clone, placez vous sur la branche en question : git checkout nom-de-la-branche\nfusionnez master dans la branche : git merge master\nrÃ©solvez les Ã©ventuels conflits dans les fichiers concernÃ©s\nfinalisez le commit de merge et poussez-le sur la branche remote, ce qui le fera apparaÃ®tre dans la pull request\n\n\n\nFusion de la branche\nSi tout est en ordre, la branche peut Ãªtre fusionnÃ©e. Seuls les mainteneurs, qui ont les droits sur master, peuvent fusionner la pull request. En termes dâ€™historique du projet, deux choix sont possibles : - â€œCreate a merge commitâ€ : tous les commits rÃ©alisÃ©s sur la branche apparaÃ®tront dans lâ€™historique du projet ; - â€œSquash and mergeâ€ : les diffÃ©rents commits rÃ©alisÃ©s sur la branche seront rassemblÃ©s en un commit unique. Cette option est gÃ©nÃ©ralement prÃ©fÃ©rable lorsquâ€™on utilise des branches de court-terme : elles permettent de garder lâ€™historique plus lisible."
  },
  {
    "objectID": "chapters/git.html#utiliser-les-issues",
    "href": "chapters/git.html#utiliser-les-issues",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Utiliser les issues",
    "text": "Utiliser les issues\nLa maniÃ¨re la plus simple de contribuer Ã  un projet open-source est dâ€™ouvrir une issue. Sur GitHub, cela se fait sur la page du projet, sous lâ€™onglet Issue (cf.Â documentation officielle). Les issues peuvent avoir diffÃ©rentes nature : - suggestion dâ€™amÃ©lioration (sans code) - notification de bug - rapports dâ€™expÃ©rience - etc.\nLes issues sont une maniÃ¨re trÃ¨s peu couteuse de contributer Ã  un projet, mais leur importance est capitale, dans la mesure oÃ¹ il est impossible pour les dÃ©veloppeurs dâ€™un projet de penser en amont Ã  toutes les utilisations possibles et donc tous les bugs possibles dâ€™une application."
  },
  {
    "objectID": "chapters/git.html#proposer-une-pull-request",
    "href": "chapters/git.html#proposer-une-pull-request",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Proposer une pull request",
    "text": "Proposer une pull request\nUne autre maniÃ¨re, plus ambitieuse, de contribuer Ã  lâ€™open source est de proposer des pull requests. ConcrÃ¨tement, lâ€™idÃ©e est de proposer une amÃ©lioration ou bien de rÃ©soudre un bug sous forme de code, que les mainteneurs du projet peuvent ensuite dÃ©cider dâ€™intÃ©grer au code existant.\nLa procÃ©dure pour proposer une pull request Ã  un projet sur lequel on nâ€™a aucun droit est trÃ¨s similaire Ã  celle dÃ©crite ci-dessus dans le cas normal. La principale diffÃ©rence est que, du fait de lâ€™absence de droits, il est impossible de pousser une branche locale sur le rÃ©pertoire du projet. On va donc devoir crÃ©er au prÃ©alable un fork, i.e.Â une copie du projet que lâ€™on crÃ©e dans son espace personnel sur GitHub. Câ€™est sur cette copie que lâ€™on va appliquer la procÃ©dure dÃ©crite prÃ©cÃ©demment, en prenant bien soin de travailler sur une branche et non sur master. Une fois les modifications pertinentes effectuÃ©es sur la branche du fork, GitHub propose de crÃ©er une pull request sur le dÃ©pÃ´t original. Cette pull request sera alors visible des mainteneurs du projet, qui pourront lâ€™Ã©valuer et dÃ©cider dâ€™adopter (ou non) les changements proposÃ©s."
  },
  {
    "objectID": "chapters/git.html#respecter-les-rÃ¨gles-de-contribution",
    "href": "chapters/git.html#respecter-les-rÃ¨gles-de-contribution",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Respecter les rÃ¨gles de contribution",
    "text": "Respecter les rÃ¨gles de contribution\nVouloir contribuer Ã  un projet open-source est trÃ¨s louable, mais ne peut pas pour autant se faire nâ€™importe comment. Un projet est constituÃ© de personnes, qui ont dÃ©veloppÃ© ensemble une maniÃ¨re de travailler, des standards de bonnes pratiques, etc. Pour sâ€™assurer que sa contribution ne reste pas lettre morte, il est indispensable de sâ€™imprÃ©gner un minimum de la culture du projet.\nPour faciliter les contributions, les projets open-source spÃ©cifient bien souvent la maniÃ¨re dont les utilisateurs peuvent contribuer ainsi que le format attendu. En gÃ©nÃ©ral, ces rÃ¨gles de contribution sont spÃ©cifiÃ©es dans un fichier CONTRIBUTING.md situÃ© Ã  la racine du projet GitHub, ou a dÃ©faut dans le README du projet. Il est essentiel de bien lire ce document sâ€™il existe afin de sâ€™assurer de proposer des contributions pertinentes."
  },
  {
    "objectID": "chapters/linux-101.html",
    "href": "chapters/linux-101.html",
    "title": "Linux 101",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des systÃ¨mes dâ€™exploitation (y compris avec Windows !). Mais comme il a la rÃ©putation dâ€™Ãªtre austÃ¨re et complexe, on utilise plutÃ´t des interfaces graphiques pour effectuer nos opÃ©rations informatiques quotidiennes.\nPourtant, avoir des notions quant Ã  lâ€™utilisation dâ€™un terminal est une vraie source dâ€™autonomie, dans la mesure oÃ¹ celui-ci permet de gÃ©rer bien plus finement les commandes que lâ€™on rÃ©alise. Pour les data scientists qui sâ€™intÃ©ressent aux bonnes pratiques et Ã  la mise en production, sa maÃ®trise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont gÃ©nÃ©ralement limitÃ©es par rapport Ã  lâ€™utilisation du programme en ligne de commande. Câ€™est par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de rÃ©aliser toutes les opÃ©rations permises par le logiciel ;\nmettre un projet de data science en production nÃ©cessite dâ€™utiliser un serveur, qui le rend disponible en permanence Ã  son public potentiel. Or lÃ  oÃ¹ Windows domine le monde des ordinateurs personnels, une large majoritÃ© des serveurs et des infrastructures cloud fonctionnent sous Linux ;\nles principaux langages de programmation des data scientists (, , etc. ) visent Ã  simplifier lâ€™exÃ©cution dâ€™opÃ©rations complexes par le biais de la ligne de commande mais hÃ©ritent nÃ©anmoins de sa logique. Utiliser la ligne de commande permet de gagner en aisance dans son travail de programmation ;\nplus gÃ©nÃ©ralement, une utilisation rÃ©guliÃ¨re du terminal est source dâ€™une meilleure comprÃ©hension du fonctionnement dâ€™un systÃ¨me de fichiers et de lâ€™exÃ©cution des processus sur un ordinateur. Ces connaissances sâ€™avÃ¨rent trÃ¨s utiles dans la pratique quotidienne du data scientist, qui nÃ©cessite de plus en plus de dÃ©velopper dans diffÃ©rents environnements dâ€™exÃ©cution.\n\nDans le cadre de ce cours, on sâ€™intÃ©ressera particuliÃ¨rement au terminal Linux puisque lâ€™Ã©crasante majoritÃ©, si ce nâ€™est lâ€™ensemble, des serveurs de mise en production sâ€™appuient sur un systÃ¨me Linux.\n\n\n\nDiffÃ©rents environnements de travail peuvent Ãªtre utilisÃ©s pour apprendre Ã  se servir dâ€™un terminal Linux :\n\nle SSP Cloud. Dans la mesure oÃ¹ les exemples de mise en production du cours seront illustrÃ©es sur cet environnement, nous recommandons de lâ€™utiliser dÃ¨s Ã  prÃ©sent pour se familiariser. Le terminal est accessible Ã  partir de diffÃ©rents services (RStudio, Jupyter, etc.), mais nous recommandons dâ€™utiliser le terminal dâ€™un service VSCode, dans la mesure oÃ¹ se servir dâ€™un IDE pour organiser notre code est en soi dÃ©jÃ  une bonne pratique ;\nKatacoda, un bac Ã  sable dans un systÃ¨me Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (Ã©mulation minimaliste dâ€™un terminal Linux), qui est installÃ©e par dÃ©faut avec Git.\n\n\n\n\nLanÃ§ons un terminal pour prÃ©senter son fonctionnement basique. On prend pour exemple le terminal dâ€™un service VSCode lancÃ© via le SSP Cloud (Application Menu tout en haut Ã  gauche de VSCode -&gt; Terminal -&gt; New Terminal). Voici Ã  quoi ressemble le terminal en question.\n\nDÃ©crivons dâ€™abord les diffÃ©rentes inscriptions qui arrivent Ã  lâ€™initialisation :\n\n(base) : cette inscription nâ€™est pas directement liÃ©e au terminal, elle provient du fait que lâ€™on utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en dÃ©tail dans le chapitre sur la portabilitÃ© ;\ncoder@vscode-824991-64744dd6d8-zbgv5 : le nom de lâ€™utilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que lâ€™on verra lÃ  encore dans le chapitre sur la portabilitÃ©\n~/work : le chemin du rÃ©pertoire courant, i.e.Â Ã  partir duquel va Ãªtre lancÃ©e toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\n\nPour Ã©viter la lourdeur des images et permettre de copier/coller facilement les commandes, on reprÃ©sentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans lâ€™exemple suivant. Les lignes commenÃ§ant par un $ sont celles avec lesquelles une commande est lancÃ©e, et les lignes sans $ reprÃ©sentent le rÃ©sultat dâ€™une commande. Attention Ã  ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement Ã  diffÃ©rencier celles-ci des rÃ©sultats.\n\n\nterminal\n\n$ echo \"une petite illustration\"\n\nune petite illustration\n\n\n\nLe terme filesystem (systÃ¨me de fichiers) dÃ©signe la maniÃ¨re dont sont organisÃ©s les fichiers au sein dâ€™un systÃ¨me dâ€™exploitation. Cette structure est hiÃ©rarchique, en forme dâ€™arbre :\n\nelle part dâ€™un rÃ©pertoire racine (le dossier qui contient tous les autres) ;\ncontient des dossiers ;\nles dossiers peuvent contenir Ã  leur tout des dossiers (sous-dossiers) ou des fichiers.\n\nIntÃ©ressons nous Ã  la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations :\n\nla racine (root) sur Linux sâ€™appelle /, lÃ  oÃ¹ elle sâ€™appelle C:\\ par dÃ©faut sur Windows ;\nle rÃ©pertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un rÃ´le essentiellement technique. Il est tout de mÃªme utile dâ€™en dÃ©crire les principaux :\n\n/bin : contient les binaires, i.e.Â les programmes exÃ©cutables ;\n/etc : contient les fichiers de configuration ;\n/home : contient lâ€™ensemble des dossiers et fichiers personnels des diffÃ©rents utilisateurs. Chaque utilisateur a un rÃ©pertoire dit â€œHOMEâ€ qui a pour chemin /home/&lt;username&gt; Ce rÃ©pertoire est souvent reprÃ©sentÃ© par le symbole ~. Câ€™Ã©tait notamment le cas dans lâ€™illustration du terminal VSCode ci-dessus, ce qui signifie quâ€™on se trouvait formellement dans le rÃ©pertoire /home/coder/work, coder Ã©tant lâ€™utilisateur par dÃ©faut du service VSCode sur le SSP Cloud.\n\n\nChaque dossier ou fichier est reprÃ©sentÃ© par un chemin dâ€™accÃ¨s, qui correspond simplement Ã  sa position dans le filesystem. Il existe deux moyens de spÃ©cifier un chemin :\n\nen utilisant un chemin absolu, câ€™est Ã  dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconnaÃ®t donc un chemin absolu par le fait quâ€™il commence forcÃ©ment par /.\nen utilisant un chemin relatif, câ€™est Ã  dire en indiquant le chemin du dossier ou fichier relativement au rÃ©pertoire courant.\n\nComme tout ce qui touche de prÃ¨s ou de loin au terminal, la seule maniÃ¨re de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront dâ€™appliquer ces concepts Ã  des cas pratiques.\n\n\n\nLe rÃ´le dâ€™un terminal est de lancer des commandes. Ces commandes peuvent Ãªtre classÃ©es en trois grandes catÃ©gories :\n\nnavigation au sein du filesystem\nmanipulations de fichiers (crÃ©er, lire, modifier des dossiers/fichiers)\nlancement de programmes\n\n\n\nLorsque lâ€™on lance un programme Ã  partir du terminal, celui-ci a pour rÃ©fÃ©rence le rÃ©pertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si lâ€™on exÃ©cute un script Python en se trouvant dans un certain rÃ©pertoire, tous les chemins des fichiers utilisÃ©s dans le script seront relatifs au rÃ©pertoire courant dâ€™exÃ©cution â€” Ã  moins dâ€™utiliser uniquement des chemins absolus, ce qui nâ€™est pas une bonne pratique en termes de reproductibilitÃ© puisque cela lie votre projet Ã  la structure de votre filesystem particulier.\nAinsi, la trÃ¨s grande majoritÃ© des opÃ©rations que lâ€™on est amenÃ© Ã  rÃ©aliser dans un terminal consiste simplement Ã  se dÃ©placer au sein du filesystem. Les commandes principales pour naviguer et se repÃ©rer dans le filesystem sont prÃ©sentÃ©es dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez pÃ©nible de manipuler des chemins absolus, qui peuvent facilement Ãªtre trÃ¨s longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient Ã  se dÃ©placer Ã  partir du rÃ©pertoire courant. Pour se faire, voici quelques utilisations trÃ¨s frÃ©quentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter dâ€™un niveau dans lâ€™arborescence (dossier parent)\n\n\ncd ~\nrevenir dans le rÃ©pertoire HOME de lâ€™utilisateur courant\n\n\n\nLa premiÃ¨re commande est lâ€™occasion de revenir sur une convention dâ€™Ã©criture importante pour les chemins relatifs :\n\n. reprÃ©sente le rÃ©pertoire courant. Ainsi, cd . revient Ã  changer de rÃ©pertoire courantâ€¦ pour le rÃ©pertoire courant, ce qui bien sÃ»r ne change rien. Mais le . est trÃ¨s utile pour la copie de fichiers (cf.Â section suivante) ou encore lorsque lâ€™on doit passer des paramÃ¨tres Ã  un programme (cf.Â section Lancement de programmes) ;\n.. reprÃ©sente le rÃ©pertoire parent du rÃ©pertoire courant.\n\nCes diffÃ©rentes commandes constituent la trÃ¨s grande majoritÃ© des usages dans un terminal. Il est essentiel de les pratiquer jusquâ€™Ã  ce quâ€™elles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup dâ€™autres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\ndÃ©placer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncrÃ©er (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncrÃ©er un fichier vide\n\n\n\nDans la mesure oÃ¹ il est gÃ©nÃ©ralement possible de rÃ©aliser toutes ces opÃ©rations Ã  lâ€™aide dâ€™interfaces graphiques (notamment, lâ€™explorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se dÃ©placer dans le filesystem. Nous vous recommandons malgrÃ© tout de les pratiquer Ã©galement, et ce pour plusieurs raisons :\n\neffectuer un maximum dâ€™opÃ©rations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ;\nen devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ;\nlorsque lâ€™on est amenÃ© Ã  manipuler un terminal pour interagir avec un serveur, il nâ€™y a souvent pas la moindre interface graphique, auquel cas il nâ€™y a pas dâ€™autre choix que dâ€™opÃ©rer uniquement Ã  partir du terminal.\n\n\n\n\nLe rÃ´le du terminal est de lancer des programmes. Lancer un programme se fait Ã  partir dâ€™un fichier dit exÃ©cutable, qui peut Ãªtre de deux formes :\n\nun binaire, i.e.Â un programme dont le code nâ€™est pas lisible par lâ€™humain ;\nun script, i.e.Â un fichier texte contenant une sÃ©rie dâ€™instructions Ã  exÃ©cuter. Le langage du terminal Linux est le shell, et les scripts associÃ©s ont pour extension .sh.\n\nDans les deux cas, la syntaxe de lancement dâ€™une commande est : le nom de lâ€™exÃ©cutable, suivi dâ€™Ã©ventuels paramÃ¨tres, sÃ©parÃ©s par des espaces. Par exemple, la commande python monscript.py exÃ©cute le binaire python et lui passe comme unique argument le nom dâ€™un script .py (contenu dans le rÃ©pertoire courant), qui va donc Ãªtre exÃ©cutÃ© via Python. De la mÃªme maniÃ¨re, toutes les commandes vues prÃ©cÃ©demment pour se dÃ©placer dans le filesystem ou manipuler des fichiers sont des exÃ©cutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier Ã  copier et le chemin dâ€™arrivÃ©e.\nDans les exemples de commandes prÃ©cÃ©dents, les paramÃ¨tres Ã©taient passÃ©s en mode positionnel : lâ€™exÃ©cutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments nâ€™est pas toujours fixÃ© Ã  lâ€™avance, du fait de la prÃ©sence de paramÃ¨tres optionnels. Ainsi, la plupart des exÃ©cutables permettent le passage dâ€™arguments optionnels, qui modifient le comportement de lâ€™exÃ©cutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier Ã  un autre endroit du filesystem, mais peut-on copier un dossier et lâ€™ensemble de son contenu avec ? Nativement non, mais lâ€™ajout dâ€™un paramÃ¨tre le permet : cp -R dossierdepart dossierarrivee permet de copier rÃ©cursivement le dossier et tout son contenu. Notons que les flags ont trÃ¨s souvent un Ã©quivalent en toute lettre, qui sâ€™Ã©crit quant Ã  lui avec deux tirers. Par exemple, la commande prÃ©cÃ©dente peut sâ€™Ã©crire de maniÃ¨re Ã©quivalente cp --recursive dossierdepart dossierarrivee. Il est frÃ©quent de voir les deux syntaxes en pratique, parfois mÃªme mÃ©langÃ©es au sein dâ€™une mÃªme commande.\n\n\n\n\nComme tout langage de programmation, le langage shell permet dâ€™assigner et dâ€™utiliser des variables dans des commandes. Pour afficher le contenu dâ€™une variable, on utilise la commande echo, qui est lâ€™Ã©quivalent de la fonction print en Python ou en R.\n\n\nterminal\n\n$ MY_VAR=\"toto\"\n$ echo $MY_VAR\n\ntoto\nQuelques remarques importantes :\n\nla syntaxe pour la crÃ©ation de variable est prÃ©cise : aucun espace dâ€™un cÃ´tÃ© comme de lâ€™autre du = ;\nen Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu Ã©crire MY_VAR=toto pour le mÃªme rÃ©sultat. Par contre, si lâ€™on veut assigner Ã  une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message dâ€™erreur ;\npour accÃ©der Ã  la valeur dâ€™une variable, on la prÃ©fixe dâ€™un $.\n\nNotre objectif avec ce tutoriel nâ€™est pas de savoir coder en shell, on ne va donc pas sâ€™attarder sur les propriÃ©tÃ©s des variables. En revanche, introduire ce concept Ã©tait nÃ©cessaire pour en prÃ©senter un autre, essentiel quant Ã  lui dans la pratique quotidienne du data scientist : les variables dâ€™environnement. Pour faire une analogie â€” un peu simpliste â€” avec les langages de programmation, ce sont des sortes de variables â€œglobalesâ€, dans la mesure oÃ¹ elles vont Ãªtre accessibles Ã  tous les programmes lancÃ©s Ã  partir dâ€™un terminal, et vont modifier leur comportement.\nLa liste des variables dâ€™environnement peut Ãªtre affichÃ©e Ã  lâ€™aide de la commande env. Il y a gÃ©nÃ©ralement un grand nombre de variables dâ€™environnement prÃ©Ã©xistantes ; en voici un Ã©chantillon obtenu Ã  partir du terminal du service VSCode.\n\n\nterminal\n\n$ env\n\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la variÃ©tÃ© des utilisations des variables dâ€™environnements : - la variable $SHELL prÃ©cise lâ€™exÃ©cutable utilisÃ© pour lancer le terminal ; - la variable $HOME donne lâ€™emplacement du rÃ©pertoire utilisateur. En fait, le symbole ~ que lâ€™on a rencontrÃ© plus haut rÃ©fÃ©rence cette mÃªme variable ; - la variable LANG spÃ©cifie la locale, un concept qui permet de dÃ©finir la langue et lâ€™encodage utilisÃ©s par dÃ©faut par Linux ; - la variable CONDA_PYTHON_EXE existe uniquement parce que lâ€™on a installÃ© conda comme systÃ¨me de gestion de packages Python. Câ€™est lâ€™existance de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous intÃ©resse.\nUne variable dâ€™environnement essentielle, et que lâ€™on est frÃ©quemment amenÃ© Ã  modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concatÃ©nation de chemins absolus, sÃ©parÃ©s par :, qui spÃ©cifie les dossiers dans lesquels Linux va chercher les exÃ©cutables lorsque lâ€™on lance une commande, ainsi que lâ€™ordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n\n\nterminal\n\n$ echo $PATH\n\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nLâ€™ordre de recherche est de gauche Ã  droite. Câ€™est donc parce que le dossier /home/coder/local/bin/conda/bin est situÃ© en premier que lâ€™interprÃ©teur Python qui sera choisi lorsque lâ€™on lance un script Python est celui issu de Conda, et non celui contenu par dÃ©faut dans /usr/bin par exemple.\nLâ€™existence et la configuration adÃ©quate des variables dâ€™environnement est essentielle pour le bon fonctionnement de nombreux outils trÃ¨s utilisÃ©s en data science, comme Git ou encore Spark par exemple. Il est donc nÃ©cessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration dâ€™un serveur en cas de bug liÃ© Ã  une variable dâ€™environnement manquante ou mal configurÃ©e.\n\n\n\nLa sÃ©curitÃ© est un enjeu central en Linux, qui permet une gestion trÃ¨s fine des permissions sur les diffÃ©rents fichiers et programmes.\nUne diffÃ©rence majeure par rapport Ã  dâ€™autres systÃ¨mes dâ€™exploitation, notamment Windows, est quâ€™aucun utilisateur nâ€™a par dÃ©faut les droits complets dâ€™administrateur (root). Il nâ€™est donc pas possible nativement dâ€™accÃ©der au parties sensibles du systÃ¨me, ou bien de lancer certains types de programme. Par exemple, si lâ€™on essaie de lister les fichiers du dossier /root, on obtient une erreur.\n\n\nterminal\n\n$ ls /root\n\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines opÃ©rations telles que lâ€™installation de binaires ou de packages nÃ©cessitent cependant des droits administrateurs. Dans ce cas, il est dâ€™usage dâ€™utiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de lâ€™exÃ©cution de la commande.\n\n\nterminal\n\n$ sudo ls /root\n\nLe dossier /root Ã©tant vide, la commande ls renvoie une chaÃ®ne de caractÃ¨res vide, mais nous nâ€™avons plus de problÃ¨me de permission. Notons quâ€™une bonne pratique de sÃ©curitÃ©, en particulier dans les scripts shell que lâ€™on peut Ãªtre amenÃ©s Ã  Ã©crire ou exÃ©cuter, est de limiter lâ€™utilisation de cette commande aux cas oÃ¹ elle sâ€™avÃ¨re nÃ©cessaire.\nUne autre subtilitÃ© concerne justement lâ€™exÃ©cution de scripts shell. Par dÃ©faut, quâ€™il soit crÃ©Ã© par lâ€™utilisateur ou tÃ©lÃ©chargÃ© dâ€™internet, un script nâ€™est pas exÃ©cutable.\n\n\nterminal\n\n1$ touch test.sh\n2$ ./test.sh\n\n\n1\n\nCrÃ©er le script test.sh (vide)\n\n2\n\nExÃ©cuter le script test.sh\n\n\nbash: ./test.sh: Permission denied\nCâ€™est bien entendu une mesure de sÃ©curitÃ© pour Ã©viter lâ€™exÃ©cution automatique de scripts potentiellement malveillants. Pour pouvoir exÃ©cuter un tel script, il faut attribuer des droits dâ€™exÃ©cution au fichier avec la commande chmod. Il devient alors possible dâ€™exÃ©cuter le script classiquement.\n\n\nterminal\n\n1$ chmod +x test.sh\n2$ ./test.sh\n$ # Le script Ã©tant vide, il ne se passe rien\n\n\n1\n\nDonner des droits dâ€™exÃ©cution au script test.sh\n\n2\n\nExÃ©cuter le script test.sh\n\n\n\n\n\nMaintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell prÃ©cÃ©demment Ã©voquÃ©s. A lâ€™instar dâ€™un script Python, un script shell permet dâ€™automatiser une sÃ©rie de commandes lancÃ©es dans un terminal. Le but de ce tutoriel nâ€™est pas de savoir Ã©crire des scripts shell complexes, travail gÃ©nÃ©ralement dÃ©volu aux les data engineers ou les sysadmin (administrateurs systÃ¨me), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces compÃ©tences sont essentielles lorsque lâ€™on se prÃ©occupe de mise en production. A titre dâ€™exemple, comme nous le verrons dans le chapitre sur la portabilitÃ©, il est frÃ©quent dâ€™utiliser un script shell comme entrypoint dâ€™une image docker, afin de spÃ©cifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement Ã  lâ€™aide dâ€™un script simple. ConsidÃ©rons les commandes suivantes, que lâ€™on met dans un fichier monscript.sh dans le rÃ©pertoire courant.\n\n\nterminal\n\n$ #!/bin/bash\n$ SECTION=$1\n$ CHAPTER=$2\n$ FORMATION_DIR=/home/coder/work/formation\n$ mkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\n$ touch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\n\nAnalysons la structure de ce script :\n\nla premiÃ¨re ligne est classique, elle se nomme le shebang : elle indique au systÃ¨me quel interprÃ©teur utiliser pour exÃ©cuter ce script. Dans notre cas, et de maniÃ¨re gÃ©nÃ©rale, on utilise bash (Bourne-Again SHell, lâ€™implÃ©mentation moderne du shell) ;\nles lignes 2 et 3 assignent Ã  des variables les arguments passÃ©s au script dans la commande. Par dÃ©faut, ceux-ci sont assignÃ©s Ã  des variables n oÃ¹ n est la position de lâ€™argument, en commenÃ§ant Ã  1 ;\nla ligne 4 assigne un chemin Ã  une variable\nla ligne 5 crÃ©e le chemin complet, dÃ©fini Ã  partir des variables crÃ©Ã©es prÃ©cÃ©demment. Le paramÃ¨tre -p est important : il prÃ©cise Ã  mkdir dâ€™agir de maniÃ¨re rÃ©cursive, câ€™est Ã  dire de crÃ©er les dossiers intermÃ©diaires qui nâ€™existent pas encore ;\nla ligne 6 crÃ©e un fichier texte vide dans le dossier crÃ©Ã© avec la commande prÃ©cÃ©dente.\n\nExÃ©cutons maintenant ce script, en prenant soin de lui donner les permission adÃ©quates au prÃ©alable.\n\n\nterminal\n\n$ chmod +x monscript.sh\n$ bash monscript.sh section2 chapitre3\n$ ls formation/section1/chapitre2/\n\ntext.txt\nOpÃ©ration rÃ©ussie : le dossier a bien Ã©tÃ© crÃ©Ã© et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash trÃ¨s bien rÃ©alisÃ©e.\n\n\n\nUne diffÃ©rence fondamentale entre Linux et Windows tient Ã  la maniÃ¨re dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier exÃ©cutable en .exe) sur le site du logiciel, et on lâ€™exÃ©cute. En Linux, on passe gÃ©nÃ©ralement par un gestionnaire de packages qui va chercher les logiciels sur un rÃ©pertoire centralisÃ©, Ã  la maniÃ¨re de pip en Python par exemple.\nPourquoi cette diffÃ©rence ? Une raison importante est que, contrairement Ã  Windows, il existe une multitude de distributions diffÃ©rentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent diffÃ©remment et peuvent avoir diffÃ©rentes versions. En utilisant le package manager (gestionnaire de paquets) propre Ã  la distribution en question, on sâ€™assure de tÃ©lÃ©charger le logiciel adaptÃ© Ã  sa distribution. Dans ce cours, on fait le choix dâ€™utiliser une distribution Debian et son gestionnaire de paquets associÃ© apt. Debian est en effet un choix populaire pour les servers de part sa stabilitÃ© et sa simplicitÃ©, et sera Ã©galement familiÃ¨re aux utilisateurs dâ€™Ubuntu, distribution trÃ¨s populaire pour les ordinateurs personnels et qui est basÃ©e sur Debian.\nLâ€™utilisation dâ€™apt est trÃ¨s simple. La seule difficultÃ© est de savoir le nom du paquet que lâ€™on souhaite installer, ce qui nÃ©cessite en gÃ©nÃ©ral dâ€™utiliser un moteur de recherche. Lâ€™installation de paquets est Ã©galement un cas oÃ¹ il faut utiliser sudo, puisque cela implique souvent lâ€™accÃ¨s Ã  des rÃ©pertoires protÃ©gÃ©s.\n\n\nterminal\n\n$ sudo apt install tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nDÃ©sinstaller un package est Ã©galement simple : câ€™est lâ€™opÃ©ration inverse. Par sÃ©curitÃ©, le terminal vous demande si vous Ãªtes sÃ»r de votre choix en vous demandant de tapper la lettre y ou la lettre n.Â On peut passer automatiquement cette Ã©tape en ajoutant le paramÃ¨tre -y\n\n\nterminal\n\n$ sudo apt remove -y tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant dâ€™installer un package, il est toujours prÃ©fÃ©rable de mettre Ã  jour la base des packages, pour sâ€™assurer quâ€™on obtiendra bien la derniÃ¨re version.\n\n\nterminal\n\n$ sudo apt update\n\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date.\n\n\n\nOn lâ€™a dit et redit : devenir Ã  lâ€™aise avec le terminal Linux est essentiel et demande de la pratique. Il existe nÃ©anmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa premiÃ¨re est lâ€™autocomplÃ©tion. DÃ¨s lors que vous Ã©crivez une commande contenant un nom dâ€™exÃ©cutable, un chemin sur le filesystem, ou autre, nâ€™hÃ©sitez pas Ã  utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majoritÃ© des cas, cela va vous faire gagner un temps prÃ©cieux.\nUne seconde astuce, qui nâ€™en est pas vraiment une, est de lire la documentation dâ€™une commande lorsque lâ€™on nâ€™est pas sÃ»r de sa syntaxe ou des paramÃ¨tres admissibles. Via le terminal, la documentation dâ€™une commande peut Ãªtre affichÃ©e en exÃ©cutant man suivie de la commande en question, par exemple : man cp. Comme il nâ€™est pas toujours trÃ¨s pratique de lire de longs textes dans un petit terminal, on peut Ã©galement chercher la documentation dâ€™une commande sur le site man7."
  },
  {
    "objectID": "chapters/linux-101.html#pourquoi-sintÃ©resser-au-terminal-linux",
    "href": "chapters/linux-101.html#pourquoi-sintÃ©resser-au-terminal-linux",
    "title": "Linux 101",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des systÃ¨mes dâ€™exploitation (y compris avec Windows !). Mais comme il a la rÃ©putation dâ€™Ãªtre austÃ¨re et complexe, on utilise plutÃ´t des interfaces graphiques pour effectuer nos opÃ©rations informatiques quotidiennes.\nPourtant, avoir des notions quant Ã  lâ€™utilisation dâ€™un terminal est une vraie source dâ€™autonomie, dans la mesure oÃ¹ celui-ci permet de gÃ©rer bien plus finement les commandes que lâ€™on rÃ©alise. Pour les data scientists qui sâ€™intÃ©ressent aux bonnes pratiques et Ã  la mise en production, sa maÃ®trise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont gÃ©nÃ©ralement limitÃ©es par rapport Ã  lâ€™utilisation du programme en ligne de commande. Câ€™est par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de rÃ©aliser toutes les opÃ©rations permises par le logiciel ;\nmettre un projet de data science en production nÃ©cessite dâ€™utiliser un serveur, qui le rend disponible en permanence Ã  son public potentiel. Or lÃ  oÃ¹ Windows domine le monde des ordinateurs personnels, une large majoritÃ© des serveurs et des infrastructures cloud fonctionnent sous Linux ;\nles principaux langages de programmation des data scientists (, , etc. ) visent Ã  simplifier lâ€™exÃ©cution dâ€™opÃ©rations complexes par le biais de la ligne de commande mais hÃ©ritent nÃ©anmoins de sa logique. Utiliser la ligne de commande permet de gagner en aisance dans son travail de programmation ;\nplus gÃ©nÃ©ralement, une utilisation rÃ©guliÃ¨re du terminal est source dâ€™une meilleure comprÃ©hension du fonctionnement dâ€™un systÃ¨me de fichiers et de lâ€™exÃ©cution des processus sur un ordinateur. Ces connaissances sâ€™avÃ¨rent trÃ¨s utiles dans la pratique quotidienne du data scientist, qui nÃ©cessite de plus en plus de dÃ©velopper dans diffÃ©rents environnements dâ€™exÃ©cution.\n\nDans le cadre de ce cours, on sâ€™intÃ©ressera particuliÃ¨rement au terminal Linux puisque lâ€™Ã©crasante majoritÃ©, si ce nâ€™est lâ€™ensemble, des serveurs de mise en production sâ€™appuient sur un systÃ¨me Linux."
  },
  {
    "objectID": "chapters/linux-101.html#environnement-de-travail",
    "href": "chapters/linux-101.html#environnement-de-travail",
    "title": "Linux 101",
    "section": "",
    "text": "DiffÃ©rents environnements de travail peuvent Ãªtre utilisÃ©s pour apprendre Ã  se servir dâ€™un terminal Linux :\n\nle SSP Cloud. Dans la mesure oÃ¹ les exemples de mise en production du cours seront illustrÃ©es sur cet environnement, nous recommandons de lâ€™utiliser dÃ¨s Ã  prÃ©sent pour se familiariser. Le terminal est accessible Ã  partir de diffÃ©rents services (RStudio, Jupyter, etc.), mais nous recommandons dâ€™utiliser le terminal dâ€™un service VSCode, dans la mesure oÃ¹ se servir dâ€™un IDE pour organiser notre code est en soi dÃ©jÃ  une bonne pratique ;\nKatacoda, un bac Ã  sable dans un systÃ¨me Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (Ã©mulation minimaliste dâ€™un terminal Linux), qui est installÃ©e par dÃ©faut avec Git."
  },
  {
    "objectID": "chapters/linux-101.html#introduction-au-terminal",
    "href": "chapters/linux-101.html#introduction-au-terminal",
    "title": "Linux 101",
    "section": "",
    "text": "LanÃ§ons un terminal pour prÃ©senter son fonctionnement basique. On prend pour exemple le terminal dâ€™un service VSCode lancÃ© via le SSP Cloud (Application Menu tout en haut Ã  gauche de VSCode -&gt; Terminal -&gt; New Terminal). Voici Ã  quoi ressemble le terminal en question.\n\nDÃ©crivons dâ€™abord les diffÃ©rentes inscriptions qui arrivent Ã  lâ€™initialisation :\n\n(base) : cette inscription nâ€™est pas directement liÃ©e au terminal, elle provient du fait que lâ€™on utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en dÃ©tail dans le chapitre sur la portabilitÃ© ;\ncoder@vscode-824991-64744dd6d8-zbgv5 : le nom de lâ€™utilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que lâ€™on verra lÃ  encore dans le chapitre sur la portabilitÃ©\n~/work : le chemin du rÃ©pertoire courant, i.e.Â Ã  partir duquel va Ãªtre lancÃ©e toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\n\nPour Ã©viter la lourdeur des images et permettre de copier/coller facilement les commandes, on reprÃ©sentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans lâ€™exemple suivant. Les lignes commenÃ§ant par un $ sont celles avec lesquelles une commande est lancÃ©e, et les lignes sans $ reprÃ©sentent le rÃ©sultat dâ€™une commande. Attention Ã  ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement Ã  diffÃ©rencier celles-ci des rÃ©sultats.\n\n\nterminal\n\n$ echo \"une petite illustration\"\n\nune petite illustration"
  },
  {
    "objectID": "chapters/linux-101.html#notions-de-filesystem",
    "href": "chapters/linux-101.html#notions-de-filesystem",
    "title": "Linux 101",
    "section": "",
    "text": "Le terme filesystem (systÃ¨me de fichiers) dÃ©signe la maniÃ¨re dont sont organisÃ©s les fichiers au sein dâ€™un systÃ¨me dâ€™exploitation. Cette structure est hiÃ©rarchique, en forme dâ€™arbre :\n\nelle part dâ€™un rÃ©pertoire racine (le dossier qui contient tous les autres) ;\ncontient des dossiers ;\nles dossiers peuvent contenir Ã  leur tout des dossiers (sous-dossiers) ou des fichiers.\n\nIntÃ©ressons nous Ã  la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations :\n\nla racine (root) sur Linux sâ€™appelle /, lÃ  oÃ¹ elle sâ€™appelle C:\\ par dÃ©faut sur Windows ;\nle rÃ©pertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un rÃ´le essentiellement technique. Il est tout de mÃªme utile dâ€™en dÃ©crire les principaux :\n\n/bin : contient les binaires, i.e.Â les programmes exÃ©cutables ;\n/etc : contient les fichiers de configuration ;\n/home : contient lâ€™ensemble des dossiers et fichiers personnels des diffÃ©rents utilisateurs. Chaque utilisateur a un rÃ©pertoire dit â€œHOMEâ€ qui a pour chemin /home/&lt;username&gt; Ce rÃ©pertoire est souvent reprÃ©sentÃ© par le symbole ~. Câ€™Ã©tait notamment le cas dans lâ€™illustration du terminal VSCode ci-dessus, ce qui signifie quâ€™on se trouvait formellement dans le rÃ©pertoire /home/coder/work, coder Ã©tant lâ€™utilisateur par dÃ©faut du service VSCode sur le SSP Cloud.\n\n\nChaque dossier ou fichier est reprÃ©sentÃ© par un chemin dâ€™accÃ¨s, qui correspond simplement Ã  sa position dans le filesystem. Il existe deux moyens de spÃ©cifier un chemin :\n\nen utilisant un chemin absolu, câ€™est Ã  dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconnaÃ®t donc un chemin absolu par le fait quâ€™il commence forcÃ©ment par /.\nen utilisant un chemin relatif, câ€™est Ã  dire en indiquant le chemin du dossier ou fichier relativement au rÃ©pertoire courant.\n\nComme tout ce qui touche de prÃ¨s ou de loin au terminal, la seule maniÃ¨re de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront dâ€™appliquer ces concepts Ã  des cas pratiques."
  },
  {
    "objectID": "chapters/linux-101.html#lancer-des-commandes",
    "href": "chapters/linux-101.html#lancer-des-commandes",
    "title": "Linux 101",
    "section": "",
    "text": "Le rÃ´le dâ€™un terminal est de lancer des commandes. Ces commandes peuvent Ãªtre classÃ©es en trois grandes catÃ©gories :\n\nnavigation au sein du filesystem\nmanipulations de fichiers (crÃ©er, lire, modifier des dossiers/fichiers)\nlancement de programmes\n\n\n\nLorsque lâ€™on lance un programme Ã  partir du terminal, celui-ci a pour rÃ©fÃ©rence le rÃ©pertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si lâ€™on exÃ©cute un script Python en se trouvant dans un certain rÃ©pertoire, tous les chemins des fichiers utilisÃ©s dans le script seront relatifs au rÃ©pertoire courant dâ€™exÃ©cution â€” Ã  moins dâ€™utiliser uniquement des chemins absolus, ce qui nâ€™est pas une bonne pratique en termes de reproductibilitÃ© puisque cela lie votre projet Ã  la structure de votre filesystem particulier.\nAinsi, la trÃ¨s grande majoritÃ© des opÃ©rations que lâ€™on est amenÃ© Ã  rÃ©aliser dans un terminal consiste simplement Ã  se dÃ©placer au sein du filesystem. Les commandes principales pour naviguer et se repÃ©rer dans le filesystem sont prÃ©sentÃ©es dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez pÃ©nible de manipuler des chemins absolus, qui peuvent facilement Ãªtre trÃ¨s longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient Ã  se dÃ©placer Ã  partir du rÃ©pertoire courant. Pour se faire, voici quelques utilisations trÃ¨s frÃ©quentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter dâ€™un niveau dans lâ€™arborescence (dossier parent)\n\n\ncd ~\nrevenir dans le rÃ©pertoire HOME de lâ€™utilisateur courant\n\n\n\nLa premiÃ¨re commande est lâ€™occasion de revenir sur une convention dâ€™Ã©criture importante pour les chemins relatifs :\n\n. reprÃ©sente le rÃ©pertoire courant. Ainsi, cd . revient Ã  changer de rÃ©pertoire courantâ€¦ pour le rÃ©pertoire courant, ce qui bien sÃ»r ne change rien. Mais le . est trÃ¨s utile pour la copie de fichiers (cf.Â section suivante) ou encore lorsque lâ€™on doit passer des paramÃ¨tres Ã  un programme (cf.Â section Lancement de programmes) ;\n.. reprÃ©sente le rÃ©pertoire parent du rÃ©pertoire courant.\n\nCes diffÃ©rentes commandes constituent la trÃ¨s grande majoritÃ© des usages dans un terminal. Il est essentiel de les pratiquer jusquâ€™Ã  ce quâ€™elles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup dâ€™autres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\ndÃ©placer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncrÃ©er (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncrÃ©er un fichier vide\n\n\n\nDans la mesure oÃ¹ il est gÃ©nÃ©ralement possible de rÃ©aliser toutes ces opÃ©rations Ã  lâ€™aide dâ€™interfaces graphiques (notamment, lâ€™explorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se dÃ©placer dans le filesystem. Nous vous recommandons malgrÃ© tout de les pratiquer Ã©galement, et ce pour plusieurs raisons :\n\neffectuer un maximum dâ€™opÃ©rations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ;\nen devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ;\nlorsque lâ€™on est amenÃ© Ã  manipuler un terminal pour interagir avec un serveur, il nâ€™y a souvent pas la moindre interface graphique, auquel cas il nâ€™y a pas dâ€™autre choix que dâ€™opÃ©rer uniquement Ã  partir du terminal.\n\n\n\n\nLe rÃ´le du terminal est de lancer des programmes. Lancer un programme se fait Ã  partir dâ€™un fichier dit exÃ©cutable, qui peut Ãªtre de deux formes :\n\nun binaire, i.e.Â un programme dont le code nâ€™est pas lisible par lâ€™humain ;\nun script, i.e.Â un fichier texte contenant une sÃ©rie dâ€™instructions Ã  exÃ©cuter. Le langage du terminal Linux est le shell, et les scripts associÃ©s ont pour extension .sh.\n\nDans les deux cas, la syntaxe de lancement dâ€™une commande est : le nom de lâ€™exÃ©cutable, suivi dâ€™Ã©ventuels paramÃ¨tres, sÃ©parÃ©s par des espaces. Par exemple, la commande python monscript.py exÃ©cute le binaire python et lui passe comme unique argument le nom dâ€™un script .py (contenu dans le rÃ©pertoire courant), qui va donc Ãªtre exÃ©cutÃ© via Python. De la mÃªme maniÃ¨re, toutes les commandes vues prÃ©cÃ©demment pour se dÃ©placer dans le filesystem ou manipuler des fichiers sont des exÃ©cutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier Ã  copier et le chemin dâ€™arrivÃ©e.\nDans les exemples de commandes prÃ©cÃ©dents, les paramÃ¨tres Ã©taient passÃ©s en mode positionnel : lâ€™exÃ©cutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments nâ€™est pas toujours fixÃ© Ã  lâ€™avance, du fait de la prÃ©sence de paramÃ¨tres optionnels. Ainsi, la plupart des exÃ©cutables permettent le passage dâ€™arguments optionnels, qui modifient le comportement de lâ€™exÃ©cutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier Ã  un autre endroit du filesystem, mais peut-on copier un dossier et lâ€™ensemble de son contenu avec ? Nativement non, mais lâ€™ajout dâ€™un paramÃ¨tre le permet : cp -R dossierdepart dossierarrivee permet de copier rÃ©cursivement le dossier et tout son contenu. Notons que les flags ont trÃ¨s souvent un Ã©quivalent en toute lettre, qui sâ€™Ã©crit quant Ã  lui avec deux tirers. Par exemple, la commande prÃ©cÃ©dente peut sâ€™Ã©crire de maniÃ¨re Ã©quivalente cp --recursive dossierdepart dossierarrivee. Il est frÃ©quent de voir les deux syntaxes en pratique, parfois mÃªme mÃ©langÃ©es au sein dâ€™une mÃªme commande."
  },
  {
    "objectID": "chapters/linux-101.html#variables-denvironnement",
    "href": "chapters/linux-101.html#variables-denvironnement",
    "title": "Linux 101",
    "section": "",
    "text": "Comme tout langage de programmation, le langage shell permet dâ€™assigner et dâ€™utiliser des variables dans des commandes. Pour afficher le contenu dâ€™une variable, on utilise la commande echo, qui est lâ€™Ã©quivalent de la fonction print en Python ou en R.\n\n\nterminal\n\n$ MY_VAR=\"toto\"\n$ echo $MY_VAR\n\ntoto\nQuelques remarques importantes :\n\nla syntaxe pour la crÃ©ation de variable est prÃ©cise : aucun espace dâ€™un cÃ´tÃ© comme de lâ€™autre du = ;\nen Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu Ã©crire MY_VAR=toto pour le mÃªme rÃ©sultat. Par contre, si lâ€™on veut assigner Ã  une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message dâ€™erreur ;\npour accÃ©der Ã  la valeur dâ€™une variable, on la prÃ©fixe dâ€™un $.\n\nNotre objectif avec ce tutoriel nâ€™est pas de savoir coder en shell, on ne va donc pas sâ€™attarder sur les propriÃ©tÃ©s des variables. En revanche, introduire ce concept Ã©tait nÃ©cessaire pour en prÃ©senter un autre, essentiel quant Ã  lui dans la pratique quotidienne du data scientist : les variables dâ€™environnement. Pour faire une analogie â€” un peu simpliste â€” avec les langages de programmation, ce sont des sortes de variables â€œglobalesâ€, dans la mesure oÃ¹ elles vont Ãªtre accessibles Ã  tous les programmes lancÃ©s Ã  partir dâ€™un terminal, et vont modifier leur comportement.\nLa liste des variables dâ€™environnement peut Ãªtre affichÃ©e Ã  lâ€™aide de la commande env. Il y a gÃ©nÃ©ralement un grand nombre de variables dâ€™environnement prÃ©Ã©xistantes ; en voici un Ã©chantillon obtenu Ã  partir du terminal du service VSCode.\n\n\nterminal\n\n$ env\n\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la variÃ©tÃ© des utilisations des variables dâ€™environnements : - la variable $SHELL prÃ©cise lâ€™exÃ©cutable utilisÃ© pour lancer le terminal ; - la variable $HOME donne lâ€™emplacement du rÃ©pertoire utilisateur. En fait, le symbole ~ que lâ€™on a rencontrÃ© plus haut rÃ©fÃ©rence cette mÃªme variable ; - la variable LANG spÃ©cifie la locale, un concept qui permet de dÃ©finir la langue et lâ€™encodage utilisÃ©s par dÃ©faut par Linux ; - la variable CONDA_PYTHON_EXE existe uniquement parce que lâ€™on a installÃ© conda comme systÃ¨me de gestion de packages Python. Câ€™est lâ€™existance de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous intÃ©resse.\nUne variable dâ€™environnement essentielle, et que lâ€™on est frÃ©quemment amenÃ© Ã  modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concatÃ©nation de chemins absolus, sÃ©parÃ©s par :, qui spÃ©cifie les dossiers dans lesquels Linux va chercher les exÃ©cutables lorsque lâ€™on lance une commande, ainsi que lâ€™ordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n\n\nterminal\n\n$ echo $PATH\n\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nLâ€™ordre de recherche est de gauche Ã  droite. Câ€™est donc parce que le dossier /home/coder/local/bin/conda/bin est situÃ© en premier que lâ€™interprÃ©teur Python qui sera choisi lorsque lâ€™on lance un script Python est celui issu de Conda, et non celui contenu par dÃ©faut dans /usr/bin par exemple.\nLâ€™existence et la configuration adÃ©quate des variables dâ€™environnement est essentielle pour le bon fonctionnement de nombreux outils trÃ¨s utilisÃ©s en data science, comme Git ou encore Spark par exemple. Il est donc nÃ©cessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration dâ€™un serveur en cas de bug liÃ© Ã  une variable dâ€™environnement manquante ou mal configurÃ©e."
  },
  {
    "objectID": "chapters/linux-101.html#permissions",
    "href": "chapters/linux-101.html#permissions",
    "title": "Linux 101",
    "section": "",
    "text": "La sÃ©curitÃ© est un enjeu central en Linux, qui permet une gestion trÃ¨s fine des permissions sur les diffÃ©rents fichiers et programmes.\nUne diffÃ©rence majeure par rapport Ã  dâ€™autres systÃ¨mes dâ€™exploitation, notamment Windows, est quâ€™aucun utilisateur nâ€™a par dÃ©faut les droits complets dâ€™administrateur (root). Il nâ€™est donc pas possible nativement dâ€™accÃ©der au parties sensibles du systÃ¨me, ou bien de lancer certains types de programme. Par exemple, si lâ€™on essaie de lister les fichiers du dossier /root, on obtient une erreur.\n\n\nterminal\n\n$ ls /root\n\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines opÃ©rations telles que lâ€™installation de binaires ou de packages nÃ©cessitent cependant des droits administrateurs. Dans ce cas, il est dâ€™usage dâ€™utiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de lâ€™exÃ©cution de la commande.\n\n\nterminal\n\n$ sudo ls /root\n\nLe dossier /root Ã©tant vide, la commande ls renvoie une chaÃ®ne de caractÃ¨res vide, mais nous nâ€™avons plus de problÃ¨me de permission. Notons quâ€™une bonne pratique de sÃ©curitÃ©, en particulier dans les scripts shell que lâ€™on peut Ãªtre amenÃ©s Ã  Ã©crire ou exÃ©cuter, est de limiter lâ€™utilisation de cette commande aux cas oÃ¹ elle sâ€™avÃ¨re nÃ©cessaire.\nUne autre subtilitÃ© concerne justement lâ€™exÃ©cution de scripts shell. Par dÃ©faut, quâ€™il soit crÃ©Ã© par lâ€™utilisateur ou tÃ©lÃ©chargÃ© dâ€™internet, un script nâ€™est pas exÃ©cutable.\n\n\nterminal\n\n1$ touch test.sh\n2$ ./test.sh\n\n\n1\n\nCrÃ©er le script test.sh (vide)\n\n2\n\nExÃ©cuter le script test.sh\n\n\nbash: ./test.sh: Permission denied\nCâ€™est bien entendu une mesure de sÃ©curitÃ© pour Ã©viter lâ€™exÃ©cution automatique de scripts potentiellement malveillants. Pour pouvoir exÃ©cuter un tel script, il faut attribuer des droits dâ€™exÃ©cution au fichier avec la commande chmod. Il devient alors possible dâ€™exÃ©cuter le script classiquement.\n\n\nterminal\n\n1$ chmod +x test.sh\n2$ ./test.sh\n$ # Le script Ã©tant vide, il ne se passe rien\n\n\n1\n\nDonner des droits dâ€™exÃ©cution au script test.sh\n\n2\n\nExÃ©cuter le script test.sh"
  },
  {
    "objectID": "chapters/linux-101.html#les-scripts-shell",
    "href": "chapters/linux-101.html#les-scripts-shell",
    "title": "Linux 101",
    "section": "",
    "text": "Maintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell prÃ©cÃ©demment Ã©voquÃ©s. A lâ€™instar dâ€™un script Python, un script shell permet dâ€™automatiser une sÃ©rie de commandes lancÃ©es dans un terminal. Le but de ce tutoriel nâ€™est pas de savoir Ã©crire des scripts shell complexes, travail gÃ©nÃ©ralement dÃ©volu aux les data engineers ou les sysadmin (administrateurs systÃ¨me), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces compÃ©tences sont essentielles lorsque lâ€™on se prÃ©occupe de mise en production. A titre dâ€™exemple, comme nous le verrons dans le chapitre sur la portabilitÃ©, il est frÃ©quent dâ€™utiliser un script shell comme entrypoint dâ€™une image docker, afin de spÃ©cifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement Ã  lâ€™aide dâ€™un script simple. ConsidÃ©rons les commandes suivantes, que lâ€™on met dans un fichier monscript.sh dans le rÃ©pertoire courant.\n\n\nterminal\n\n$ #!/bin/bash\n$ SECTION=$1\n$ CHAPTER=$2\n$ FORMATION_DIR=/home/coder/work/formation\n$ mkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\n$ touch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\n\nAnalysons la structure de ce script :\n\nla premiÃ¨re ligne est classique, elle se nomme le shebang : elle indique au systÃ¨me quel interprÃ©teur utiliser pour exÃ©cuter ce script. Dans notre cas, et de maniÃ¨re gÃ©nÃ©rale, on utilise bash (Bourne-Again SHell, lâ€™implÃ©mentation moderne du shell) ;\nles lignes 2 et 3 assignent Ã  des variables les arguments passÃ©s au script dans la commande. Par dÃ©faut, ceux-ci sont assignÃ©s Ã  des variables n oÃ¹ n est la position de lâ€™argument, en commenÃ§ant Ã  1 ;\nla ligne 4 assigne un chemin Ã  une variable\nla ligne 5 crÃ©e le chemin complet, dÃ©fini Ã  partir des variables crÃ©Ã©es prÃ©cÃ©demment. Le paramÃ¨tre -p est important : il prÃ©cise Ã  mkdir dâ€™agir de maniÃ¨re rÃ©cursive, câ€™est Ã  dire de crÃ©er les dossiers intermÃ©diaires qui nâ€™existent pas encore ;\nla ligne 6 crÃ©e un fichier texte vide dans le dossier crÃ©Ã© avec la commande prÃ©cÃ©dente.\n\nExÃ©cutons maintenant ce script, en prenant soin de lui donner les permission adÃ©quates au prÃ©alable.\n\n\nterminal\n\n$ chmod +x monscript.sh\n$ bash monscript.sh section2 chapitre3\n$ ls formation/section1/chapitre2/\n\ntext.txt\nOpÃ©ration rÃ©ussie : le dossier a bien Ã©tÃ© crÃ©Ã© et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash trÃ¨s bien rÃ©alisÃ©e."
  },
  {
    "objectID": "chapters/linux-101.html#gestionnaire-de-paquets",
    "href": "chapters/linux-101.html#gestionnaire-de-paquets",
    "title": "Linux 101",
    "section": "",
    "text": "Une diffÃ©rence fondamentale entre Linux et Windows tient Ã  la maniÃ¨re dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier exÃ©cutable en .exe) sur le site du logiciel, et on lâ€™exÃ©cute. En Linux, on passe gÃ©nÃ©ralement par un gestionnaire de packages qui va chercher les logiciels sur un rÃ©pertoire centralisÃ©, Ã  la maniÃ¨re de pip en Python par exemple.\nPourquoi cette diffÃ©rence ? Une raison importante est que, contrairement Ã  Windows, il existe une multitude de distributions diffÃ©rentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent diffÃ©remment et peuvent avoir diffÃ©rentes versions. En utilisant le package manager (gestionnaire de paquets) propre Ã  la distribution en question, on sâ€™assure de tÃ©lÃ©charger le logiciel adaptÃ© Ã  sa distribution. Dans ce cours, on fait le choix dâ€™utiliser une distribution Debian et son gestionnaire de paquets associÃ© apt. Debian est en effet un choix populaire pour les servers de part sa stabilitÃ© et sa simplicitÃ©, et sera Ã©galement familiÃ¨re aux utilisateurs dâ€™Ubuntu, distribution trÃ¨s populaire pour les ordinateurs personnels et qui est basÃ©e sur Debian.\nLâ€™utilisation dâ€™apt est trÃ¨s simple. La seule difficultÃ© est de savoir le nom du paquet que lâ€™on souhaite installer, ce qui nÃ©cessite en gÃ©nÃ©ral dâ€™utiliser un moteur de recherche. Lâ€™installation de paquets est Ã©galement un cas oÃ¹ il faut utiliser sudo, puisque cela implique souvent lâ€™accÃ¨s Ã  des rÃ©pertoires protÃ©gÃ©s.\n\n\nterminal\n\n$ sudo apt install tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nDÃ©sinstaller un package est Ã©galement simple : câ€™est lâ€™opÃ©ration inverse. Par sÃ©curitÃ©, le terminal vous demande si vous Ãªtes sÃ»r de votre choix en vous demandant de tapper la lettre y ou la lettre n.Â On peut passer automatiquement cette Ã©tape en ajoutant le paramÃ¨tre -y\n\n\nterminal\n\n$ sudo apt remove -y tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant dâ€™installer un package, il est toujours prÃ©fÃ©rable de mettre Ã  jour la base des packages, pour sâ€™assurer quâ€™on obtiendra bien la derniÃ¨re version.\n\n\nterminal\n\n$ sudo apt update\n\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date."
  },
  {
    "objectID": "chapters/linux-101.html#tricks",
    "href": "chapters/linux-101.html#tricks",
    "title": "Linux 101",
    "section": "",
    "text": "On lâ€™a dit et redit : devenir Ã  lâ€™aise avec le terminal Linux est essentiel et demande de la pratique. Il existe nÃ©anmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa premiÃ¨re est lâ€™autocomplÃ©tion. DÃ¨s lors que vous Ã©crivez une commande contenant un nom dâ€™exÃ©cutable, un chemin sur le filesystem, ou autre, nâ€™hÃ©sitez pas Ã  utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majoritÃ© des cas, cela va vous faire gagner un temps prÃ©cieux.\nUne seconde astuce, qui nâ€™en est pas vraiment une, est de lire la documentation dâ€™une commande lorsque lâ€™on nâ€™est pas sÃ»r de sa syntaxe ou des paramÃ¨tres admissibles. Via le terminal, la documentation dâ€™une commande peut Ãªtre affichÃ©e en exÃ©cutant man suivie de la commande en question, par exemple : man cp. Comme il nâ€™est pas toujours trÃ¨s pratique de lire de longs textes dans un petit terminal, on peut Ã©galement chercher la documentation dâ€™une commande sur le site man7."
  },
  {
    "objectID": "chapters/projects-architecture.html",
    "href": "chapters/projects-architecture.html",
    "title": "Structure des projets",
    "section": "",
    "text": "DÃ©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein Ã©cran."
  },
  {
    "objectID": "chapters/projects-architecture.html#dÃ©monstration-par-lexemple",
    "href": "chapters/projects-architecture.html#dÃ©monstration-par-lexemple",
    "title": "Structure des projets",
    "section": "DÃ©monstration par lâ€™exemple",
    "text": "DÃ©monstration par lâ€™exemple\nVoici un exemple dâ€™organisation de projet, qui vous rappellera peut-Ãªtre des souvenirs :\nâ”œâ”€â”€ report.qmd\nâ”œâ”€â”€ correlation.png\nâ”œâ”€â”€ data.csv\nâ”œâ”€â”€ data2.csv\nâ”œâ”€â”€ fig1.png\nâ”œâ”€â”€ figure 2 (copy).png\nâ”œâ”€â”€ report.pdf\nâ”œâ”€â”€ partial data.csv\nâ”œâ”€â”€ script.R\nâ””â”€â”€ script_final.py\nSource : eliocamp.github.io\nLa structure du projet suivante rend compliquÃ©e la comprÃ©hension du projet. Parmi les principales questions :\n\nQuelles sont les donnÃ©es en entrÃ©e de chaine ?\nDans quel ordre les donnÃ©es intermÃ©diaires sont-elles crÃ©Ã©es ?\nQuel est lâ€™objet des productions graphiques ?\nTous les codes sont-ils utilisÃ©s dans ce projet ?\n\nEn structurant le dossier en suivant des rÃ¨gles simples, par exemple en organisant le projet par des dossiers inputs, outputs, on amÃ©liore dÃ©jÃ  grandement la lisibilitÃ© du projet\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ data\nâ”‚   â”œâ”€â”€ raw\nâ”‚   â”‚   â”œâ”€â”€ data.csv\nâ”‚   â”‚   â””â”€â”€ data2.csv\nâ”‚   â””â”€â”€ derived\nâ”‚       â””â”€â”€ partial data.csv\nâ”œâ”€â”€ src\n|   â”œâ”€â”€ script.py\nâ”‚   â”œâ”€â”€ script_final.py\nâ”‚   â””â”€â”€ report.qmd\nâ””â”€â”€ output\n    â”œâ”€â”€ fig1.png\n    â”œâ”€â”€ figure 2 (copy).png\n    â”œâ”€â”€ figure10.png\n    â”œâ”€â”€ correlation.png\n    â””â”€â”€ report.pdf\n\n\n\n\n\n\nNote\n\n\n\nComme Git est un prÃ©requis, tout projet prÃ©sente un fichier .gitignore (il est trÃ¨s important, surtout quand on manipule des donnÃ©es qui ne doivent pas se retrouver sur Github ou Gitlab).\nUn projet prÃ©sente aussi un fichier README.md Ã  la racine, nous reviendrons dessus.\nUn projet qui utilise lâ€™intÃ©gration continue contiendra Ã©galement des fichiers spÃ©cifiques :\n\nsi vous utilisez Gitlab, les instructions sont stockÃ©es dans le fichier gitlab-ci.yml\nsi vous utilisez Github, cela se passe dans le dossier .github/workflows\n\n\n\nEn changeant simplement le nom des fichiers, on rend la structure du projet trÃ¨s lisible :\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ data\nâ”‚   â”œâ”€â”€ raw\nâ”‚   â”‚   â”œâ”€â”€ dpe_logement_202103.csv\nâ”‚   â”‚   â””â”€â”€ dpe_logement_202003.csv\nâ”‚   â””â”€â”€ derived\nâ”‚       â””â”€â”€ dpe_logement_merged_preprocessed.csv\nâ”œâ”€â”€ src\n|   â”œâ”€â”€ preprocessing.py\nâ”‚   â”œâ”€â”€ generate_plots.py\nâ”‚   â””â”€â”€ report.qmd\nâ””â”€â”€ output\n    â”œâ”€â”€ histogram_energy_diagnostic.png\n    â”œâ”€â”€ barplot_consumption_pcs.png\n    â”œâ”€â”€ correlation_matrix.png\n    â””â”€â”€ report.pdf\nMaintenant, le type de donnÃ©es en entrÃ©e de chaine est clair, le lien entre les scripts, les donnÃ©es intermÃ©diaires et les output est transparent."
  },
  {
    "objectID": "chapters/projects-architecture.html#vers-la-sÃ©paration-du-stockage-du-code-des-donnÃ©es-et-de-lenvironnement-dexÃ©cution",
    "href": "chapters/projects-architecture.html#vers-la-sÃ©paration-du-stockage-du-code-des-donnÃ©es-et-de-lenvironnement-dexÃ©cution",
    "title": "Structure des projets",
    "section": "Vers la sÃ©paration du stockage du code, des donnÃ©es et de lâ€™environnement dâ€™exÃ©cution",
    "text": "Vers la sÃ©paration du stockage du code, des donnÃ©es et de lâ€™environnement dâ€™exÃ©cution\nLa sÃ©paration du stockage du code et des donnÃ©es ainsi que de lâ€™environnement dâ€™exÃ©cution est importante pour plusieurs raisons.\nTout dâ€™abord, cela permet de garantir la sÃ©curitÃ© et lâ€™intÃ©gritÃ© des donnÃ©es. En sÃ©parant les donnÃ©es du code, il devient plus difficile pour nâ€™importe qui dâ€™accÃ©der aux informations sensibles stockÃ©es dans les donnÃ©es.\nEn sÃ©parant lâ€™environnement dâ€™exÃ©cution, il est possible de sâ€™assurer que le code fonctionne de maniÃ¨re cohÃ©rente et sans conflit avec dâ€™autres programmes exÃ©cutÃ©s sur le mÃªme systÃ¨me ou nâ€™est pas altÃ©rÃ© par des configurations systÃ¨mes difficiles Ã  reproduire. Cette sÃ©paration facilite Ã©galement la portabilitÃ© et lâ€™adaptation de lâ€™application Ã  diffÃ©rentes plateformes, en permettant de modifier lâ€™environnement dâ€™exÃ©cution sans avoir Ã  modifier le code ou les donnÃ©es.\nLe prochain chapitre sera consacrÃ© Ã  la gestion des dÃ©pendances. Il illustrera la maniÃ¨re dont environnement dâ€™exÃ©cution et code dâ€™un projet peuvent Ãªtre reliÃ©s afin de crÃ©er de la portabilitÃ©.\nLâ€™exÃ©cution dâ€™un code peut dÃ©pendre dâ€™Ã©lÃ©ments de configuration comme des jetons dâ€™authentification ou des mots de passe de connexion Ã  des services qui sont personnels. Ces Ã©lÃ©ments de configuration nâ€™ont pas vocation Ã  Ãªtre partagÃ©s par du code et il est recommandÃ© de les exclure du code. La meilleure maniÃ¨re de transformer ces configurations en paramÃ¨tre est de les isoler dans un script sÃ©parÃ©, qui nâ€™est pas partagÃ©, et utiliser les variables crÃ©Ã©es Ã  cette occasion dans le reste du programme.\nLa maniÃ¨re privilÃ©giÃ©e de conserver ce type dâ€™information est le format YAML. Ce format de fichier permet de stocker des informations de maniÃ¨re hiÃ©rarchisÃ©e et flexible, mais de maniÃ¨re plus lisible que le JSON. Ce format sera transformÃ© en dictionnaire Python ce qui permet des recherches facilitÃ©es.\nPrenons le YAML suivant :\n\n\nsecrets.yaml\n\ntoken:\n    api_insee: \"toto\"\n    api_github: \"tokengh\"\npwd:\n    base_pg: \"monmotdepasse\"\n\nLâ€™import de ce fichier se fait avec le package yaml de la maniÃ¨re suivante :\nimport yaml\n\nwith open('secrets.yaml') as f:\n    secrets = yaml.safe_load(f)\n\n# utilisation du secret\njeton_insee = secrets['token']['api_insee']"
  },
  {
    "objectID": "chapters/projects-architecture.html#les-cookiecutters",
    "href": "chapters/projects-architecture.html#les-cookiecutters",
    "title": "Structure des projets",
    "section": "Les cookiecutters",
    "text": "Les cookiecutters\nEn Python il existe des modÃ¨les de structure de projets : les cookiecutters. Il sâ€™agit de modÃ¨les dâ€™arborescences de fichiers (fichiers Python mais Ã©galement tout type de fichiers) proposÃ©s par la communautÃ© et tÃ©lÃ©chargeables comme point de dÃ©part dâ€™un projet.\nLâ€™idÃ©e de cookiecutter est de proposer des templates que lâ€™on utilise pour initialiser un projet, afin de bÃ¢tir Ã  lâ€™avance une structure Ã©volutive. On va sâ€™inspirer de la structure du template datascience dÃ©veloppÃ© par la communautÃ©. La syntaxe Ã  utiliser dans ce cas est la suivante :\n\n\nterminal\n\n$ $ pip install cookiecutter\n$ $ cookiecutter https://github.com/drivendata/cookiecutter-data-science\n\nLe modÃ¨le est personnalisable, notamment pour faciliter lâ€™interaction entre un systÃ¨me de stockage distant. Lâ€™arborescence gÃ©nÃ©rÃ©e est assez massive pour permettre une grande diversitÃ© de projet. Il nâ€™est souvent pas nÃ©cessaire dâ€™avoir toutes les composantes du cookiecutter.\n\n\nStructure complÃ¨te gÃ©nÃ©rÃ©e par le cookiecutter data science\n\nâ”œâ”€â”€ LICENSE\nâ”œâ”€â”€ Makefile           &lt;- Makefile with commands like `make data` or `make train`\nâ”œâ”€â”€ README.md          &lt;- The top-level README for developers using this project.\nâ”œâ”€â”€ data\nâ”‚   â”œâ”€â”€ external       &lt;- Data from third party sources.\nâ”‚   â”œâ”€â”€ interim        &lt;- Intermediate data that has been transformed.\nâ”‚   â”œâ”€â”€ processed      &lt;- The final, canonical data sets for modeling.\nâ”‚   â””â”€â”€ raw            &lt;- The original, immutable data dump.\nâ”‚\nâ”œâ”€â”€ docs               &lt;- A default Sphinx project; see sphinx-doc.org for details\nâ”‚\nâ”œâ”€â”€ models             &lt;- Trained and serialized models, model predictions, or model summaries\nâ”‚\nâ”œâ”€â”€ notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\nâ”‚                         the creator's initials, and a short `-` delimited description, e.g.\nâ”‚                         `1.0-jqp-initial-data-exploration`.\nâ”‚\nâ”œâ”€â”€ references         &lt;- Data dictionaries, manuals, and all other explanatory materials.\nâ”‚\nâ”œâ”€â”€ reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\nâ”‚   â””â”€â”€ figures        &lt;- Generated graphics and figures to be used in reporting\nâ”‚\nâ”œâ”€â”€ requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\nâ”‚                         generated with `pip freeze &gt; requirements.txt`\nâ”‚\nâ”œâ”€â”€ setup.py           &lt;- Make this project pip installable with `pip install -e`\nâ”œâ”€â”€ src                &lt;- Source code for use in this project.\nâ”‚   â”œâ”€â”€ __init__.py    &lt;- Makes src a Python module\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ data           &lt;- Scripts to download or generate data\nâ”‚   â”‚   â””â”€â”€ make_dataset.py\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ features       &lt;- Scripts to turn raw data into features for modeling\nâ”‚   â”‚   â””â”€â”€ build_features.py\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ models         &lt;- Scripts to train models and then use trained models to make\nâ”‚   â”‚   â”‚                 predictions\nâ”‚   â”‚   â”œâ”€â”€ predict_model.py\nâ”‚   â”‚   â””â”€â”€ train_model.py\nâ”‚   â”‚\nâ”‚   â””â”€â”€ visualization  &lt;- Scripts to create exploratory and results oriented visualizations\nâ”‚       â””â”€â”€ visualize.py\nâ”‚\nâ””â”€â”€ tox.ini            &lt;- tox file with settings for running tox; see tox.readthedocs.io\n\n\n\n\n\n\n\nTests unitaires\n\n\n\n\n\nLes tests unitaires sont des tests automatisÃ©s qui vÃ©rifient le bon fonctionnement dâ€™une unitÃ© de code, comme une fonction ou une mÃ©thode. Lâ€™objectif est de sâ€™assurer que chaque unitÃ© de code fonctionne correctement avant dâ€™Ãªtre intÃ©grÃ©e dans le reste du programme.\nLes tests unitaires sont utiles lorsquâ€™on travaille sur un code de taille consÃ©quente ou lorsquâ€™on partage son code Ã  dâ€™autres personnes, car ils permettent de sâ€™assurer que les modifications apportÃ©es ne crÃ©ent pas de nouvelles erreurs.\nEn Python, on peut utiliser le package unittest pour Ã©crire des tests unitaires. Voici un exemple tirÃ© de ce site :\n# fichier test_str.py\nimport unittest\n\n\nclass ChaineDeCaractereTest(unittest.TestCase):\n\n    def test_reversed(self):\n        resultat = reversed(\"abcd\")\n        self.assertEqual(\"dcba\", \"\".join(resultat))\n\n    def test_sorted(self):\n        resultat = sorted(\"dbca\")\n        self.assertEqual(['a', 'b', 'c', 'd'], resultat)\n\n    def test_upper(self):\n        resultat = \"hello\".upper()\n        self.assertEqual(\"HELLO\", resultat)\n\n    def test_erreur\n\n\nif __name__ == '__main__':\n    unittest.main()\nPour vÃ©rifier que les tests fonctionnent, on exÃ©cute ce script depuis la ligne de commande :\npython3 test_str.py\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nSi on Ã©crit des tests unitaires, il est important de les maintenir !\nPrendre du temps pour Ã©crire des tests unitaires qui ne sont pas maintenus et donc ne renvoie plus de diagnostics pertinents est du temps perdu."
  },
  {
    "objectID": "chapters/projects-architecture.html#transformer-son-projet-en-package-python",
    "href": "chapters/projects-architecture.html#transformer-son-projet-en-package-python",
    "title": "Structure des projets",
    "section": "Transformer son projet en package Python",
    "text": "Transformer son projet en package Python\nLe package est la structure aboutie dâ€™un projet Python autosuffisant. Il sâ€™agit dâ€™une maniÃ¨re formelle de contrÃ´ler la reproductibilitÃ© dâ€™un projet car :\n\nle package assure une gestion cohÃ©rente des dÃ©pendances\nle package offre une certaine structure pour la documentation\nle package facilite la rÃ©utilisation du code\nle package permet des Ã©conomies dâ€™Ã©chelle, car on peut rÃ©utiliser lâ€™un des packages pour un autre projet\nle package facilite le debuggage car il est plus facile dâ€™identifier une erreur quand elle est dans un package\nâ€¦\n\nEn Python, le package est une structure peu contraignante si on a adoptÃ© les bonnes pratiques de structuration de projet. Ã€ partir de la structure modulaire prÃ©cÃ©demment Ã©voquÃ©e, il nâ€™y a quâ€™un pas vers le package : lâ€™ajout dâ€™un fichier pyproject.toml qui contrÃ´le la construction du package (voir ici).\nIl existe plusieurs outils pour installer un package dans le systÃ¨me Ã  partir dâ€™une structure de fichiers locale. Les deux principaux sont\n\nsetuptools\npoetry\n\nLe package fait la transition entre un code modulaire et un code portable, concept sur lequel nous reviendrons dans le prochain chapitre."
  },
  {
    "objectID": "chapters/projects-architecture.html#footnotes",
    "href": "chapters/projects-architecture.html#footnotes",
    "title": "Structure des projets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA cet Ã©gard, Python est beaucoup plus fiable que R. Dans R, si deux scripts utilisent des fonctions dont le nom est identique mais issues de packages diffÃ©rents, il y aura un conflit. En Python chaque module sera importÃ© comme un package en soi.â†©ï¸"
  },
  {
    "objectID": "chapters/galerie/2024/primePredict.html",
    "href": "chapters/galerie/2024/primePredict.html",
    "title": "PrimePredict",
    "section": "",
    "text": "Avec PrimePredict il est possible de calculer les primes annuelles matÃ©rielles dâ€™assurance Ã  partir de donnÃ©es fourni. Le modÃ¨le prÃ©dit Ã  lafois le coÃ»t des dommages matÃ©riels et la frÃ©quence des incidents matÃ©riels."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mise en production",
    "section": "",
    "text": "Cours dans le parcours data science en derniÃ¨re annÃ©e Ã  lâ€™ENSAE construit par Romain Avouac et Lino Galiana.\nLes slides associÃ©es au cours sont disponibles Ã  cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "index.html#mise-en-production-des-projets-de-data-science",
    "href": "index.html#mise-en-production-des-projets-de-data-science",
    "title": "Mise en production",
    "section": "",
    "text": "Cours dans le parcours data science en derniÃ¨re annÃ©e Ã  lâ€™ENSAE construit par Romain Avouac et Lino Galiana.\nLes slides associÃ©es au cours sont disponibles Ã  cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "chapters/portability.html",
    "href": "chapters/portability.html",
    "title": "PortabilitÃ©",
    "section": "",
    "text": "DÃ©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein Ã©cran."
  },
  {
    "objectID": "chapters/portability.html#introduction",
    "href": "chapters/portability.html#introduction",
    "title": "PortabilitÃ©",
    "section": "Introduction",
    "text": "Introduction\nPour illustrer lâ€™importance de travailler avec des environnements virtuels, mettons-nous Ã  la place dâ€™un.e aspirant.e data scientist qui commencerait ses premiers projets.\nSelon toute vraisemblance, on va commencer par installer une distribution de Python â€” souvent, via Anaconda â€” sur son poste et commencer Ã  dÃ©velopper, projet aprÃ¨s projet. Sâ€™il est nÃ©cessaire dâ€™installer une librairie supplÃ©mentaire, on le fera sans trop se poser de question. Puis, on passera au projet suivant en adoptant la mÃªme dÃ©marche. Et ainsi de suite.\nCette dÃ©marche naturelle prÃ©sentera lâ€™avantage de permettre dâ€™aller vite dans les expÃ©rimentations. NÃ©anmoins, elle deviendra problÃ©matique sâ€™il devient nÃ©cessaire de partager son projet, ou de reprendre celui-ci dans le futur.\nDans cette approche, les diffÃ©rents packages quâ€™on va Ãªtre amenÃ© Ã  utiliser vont Ãªtre installÃ©s au mÃªme endroit. Ceci peut apparaÃ®tre secondaire, aprÃ¨s tout nous utilisons Python pour sa simplicitÃ© dâ€™usage qui ne nÃ©cessite pas de passer des heures Ã  se poser des questions avant dâ€™Ã©crire la moindre ligne de code, mais cela va finir par nous poser plusieurs problÃ¨mes :\n\nconflits de version : une application A peut dÃ©pendre de la version 1 dâ€™un package lÃ  oÃ¹ une application B peut dÃ©pendre de la version 2 de ce mÃªme package. Ces versions dâ€™un mÃªme package peuvent avoir des incompatibilitÃ©s2. Une seule application peut donc fonctionner dans cette configuration ;\nversion de Python fixe â€” on ne peut avoir quâ€™une seule installation par systÃ¨me â€” lÃ  oÃ¹ on voudrait pouvoir avoir des versions diffÃ©rentes selon le projet ;\nreproductiblitÃ© limitÃ©e : difficile de dire quel projet repose sur tel package, dans la mesure oÃ¹ ceux-ci sâ€™accumulent en un mÃªme endroit au fil des projets ;\nportabilitÃ© limitÃ©e : consÃ©quence du point prÃ©cÃ©dent, il est difficile de fixer dans un fichier les dÃ©pendances spÃ©cifiques Ã  un projet, et exclusivement celles-ci.\n\nLes environnements virtuels constituent une solution Ã  ces diffÃ©rents problÃ¨mes."
  },
  {
    "objectID": "chapters/portability.html#fonctionnement",
    "href": "chapters/portability.html#fonctionnement",
    "title": "PortabilitÃ©",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLe concept dâ€™environnement virtuel est techniquement trÃ¨s simple. On peut lui donner la dÃ©finition suivante pour Python :\n\nâ€œdossier auto-suffisant qui contient une installation de Python pour une version particuliÃ¨re de Python ainsi que des packages additionnels et qui est isolÃ© des autres environnements existants.â€\n\nOn peut donc simplement voir les environnements virtuels comme un moyen de faire cohabiter sur un mÃªme systÃ¨me diffÃ©rentes installations de Python avec chacune leur propre liste de packages installÃ©s et leurs versions. DÃ©velopper dans des environnements virtuels vierges Ã  chaque dÃ©but de projet est une trÃ¨s bonne pratique pour accroÃ®tre la reproductibilitÃ© des analyses."
  },
  {
    "objectID": "chapters/portability.html#implÃ©mentations",
    "href": "chapters/portability.html#implÃ©mentations",
    "title": "PortabilitÃ©",
    "section": "ImplÃ©mentations",
    "text": "ImplÃ©mentations\nIl existe diffÃ©rentes implÃ©mentations des environnements virtuels en Python, dont chacune ont leurs spÃ©cificitÃ©s et leur communautÃ© dâ€™utilisateurs :\n\nLâ€™implÃ©mentation standard en Python est venv.\nconda propose une implÃ©mentation plus complÃ¨te.\n\nEn pratique pour les utilisateurs, ces implÃ©mentations sont relativement proches. La diffÃ©rence conceptuelle majeure est que conda est Ã  la fois un package manager (comme pip) et un gestionnaire dâ€™environnements virtuels (comme venv).\nPendant longtemps, conda en tant que package manager sâ€™est avÃ©rÃ© trÃ¨s pratique en data science, dans la mesure oÃ¹ il gÃ©rait non seulement les dÃ©pendances Python mais aussi dans dâ€™autres langages, comme des dÃ©pendances C, trÃ¨s utilisÃ©es par les principales librairies de data science et dont lâ€™installation peut Ãªtre complexe sur certains systÃ¨mes dâ€™exploitation. NÃ©anmoins, depuis quelques annÃ©es, lâ€™installation de packages par pip se fait de plus en plus par le biais de wheels qui sont des versions prÃ©-compilÃ©es des librairies systÃ¨mes, propres Ã  chaque configuration systÃ¨me.\n\n\n\n\n\n\nUne diffÃ©rence conceptuelle entre pip et conda\n\n\n\n\n\nLâ€™autre diffÃ©rence majeure avec pip est que Conda utilise une mÃ©thode plus avancÃ©e â€” et donc Ã©galement plus coÃ»teuse en temps â€” de rÃ©solution des dÃ©pendances.\nEn effet, diffÃ©rents packages peuvent spÃ©cifier diffÃ©rentes versions dâ€™un mÃªme package dont ils dÃ©pendent tous les deux, ce qui provoque un conflit de version. Conda va par dÃ©faut appliquer un algorithme qui vise Ã  gÃ©rer au mieux ces conflits, lÃ  oÃ¹ pip va choisir une approche plus minimaliste3.\n\n\n\npip+venv prÃ©sente lâ€™avantage de la simplicitÃ©, conda de la fiabilitÃ©. Selon les projets, on privilÃ©giera lâ€™un ou lâ€™autre. NÃ©anmoins, si le projet est amenÃ© Ã  fonctionner de maniÃ¨re isolÃ©e dans un conteneur, venv suffira amplement car lâ€™isolation sera fournie par le conteneur comme nous le verrons ultÃ©rieurement.\n\n\n\n\n\n\nCâ€™est diffÃ©rent en  ?\n\n\n\n\n\nOn lit souvent, notamment chez les afficionados de  que la gestion des environnements en Python est chaotique. Câ€™Ã©tait vrai au dÃ©but des annÃ©es 2010 mais câ€™est quelques peu exagÃ©rÃ© aujourdâ€™hui.\nLa qualitÃ© supÃ©rieure des outils  pour la gestion des dÃ©pendances ne saute pas aux yeux: renv est trÃ¨s intÃ©ressant mais ne permet pas de dÃ©finir la version de  :\n\nR version: renv tracks, but doesnâ€™t help with, the version of R used with the packge. renv canâ€™t easily help with this because itâ€™s run inside of R, but you might find tools like rig helpful, as they make it easier to switch between multiple version of R on one computer.\n\nCâ€™est, en fait, le problÃ¨me principal des outils  pour la reproductibilitÃ©. Pour les utiliser, il faut souvent se trouver dans une session , avec ses spÃ©cificitÃ©s. Les outils  qui sâ€™utilisent pas le biais de la ligne de commande offrent une robustesse plus importante. venv est certes dÃ©pendant de la version de  utilisÃ©e lors de la crÃ©ation de lâ€™environnement mais le fait de passer par le terminal permet de choisir la version de  qui servira Ã  crÃ©er lâ€™environnement. Quant Ã  conda, la version de  est dÃ©finie dans le environment.yml ce qui donne une grande libertÃ©.\n\n\n\nPuisquâ€™il nâ€™y a pas de raison absolue dâ€™imposer pip+venv ou conda, nous recommandons le pragmatisme. Personnellement, nous utilisons plutÃ´t venv car nous travaillons principalement dans des microservices basÃ©s sur des conteneurs et non sur des postes personnels, ce qui est lâ€™approche moderne dans le monde de la data science. Nous prÃ©sentons nÃ©anmoins les deux approches par la suite. Lâ€™application fil rouge propose les deux approches, Ã  vous de choisir celle que vous dÃ©sirez privilÃ©gier."
  },
  {
    "objectID": "chapters/portability.html#guide-pratique-dutilisation-dun-environnement-virtuel",
    "href": "chapters/portability.html#guide-pratique-dutilisation-dun-environnement-virtuel",
    "title": "PortabilitÃ©",
    "section": "Guide pratique dâ€™utilisation dâ€™un environnement virtuel",
    "text": "Guide pratique dâ€™utilisation dâ€™un environnement virtuel\n\nInstallation\n\nvenvconda\n\n\nvenv est un module inclus par dÃ©faut dans Python, ce qui le rend facilement accessible pour la gestion dâ€™environnements virtuels.\nLes instructions pour utiliser venv, lâ€™outil de crÃ©ation dâ€™environnements virtuels intÃ©grÃ© Ã  Python, sont dÃ©taillÃ©es dans la documentation officielle de Python.\n\n\n\nIllustration du principe (Source: dataquest)\n\n\n\n\nLes instructions Ã  suivre pour installer conda sont dÃ©taillÃ©es dans la documentation officielle. conda seul Ã©tant peu utile en pratique, il est gÃ©nÃ©ralement installÃ© dans le cadre de distributions. Les deux plus populaires sont :\n\nMiniconda : une distribution minimaliste contenant conda, Python ainsi quâ€™un petit nombre de packages techniques trÃ¨s utiles ;\nAnaconda : une distribution assez volumineuse contenant conda, Python, dâ€™autres logiciels (R, Spyder, etc.) ainsi quâ€™un ensemble de packages utiles pour la data science (SciPy, NumPy, etc.).\n\n\nLe choix de la distribution importe assez peu en pratique, dans la mesure oÃ¹ nous allons de toute maniÃ¨re utiliser des environnements virtuels vierges pour dÃ©velopper nos projets.\n\n\n\n\n\nCrÃ©er un environnement\n\nvenvconda\n\n\nPour commencer Ã  utiliser venv, commenÃ§ons par crÃ©er un environnement vierge, nommÃ© dev. Pour crÃ©er un environnement virtuel, cela se fait en ligne de commande par le biais de Python. Cela signifie que la version de Python utilisÃ©e par cet environnement sera celle utilisÃ©e lors de la crÃ©ation de celui-ci.\n\n\nterminal\n\n1$ python -m venv dev\n\n\n1\n\nSur un systÃ¨me Windows, ce sera python.exe -m venv dev\n\n\nCette commande crÃ©e un dossier nommÃ© dev/ contenant une installation Python isolÃ©e.\n\n\nExemple sur un systÃ¨me Linux\n\n\n\n\nExemple sur un systÃ¨me Linux\n\n\n\nCelle-ci est de la version de Python enregistrÃ©e par dÃ©faut dans le PATH, en lâ€™occurrence Python 3.11. Pour crÃ©er un environnement virtuel avec une autre version de Python, il faudra dÃ©finir le chemin de maniÃ¨re formelle, par exemple:\n\n\nterminal\n\n$ /chemin_local/python3.8 -m venv dev-old\n\n\n\nPour commencer Ã  utiliser conda, commenÃ§ons par crÃ©er un environnement vierge, nommÃ© dev, en spÃ©cifiant la version de Python que lâ€™on souhaite installer pour notre projet.\n\n\nterminal\n\n$ conda create -n dev python=3.9.7\n\nRetrieving notices: ...working... done\nChannels:\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/mamba/envs/dev\n\n  added / updated specs:\n    - python=3.9.7\n\n\nThe following packages will be downloaded:\n...\nThe following NEW packages will be INSTALLED:\n...\nProceed ([y]/n)? y\nDownloading and Extracting Packages\n...\nComme indiquÃ© dans les logs, Conda a crÃ©Ã© notre environnement et nous indique son emplacement sur le filesystem. En rÃ©alitÃ©, lâ€™environnement nâ€™est jamais vraiment vierge : Conda nous demande â€” et il faut rÃ©pondre oui en tapant y â€” dâ€™installer un certain nombre de packages, qui sont ceux qui viennent avec la distribution Miniconda.\nOn peut vÃ©rifier que lâ€™environnement a bien Ã©tÃ© crÃ©Ã© en listant les environnements installÃ©s sur le systÃ¨me.\n\n\nterminal\n\n$ conda info --envs\n\n# conda environments:                                                                                                                             \n#                                                                                                                                                 \nbase                  *  /opt/mamba                                                                                                               \ndev                      /opt/mamba/envs/dev\n\n\n\n\n\nActiver un environnement\nComme plusieurs environnements peuvent coexister sur un mÃªme systÃ¨me, il faut dire Ã  notre gestionnaire dâ€™environnement dâ€™activer celui-ci. DÃ¨s lors, ce sera celui-ci qui sera utilisÃ© implicitement lorsquâ€™on utilisera python, pip, etc. dans la ligne de commande active4.\n\nvenvconda\n\n\n\n\nterminal\n\n$ source dev/bin/activate\n\nvenv active lâ€™environnement virtuel dev, indiquÃ© par le changement du nom de lâ€™environnement qui apparaÃ®t au dÃ©but de la ligne de commande dans le terminal. Une fois activÃ©, dev devient temporairement notre environnement par dÃ©faut pour les opÃ©rations Python. Pour confirmer cela, nous pouvons utiliser la commande which pour dÃ©terminer lâ€™emplacement de lâ€™interprÃ©teur Python qui sera utilisÃ© pour exÃ©cuter des scripts comme python mon-script.py.\n\n\nterminal\n\n(dev) $ which python\n\n/home/onyxia/work/dev/bin/python\n\n\n\n\nterminal\n\n$ conda activate dev\n\nConda nous indique que lâ€™on travaille Ã  partir de maintenant dans lâ€™environnement dev en indiquant son nom entre parenthÃ¨ses au dÃ©but de la ligne de commandes. Autrement dit, dev devient pour un temps notre environnement par dÃ©faut. Pour sâ€™en assurer, vÃ©rifions avec la commande which lâ€™emplacement de lâ€™interprÃ©teur Python qui sera utilisÃ© si on lance une commande du type python mon-script.py.\n\n\nterminal\n\n(dev) $ which python \n\n/opt/mamba/envs/dev/bin/python\n\n\n\nOn travaille bien dans lâ€™environnement attendu : lâ€™interprÃ©teur qui se lance nâ€™est pas celui du systÃ¨me global, mais bien celui spÃ©cifique Ã  notre environnement virtuel.\n\n\nLister les packages installÃ©s\nUne fois lâ€™environnement activÃ©, on peut lister les packages installÃ©s et leur version. Cela confirme quâ€™un certain nombre de packages sont installÃ©s par dÃ©faut lors de la crÃ©ation dâ€™un environnement virtuel.\n\nvenvconda\n\n\nOn part dâ€™un environnement vraiment rÃ©duit Ã  lâ€™os:\n\n\nterminal\n\n(dev) $ pip list\n\nPackage    Version\n---------- -------\npip        23.3.2\nsetuptools 69.0.3\nwheel      0.42.0\n\n\nLâ€™environnement est assez minimaliste, quoique plus garni que lors de la crÃ©ation dâ€™un environnement virtuel par venv\n\n\nterminal\n\n(dev) $ conda list\n\n# packages in environment at /opt/mamba/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\nca-certificates           2023.11.17           hbcca054_0    conda-forge\n...\n\n\n\nPour se convaincre, on peut vÃ©rifier que Numpy est bien absent de notre environnement:\n\n\nterminal\n\n(dev) $ python -c \"import numpy as np\"\n\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'numpy'\n\n\nInstaller un package\nNotre environnement peut Ãªtre enrichi, lorsque nÃ©cessaire, avec lâ€™installation dâ€™un package par le biais de la ligne de commande. La procÃ©dure est trÃ¨s similaire entre pip (pour les environnements venv) et conda.\n\n\n\n\n\n\nMÃ©langer pip et conda\n\n\n\n\n\nIl est techniquement possible dâ€™installer des packages par le biais de pip en Ã©tant situÃ© dans un environnement virtuel conda5. Ce nâ€™est pas un problÃ¨me pour de lâ€™expÃ©rimentation et Ã§a permet de dÃ©velopper rapidement.\nNÃ©anmoins, dans un environnement de production câ€™est une pratique Ã  Ã©viter.\n\nSoit on initialise un environnement conda autosuffisant avec un env.yml (voir plus bas) ;\nSoit on crÃ©e un environnement venv et on fait exclusivement des pip install.\n\n\n\n\n\nvenvconda\n\n\n\n\nterminal\n\n(dev) $ pip install nom_du_package\n\n\n\n\n\nterminal\n\n(dev) $ conda install nom_du_package\n\n\n\n\nLa diffÃ©rence est que lÃ  oÃ¹ pip install va installer un package Ã  partir du rÃ©pertoire PyPI, conda install va chercher le package sur les rÃ©pertoires maintenus par les dÃ©veloppeurs de Conda6.\nInstallons par exemple le package phare de machine learning scikit-learn.\n\nvenvconda\n\n\n\n\nterminal\n\n(dev) $ conda install scikit-learn\n\npip install scikit-learn\nCollecting scikit-learn\n  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/5b/be/208f17ce87a5e55094b0e8ffd55b06919ab9b56e7e4ce2a64cd9095ec5d2/scikit_learn-1.4.0-1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading scikit_learn-1.4.0-1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting numpy&lt;2.0,&gt;=1.19.5 (from scikit-learn)\n  Obtaining dependency information for numpy&lt;2.0,&gt;=1.19.5 from https://files.pythonhosted.org/packages/5a/62/007b63f916aca1d27f5fede933fda3315d931\n...\nLes dÃ©pendances nÃ©cessaires (par exemple Numpy sont automatiquement installÃ©es). Lâ€™environnement sâ€™enrichit donc:\n\n\nterminal\n\n(dev) $ pip list\n\nPackage       Version\n------------- -------\njoblib        1.3.2\nnumpy         1.26.3\npip           23.2.1\nscikit-learn  1.4.0\nscipy         1.12.0\nsetuptools    65.5.0\nthreadpoolctl 3.2.0\n\n\n\n\nterminal\n\n(dev) $ conda install scikit-learn\n\n\n\nVoir la sortie\n\nChannels:\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/mamba/envs/dev\n\n  added / updated specs:\n    - scikit-learn\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    joblib-1.3.2               |     pyhd8ed1ab_0         216 KB  conda-forge\n    libblas-3.9.0              |21_linux64_openblas          14 KB  conda-forge\n    libcblas-3.9.0             |21_linux64_openblas          14 KB  conda-forge\n    libgfortran-ng-13.2.0      |       h69a702a_3          23 KB  conda-forge\n    libgfortran5-13.2.0        |       ha4646dd_3         1.4 MB  conda-forge\n    liblapack-3.9.0            |21_linux64_openblas          14 KB  conda-forge\n    libopenblas-0.3.26         |pthreads_h413a1c8_0         5.3 MB  conda-forge\n    libstdcxx-ng-13.2.0        |       h7e041cc_3         3.7 MB  conda-forge\n    numpy-1.26.3               |   py39h474f0d3_0         6.6 MB  conda-forge\n    python_abi-3.9             |           4_cp39           6 KB  conda-forge\n    scikit-learn-1.4.0         |   py39ha22ef79_0         8.7 MB  conda-forge\n    scipy-1.12.0               |   py39h474f0d3_2        15.6 MB  conda-forge\n    threadpoolctl-3.2.0        |     pyha21a80b_0          20 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:        41.6 MB\n\nThe following NEW packages will be INSTALLED:\n\n  joblib             conda-forge/noarch::joblib-1.3.2-pyhd8ed1ab_0 \n  libblas            conda-forge/linux-64::libblas-3.9.0-21_linux64_openblas \n  libcblas           conda-forge/linux-64::libcblas-3.9.0-21_linux64_openblas \n  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-13.2.0-h69a702a_3 \n  libgfortran5       conda-forge/linux-64::libgfortran5-13.2.0-ha4646dd_3 \n  liblapack          conda-forge/linux-64::liblapack-3.9.0-21_linux64_openblas \n  libopenblas        conda-forge/linux-64::libopenblas-0.3.26-pthreads_h413a1c8_0 \n  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-13.2.0-h7e041cc_3 \n  numpy              conda-forge/linux-64::numpy-1.26.3-py39h474f0d3_0 \n  python_abi         conda-forge/linux-64::python_abi-3.9-4_cp39 \n  scikit-learn       conda-forge/linux-64::scikit-learn-1.4.0-py39ha22ef79_0 \n  scipy              conda-forge/linux-64::scipy-1.12.0-py39h474f0d3_2 \n  threadpoolctl      conda-forge/noarch::threadpoolctl-3.2.0-pyha21a80b_0 \n\n\n\nDownloading and Extracting Packages:\n                                                                                                                                                  \nPreparing transaction: done                                                                                                                       \nVerifying transaction: done                                                                                                                       \nExecuting transaction: done \n\nLÃ  encore, conda nous demande dâ€™installer dâ€™autres packages, qui sont des dÃ©pendances de scikit-learn. Par exemple, la librairie de calcul scientifique NumPy.\n(dev) $ conda list\n# packages in environment at /opt/mamba/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\nca-certificates           2023.11.17           hbcca054_0    conda-forge\njoblib                    1.3.2              pyhd8ed1ab_0    conda-forge\nld_impl_linux-64          2.40                 h41732ed_0    conda-forge\nlibblas                   3.9.0           21_linux64_openblas    conda-forge\nlibcblas                  3.9.0           21_linux64_openblas    conda-forge\nlibffi                    3.4.2                h7f98852_5    conda-forge\nlibgcc-ng                 13.2.0               h807b86a_3    conda-forge\nlibgfortran-ng            13.2.0               h69a702a_3    conda-forge\nlibgfortran5              13.2.0               ha4646dd_3    conda-forge\nlibgomp                   13.2.0               h807b86a_3    conda-forge\nliblapack                 3.9.0           21_linux64_openblas    conda-forge\nlibopenblas               0.3.26          pthreads_h413a1c8_0    conda-forge\nlibsqlite                 3.44.2               h2797004_0    conda-forge\nlibstdcxx-ng              13.2.0               h7e041cc_3    conda-forge\nlibzlib                   1.2.13               hd590300_5    conda-forge\nncurses                   6.4                  h59595ed_2    conda-forge\nnumpy                     1.26.3           py39h474f0d3_0    conda-forge\nopenssl                   3.2.0                hd590300_1    conda-forge\npip                       23.3.2             pyhd8ed1ab_0    conda-forge\npython                    3.9.7           hf930737_3_cpython    conda-forge\npython_abi                3.9                      4_cp39    conda-forge\nreadline                  8.2                  h8228510_1    conda-forge\nscikit-learn              1.4.0            py39ha22ef79_0    conda-forge\nscipy                     1.12.0           py39h474f0d3_2    conda-forge\nsetuptools                69.0.3             pyhd8ed1ab_0    conda-forge\nsqlite                    3.44.2               h2c6b66d_0    conda-forge\nthreadpoolctl             3.2.0              pyha21a80b_0    conda-forge\ntk                        8.6.13          noxft_h4845f30_101    conda-forge\ntzdata                    2023d                h0c530f3_0    conda-forge\nwheel                     0.42.0             pyhd8ed1ab_0    conda-forge\nxz                        5.2.6                h166bdaf_0    conda-forge\nzlib                      1.2.13               hd590300_5    conda-forge\n\n\n\n\n\nExporter les spÃ©cifications de lâ€™environnement\nDÃ©velopper Ã  partir dâ€™un environnement vierge est une bonne pratique de reproductibilitÃ© : en partant dâ€™une base minimale, on sâ€™assure que seuls les packages effectivement nÃ©cessaires au bon fonctionnement de notre application ont Ã©tÃ© installÃ©s au fur et Ã  mesure du projet.\nCela rend Ã©galement notre projet plus aisÃ© Ã  rendre portable. On peut exporter les spÃ©cifications de lâ€™environnement dans un fichier spÃ©cial qui peut permettre de crÃ©er un nouvel environnement similaire Ã  celui ayant servi initialement.\n\nvenvconda\n\n\n\n\nterminal\n\n(dev) $ pip freeze &gt; requirements.txt\n\n\n\nVoir le fichier requirements.txt gÃ©nÃ©rÃ©\n\n\n\nrequirements.txt\n\njoblib==1.3.2\nnumpy==1.26.3\nscikit-learn==1.4.0\nscipy==1.12.0\nthreadpoolctl==3.2.0\n\n\n\n\n\n\nterminal\n\n(dev) $ conda env export &gt; environment.yml\n\n\n\nVoir le fichier environment.yml gÃ©nÃ©rÃ©\n\n\n\nenvironment.yml\n\nname: dev\nchannels:\n  - conda-forge\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=2_gnu\n  - ca-certificates=2023.11.17=hbcca054_0\n  - joblib=1.3.2=pyhd8ed1ab_0\n  - ld_impl_linux-64=2.40=h41732ed_0\n  - libblas=3.9.0=21_linux64_openblas\n  - libcblas=3.9.0=21_linux64_openblas\n  - libffi=3.4.2=h7f98852_5\n  - libgcc-ng=13.2.0=h807b86a_3\n  - libgfortran-ng=13.2.0=h69a702a_3\n  - libgfortran5=13.2.0=ha4646dd_3\n  - libgomp=13.2.0=h807b86a_3\n  - liblapack=3.9.0=21_linux64_openblas\n  - libopenblas=0.3.26=pthreads_h413a1c8_0\n  - libsqlite=3.44.2=h2797004_0\n  - libstdcxx-ng=13.2.0=h7e041cc_3\n  - libzlib=1.2.13=hd590300_5\n  - ncurses=6.4=h59595ed_2\n  - numpy=1.26.3=py39h474f0d3_0\n  - openssl=3.2.0=hd590300_1\n  - pip=23.3.2=pyhd8ed1ab_0\n  - python=3.9.7=hf930737_3_cpython\n  - python_abi=3.9=4_cp39\n  - readline=8.2=h8228510_1\n  - scikit-learn=1.4.0=py39ha22ef79_0\n  - scipy=1.12.0=py39h474f0d3_2\n  - setuptools=69.0.3=pyhd8ed1ab_0\n  - sqlite=3.44.2=h2c6b66d_0\n  - threadpoolctl=3.2.0=pyha21a80b_0\n  - tk=8.6.13=noxft_h4845f30_101\n  - tzdata=2023d=h0c530f3_0\n  - wheel=0.42.0=pyhd8ed1ab_0\n  - xz=5.2.6=h166bdaf_0\n  - zlib=1.2.13=hd590300_5\nprefix: /opt/mamba/envs/dev\n\n\n\n\n\nCe fichier est mis par convention Ã  la racine du dÃ©pÃ´t Git du projet. Ainsi, les personnes souhaitant tester lâ€™application peuvent recrÃ©er le mÃªme environnement Conda que celui qui a servi au dÃ©veloppement via la commande suivante.\n\nvenvconda\n\n\nOn refait la dÃ©marche prÃ©cÃ©dente de crÃ©ation dâ€™un environnement vierge puis un pip install -r requirements.txt\n\n\nterminal\n\n$ python -m venv newenv\n$ source newenv/bin/activate\n\n\n\nterminal\n\n(newenv) $ pip install -r requirements.txt\n\n\n\nCela se fait en une seule commande:\n\n\nterminal\n\n$ $ conda env create -f environment.yml\n\n\n\n\n\n\nChanger dâ€™environnement\n\nvenvconda\n\n\nPour changer dâ€™environnement virtuel, il suffit dâ€™en activer un autre.\n\n\nterminal\n\n(myenv) $ deactivate\n$ source anotherenv/bin/activate\n(anotherenv) $ which python\n/chemin/vers/anotherenv/bin/python\n\nPour quitter lâ€™environnement virtuel actif, on utilise simplement la commande deactivate :\n\n\nterminal\n\n(anotherenv) $ deactivate\n$ \n\n\n\nPour changer dâ€™environnement, il suffit dâ€™en activer un autre.\n\n\nterminal\n\n(dev) $ conda activate base\n(base) $ which python\n/opt/mamba/bin/python\n\nPour sortir de tout environnement conda, on utilise la commande conda deactivate :\n\n\nterminal\n\n(base) $ conda deactivate\n$ \n\n\n\n\n\n\nAide-mÃ©moire\n\n\n\n\n\n\n\n\nvenv\nconda\nPrincipe\n\n\n\n\npython -m venv &lt;env_name&gt;\nconda create -n &lt;env_name&gt; python=&lt;python_version&gt;\nCrÃ©ation dâ€™un environnement nommÃ© &lt;env_name&gt; dont la version de Python est &lt;python_version&gt;\n\n\n\nconda info --envs\nLister les environnements\n\n\nsource &lt;env_name&gt;/bin/activate\nconda activate &lt;env_name&gt;\nUtiliser lâ€™environnement &lt;env_name&gt; pour la session du terminal\n\n\npip list\nconda list\nLister les packages dans lâ€™environnement actif\n\n\npip install &lt;pkg&gt;\nconda install &lt;pkg&gt;\nInstaller le package &lt;pkg&gt; dans lâ€™environnement actif\n\n\npip freeze &gt; requirements.txt\nconda env export &gt; environment.yml\nExporter les spÃ©cifications de lâ€™environnement dans un fichier requirements.txt"
  },
  {
    "objectID": "chapters/portability.html#limites",
    "href": "chapters/portability.html#limites",
    "title": "PortabilitÃ©",
    "section": "Limites",
    "text": "Limites\nDÃ©velopper dans des environnements virtuels est une bonne pratique, car cela accroÃ®t la portabilitÃ© dâ€™une application. NÃ©anmoins, il y a plusieurs limites Ã  leur utilisation :\n\nles librairies systÃ¨me nÃ©cessaires Ã  lâ€™installation des packages ne sont pas gÃ©rÃ©es ;\nles environnements virtuels ne permettent pas toujours de gÃ©rer des projets faisant intervenir diffÃ©rents langages de programmation ;\ndevoir installer conda, Python, et les packages nÃ©cessaires Ã  chaque changement dâ€™environnement peut Ãªtre assez long et pÃ©nible en pratique ;\ndans un environnement de production, gÃ©rer des environnements virtuels diffÃ©rents pour chaque projet peut sâ€™avÃ©rer rapidement complexe pour les administrateurs systÃ¨me.\n\nLa technologie des conteneurs permet de rÃ©pondre Ã  ces diffÃ©rents problÃ¨mes."
  },
  {
    "objectID": "chapters/portability.html#introduction-1",
    "href": "chapters/portability.html#introduction-1",
    "title": "PortabilitÃ©",
    "section": "Introduction",
    "text": "Introduction\nAvec les environnements virtuels, lâ€™idÃ©e Ã©tait de permettre Ã  chaque utilisateur potentiel de notre projet dâ€™installer sur son environnement dâ€™exÃ©cution les packages nÃ©cessaires Ã  la bonne exÃ©cution du projet.\nNÃ©anmoins, comme on lâ€™a vu, cette approche ne garantit pas une reproductibilitÃ© parfaite et a lâ€™inconvÃ©nient de demander beaucoup de gestion manuelle.\nChangeons de perspective : au lieu de distribuer une recette permettant Ã  lâ€™utilisateur de recrÃ©er lâ€™environnement nÃ©cessaire sur sa machine, ne pourrait-on pas directement distribuer Ã  lâ€™utilisateur une machine contenant lâ€™environnement prÃ©-configurÃ© ?\nBien entendu, on ve pas configurer et envoyer des ordinateurs portables Ã  tous les utilisateurs potentiels dâ€™un projet. On va donc essayer de livrer une version virtuelle de notre ordinateur. Il existe deux approches principales pour cela:\n\nLes machines virtuelles. Cette approche nâ€™est pas nouvelle. Elle consiste Ã  recrÃ©er, sur un serveur, un environnement informatique complet (matÃ©riel et systÃ¨me dâ€™exploitation) qui rÃ©plique le comportement dâ€™un vÃ©ritable ordinateur.\nLes conteneurs, une solution plus lÃ©gÃ¨re pour empaqueter un environnement informatique afin de rÃ©pliquer le comportement dâ€™une machine rÃ©elle."
  },
  {
    "objectID": "chapters/portability.html#fonctionnement-1",
    "href": "chapters/portability.html#fonctionnement-1",
    "title": "PortabilitÃ©",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLes machines virtuelles ont lâ€™inconvÃ©nient dâ€™Ãªtre assez lourdes, et complexes Ã  rÃ©pliquer et distribuer. Pour pallier ces diffÃ©rentes limites, les conteneurs se sont imposÃ©s au cours de la derniÃ¨re dÃ©cennie. Toutes les infrastructures cloud modernes ont progressivement abandonnÃ© les machines virtuelles pour privilÃ©gier des conteneurs pour les raisons que nous allons Ã©voquer ultÃ©rieurement.\nComme les machines virtuelles, les conteneurs permettent dâ€™empaqueter complÃ¨tement lâ€™environnement (librairies systÃ¨mes, application, configuration) qui permet de faire tourner lâ€™application. Mais Ã  lâ€™inverse dâ€™une machine virtuelle, le conteneur nâ€™inclut pas de systÃ¨me dâ€™exploitation propre, il utilise celui de la machine hÃ´te qui lâ€™exÃ©cute. Cela signifie que si on dÃ©sire reproduire le comportement dâ€™une machine Windows, il nâ€™est pas nÃ©cessaire dâ€™avoir un gros serveur avec Windows. Il est tout Ã  fait possible dâ€™avoir un serveur Linux, ce qui est la norme, et de rÃ©pliquer le comportement dâ€™une machine Windows Ã  lâ€™intÃ©rieur. A lâ€™inverse, cela peut permettre de tester des configurations Linux ou Mac sur un ordinateur Windows. Câ€™est le rÃ´le du logiciel de conteneurisation qui fera la traduction entre les instructions voulues par le software et le systÃ¨me dâ€™exploitation du hardware.\nLa technologie des conteneurs permet ainsi de garantir une trÃ¨s forte reproductibilitÃ© tout en restant suffisamment lÃ©gÃ¨re pour permettre une distribution et un dÃ©ploiement simple aux utilisateurs. En effet, lâ€™adhÃ©rence forte entre un systÃ¨me dâ€™exploitation et un logiciel dans lâ€™approche des machines virtuelles rend plus complexe la montÃ©e en charge dâ€™un serveur. Si plus dâ€™utilisateurs commencent Ã  utiliser lâ€™application, il est nÃ©cessaire de sâ€™assurer que des serveurs correspondant aux besoins de lâ€™application (systÃ¨me dâ€™exploitation, configurations techniques, etc.) soient disponibles. Avec les conteneurs, la montÃ©e en charge est plus simple puisque les restrictions matÃ©rielles sont moins fortes: ajouter un serveur Linux avec les logiciels adÃ©quats peut Ãªtre suffisant.\n\n\n\nDiffÃ©rences entre lâ€™approche conteneurs (gauche) et lâ€™approche machines virtuelles (droite) (Source : docker.com )\n\n\nDu point de vue de lâ€™utilisateur, la diffÃ©rence nâ€™est pas toujours perceptible pour des besoins standards. Lâ€™utilisateur accÃ©dera Ã  son application par une application dÃ©diÃ©e (un navigateur, un logiciel spÃ©cialisÃ©â€¦) et les calculs issus des opÃ©rations effectuÃ©es seront dÃ©portÃ©s sur les serveurs oÃ¹ est hÃ©bergÃ©e cette application. NÃ©anmoins, pour lâ€™organisation qui gÃ¨re cette application, les conteneurs offriront plus de libertÃ© et de flexibilitÃ© comme nous lâ€™avons Ã©voquÃ©."
  },
  {
    "objectID": "chapters/portability.html#docker-limplÃ©mentation-standard",
    "href": "chapters/portability.html#docker-limplÃ©mentation-standard",
    "title": "PortabilitÃ©",
    "section": "Docker , lâ€™implÃ©mentation standard",
    "text": "Docker , lâ€™implÃ©mentation standard\nComme nous lâ€™avons Ã©voquÃ©, le logiciel de conteneurisation fait office de couche tampon entre les applications et le systÃ¨me dâ€™exploitation du serveur.\nComme pour les environnements virtuels, il existe diffÃ©rentes implÃ©mentations de la technologie des conteneurs. En pratique, lâ€™implÃ©mentation offerte par Docker est devenue largement prÃ©dominante, au point quâ€™il est devenu courant dâ€™utiliser de maniÃ¨re interchangeable les termes â€œconteneuriserâ€ et â€œDockeriserâ€ une application. Câ€™est donc cette implÃ©mentation que nous allons Ã©tudier et utiliser dans ce cours.\n\nInstallation et environnements bacs Ã  sable\nDocker  est un logiciel qui peut sâ€™installer sur diffÃ©rents systÃ¨me dâ€™exploitation. Les instructions sont dÃ©taillÃ©es dans la documentation officielle. Il est nÃ©cessaire dâ€™avoir des droits administrateurs sur son poste pour pouvoir faire cette installation.\n\n\n\n\n\n\nBesoins en espace disque\n\n\n\n\n\nIl est Ã©galement recommandÃ© dâ€™avoir de lâ€™espace disque libre car certaines images (concept sur lequel nous reviendrons), une fois dÃ©compressÃ©es et construites, peuvent Ãªtre lourdes selon la richesse des librairies installÃ©es dessus. Elles peuvent rapidement prendre quelques Gigas dâ€™espace disque.\nCeci est nÃ©anmoins Ã  comparer Ã  lâ€™espace disque monstrueux que peut prendre un systÃ¨me dâ€™exploitation complet (autour de 15GB pour Ubuntu ou Mac OS, 20GB par exemple pour Windowsâ€¦). La distribution Linux la plus minimaliste (Alpine) ne fait que 3Mo compressÃ©e et 5Mo une fois dÃ©compressÃ©e.\n\n\n\nIl existe Ã©galement des environnements en ligne gratuits pouvant servir de bacs Ã  sable sâ€™il nâ€™est pas possible pour vous dâ€™installer Docker.\nPlay with Docker permet de tester en ligne Docker comme on pourrait le faire sur une installation personnelle. NÃ©anmoins, ces services sont limitÃ©s: la taille maximale des images dÃ©compressÃ©es est limitÃ©e Ã  2Go, les services connaissent des coupures en cas dâ€™utilisation massiveâ€¦\nComme nous le verrons ultÃ©rieurement, lâ€™utilisation de Docker en interactif est pratique pour apprendre et expÃ©rimenter. NÃ©anmoins, en pratique, on utilise principalement Docker par le biais de lâ€™intÃ©gration continue via Github Actions ou Gitlab CI.\n\n\nPrincipes\n\n\n\nSource : k21academy.com\n\n\nUn conteneur Docker est mis Ã  disposition sous la forme dâ€™une image, câ€™est Ã  dire dâ€™un fichier binaire qui contient lâ€™environnement nÃ©cessaire Ã  lâ€™exÃ©cution de lâ€™application. Celui-ci est mis Ã  disposition de tous sous une forme compressÃ©e sur un dÃ©pÃ´t dâ€™images publiques (le plus connu est Dockerhub).\nAvant de mettre Ã  disposition une image, il est nÃ©cessaire de la construire (build). Pour cela on utilise un Dockerfile, un fichier texte qui contient la recette â€” sous forme de commandes Linux â€” de construction de lâ€™environnement.\nUne fois lâ€™image construite, il est possible de faire deux actions:\n\nLa lancer (run) en local. Cela permet de tester lâ€™application, Ã©ventuellement de la corriger en cas de mauvais fonctionnement. Le lancement de lâ€™application permet de faire tourner lâ€™image dans un environnement isolÃ© quâ€™on appelle le conteneur (container), une instance vivante de lâ€™image en quelques sortes7.\nLa mettre Ã  disposition sur un dÃ©pÃ´t public pour permettre Ã  dâ€™autres (ou Ã  soi-mÃªme) de la tester. Lâ€™image va Ãªtre uploadÃ©e (push) sur un dÃ©pÃ´t (registry), public ou privÃ©, depuis lequel les utilisateurs vont pouvoir tÃ©lÃ©charger lâ€™image (pull).\n\n\n\n\n\n\n\nMettre Ã  disposition son image Docker\n\n\n\n\n\nLe rÃ©pertoire dâ€™images publiques le plus connu est DockerHub. Il sâ€™agit dâ€™un rÃ©pertoire oÃ¹ nâ€™importe qui peut proposer une image Docker, associÃ©e ou non Ã  un projet disponible sur Github ou Gitlab. Il est possible de mettre Ã  disposition de maniÃ¨re manuelle des images mais, comme nous le montrerons dans le chapitre sur la mise en production, il est beaucoup plus pratique dâ€™utiliser des fonctionalitÃ©s dâ€™interaction automatique entre DockerHub et un dÃ©pÃ´t GitHub."
  },
  {
    "objectID": "chapters/portability.html#application",
    "href": "chapters/portability.html#application",
    "title": "PortabilitÃ©",
    "section": "Application",
    "text": "Application\nAfin de prÃ©senter lâ€™utilisation de Docker en pratique, nous allons prÃ©senter les diffÃ©rentes Ã©tapes permettant de â€œdockeriserâ€ une application web minimaliste construite avec le framework Python Flask8.\nLa structure de notre projet est la suivante.\nâ”œâ”€â”€ myflaskapp\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ hello-world.py\nâ”‚   â””â”€â”€ requirements.txt\nLe script hello-world.py contient le code dâ€™une application minimaliste, qui affiche simplement â€œHello, World!â€ sur une page web. Nous verrons dans lâ€™application fil rouge comment construire une application interactive plus complÃ¨te.\n\n\nhello-world.py\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef hello_world():\n    return \"&lt;p&gt;Hello, World!&lt;/p&gt;\"\n\nPour faire tourner lâ€™application, il nous faut donc Ã  la fois Python et le package Flask. Il est donc nÃ©cessaire de contrÃ´ler lâ€™environnement virtuel autour de Python ce qui va impliquer:\n\nInstaller Python ;\nInstaller les packages nÃ©cessaires Ã  lâ€™exÃ©cution de notre code. En lâ€™occurrence, on nâ€™a besoin que de Flask.\n\nSi la version de Python utilisÃ©e par notre application nous importe peu, il est plus simple dâ€™adopter un environnement virtuel venv que conda. Nous allons donc proposer dâ€™utiliser ceci ce qui tombe bien car nous avons dÃ©jÃ  notre requirements.txt qui prend la forme suivante:\n\n\nrequirements.txt\n\nFlask==2.1.1\n\nCes installations en deux temps (Python et packages nÃ©cessaires) doivent Ãªtre spÃ©cifiÃ©es dans le Dockerfile (cf.Â section suivante)."
  },
  {
    "objectID": "chapters/portability.html#dockerfile",
    "href": "chapters/portability.html#dockerfile",
    "title": "PortabilitÃ©",
    "section": "Le Dockerfile",
    "text": "Le Dockerfile\nPour faire un plat, il faut une recette. Pour faire une image image Docker, il faut un Dockerfile.\nCe fichier texte contient une sÃ©rie de commandes qui permettent de construire lâ€™image. Ces fichiers peuvent Ãªtre plus ou moins complexes selon lâ€™application que lâ€™on cherche Ã  conteneuriser, mais leur structure est assez normalisÃ©e.\nLâ€™idÃ©e est de partir dâ€™une couche de base (une distribution Linux minimaliste) et y ajouter des couches en fonction des besoins de notre application.\nPour illustrer cela, analysons ligne Ã  ligne le Dockerfile nÃ©cessaire pour construire une image Docker de notre application Flask.\n\n\n\nDockerfile\n\n1FROM ubuntu:20.04\n\nRUN apt-get update -y && \\ \n2    apt-get install -y python3-pip python3-dev\n    \n3WORKDIR /app\n\n4COPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY . /app\n\n5ENV FLASK_APP=\"hello-world.py\"\n6EXPOSE 5000\n\n7CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n\n\n\n1\n\nFROM : spÃ©cifie lâ€™image de base. Une image Docker hÃ©rite toujours dâ€™une image de base. Ici, on choisit lâ€™image Ubuntu version 20.04, tout va donc se passer comme si lâ€™on dÃ©veloppait sur une machine virtuelle vierge ayant pour systÃ¨me dâ€™exploitation Ubuntu 20.04  ;\n\n2\n\nRUN : lance une commande Linux. Ici, on met dâ€™abord Ã  jour la liste des packages tÃ©lÃ©chargeables via apt, puis on installe Python  ainsi que des librairies systÃ¨me nÃ©cessaires au bon fonctionnement de notre application ;\n\n3\n\nWORKDIR : spÃ©cifie le rÃ©pertoire de travail de lâ€™image. Ainsi, toutes les commandes suivantes seront exÃ©cutÃ©es depuis ce rÃ©pertoire. Câ€™est lâ€™Ã©quivalent Docker de la commande cd (voir Linux 101) ;\n\n4\n\nCOPY : copie un fichier local sur lâ€™image Docker. Cela est liÃ© Ã  la maniÃ¨re dont fonctionne Docker. Pour ne pas polluer lâ€™image de fichiers non nÃ©cessaires (qui affecteront de maniÃ¨re incidente sa configuration et le poids de celle-ci), par dÃ©faut, votre image ne contient pas de fichiers de votre projet. Si certains sont nÃ©cessaires pour construire lâ€™image, il faut explicitement le dire Ã  Docker. Ici, on copie dâ€™abord le fichier requirements.txt du projet, qui spÃ©cifie les dÃ©pendances Python de notre application, puis on les installe avec une commande RUN. La seconde instruction COPY copie le rÃ©pertoire du projet sur lâ€™image ;\n\n5\n\nENV : crÃ©e une variable dâ€™environnement qui sera accessible Ã  lâ€™application dans le conteneur. Ici, on dÃ©finit une variable dâ€™environnement attendue par Flask, qui spÃ©cifie le nom du script permettant de lancer lâ€™application ;\n\n6\n\nEXPOSE : informe Docker que le conteneur â€œÃ©couteâ€ sur le port 5000, qui est le port par dÃ©faut utilisÃ© par le serveur web de Flask. Ceci est liÃ© Ã  la nature du fonctionnement de Flask qui lance un localhost sur un port donnÃ©, en lâ€™occurrence le port 5000. ;\n\n7\n\nCMD : spÃ©cifie la commande que doit exÃ©cuter le conteneur lors de son lancement. Il sâ€™agit dâ€™une liste, qui contient les diffÃ©rentes parties de la commande sous forme de chaÃ®nes de caractÃ¨res. Ici, on lance la commande flask run qui sait automatiquement quelle application lancer du fait de la commande ENV spÃ©cifiÃ©e prÃ©cÃ©demment. On ajoute lâ€™option --host=0.0.0.0 pour que ce soit lâ€™application dÃ©ployÃ©e sur le localhost (notre application Flask) qui soit mis Ã  disposition de lâ€™utilisateur final.\n\n\n\n\n\n\n\n\n\n\nChoisir lâ€™image de base\n\n\n\n\n\nDans lâ€™idÃ©al, on essaie de partir dâ€™une couche la plus petite possible pour limiter la taille de lâ€™image finalement obtenue. Il nâ€™est en effet pas nÃ©cessaire dâ€™utiliser une image disposant de  si on nâ€™utilise que du .\nEn gÃ©nÃ©ral, les diffÃ©rents langages proposent des images de petite taille dans lequel un interprÃ©teur est dÃ©jÃ  installÃ© et proprement configurÃ©. Dans cette application, on aurait par exemple pu utiliser lâ€™image python:3.9-slim-buster.\n\n\n\nAvec la premiÃ¨re commande RUN du Dockerfile, nous installons Python mais aussi des librairies systÃ¨me nÃ©cessaires au bon fonctionnement de lâ€™application. Mais comment les avons-nous trouvÃ©es ?\nPar essai et erreur. Lors de lâ€™Ã©tape de build que lâ€™on verra juste aprÃ¨s, le moteur Docker va essayer de construire lâ€™image selon les spÃ©cifications du Dockerfile, comme sâ€™il partait dâ€™un ordinateur vide contenant simplement Ubuntu 20.04. Si des librairies manquent, le processus de build devrait renvoyer une erreur, qui sâ€™affichera dans les logs de lâ€™application, affichÃ©s par dÃ©faut dans la console. Quand on a de la chance, les logs dÃ©crivent explicitement les librairies systÃ¨me manquantes. Mais souvent, les messages dâ€™erreur ne sont pas trÃ¨s explicites, et il faut alors les copier dans un moteur de recherche bien connu pour trouver la rÃ©ponse, souvent sur StackOverflow.\nIl est recommandÃ©, avant dâ€™essayer de crÃ©er une image Docker, de passer par lâ€™Ã©tape intermÃ©diaire dans la dÃ©marche de reproductibilitÃ© quâ€™est la crÃ©ation dâ€™un script shell (.sh). Cette approche graduelle est illustrÃ©e dans lâ€™application fil rouge.\n\n\n\n\n\n\nLâ€™instruction COPY\n\n\n\n\n\nLa recette prÃ©sente dans le Dockerfile peut nÃ©cessiter lâ€™utilisation de fichiers appartenant au dossier de travail. Pour que Docker les trouve dans son contexte, il est nÃ©cessaire dâ€™introduire une commande COPY. Câ€™est un petit peu comme pour la cuisine: pour utiliser un produit dans une recette, il faut le sortir du frigo (fichier local) et le mettre sur la table.\n\n\n\nNous nâ€™avons ici vu que les commandes Docker les plus frÃ©quentes, il en existe beaucoup dâ€™autres en pratique. Nâ€™hÃ©sitez pas Ã  consulter la documentation officielle pour comprendre leur utilisation."
  },
  {
    "objectID": "chapters/portability.html#build",
    "href": "chapters/portability.html#build",
    "title": "PortabilitÃ©",
    "section": "Construction dâ€™une image Docker",
    "text": "Construction dâ€™une image Docker\nPour construire une image Ã  partir dâ€™un Dockerfile, il suffit dâ€™utiliser la commande docker build depuis la ligne de commande9. Il faut ensuite spÃ©cifier deux Ã©lÃ©ments importants :\n\nle build context. Il faut indiquer Ã  Docker le chemin de notre projet, qui doit contenir le Dockerfile. En pratique, il est plus simple de se mettre dans le dossier du projet via la commande cd, puis de passer . comme build context pour indiquer Ã  Docker de build â€œdâ€™iciâ€ ;\nle tag, câ€™est Ã  dire le nom de lâ€™image. Tant que lâ€™on utilise Docker en local, le tag importe peu. On verra par la suite que la structure du tag a de lâ€™importance lorsque lâ€™on souhaite exporter ou importer une image Docker Ã  partir dâ€™un dÃ©pÃ´t distant.\n\nRegardons ce qui se passe en pratique lorsque lâ€™on essaie de construire notre image. Le tag de celle-ci est myflaskapp:\n\n\nterminal\n\n$ docker build -t myflaskapp .\n\nSending build context to Docker daemon     47MB\nStep 1/8 : FROM ubuntu:20.04\n ---&gt; 825d55fb6340\nStep 2/8 : RUN apt-get update && apt-get install -y python3-pip python3-dev\n ---&gt; Running in 92b42d579cfa\n...\ndone.\nRemoving intermediate container 92b42d579cfa\n ---&gt; 8826d53e3c01\nStep 3/8 : WORKDIR /app\n ---&gt; Running in 153b32893c23\nRemoving intermediate container 153b32893c23\n ---&gt; 7b4d22021986\nStep 4/8 : COPY requirements.txt /app/requirements.txt\n...\nSuccessfully built 125bd8da70ff\nSuccessfully tagged myflaskapp:latest\nLe moteur Docker essaie de construire notre image sÃ©quentiellement Ã  partir des commandes spÃ©cifiÃ©es dans le Dockerfile. Sâ€™il rencontre une erreur, la procÃ©dure sâ€™arrÃªte, et il faut alors trouver la source du problÃ¨me dans les logs et adapter le Dockerfile en consÃ©quence.\nSi tout se passe bien, Docker nous indique que le build a rÃ©ussi et lâ€™image est prÃªte Ã  Ãªtre utilisÃ©e. On peut vÃ©rifier que lâ€™image est bien disponible Ã  lâ€™aide de la commande docker images.\n\n\nterminal\n\n$ docker images\n\nREPOSITORY                               TAG       IMAGE ID       CREATED          SIZE\nmyflaskapp                               latest    57d2f410a631   2 hours ago      433MB\nIntÃ©ressons nous un peu plus en dÃ©tail aux logs de lâ€™Ã©tape de build ğŸ‘†ï¸.\nEntre les Ã©tapes, Docker affiche des suites de lettres et de chiffres un peu Ã©sotÃ©riques, et nous parle de conteneurs intermÃ©diaires. En fait, il faut voir une image Docker comme un empilement de couches (layers), qui sont elles-mÃªmes des images Docker. Quand on hÃ©rite dâ€™une image avec lâ€™instruction FROM, on spÃ©cifie donc Ã  Docker la couche initiale, sur laquelle il va construire le reste de notre environnement. A chaque Ã©tape sa nouvelle couche, et Ã  chaque couche son hash, un identifiant unique fait de lettres et de chiffres.\nCela peut ressembler Ã  des dÃ©tails techniques, mais câ€™est en fait extrÃªmement utile en pratique car cela permet Ã  Docker de faire du caching. Lorsquâ€™on dÃ©veloppe un Dockerfile, il est frÃ©quent de devoir modifier ce dernier de nombreuses fois avant de trouver la bonne recette, et on aimerait bien ne pas avoir Ã  rebuild lâ€™environnement complet Ã  chaque fois. Docker gÃ¨re cela trÃ¨s bien : il cache chacune des couches intermÃ©diaires10.\nPar exemple, si lâ€™on modifie la 5Ã¨me commande du Dockerfile, Docker va utiliser le cache pour ne pas avoir Ã  recalculer les Ã©tapes prÃ©cÃ©dentes, qui nâ€™ont pas changÃ©. Cela sâ€™appelle lâ€™â€œinvalidation du cacheâ€ : dÃ¨s lors quâ€™une Ã©tape du Dockerfile est modifiÃ©e, Docker va recalculer toutes les Ã©tapes suivantes, mais seulement celles-ci. ConsÃ©quence directe de cette observation : il faut toujours ordonner les Ã©tapes dâ€™un Dockerfile de sorte Ã  ce qui est le plus susceptible dâ€™Ãªtre souvent modifiÃ© soit Ã  la fin du fichier, et inversement.\nPour illustrer cela, regardons ce qui se passe si lâ€™on modifie le nom du script qui lance lâ€™application, et donc la valeur de la variable dâ€™environnement FLASK_APP dans le Dockerfile.\n\n\nterminal\n\n$ docker build . -t myflaskapp\n\n\nSending build context to Docker daemon  4.096kB\nStep 1/10 : FROM ubuntu:20.04\n ---&gt; 825d55fb6340\nStep 2/10 : ENV DEBIAN_FRONTEND=noninteractive\n ---&gt; Using cache\n ---&gt; ea1c7c083ac9\nStep 3/10 : RUN apt-get update -y &&     apt-get install -y python3-pip python3-dev\n ---&gt; Using cache\n ---&gt; 078b8ac0e1cb\nStep 4/10 : WORKDIR /app\n ---&gt; Using cache\n ---&gt; cd19632825b3\nStep 5/10 : COPY requirements.txt /app/requirements.txt\n ---&gt; Using cache\n ---&gt; 271cd1686899\nStep 6/10 : RUN pip install -r requirements.txt\n ---&gt; Using cache\n ---&gt; 3ea406fdf383\nStep 7/10 : COPY . /app\n ---&gt; 3ce5bd3a9572\nStep 8/10 : ENV FLASK_APP=\"new.py\"\n ---&gt; Running in b378d16bb605\nRemoving intermediate container b378d16bb605\n ---&gt; e1f50490287b\nStep 9/10 : EXPOSE 5000\n ---&gt; Running in ab53c461d3de\nRemoving intermediate container ab53c461d3de\n ---&gt; 0b86eca40a80\nStep 10/10 : CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n ---&gt; Running in 340eec151a51\nRemoving intermediate container 340eec151a51\n ---&gt; 16d7a5b8db28\nSuccessfully built 16d7a5b8db28\nSuccessfully tagged myflaskapp:latest\n\nLâ€™Ã©tape de build a pris quelques secondes au lieu de plusieurs minutes, et les logs montrent bien lâ€™utilisation du cache faite par Docker : les Ã©tapes prÃ©cÃ©dant le changement rÃ©utilisent les couches cachÃ©es, mais celle dâ€™aprÃ¨s sont recalculÃ©es."
  },
  {
    "objectID": "chapters/portability.html#execution",
    "href": "chapters/portability.html#execution",
    "title": "PortabilitÃ©",
    "section": "ExÃ©cuter (run) une image Docker",
    "text": "ExÃ©cuter (run) une image Docker\nLâ€™Ã©tape de build a permis de crÃ©er une image Docker. Une image doit Ãªtre vue comme un template : elle permet dâ€™exÃ©cuter lâ€™application sur nâ€™importe quel environnement dâ€™exÃ©cution sur lequel un moteur Docker est installÃ©.\nEn lâ€™Ã©tat, on a donc juste construit, mais rien lancÃ© : notre application ne tourne pas encore. Pour cela, il faut crÃ©er un conteneur, i.e.Â une instance vivante de lâ€™image qui permet dâ€™accÃ©der Ã  lâ€™application. Cela se fait via la commande docker run.\n\n\nterminal\n\n$ docker run -d -p 8000:5000 myflaskapp:latest\n6a2ab0d82d051a3829b182ede7b9152f7b692117d63fa013e7dfe6232f1b9e81\n\nDÃ©taillons la syntaxe de cette commande :\n\ndocker run tag : lance lâ€™image dont on fournit le tag. Le tag est de la forme repository/projet:version. Ici, il nâ€™y a pas de repository puisque tout est fait en local ;\n-d : â€œdÃ©tacheâ€ le conteneur du terminal qui le lance ;\n-p : effectue un mapping entre un port de la machine qui exÃ©cute le conteneur, et le conteneur lui-mÃªme. Notre conteneur Ã©coute sur le port 5000, et lâ€™on veut que notre application soit exposÃ©e sur le port 8000 de notre machine.\n\nLorsque lâ€™on exÃ©cute docker run, Docker nous rÃ©pond simplement un hash qui identifie le conteneur que lâ€™on a lancÃ©. On peut vÃ©rifier quâ€™il tourne bien avec la commande docker ps, qui renvoie toutes les informations associÃ©es au conteneur.\n\n\nterminal\n\n$ docker ps\n\nCONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS                                   NAMES\n6a2ab0d82d05   myflaskapp   \"flask run --host=0.â€¦\"   7 seconds ago   Up 6 seconds   0.0.0.0:8000-&gt;5000/tcp, :::8000-&gt;5000/tcp   vigorous_kalam\nLes conteneurs peuvent Ãªtre utilisÃ©s pour rÃ©aliser des tÃ¢ches trÃ¨s diffÃ©rentes. GrossiÃ¨rement, on peut distinguer deux situations :\n\nle conteneur effectue une tÃ¢che â€œone-shotâ€, câ€™est Ã  dire une opÃ©ration qui a vocation Ã  sâ€™effectuer en un certain temps, suite Ã  quoi le conteneur peut sâ€™arrÃªter ;\nle conteneur exÃ©cute une application. Dans ce cas, on souhaite que le conteneur reste en vie aussi longtemps que lâ€™on souhaite utiliser lâ€™application en question.\n\nDans notre cas dâ€™application, on se situe dans la seconde configuration puisque lâ€™on veut exÃ©cuter une application web. Lorsque lâ€™application tourne, elle expose sur le localhost, accessible depuis un navigateur web â€” en lâ€™occurence, Ã  lâ€™adresse localhost:5000/. Les calculs sont effectuÃ©s sur un serveur local, et le navigateur sert dâ€™interface avec lâ€™utilisateur â€” comme lorsque vous utilisez un notebook Jupyter par exemple.\nFinalement, on a pu dÃ©velopper et exÃ©cuter une application complÃ¨te sur notre environnement local, sans avoir eu Ã  installer quoi que ce soit sur notre machine personnelle, Ã  part Docker."
  },
  {
    "objectID": "chapters/portability.html#exp-docker",
    "href": "chapters/portability.html#exp-docker",
    "title": "PortabilitÃ©",
    "section": "Exporter une image Docker",
    "text": "Exporter une image Docker\nJusquâ€™Ã  maintenant, toutes les commandes Docker que nous avons exÃ©cutÃ©es se sont passÃ©es en local. Ce mode de fonctionnement peut Ãªtre intÃ©ressant pour la phase de dÃ©veloppement et dâ€™expÃ©rimentation. Mais comme on lâ€™a vu, un des gros avantages de Docker est la facilitÃ© de redistribution des images construites, qui peuvent ensuite Ãªtre utilisÃ©es par de nombreux utilisateurs pour faire tourner notre application. Pour cela, il nous faut uploader notre image sur un dÃ©pÃ´t distant, Ã  partir duquel les utilisateurs pourront la tÃ©lÃ©charger.\nPlusieurs possibilitÃ©s existent selon le contexte de travail : une entreprise peut avoir un dÃ©pÃ´t interne par exemple. Si le projet est open source, on peut utiliser le DockerHub.\nLe workflow pour uploader une image est le suivant :\n\ncrÃ©er un compte sur DockerHub ;\ncrÃ©er un projet (public) sur DockerHub, qui va hÃ©berger les images Docker du projet ;\nsur un terminal, utiliser docker login pour sâ€™authentifier au DockerHub ;\non va modifier le tag que lâ€™on fournit lors du build pour spÃ©cifier le chemin attendu. Dans notre cas : docker build -t compte/projet:version . ;\nuploader lâ€™image avec docker push compte/projet:version\n\n\n\nterminal\n\n$ docker push avouacr/myflaskapp:1.0.0\n\nThe push refers to repository [docker.io/avouacr/myflaskapp]\n71db96687fe6: Pushed \n624877ac887b: Pushed \nea4ab6b86e70: Pushed \nb5120a5bc48d: Pushed \n5fa484a3c9d8: Pushed \nc5ec52c98b31: Pushed \n1.0.0: digest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9 size: 1574"
  },
  {
    "objectID": "chapters/portability.html#imp-docker",
    "href": "chapters/portability.html#imp-docker",
    "title": "PortabilitÃ©",
    "section": "Importer une image Docker",
    "text": "Importer une image Docker\nEn supposant que le dÃ©pÃ´t utilisÃ© pour uploader lâ€™image est public, la procÃ©dure que doit suivre un utilisateur pour la tÃ©lÃ©charger se rÃ©sume Ã  utiliser la commande docker pull compte/projet:version\n\n\nterminal\n\n$ docker pull avouacr/myflaskapp:1.0.0\n\n1.0.0: Pulling from avouacr/myflaskapp\ne0b25ef51634: Pull complete \nc0445e4b247e: Pull complete \n48ba4e71d1c2: Pull complete \nffd728caa80a: Pull complete \n906a95f00510: Pull complete \nd7d49b6e17ab: Pull complete \nDigest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9\nStatus: Downloaded newer image for avouacr/myflaskapp:1.0.0\ndocker.io/avouacr/myflaskapp:1.0.0\nDocker tÃ©lÃ©charge et extrait chacune des couches qui constituent lâ€™image (ce qui peut parfois Ãªtre long). Lâ€™utilisateur peut alors crÃ©er un conteneur Ã  partir de lâ€™image, en utilisant docker run comme illustrÃ© prÃ©cÃ©demment."
  },
  {
    "objectID": "chapters/portability.html#aide-mÃ©moire-1",
    "href": "chapters/portability.html#aide-mÃ©moire-1",
    "title": "PortabilitÃ©",
    "section": "Aide-mÃ©moire",
    "text": "Aide-mÃ©moire\nVoici une premiÃ¨re aide-mÃ©moire sur les principales commandes Ã  intÃ©grer dans un Dockerfile:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\nFROM &lt;image&gt;:&lt;tag&gt;\nUtiliser comme point de dÃ©part lâ€™image &lt;image&gt; ayant le tag &lt;tag&gt;\n\n\nRUN &lt;instructions&gt;\nUtiliser la suite dâ€™instructions &lt;instructions&gt; dans un terminal Linux. Pour passer plusieurs commandes dans un RUN, utiliser &&. Cette suite de commande peut avoir plusieurs lignes, dans ce cas, mettre \\ en fin de ligne\n\n\nCOPY &lt;source&gt; &lt;dest&gt;\nRÃ©cupÃ©rer le fichier prÃ©sent dans le systÃ¨me de fichier local Ã  lâ€™emplacement &lt;source&gt; pour que les instructions ultÃ©rieures puissent le trouver Ã  lâ€™emplacement &lt;source&gt;\n\n\nADD &lt;source&gt; &lt;dest&gt;\nGlobalement, mÃªme rÃ´le que COPY\n\n\nENV MY_NAME=\"John Doe\"\nCrÃ©ation dâ€™une variable dâ€™environnement (qui devient disponible sous lâ€™alias $MY_NAME)\n\n\nWORKDIR &lt;path&gt;\nDÃ©finir le working directory du conteuneur Docker dans le dossier &lt;path&gt;\n\n\nUSER &lt;username&gt;\nCrÃ©ation dâ€™un utilisateur non root nommÃ© &lt;username&gt;\n\n\nEXPOSE &lt;PORT_ID&gt;\nLorsquâ€™elle tournera, lâ€™application sera disponible depuis le port &lt;PORT_ID&gt;\n\n\nCMD [\"executable\",\"param1\",\"param2\"]\nAu lancement de lâ€™instance Docker la commande executable (par exemple python3) sera lancÃ©e avec les paramÃ¨tres additionnels fournis\n\n\n\nUne seconde aide-mÃ©moire pour les principales commandes Linux est disponible ci-dessous:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\ndocker build . -t &lt;tag&gt;\nConstruire lâ€™image Docker Ã  partir des fichiers dans le rÃ©pertoire courant (.) en lâ€™identifiant avec le tag &lt;tag&gt;\n\n\ndocker run -it &lt;tag&gt;\nLancer lâ€™instance docker identifiÃ©e par &lt;tag&gt;\n\n\ndocker images\nLister les images disponibles sur la machine et quelques unes de leurs propriÃ©tÃ©s (tags, volume, etc.)\n\n\ndocker system prune\nFaire un peu de mÃ©nage dans ses images Docker (bien rÃ©flÃ©chir avant de faire tourner cette commande)"
  },
  {
    "objectID": "chapters/portability.html#footnotes",
    "href": "chapters/portability.html#footnotes",
    "title": "PortabilitÃ©",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNous reviendrons plus tard sur la maniÃ¨re dont la mise Ã  disposition de packages sous forme prÃ©compilÃ©e par le biais de wheels offre une solution Ã  ce problÃ¨me.â†©ï¸\nSâ€™il est impossible de suivre les Ã©volutions de tous les packages de la data science, il est recommandÃ© de faire une veille sur les principaux comme Pandas ou Scikit en suivant les release notes des versions majeures qui introduisent gÃ©nÃ©ralement des non-compatibilitÃ©s.â†©ï¸\nLe solver de conda, qui est un algorithme de recherche de chemin optimal dans des graphes pour gÃ©rer les (in)compatibilitÃ©s de versions, est lourd Ã  mettre en oeuvre. Le projet mamba a permis dâ€™offrir une rÃ©implÃ©mentation de Conda en C++ par le biais dâ€™un solver plus efficace. Cela a permis de franchement accÃ©lÃ©rer la vitesse dâ€™installation des packages par le biais de conda. NÃ©anmoins, lâ€™accÃ¨s de plus en plus frÃ©quent Ã  des wheels a permis un retour en grÃ¢ce des environnements virtuels implÃ©mentÃ©s par venv au cours des derniÃ¨res annÃ©es.â†©ï¸\nCela signifie que si on ouvre un nouveau terminal, il faudra Ã  nouveau activer cet environnement si on dÃ©sire lâ€™utiliser. Si on dÃ©sire activer par dÃ©faut un environnement, il est possible de configurer le terminal pour quâ€™il active automatiquement un environnement spÃ©cifique lors de son ouverture. Cela peut Ãªtre rÃ©alisÃ© en modifiant les fichiers de configuration du shell, par le biais par exemple du script .bashrc sur Linux.â†©ï¸\nDâ€™ailleurs, si vous utilisez pip sur le SSPCloud, câ€™est ce que vous faites, sans vous en rendre compte.â†©ï¸\nCes rÃ©pertoires sont, dans le langage conda, les canaux. Le canal par dÃ©faut est maintenu par les dÃ©veloppeurs dAnaconda. Cependant, pour en assurer la stabilitÃ©, ce canal a une forte inertie. La conda-forge a Ã©mergÃ© pour offrir plus de flexibilitÃ© aux dÃ©veloppeurs de package qui peuvent ainsi mettre Ã  disposition des versions plus rÃ©centes de leurs packages, comme sur PyPI.â†©ï¸\nPar abus de langage, on mÃ©lange souvent les termes â€œimageâ€ et â€œconteneurâ€. En pratique ces deux concepts sont trÃ¨s proches. Le second correspond Ã  la version vivante du premier.â†©ï¸\nFlask est un framework permettant de dÃ©ployer, de maniÃ¨re lÃ©gÃ¨re, des applications reposant sur Python.â†©ï¸\nSi vous Ãªtes sur Windows, les lignes de commande disponibles par dÃ©faut (cmd ou Powershell) sont peu pratiques. Il est recommandÃ© dâ€™utiliser la ligne de commande de Git Bash (une Ã©mulation minimaliste dâ€™une ligne de commande Linux) qui vous permettra de faire des opÃ©rations en ligne de commande.â†©ï¸\nLe cache est trÃ¨s pratique pour une construction expÃ©rimentale en local. Malheureusement, lorsquâ€™on passe par des services dâ€™intÃ©gration continue, lâ€™utilisation du cache est moins Ã©vidente car chaque run se fait sur une machine indÃ©pendante de la prÃ©cÃ©dente.â†©ï¸"
  }
]