[
  {
    "objectID": "chapters/advanced-notions.html",
    "href": "chapters/advanced-notions.html",
    "title": "Des ressources pour aller plus loin dans lâ€™industrialisation de son projet",
    "section": "",
    "text": "Ce quâ€™on aurait aimer faire mais on nâ€™avait pas encore eu le temps de lâ€™intÃ©grer :\n\nMamba\nTests unitaires\nMonitoring\n\nLogging\n\nVersioning avancÃ©\n\nData version control\nMLOps\n\nOptimisation de performance\n\nSin of early optimisation\nProfiling\nRessources\n\nR : Advanced R, R inferno\nPython : High Performance Python\n\n\nBonnes pratiques Dockerfile\nEnjeux de sÃ©curitÃ©"
  },
  {
    "objectID": "chapters/application.html",
    "href": "chapters/application.html",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "",
    "text": "Lâ€™objectif de cette mise en application est dâ€™illustrer les diffÃ©rentes Ã©tapes qui sÃ©parent la phase de dÃ©veloppement dâ€™un projet de celle de la mise en production. Elle permettra de mettre en pratique les diffÃ©rents concepts prÃ©sentÃ©s tout au long du cours.\nNous nous plaÃ§ons dans une situation initiale correspondant Ã  la fin de la phase de dÃ©veloppement dâ€™un projet de data science. On a un notebook un peu monolithique, qui rÃ©alise les Ã©tapes classiques dâ€™un pipeline de machine learning :\nLâ€™objectif est dâ€™amÃ©liorer le projet de maniÃ¨re incrÃ©mentale jusquâ€™Ã  pouvoir le mettre en production, en le valorisant sous une forme adaptÃ©e."
  },
  {
    "objectID": "chapters/application.html#etape-1-forker-le-dÃ©pÃ´t-dexemple-et-crÃ©er-une-branche-de-travail",
    "href": "chapters/application.html#etape-1-forker-le-dÃ©pÃ´t-dexemple-et-crÃ©er-une-branche-de-travail",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 1 : forker le dÃ©pÃ´t dâ€™exemple et crÃ©er une branche de travail",
    "text": "Etape 1 : forker le dÃ©pÃ´t dâ€™exemple et crÃ©er une branche de travail\n\nOuvrir un service VSCode sur le SSP Cloud. Vous pouvez aller dans la page My Services et cliquer sur New service. Sinon, vous pouvez lancer le service en cliquant directement ici.\nGÃ©nÃ©rer un jeton dâ€™accÃ¨s (token) sur GitHub afin de permettre lâ€™authentification en ligne de commande Ã  votre compte. La procÃ©dure est dÃ©crite ici. Garder le jeton gÃ©nÃ©rÃ© de cÃ´tÃ©.\nForker le dÃ©pÃ´t Github : https://github.com/ensae-reproductibilite/application-correction\nClÃ´ner votre dÃ©pÃ´t Github en utilisant le terminal depuis Visual Studio (Terminal > New Terminal) :\n\n$ git clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application-correction.git\noÃ¹ <TOKEN> et <USERNAME> sont Ã  remplacer, respectivement, par le jeton que vous avez gÃ©nÃ©rÃ© prÃ©cÃ©demment et votre nom dâ€™utilisateur.\n\nSe placer avec le terminal dans le dossier en question :\n\n$ cd ensae-reproductibilite-application-correction"
  },
  {
    "objectID": "chapters/application.html#etape-1-sassurer-que-le-script-sexÃ©cute-correctement",
    "href": "chapters/application.html#etape-1-sassurer-que-le-script-sexÃ©cute-correctement",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 1 : sâ€™assurer que le script sâ€™exÃ©cute correctement",
    "text": "Etape 1 : sâ€™assurer que le script sâ€™exÃ©cute correctement\nOn va partir du fichier notebook.py qui reprend le contenu du notebook1 mais dans un script classique.\nLa premiÃ¨re Ã©tape est simple, mais souvent oubliÃ©e : vÃ©rifier que le code fonctionne correctement.\n\n\n\n\n\n\nApplication 1: corriger les erreurs\n\n\n\n\nOuvrir dans VSCode le script titanic.py ;\nExÃ©cuter le script ligne Ã  ligne pour dÃ©tecter les erreurs ;\nCorriger les deux erreurs qui empÃªchent la bonne exÃ©cution ;\nVÃ©rifier le fonctionnement du script en utilisant la ligne de commande\n\npython titanic.py\n\n\nIl est maintenant temps de commit les changements effectuÃ©s avec Git2 :\n$ git add titanic.py\n$ git commit -m \"Corrige l'erreur qui empÃªchait l'exÃ©cution\"\n$ git push\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli1\nou\nScript checkpoint"
  },
  {
    "objectID": "chapters/application.html#etape-2-utiliser-un-linter-puis-un-formatter",
    "href": "chapters/application.html#etape-2-utiliser-un-linter-puis-un-formatter",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 2: utiliser un linter puis un formatter",
    "text": "Etape 2: utiliser un linter puis un formatter\nOn va maintenant amÃ©liorer la qualitÃ© de notre code en appliquant les standards communautaires. Pour cela, on va utiliser le linter classique PyLint.\n\n\n\n\n\n\nNote\n\n\n\nNâ€™hÃ©sitez pas Ã  taper un code dâ€™erreur sur un moteur de recherche pour obtenir plus dâ€™informations si jamais le message nâ€™est pas clair !\n\n\nPour appliquer le linter Ã  un script .py, la syntaxe Ã  entrer dans le terminal est la suivante :\n$ pylint mon_script.py\n\n\n\n\n\n\nImportant\n\n\n\nPyLint et Black sont des packages Python qui sâ€™utilisent principalement en ligne de commande.\nSi vous avez une erreur qui suggÃ¨re que votre terminal ne connait pas PyLint ou Black, nâ€™oubliez pas dâ€™exÃ©cuter la commande pip install pylint ou pip install black.\n\n\nLe linter renvoie alors une sÃ©rie dâ€™irrÃ©gularitÃ©s, en prÃ©cisant Ã  chaque fois la ligne de lâ€™erreur et le message dâ€™erreur associÃ© (ex : mauvaise identation). Il renvoie finalement une note sur 10, qui estime la qualitÃ© du code Ã  lâ€™aune des standards communautaires Ã©voquÃ©s dans la partie QualitÃ© du code.\n\n\n\n\n\n\nApplication 2: rendre lisible le script\n\n\n\n\nDiagnostiquer et Ã©valuer la qualitÃ© de titanic.py avec PyLint. Regarder la note obtenue.\nUtiliser black titanic.py --diff --color pour observer les changements de forme que va induire lâ€™utilisation du formatter Black\nAppliquer le formatter Black\nRÃ©utiliser PyLint pour diagnostiquer lâ€™amÃ©lioration de la qualitÃ© du script et le travail qui reste Ã  faire.\nComme la majoritÃ© du travail restant est Ã  consacrer aux imports:\n\nMettre tous les imports ensemble en dÃ©but de script\nRetirer les imports redondants en sâ€™aidant des diagnostics de votre Ã©diteur\nRÃ©ordonner les imports si PyLint vous indique de le faire\nCorriger les derniÃ¨res fautes formelles suggÃ©rÃ©es par PyLint\n\nDÃ©limiter des parties dans votre code pour rendre sa structure plus lisible\n\n\n\nLe code est maintenant lisible, il obtient Ã  ce stade une note formelle proche de 10. Mais il nâ€™est pas encore totalement intelligible ou fiable. Il y a notamment beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. NÃ©anmoins, avant cela, occupons-nous de mieux gÃ©rer certains paramÃ¨tres du script: jetons dâ€™API et chemin des fichiers.\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli2\nou\ntitanic.py"
  },
  {
    "objectID": "chapters/application.html#etape-3-gestion-des-paramÃ¨tres",
    "href": "chapters/application.html#etape-3-gestion-des-paramÃ¨tres",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 3: gestion des paramÃ¨tres",
    "text": "Etape 3: gestion des paramÃ¨tres\nLâ€™exÃ©cution du code et les rÃ©sultats obtenus dÃ©pendent de certains paramÃ¨tres. Lâ€™Ã©tude de rÃ©sultats alternatifs, en jouant sur des variantes des paramÃ¨tres, est Ã  ce stade compliquÃ©e car il est nÃ©cessaire de parcourir le code pour trouver ces paramÃ¨tres. De plus, certains paramÃ¨tres personnels comme des jetons dâ€™API ou des mots de passe nâ€™ont pas vocation Ã  Ãªtre prÃ©sents dans le code.\nIl est plus judicieux de considÃ©rer ces paramÃ¨tres comme des variables dâ€™entrÃ©e du script. Cela peut Ãªtre fait de deux maniÃ¨res:\n\nAvec des arguments optionnels appelÃ©s depuis la ligne de commande. Cela peut Ãªtre pratique pour mettre en oeuvre des tests automatisÃ©s3 mais nâ€™est pas forcÃ©ment pertinent pour toutes les variables. Nous allons montrer cet usage avec le nombre dâ€™arbres de notre random forest ;\nEn utilisant un fichier de configuration dont les valeurs sont importÃ©es dans le script principal. Nous allons le mettre en oeuvre pour deux types de fichiers: les Ã©lÃ©ments de configuration Ã  partager et ceux Ã  conserver pour soi mais pouvant servir.\n\n\n\n\n\n\n\nApplication 3: ParamÃ©trisation du script\n\n\n\n\nEn sâ€™inspirant de cette rÃ©ponse, crÃ©er une variable n_trees qui peut Ã©ventuellement Ãªtre paramÃ©trÃ©e en ligne de commande et dont la valeur par dÃ©faut est 20.\nTester cette paramÃ©trisation en ligne de commande avec la valeur par dÃ©faut puis 2, 10 et 50 arbres\nRepÃ©rer le jeton dâ€™API dans le code. Retirer le jeton dâ€™API du code et crÃ©er Ã  la racine du projet un fichier YAML nommÃ© secrets.yaml oÃ¹ vous Ã©crivez ce secret sous la forme key: value\nPour Ã©viter dâ€™avoir Ã  le faire plus tard, crÃ©er une fonction import_yaml_config qui prend en argument le chemin dâ€™un fichier YAML et renvoie le contenu de celui-ci en output. Vous pouvez suivre le conseil du chapitre sur la QualitÃ© du code en adoptant le type hinting.\nCrÃ©er la variable API_TOKEN ayant la valeur stockÃ©e dans secrets.yaml4.\nTester en ligne de commande que lâ€™exÃ©cution du fichier est toujours sans erreur\nRefaire un diagnostic avec PyLint et corriger les Ã©ventuels messages.\nCrÃ©er un fichier config.yaml stockant trois informations: le chemin des donnÃ©es dâ€™entraÃ®nement, des donnÃ©es de test et la rÃ©partition train/test utilisÃ©e dans le code. CrÃ©er les variables correspondantes dans le code aprÃ¨s avoir utilisÃ© import_yaml_config\nCrÃ©er un fichier .gitignore. Ajouter dans ce fichier secrets.yaml car il ne faut pas committer ce fichier.\nCrÃ©er un fichier README.md oÃ¹ vous indiquez quâ€™il faut crÃ©er un fichier secrets.yaml pour pouvoir utiliser lâ€™API.\n\n\n\nIndice si vous ne trouvez pas comment lire un fichier YAML\n\nSi le fichier sâ€™appelle toto.yaml, vous pouvez lâ€™importer de cette maniÃ¨re:\nwith open(\"toto.yaml\", \"r\", encoding=\"utf-8\") as stream:\n    dict_config = yaml.safe_load(stream)\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli3\nou\n\ntitanic.py\nREADME.md\nconfig.yaml\nsecrets.yaml\n.gitignore"
  },
  {
    "objectID": "chapters/application.html#etape-4-adopter-la-programmation-fonctionnelle",
    "href": "chapters/application.html#etape-4-adopter-la-programmation-fonctionnelle",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 4 : Adopter la programmation fonctionnelle",
    "text": "Etape 4 : Adopter la programmation fonctionnelle\nNous allons mettre en fonctions les parties importantes de lâ€™analyse, et les mettre dans un module afin de pouvoir les importer directement depuis le notebook.\nCet exercice Ã©tant chronophage, il nâ€™est pas obligatoire de le rÃ©aliser en entier. Lâ€™important est de comprendre la dÃ©marche et dâ€™adopter frÃ©quemment une approche fonctionnelle5. Pour obtenir une chaine entiÃ¨rement fonctionnalisÃ©e, vous pouvez reprendre le checkpoint.\n\n\n\n\n\n\nApplication 4: adoption des standards de programmation fonctionnelle\n\n\n\n\nCrÃ©er une fonction qui importe les donnÃ©es dâ€™entraÃ®nement (train.csv) et de test (test.csv) et renvoie des DataFrames Pandas ;\nEn fonction du temps disponible, crÃ©er plusieurs fonctions pour rÃ©aliser les Ã©tapes de feature engineering:\n\nLa crÃ©ation de la variable â€œTitleâ€ peut Ãªtre automatisÃ©e en vertu du principe â€œdo not repeat yourselfâ€6.\nRegrouper ensemble les fillna et essayer de crÃ©er une fonction gÃ©nÃ©ralisant lâ€™opÃ©ration.\nLes label encoders peuvent Ãªtre transformÃ©s en deux fonctions: une premiÃ¨re pour encoder une colonne puis une seconde qui utilise la premiÃ¨re de maniÃ¨re rÃ©pÃ©tÃ©e pour encoder plusieurs colonnes. Remarquez les erreurs de copier-coller que cela corrige\nFinaliser les derniÃ¨res transformations avec des fonctions\n\nCrÃ©er une fonction qui rÃ©alise le split train/test de validation en fonction dâ€™un paramÃ¨tre reprÃ©sentant la proportion de lâ€™Ã©chantillon de test.\nCrÃ©er une fonction qui entraÃ®ne et Ã©value un classifieur RandomForest, et qui prend en paramÃ¨tre le nombre dâ€™arbres (n_estimators). La fonction doit imprimer Ã  la fin la performance obtenue et la matrice de confusion.\nDÃ©placer toutes les fonctions ensemble, en dÃ©but de script. \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLe fait dâ€™appliquer des fonctions a dÃ©jÃ  amÃ©liorÃ© la fiabilitÃ© du processus en rÃ©duisant le nombre dâ€™erreurs de copier-coller. NÃ©anmoins, pour vraiment fiabiliser le processus, il faudrait utiliser un pipeline de transformations de donnÃ©es.\nCeci nâ€™est pas encore au programme du cours mais le sera dans une prochaine version.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli4\nou\n\ntitanic.py\n\nLes autres fichiers inchangÃ©s:\n\nREADME.md\nconfig.yaml\nsecrets.yaml\n.gitignore"
  },
  {
    "objectID": "chapters/application.html#etape-1-modularisation",
    "href": "chapters/application.html#etape-1-modularisation",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 1 : modularisation",
    "text": "Etape 1 : modularisation\nFini le temps de lâ€™expÃ©rimentation : on va maintenant essayer de se passer complÃ¨tement du notebook. Pour cela, on va utiliser un main script, câ€™est Ã  dire un script qui reproduit lâ€™analyse en important et en exÃ©cutant les diffÃ©rentes fonctions dans lâ€™ordre attendu.\n\n\n\n\n\n\nApplication 5: modularisation\n\n\n\n\nDÃ©placer les fonctions dans une sÃ©rie de fichiers dÃ©diÃ©s:\n\nimport_data.py: fonctions dâ€™import de donnÃ©es\nbuild_features.py: fonctions regroupant les Ã©tapes de feature engineering\ntrain_evaluate.py: fonctions dâ€™entrainement et dâ€™Ã©valuation du modÃ¨le\n\nSpÃ©cifier les dÃ©pendances (i.e.Â les packages Ã  importer) dans les modules pour que ceux-ci puissent sâ€™exÃ©cuter indÃ©pendamment ;\nRenommer titanic.py en main.py pour suivre la convention de nommage des projets Python ;\nImporter les fonctions nÃ©cessaires Ã  partir des modules. âš ï¸ Ne pas utiliser from XXX import *, ce nâ€™est pas une bonne pratique !\nVÃ©rifier que tout fonctionne bien en exÃ©cutant le script main Ã  partir de la ligne de commande :\n\n$ python main.py\n\n\nOn dispose maintenant dâ€™une application Python fonctionnelle. NÃ©anmoins, le projet est certes plus fiable mais sa structuration laisse Ã  dÃ©sirer et il serait difficile de rentrer Ã  nouveau dans le projet dans quelques temps.\n\n\nEtat actuel du projet ğŸ™ˆ\n\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ train.csv\nâ”œâ”€â”€ test.csv\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ config.yaml\nâ”œâ”€â”€ secrets.yaml\nâ”œâ”€â”€ import_data.py\nâ”œâ”€â”€ build_features.py\nâ”œâ”€â”€ train_evaluate.py\nâ””â”€â”€main.py\n\nComme cela est expliquÃ© dans la partie Structure des projets, on va adopter une structure certes arbitraire mais qui va faciliter lâ€™autodocumentation de notre projet.\nDe plus, une telle structure va faciliter des Ã©volutions optionnelles comme la packagisation du projet. Passer dâ€™une structure modulaire bien faite Ã  un package est quasi-immÃ©diat en Python.\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli5\nou\n\nbuild_features.py\nimport_data.py\ntrain_evaluate.py\nmain.py\n\nLes autres fichiers inchangÃ©s:\n\nREADME.md\nconfig.yaml\nsecrets.yaml\n.gitignore"
  },
  {
    "objectID": "chapters/application.html#etape-2-adopter-une-architecture-standardisÃ©e-de-projet",
    "href": "chapters/application.html#etape-2-adopter-une-architecture-standardisÃ©e-de-projet",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 2 : adopter une architecture standardisÃ©e de projet",
    "text": "Etape 2 : adopter une architecture standardisÃ©e de projet\nOn va maintenant modifier lâ€™architecture de notre projet pour la rendre plus standardisÃ©e. Pour cela, on va sâ€™inspirer des structures cookiecutter qui gÃ©nÃ¨rent des templates de projet.\nOn va sâ€™inspirer de la structure du template datascience dÃ©veloppÃ© par la communautÃ©.\n\n\n\n\n\n\nNote\n\n\n\nLâ€™idÃ©e de cookiecutter est de proposer des templates que lâ€™on utilise pour initialiser un projet, afin de bÃ¢tir Ã  lâ€™avance une structure Ã©volutive. La syntaxe Ã  utiliser dans ce cas est la suivante :\n$ pip install cookiecutter\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science\nIci, on a dÃ©jÃ  un projet, on va donc faire les choses dans lâ€™autre sens : on va sâ€™inspirer de la structure proposÃ©e afin de rÃ©organiser celle de notre projet selon les standards communautaires.\n\n\nEn sâ€™inspirant du cookiecutter data science on va adopter la structure suivante:\nensae-reproductibilite-application\nâ”œâ”€â”€ main.py\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ data\nâ”‚   â””â”€â”€ raw\nâ”‚       â”œâ”€â”€ test.csv\nâ”‚       â””â”€â”€ train.csv\nâ”œâ”€â”€ configuration\nâ”‚   â”œâ”€â”€ secrets.yaml\nâ”‚   â””â”€â”€ config.yaml\nâ”œâ”€â”€ notebooks\nâ”‚   â””â”€â”€ titanic.ipynb\nâ””â”€â”€ src\n    â”œâ”€â”€ data\n    â”‚   â””â”€â”€ import_data.py\n    â”œâ”€â”€ features\n    â”‚   â””â”€â”€ build_features.py\n    â””â”€â”€ models\n        â””â”€â”€ train_evaluate.py\n\n\n\n\n\n\nApplication 6: adopter une structure lisible\n\n\n\n\n(optionnel) Analyser et comprendre la structure de projet proposÃ©e par le template\nModifier lâ€™arborescence du projet selon le modÃ¨le\nAdapter les scripts et les fichiers de configuration Ã  la nouvelle arborescence\nAjouter le dossier pycache au .gitignore7 et le dossier data\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli6\nou\n\nbuild_features.py\nimport_data.py\ntrain_evaluate.py\nmain.py\n\nLes autres fichiers sont inchangÃ©s, Ã  lâ€™exception de leur emplacement.\n\n\n\n\nEtape 3: indiquer lâ€™environnement minimal de reproductibilitÃ©\nLe script main.py nÃ©cessite un certain nombre de packages pour Ãªtre fonctionnel. Chez vous les packages nÃ©cessaires sont bien sÃ»r installÃ©s mais Ãªtes-vous assurÃ© que câ€™est le cas chez la personne qui testera votre code ?\nAfin de favoriser la portabilitÃ© du projet, il est dâ€™usage de â€œfixer lâ€™environnementâ€, câ€™est-Ã -dire dâ€™indiquer dans un fichier toutes les dÃ©pendances utilisÃ©es ainsi que leurs version. Nous proposons de crÃ©er un fichier requirements.txt minimal, sur lequel nous reviendrons dans la partie consacrÃ©e aux environnements reproductibles.\nLe fichier requirements.txt est conventionnellement localisÃ© Ã  la racine du projet. Ici on ne va pas fixer les versions, on raffinera ce fichier plus tard.\n\n\n\n\n\n\nApplication 7: crÃ©ation du requirements.txt\n\n\n\n\nCrÃ©er un fichier requirements.txt avec la liste des packages nÃ©cessaires\nAjouter une indication dans README.md sur lâ€™installation des packages grÃ¢ce au fichier requirements.txt\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli7\nou\n\nrequirements.txt\nREADME.md"
  },
  {
    "objectID": "chapters/application.html#stockageS3",
    "href": "chapters/application.html#stockageS3",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 3 : stocker les donnÃ©es de maniÃ¨re externe",
    "text": "Etape 3 : stocker les donnÃ©es de maniÃ¨re externe\n\n\n\n\n\n\nWarning\n\n\n\nPour mettre en oeuvre cette Ã©tape, il peut Ãªtre utile de comprendre un peu comme fonctionne le SSP Cloud. Vous devrez suivre la documentation du SSP Cloud pour la rÃ©aliser. Une aide-mÃ©moire est Ã©galement disponible dans le cours de 2e annÃ©e de lâ€™ENSAE Python pour la data science\n\n\nComme on lâ€™a vu dans le cours (partie structure des projets), les donnÃ©es ne sont pas censÃ©es Ãªtre versionnÃ©es sur un projet Git.\nLâ€™idÃ©al pour Ã©viter cela tout en maintenant la reproductibilitÃ© est dâ€™utiliser une solution de stockage externe. On va utiliser pour cela MinIO, la solution de stockage de type S3 offerte par le SSP Cloud.\n\n\n\n\n\n\nApplication 8: utilisation dâ€™un systÃ¨me de stockage distant\n\n\n\nA partir de la ligne de commande, utiliser lâ€™utilitaire MinIO pour copier les donnÃ©es data/raw/train.csv et data/raw/test.csv vers votre bucket personnel, respectivement dans les dossiers ensae-reproductibilite/data/raw/train.csv et ensae-reproductibilite/data/raw/test.csv.\n\n\nIndice\n\nStructure Ã  adopter:\n$ mc cp data/raw/train.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv\n$ mc cp data/raw/test.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv\nen modifiant lâ€™emplacement de votre bucket personnel\n\n\nPour se simplifier la vie, on va utiliser des URL de tÃ©lÃ©chargement des fichiers (comme si ceux-ci Ã©taient sur nâ€™importe quel espace de stockage) plutÃ´t que dâ€™utiliser une librairie S3 compatible comme boto3 ou s3fs. Pour cela, en ligne de commande, faire:\n\nmc anonymous set download s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/\nen modifiant <BUCKET_PERSONNEL>. Les URL de tÃ©lÃ©chargement seront de la forme https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv et https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv\n\nModifier configuration.yaml pour utiliser directement les URL dans lâ€™import\nSupprimer les fichiers .csv du dossier data de votre projet, on nâ€™en a plus besoin vu quâ€™on les importe de lâ€™extÃ©rieur\nVÃ©rifier le bon fonctionnement de votre application\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli8\nou\n\nconfig.yaml"
  },
  {
    "objectID": "chapters/application.html#etape-1-proposer-des-tests-unitaires-optionnel",
    "href": "chapters/application.html#etape-1-proposer-des-tests-unitaires-optionnel",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 1 : proposer des tests unitaires (optionnel)",
    "text": "Etape 1 : proposer des tests unitaires (optionnel)\nNotre code comporte un certain nombre de fonctions gÃ©nÃ©riques. On peut vouloir tester leur usage sur des donnÃ©es standardisÃ©es, diffÃ©rentes de celles du Titanic.\nMÃªme si la notion de tests unitaires prend plus de sens dans un package, nous pouvons proposer dans le projet des exemples dâ€™utilisation de la fonction, ceci peut Ãªtre pÃ©dagogique.\nNous allons utiliser unittest pour effectuer des tests unitaires. Cette approche nÃ©cessite une maÃ®trise de la programmation orientÃ©e objet.\n\n\n\n\n\n\nApplication 9: test unitaire (optionnel)\n\n\n\nDans le dossier src/data/, crÃ©er un fichier test_create_variable_title.py8.\nEn sâ€™inspirant de lâ€™exemple de base, crÃ©er une classe TestCreateVariableTitle qui effectue les opÃ©rations suivantes:\n\nCrÃ©ation dâ€™une fonction test_create_variable_title_default_variable_name qui permet de comparer les objets suivants:\n\nCrÃ©ation dâ€™un DataFrame de test :\n\ndf = pd.DataFrame({\n            'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)',\n                    'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath (Lily May Peel)',\n                    'Allen, Mr. William Henry', 'Moran, Mr. James',\n                    'McCarthy, Mr. Timothy J', 'Palsson, Master. Gosta Leonard',\n                    'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)',\n                    'Nasser, Mrs. Nicholas (Adele Achem)'],\n            'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],\n            'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n        })\n\nUtilisation de la fonction create_variable_title sur ce DataFrame\nComparaison au DataFrame attendu:\n\nexpected_result = pd.DataFrame({\n            'Title': ['Mr.', 'Mrs.', 'Miss.', 'Mrs.', 'Mr.', 'Mr.', 'Mr.', 'Master.', 'Mrs.', 'Mrs.'],\n            'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],\n            'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n        })\nEffectuer le test unitaire en ligne de commande avec unittest. Corriger le test unitaire en cas dâ€™erreur.\nSi le temps le permet, proposer des variantes pour tenir compte de paramÃ¨tres (comme la variable variable_name) ou dâ€™exceptions (comme la gestion du cas â€œDonaâ€)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nLorsquâ€™on effectue des tests unitaires, on cherche gÃ©nÃ©ralement Ã  tester le plus de lignes possibles de son code. On parle de taux de couverture (coverage rate) pour dÃ©signer la statistique mesurant cela.\nCela peut sâ€™effectuer de la maniÃ¨re suivante avec le package coverage:\n$ coverage run -m pytest test_create_variable_title.py\n$ coverage report -m\n\nName                            Stmts   Miss  Cover   Missing\n-------------------------------------------------------------\nimport_data.py                     15      6    60%   16-19, 31-34\ntest_create_variable_title.py      21      1    95%   54\n-------------------------------------------------------------\nTOTAL                              36      7    81%\nLe taux de couverture est souvent mis en avant par les gros projets comme indicateur de leur qualitÃ©. Il existe dâ€™ailleurs des badges Github dÃ©diÃ©s.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli9\nou\n\ntest_create_variable_title.py\n\nLes autres fichiers sont inchangÃ©s."
  },
  {
    "objectID": "chapters/application.html#etape-2-transformer-son-projet-en-package-optionnel",
    "href": "chapters/application.html#etape-2-transformer-son-projet-en-package-optionnel",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 2 : transformer son projet en package (optionnel)",
    "text": "Etape 2 : transformer son projet en package (optionnel)\nNotre projet est modulaire, ce qui le rend assez simple Ã  transformer en package, en sâ€™inspirant du cookiecutter adaptÃ©, issu de cet ouvrage.\n\n\nStructure visÃ©e\n\nensae-reproductibilite-application\nâ”œâ”€â”€ docs                                    â” \nâ”‚   â”œâ”€â”€ main.py                             â”‚ \nâ”‚   â””â”€â”€ notebooks                           â”‚ Package documentation and examples\nâ”‚       â”œâ”€â”€ titanic.ipynb                   â”‚ \nâ”œâ”€â”€ README.md                               â”˜ \nâ”œâ”€â”€ pyproject.toml                          â” \nâ”œâ”€â”€ requirements.txt                        â”‚\nâ”œâ”€â”€ src                                     â”‚\nâ”‚   â””â”€â”€ titanicml                           â”‚ Package source code, metadata,\nâ”‚       â”œâ”€â”€ __init__.py                     â”‚ and build instructions \nâ”‚       â”œâ”€â”€ config.yaml                     â”‚\nâ”‚       â”œâ”€â”€ import_data.py                  â”‚\nâ”‚       â”œâ”€â”€ build_features.py               â”‚ \nâ”‚       â””â”€â”€ train_evaluate.py               â”˜\nâ””â”€â”€ tests                                   â”\n    â””â”€â”€ test_create_variable_title.py       â”˜ Package tests\n\n\n\nRappel: structure actuelle\n\nensae-reproductibilite-application\nâ”œâ”€â”€ notebooks                                 \nâ”‚   â””â”€â”€ titanic.ipynb                  \nâ”œâ”€â”€ configuration                                 \nâ”‚   â””â”€â”€ config.yaml                  \nâ”œâ”€â”€ main.py                              \nâ”œâ”€â”€ README.md                 \nâ”œâ”€â”€ requirements.txt                      \nâ””â”€â”€ src \n    â”œâ”€â”€ data                                \n    â”‚   â”œâ”€â”€ import_data.py                    \n    â”‚   â””â”€â”€ test_create_variable_title.py      \n    â”œâ”€â”€ features                           \n    â”‚   â””â”€â”€ build_features.py      \n    â””â”€â”€ models                          \n        â””â”€â”€ train_evaluate.py              \n\n\n\n\n\n\n\nApplication 10: packagisation (optionnel)\n\n\n\n\nDÃ©placer les fichiers dans le dossier src pour respecter la nouvelle arborescence ;\nDans src/titanicml, crÃ©er un fichier vide __init__.py9 ;\nDÃ©placer le fichier de configuration dans le package (nÃ©cessaire Ã  la reproductibilitÃ©) ;\nCrÃ©er le dossier docs et mettre les fichiers indiquÃ©s dedans\nModifier src/titanicml/import_data.py :\n\nAjouter la variable config_file = os.path.join(os.path.dirname(__file__), \"config.yaml\"). Cela permettra dâ€™utiliser directement le fichier ;\nProposer un argument par dÃ©faut Ã  la fonction import_config_yaml Ã©gal Ã  config_file\n\nCrÃ©er un fichier pyproject.toml Ã  partir du contenu de ce modÃ¨le de pyproject10\nInstaller le package en local avec pip install .\nModifier le contenu de docs/main.py pour importer les fonctions de notre package titanicml et tester en ligne de commande notre fichier main.py\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPour crÃ©er la structure minimale dâ€™un package, le plus simple est dâ€™utiliser le cookiecutter adaptÃ©, issu de cet ouvrage.\nComme on a dÃ©jÃ  une structure trÃ¨s modulaire, on va plutÃ´t recrÃ©er cette structure dans notre projet dÃ©jÃ  existant. En fait, il ne manque quâ€™un fichier essentiel, le principal distinguant un projet classique dâ€™un package : pyproject.toml.\ncookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git\n\n\nDÃ©rouler pour voir les choix possibles\n\nauthor_name [Monty Python]: Daffy Duck\npackage_name [mypkg]: titanicml\npackage_short_description []: Impressive Titanic survival analysis\npackage_version [0.1.0]: \npython_version [3.9]: \nSelect open_source_license:\n1 - MIT\n2 - Apache License 2.0\n3 - GNU General Public License v3.0\n4 - Creative Commons Attribution 4.0\n5 - BSD 3-Clause\n6 - Proprietary\n7 - None\nChoose from 1, 2, 3, 4, 5, 6 [1]: \nSelect include_github_actions:\n1 - no\n2 - ci\n3 - ci+cd\nChoose from 1, 2, 3 [1]:\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli10"
  },
  {
    "objectID": "chapters/application.html#anaconda",
    "href": "chapters/application.html#anaconda",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 1 : un environnement pour rendre le projet portable",
    "text": "Etape 1 : un environnement pour rendre le projet portable\nPour quâ€™un projet soit portable, il doit remplir deux conditions:\n\nNe pas nÃ©cessiter de dÃ©pendance qui ne soient pas renseignÃ©es quelque part\nNe pas proposer des dÃ©pendances inutiles, qui ne sont pas utilisÃ©es dans le cadre du projet.\n\n\nEnvironnement virtuelEnvironnement conda\n\n\nLâ€™approche la plus lÃ©gÃ¨re est lâ€™environnement virtuel. Nous avons en fait implicitement dÃ©jÃ  commencÃ© Ã  aller vers cette direction en crÃ©ant un fichier requirements.txt.\n\n\n\n\n\n\nApplication 11a: environnement virtuel venv\n\n\n\n\nExÃ©cuter pip freeze en ligne de commande et observer la (trÃ¨s) longue liste de package\nCrÃ©er lâ€™environnement virtuel titanic en sâ€™inspirant de la documentation officielle11\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin installÃ©\nActiver lâ€™environnement et vÃ©rifier lâ€™installation de Python maintenant utilisÃ©e par votre machine \nVÃ©rifier directement depuis la ligne de commande que Python exÃ©cute bien une commande avec:\npython -c \"print('Hello')\"\nFaire la mÃªme chose mais avec import pandas as pd\nInstaller les packages Ã  partir du requirements.txt. Tester Ã  nouveau import pandas as pd pour comprendre la diffÃ©rence.\nExÃ©cuter pip freeze et comprendre la diffÃ©rence avec la situation prÃ©cÃ©dente.\nVÃ©rifier que le script main.py fonctionne bien. Sinon ajouter les packages manquants dans le requirements.txt et reprendre de maniÃ¨re itÃ©rative Ã  partir de la question 7\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier Ã  Git\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli11a\n\n\n\n\n\nLes environnements conda sont plus lourds Ã  mettre en oeuvre que les environnements virtuels mais peuvent permettre un contrÃ´le plus formel des dÃ©pendances.\nconda est Ã  la fois un gestionnaire de packages (alternative Ã  pip) et dâ€™environnements virtuels. Lâ€™inconvÃ©nient de lâ€™utilisation de conda pour gÃ©rer les environnements virtuels est que cet outil est assez lent car lâ€™algorithme de vÃ©rification des conflits de version nâ€™est pas extrÃªmement rapide.\nPour cette raison, nous allons utiliser mamba, un utilitaire de gestion des environnements conda qui est plus rapide.\n\n\n\n\n\n\nApplication 11b: environnement conda\n\n\n\n\nExÃ©cuter conda env export en ligne de commande et observer la (trÃ¨s) longue liste de package\nTester lâ€™utilisation dâ€™un package quâ€™on nâ€™utilise pas dans notre chaine de production, par exemple seaborn:\npython -c \"import seaborn as sns\"\nCrÃ©er un environnement titanic avec mamba create en listant les packages que vous aviez mis dans le requirements.txt et en ajoutant lâ€™option -c conda-forge Ã  la fin pour utiliser la conda forge\nActiver lâ€™environnement et vÃ©rifier lâ€™installation de Python maintenant utilisÃ©e par votre machine \n(optionnel) Utiliser ls dans le dossier parent de Python pour observer et comprendre le contenu de celui-ci\nVÃ©rifier que cette fois seaborn nâ€™est pas installÃ© dans lâ€™environnement :\npython -c \"import seaborn as sns\"\nExÃ©cuter Ã  nouveau conda env export et comprendre la diffÃ©rence avec la situation prÃ©cÃ©dente12.\nVÃ©rifier que le script main.py fonctionne bien. Sinon utiliser mamba install avec les packages manquants jusquâ€™Ã  ce que la chaine de production fonctionne\nCrÃ©er le fichier environment.yml Ã  partir de conda env export:\nconda env export > environment.yml\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier Ã  Git\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli11b"
  },
  {
    "objectID": "chapters/application.html#etape-2-construire-lenvironnement-de-notre-application-via-un-script-shell",
    "href": "chapters/application.html#etape-2-construire-lenvironnement-de-notre-application-via-un-script-shell",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 2: construire lâ€™environnement de notre application via un script shell",
    "text": "Etape 2: construire lâ€™environnement de notre application via un script shell\nLes environnements virtuels permettent de mieux spÃ©cifier les dÃ©pendances de notre projet, mais ne permettent pas de garantir une portabilitÃ© optimale. Pour cela, il faut recourir Ã  la technologie des conteneurs. Lâ€™idÃ©e est de construire une machine, en partant dâ€™une base quasi-vierge, qui permette de construire Ã©tape par Ã©tape lâ€™environnement nÃ©cessaire au bon fonctionnement de notre projet. Câ€™est le principe des conteneurs Docker.\nLeur mÃ©thode de construction Ã©tant un peu difficile Ã  prendre en main au dÃ©but, nous allons passer par une Ã©tape intermÃ©diaire afin de bien comprendre le processus de production.\n\nNous allons dâ€™abord crÃ©er un script shell, câ€™est Ã  dire une suite de commandes Linux permettant de construire lâ€™environnement Ã  partir dâ€™une machine vierge ;\nNous transformerons celui-ci en Dockerfile dans un deuxiÃ¨me temps. Câ€™est lâ€™objet de lâ€™Ã©tape suivante.\n\n\nEnvironnement virtuelEnvironnement conda\n\n\n\n\n\n\n\n\nApplication 12a : crÃ©er un fichier dâ€™installation de A Ã  Z\n\n\n\n\nCrÃ©er un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le dÃ©pÃ´t\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11a avec git checkout appli11a\nVia lâ€™explorateur de fichiers, crÃ©er le fichier install.sh Ã  la racine du projet avec le contenu suivant:\n\n\n\nScript Ã  crÃ©er sous le nom install.sh\n\n#!/bin/bash\n\n# Install Python\napt-get -y update\napt-get install -y python3-pip python3-venv\n\n# Create empty virtual environment\npython3 -m venv titanic\nsource titanic/bin/activate\n\n# Install project dependencies\npip install -r requirements.txt\n\n\nChanger les permissions sur le script pour le rendre exÃ©cutable\n\nchmod +x install.sh\n\nExÃ©cuter le script depuis la ligne de commande avec des droits de super-utilisateur (nÃ©cessaires pour installer des packages via apt)\n\nsudo ./install.sh\n\nVÃ©rifier que le script main.py fonctionne correctement dans lâ€™environnement virtuel crÃ©Ã©\n\nsource titanic/bin/activate\npython3 main.py\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli12a\n\n\n\n\n\n\n\n\n\n\n\nApplication 12b : crÃ©er un fichier dâ€™installation de A Ã  Z\n\n\n\n\nCrÃ©er un service ubuntu sur le SSP Cloud en cliquant sur ce lien\nCloner le dÃ©pÃ´t et se placer au niveau du checkpoint 11b avec git checkout appli11b\nSe placer dans le dossier du projet avec cd\nOn va se placer en super-utilisateur dans la ligne de commande en tapant\n\nsudo bash\n\nCrÃ©er le fichier install.sh avec le contenu suivant:\n\n\n\nScript Ã  crÃ©er sous le nom install.sh\n\napt-get -y update && apt-get -y install wget\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \\\n    rm -f Miniconda3-latest-Linux-x86_64.sh\n\nPATH=\"/miniconda/bin:${PATH}\"\n\n# Create environment\nconda install mamba -c conda-forge\nmamba create -n titanic pandas PyYAML scikit-learn -c conda-forge\nmamba activate titanic\n\nPATH=\"/miniconda/envs/titanic/bin:${PATH}\"\n\npython main.py\n\n\nChanger les permissions sur le fichier\n\nchmod u+x ./install.sh\n\nExÃ©cuter le script depuis la ligne de commande\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli12b"
  },
  {
    "objectID": "chapters/application.html#docker",
    "href": "chapters/application.html#docker",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 3: conteneuriser lâ€™application avec Docker",
    "text": "Etape 3: conteneuriser lâ€™application avec Docker\n\n\n\n\n\n\nNote\n\n\n\nCette application nÃ©cessite lâ€™accÃ¨s Ã  une version interactive de Docker. Il nâ€™y a pas beaucoup dâ€™instances en ligne disponibles.\nNous proposons deux solutions:\n\nInstaller Docker sur sa machine ;\nSe rendre sur lâ€™environnement bac Ã  sable Play with Docker\n\n\n\nMaintenant quâ€™on sait que ce script prÃ©paratoire fonctionne, on va le transformer en Dockerfile (la syntaxe Docker est lÃ©gÃ¨rement diffÃ©rente de la syntaxe Linux classique). Puis on va le tester dans un environnement bac Ã  sable (pour ensuite pouvoir plus facilement automatiser la construction de lâ€™image Docker par la suite).\n\n\n\n\n\n\nApplication 13: crÃ©ation de lâ€™image Docker\n\n\n\nSe placer dans un environnement avec Docker\n\nDans le terminal Linux, cloner votre dÃ©pÃ´t Github\nCrÃ©er via la ligne de commande un fichier texte vierge nommÃ© Dockerfile (la majuscule au dÃ©but du mot est importante)\n\n\n\nCommande pour crÃ©er un Dockerfile vierge depuis la ligne de commande\n\ntouch Dockerfile\n\n\nOuvrir ce fichier via un Ã©diteur de texte et copier le contenu suivant dedans:\n\n\n\nPremier Dockerfile\n\nFROM ubuntu:22.04\n\nWORKDIR ${HOME}/titanic\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCMD [\"python3\", \"main.py\"]\n\n\nConstruire lâ€™image\nLe Dockerfile est la recette de construction de lâ€™image. La construction effective de lâ€™image Ã  partir de cette recette sâ€™appelle lâ€™Ã©tape de build.\n\nUtiliser docker build pour crÃ©er une image avec le tag my-python-app\n\ndocker build . -t my-python-app\n\nVÃ©rifier les images dont vous disposez. Vous devriez avoir un rÃ©sultat proche de celui-ci :\n\n$ docker images\n\nREPOSITORY      TAG       IMAGE ID       CREATED         SIZE\nmy-python-app   latest    c0dfa42d8520   6 minutes ago   836MB\nubuntu          25.04     825d55fb6340   6 days ago      77.8MB\n\n\nTester lâ€™image: dÃ©couverte du cache\nLâ€™Ã©tape de build a fonctionnÃ©: une image a Ã©tÃ© construite.\nMais fait-elle effectivement ce que lâ€™on attend dâ€™elle ?\nPour le savoir, il faut passer Ã  lâ€™Ã©tape suivante, lâ€™Ã©tape de run.\n$ docker run -it my-python-app\n\npython3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory\nLe message dâ€™erreur est clair : Docker ne sait pas oÃ¹ trouver le fichier main.py. Dâ€™ailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont nÃ©cessaires pour faire tourner le code: config.yaml et le dossier src.\n\nAvant lâ€™Ã©tape CMD, copier les fichiers nÃ©cessaires sur lâ€™image afin que lâ€™application dispose de tous les Ã©lÃ©ments nÃ©cessaires pour Ãªtre en mesure de fonctionner.\n\n\n\nNouveau Dockerfile\n\nFROM ubuntu:22.04\n\nWORKDIR ${HOME}/titanic\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY main.py .\nCOPY src ./src\nCOPY configuration ./configuration\nCMD [\"python3\", \"main.py\"]\n\n\nRefaire tourner lâ€™Ã©tape de build\nRefaire tourner lâ€™Ã©tape de run. A ce stade, la matrice de confusion doit fonctionner ğŸ‰. Vous avez crÃ©Ã© votre premiÃ¨re application reproductible !\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIci, le cache permet dâ€™Ã©conomiser beaucoup de temps. Par besoin de refaire tourner toutes les Ã©tapes, Docker agit de maniÃ¨re intelligente en faisant tourner uniquement les Ã©tapes qui ont changÃ©.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli13"
  },
  {
    "objectID": "chapters/application.html#etape-prÃ©liminaire",
    "href": "chapters/application.html#etape-prÃ©liminaire",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape prÃ©liminaire",
    "text": "Etape prÃ©liminaire\nPour ne pas risquer de tout casser sur notre branche master, nous allons nous placer sur une branche nommÃ©e dev:\n\nsi dans lâ€™Ã©tape suivante vous appliquez la mÃ©thode la plus simple, vous allez pouvoir la crÃ©er depuis lâ€™interface de Github ;\nsi vous utilisez lâ€™autre mÃ©thode, vous allez devoir la crÃ©er en local ( via la commande git checkout -b dev)"
  },
  {
    "objectID": "chapters/application.html#etape-1-mise-en-place-de-tests-automatisÃ©s",
    "href": "chapters/application.html#etape-1-mise-en-place-de-tests-automatisÃ©s",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 1: mise en place de tests automatisÃ©s",
    "text": "Etape 1: mise en place de tests automatisÃ©s\nAvant dâ€™essayer de mettre en oeuvre la crÃ©ation de notre image Docker de maniÃ¨re automatisÃ©e, nous allons prÃ©senter la logique de lâ€™intÃ©gration continue en testant de maniÃ¨re automatisÃ©e notre script main.py.\nPour cela, nous allons partir de la structure proposÃ©e dans lâ€™action officielle. La documentation associÃ©e est ici\n\n\n\n\n\n\nApplication 14: premier script dâ€™intÃ©gration continue\n\n\n\nA partir de lâ€™exemple prÃ©sent dans la documentation officielle de Github, on a dÃ©jÃ  une base de dÃ©part qui peut Ãªtre modifiÃ©e:\n\nCrÃ©er un fichier .github/workflows/test.yml avec le contenu de lâ€™exemple de la documentation\nNâ€™utiliser que les versions 3.9 Ã  3.11 de Python\nUtiliser le fichier requirements.txt pour installer les dÃ©pendances. Ajouter lâ€™argument cache: 'pip' au moment de lâ€™Ã©tape actions/setup-python@v4 (voir documentation) pour mettre en cache les dÃ©pendances afin de gagner du temps sur les installations ultÃ©rieures\nRemplacer russ par pylint pour vÃ©rifier la qualitÃ© du code. Ajouter lâ€™argument --fail-under=6 pour renvoyer une erreur en cas de note trop basse13\nPlutÃ´t que pytest, utiliser python main.py pour tester que la matrice de confusion sâ€™affiche bien.\n\n\n\nFichier .github/workflows/test.yml obtenu\n\nname: Python package\n\non: [push]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.9\", \"3.10\", \"3.11\"]\n\n    steps:\n      - uses: actions/checkout@v3\n      - name: Set up Python ${{ matrix.python-version }}\n        uses: actions/setup-python@v4\n        with:\n          python-version: ${{ matrix.python-version }}\n          cache: 'pip'\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install pylint\n          pip install -r requirements.txt\n      - name: Lint\n        run: |\n          pylint src --fail-under=6\n      - name: Test workflow\n        run: |\n          python main.py\n\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli14\n\n\n\nMaintenant, nous pouvons observer que lâ€™onglet Actions sâ€™est enrichi. Chaque commit va entraÃ®ner une action pour tester nos scripts.\nSi la note est mauvaise, nous aurons une croix rouge (et nous recevrons un mail). On pourra ainsi dÃ©tecter, en dÃ©veloppant son projet, les moments oÃ¹ on dÃ©grade la qualitÃ© du script afin de la rÃ©tablir immÃ©diatemment."
  },
  {
    "objectID": "chapters/application.html#etape-2-automatisation-de-la-livraison-de-limage-docker",
    "href": "chapters/application.html#etape-2-automatisation-de-la-livraison-de-limage-docker",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 2: Automatisation de la livraison de lâ€™image Docker",
    "text": "Etape 2: Automatisation de la livraison de lâ€™image Docker\nMaintenant, nous allons automatiser la mise Ã  disposition de notre image sur DockerHub. Cela facilitera sa rÃ©utilisation mais aussi des valorisations ultÃ©rieures.\nLÃ  encore, nous allons utiliser une sÃ©rie dâ€™actions prÃ©-configurÃ©es.\nPour que Github puisse sâ€™authentifier auprÃ¨s de DockerHub, il va falloir dâ€™abord interfacer les deux plateformes. Pour cela, nous allons utiliser un jeton (token) DockerHub que nous allons mettre dans un espace sÃ©curisÃ© associÃ© Ã  votre dÃ©pÃ´t Github.\n\n\n\n\n\n\nApplication 15a: configuration\n\n\n\n\nSe rendre sur https://hub.docker.com/ et crÃ©er un compte.\nCrÃ©er un dÃ©pÃ´t public application-correction\nAller dans les paramÃ¨tres (https://hub.docker.com/settings/general) et cliquer, Ã  gauche, sur Security\nCrÃ©er un jeton personnel dâ€™accÃ¨s, ne fermez pas lâ€™onglet en question, vous ne pouvez voir sa valeur quâ€™une fois.\nDans votre dÃ©pÃ´t Github, cliquer sur lâ€™onglet Settings et cliquer, Ã  gauche, sur Actions. Sur la page qui sâ€™affiche, cliquer sur New repository secret\nDonner le nom DOCKERHUB_TOKEN Ã  ce jeton et copier la valeur. Valider\nCrÃ©er un deuxiÃ¨me secret nommÃ© DOCKERHUB_USERNAME ayant comme valeur le nom dâ€™utilisateur que vous avez crÃ©Ã© sur Dockerhub\n\n\n\nA ce stade, nous avons donnÃ© les moyens Ã  Github de sâ€™authentifier avec notre identitÃ© sur Dockerhub. Il nous reste Ã  mettre en oeuvre lâ€™action en sâ€™inspirant de https://github.com/docker/build-push-action/#usage. On ne va modifier que trois Ã©lÃ©ments dans ce fichier. Effectuer les actions suivantes:\n\n\n\n\n\n\nApplication 15b: automatisation de lâ€™image Docker\n\n\n\n\nCrÃ©er depuis VSCode un fichier .github/workflows/docker.yml et coller le contenu du template dedans ;\nChanger le tag Ã  la fin pour mettre <username>/application-correction:latest oÃ¹ username est le nom dâ€™utilisateur sur DockerHub;\nChanger le nom en un titre plus signifiant (par exemple â€œProduction de lâ€™image Dockerâ€) ;\nSi besoin, changer les branches sur lesquelles peut tourner le pipeline ;\nFaire un commit et un push de ces fichiers\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nComme on est fier de notre travail, on va afficher Ã§a avec un badge sur le README.\n\nApplication 15c: Afficher un badge dans le README\n\nSe rendre dans lâ€™onglet Actions et cliquer sur un des scripts en train de tourner.\nEn haut Ã  droite, cliquer sur ...\nSÃ©lectionner Create status badge\nRÃ©cupÃ©rer le code Markdown proposÃ©\nCopier dans le README depuis VSCode\nFaire de mÃªme pour lâ€™autre workflow\n\n\n\n\nMaintenant, il nous reste Ã  tester notre application dans lâ€™espace bac Ã  sable ou en local, si Docker est installÃ©.\n\n\n\n\n\n\nApplication 15d: Tester lâ€™application\n\n\n\n\nSe rendre sur lâ€™environnement bac Ã  sable Play with Docker\nRÃ©cupÃ©rer lâ€™image :\n\ndocker image build <username_dockerhub>/application-correction\n\nTester lâ€™image avec run\n\ndocker run -it <username_dockerhub>/application-correction\nğŸ‰ La matrice de confusion doit sâ€™afficher ! Vous avez grandement facilitÃ© la rÃ©utilisation de votre image.\n\n\n\n\n\n\n\n\nCheckpoint\n\n\n\n\n\ngit checkout appli15"
  },
  {
    "objectID": "chapters/application.html#application-15c-afficher-un-badge-dans-le-readme",
    "href": "chapters/application.html#application-15c-afficher-un-badge-dans-le-readme",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Application 15c: Afficher un badge dans le README",
    "text": "Application 15c: Afficher un badge dans le README\n\nSe rendre dans lâ€™onglet Actions et cliquer sur un des scripts en train de tourner.\nEn haut Ã  droite, cliquer sur ...\nSÃ©lectionner Create status badge\nRÃ©cupÃ©rer le code Markdown proposÃ©\nCopier dans le README depuis VSCode\nFaire de mÃªme pour lâ€™autre workflow"
  },
  {
    "objectID": "chapters/application.html#etape-3-crÃ©ation-dun-rapport-automatique",
    "href": "chapters/application.html#etape-3-crÃ©ation-dun-rapport-automatique",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 3: crÃ©ation dâ€™un rapport automatique",
    "text": "Etape 3: crÃ©ation dâ€™un rapport automatique\nMaintenant, nous allons crÃ©er et dÃ©ployer un site web pour valoriser notre travail. Cela va impliquer trois Ã©tapes:\n\nTester en local le logiciel quarto et crÃ©er un rapport minimal qui sera compilÃ© par quarto ;\nEnrichir lâ€™image docker avec le logiciel quarto ;\nCompiler le document en utilisant cette image sur les serveurs de Github ;\nDÃ©ployer ce rapport minimal pour le rendre disponible Ã  tous sur le web.\n\nLe but est de proposer un rapport minimal qui illustre la performance du modÃ¨le est la feature importance. Pour ce dernier Ã©lÃ©ment, le rapport qui sera proposÃ© utilise shap qui est une librairie dÃ©diÃ©e Ã  lâ€™interprÃ©tabilitÃ© des modÃ¨les de machine learning\n\n1. Rapport minimal en local\n1ï¸âƒ£ La premiÃ¨re Ã©tape consiste Ã  installer quarto sur notre machine Linux sur laquelle tourne VSCode:\n\nDans un terminal, installer quarto avec les commandes suivantes:\n\nQUARTO_VERSION=\"0.9.287\"\nwget \"https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb\"\nsudo apt install \"./quarto-${QUARTO_VERSION}-linux-amd64.deb\"\n\nSâ€™assurer quâ€™on travaille bien depuis lâ€™environnement conda monenv. Sinon lâ€™activer\n\n2ï¸âƒ£ Il va Ãªtre nÃ©cessaire dâ€™enrichir lâ€™environnement conda. Certaines dÃ©pendances sont nÃ©cessaires pour que quarto fonctionne bien avec Python (jupyter, nbclientâ€¦) alors que dâ€™autres ne sont nÃ©cessaires que parce quâ€™ils sont utilisÃ©s dans le document (seaborn, shapâ€¦). Changer la section dependencies avec la liste suivante:\ndependencies:\n  - python=3.10.0\n  - ipykernel==6.13.0\n  - jupyter==1.0.0\n  - matplotlib==3.5.1\n  - nbconvert==6.5.0\n  - nbclient==0.6.0\n  - nbformat==5.3.0\n  - pandas==1.4.1\n  - PyYAML==6.0\n  - s3fs==2022.2.0\n  - scikit-learn==1.0.2\n  - seaborn==0.11.2\n  - shap==0.40.0\n\nthree: CrÃ©er un fichier nommÃ© report.qmd\n\n\n~markdown\n\n\n\n\ntitle: â€œComprendre les facteurs de survie sur le Titanicâ€\n\n\nsubtitle: â€œUn rapport innovantâ€\n\n\nformat:\n\n\nhtml:\n\n\nself-contained: true\n\n\nipynb: default\n\n\njupyter: python3\n\n\n\nVoici un rapport prÃ©sentant quelques intuitions issues dâ€™un modÃ¨le random forest sur le jeu de donnÃ©es Titanic entraÃ®nÃ© et dÃ©ployÃ© de maniÃ¨re automatique.\nIl est possible de tÃ©lÃ©charger cette page sous format Jupyter Notebook ici\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nimport main\nX_train = main.X_train\ny_train = main.y_train\ntraining_data = main.training_data\nrdmf = RandomForestClassifier(n_estimators=20)\nrdmf.fit(X_train, y_train)"
  },
  {
    "objectID": "chapters/application.html#etape-4-dÃ©ploiement-de-ce-rapport-automatique-sur-le-web",
    "href": "chapters/application.html#etape-4-dÃ©ploiement-de-ce-rapport-automatique-sur-le-web",
    "title": "Appliquer les concepts Ã©tudiÃ©s Ã  un projet de data science",
    "section": "Etape 4: DÃ©ploiement de ce rapport automatique sur le web",
    "text": "Etape 4: DÃ©ploiement de ce rapport automatique sur le web\n1ï¸âƒ£ Dans un premier temps, nous allons connecter notre dÃ©pÃ´t Github au service tiers Netlify\n\nAller sur https://www.netlify.com/ et faire Sign up (utiliser son compte Github)\nDans la page dâ€™accueil de votre profil, vous pouvez cliquer sur Add new site > Import an existing project\nCliquer sur Github. Sâ€™il y a des autorisations Ã  donner, les accorder. Rechercher votre projet dans la liste de vos projets Github\nCliquer sur le nom du projet et laisser les paramÃ¨tres par dÃ©faut (nous allons modifier par la suite)\nCliquer sur Deploy site\n\n2ï¸âƒ£ A ce stade, votre dÃ©ploiement devrait Ã©chouer. Câ€™est normal, vous essayez de dÃ©ployer depuis master qui ne comporte pas de html. Mais le rapport nâ€™est pas non plus prÃ©sent dans la branche dev. En fait, aucune branche ne comporte le rapport: celui-ci est gÃ©nÃ©rÃ© dans votre pipeline mais nâ€™est jamais prÃ©sent dans le dÃ©pÃ´t car il sâ€™agit dâ€™un output. On va dÃ©sactiver le dÃ©ploiement automatique pour privilÃ©gier un dÃ©ploiement depuis Github Actions:\n\nAller dans Site Settings puis, Ã  gauche, cliquer sur Build and Deploy\nDans la section Build settings, cliquer sur Stop builds et valider\n\nOn vient de dÃ©sactiver le dÃ©ploiement automatique par dÃ©faut. On va faire communiquer notre dÃ©pÃ´t Github et Netlify par le biais de lâ€™intÃ©gration continue.\n3ï¸âƒ£ Pour cela, il faut crÃ©er un jeton Netlify pour que les serveurs de Github, lorsquâ€™ils disposent dâ€™un rapport, puissent lâ€™envoyer Ã  Netlify pour la mise sur le web. Il va Ãªtre nÃ©cessaire de crÃ©er deux variables dâ€™environnement pour connecter Github et Netlify: lâ€™identifiant du site et le token\n\nPour le token :\n\nCrÃ©er un jeton en cliquant, en haut Ã  droite, sur lâ€™icone de votre profil. Aller dans User settings. A gauche, cliquer sur Applications et crÃ©er un jeton personnel dâ€™accÃ¨s avec un nom signifiant (par exemple PAT_ENSAE_reproductibilite)\nMettre de cÃ´tÃ© (conseil : garder lâ€™onglet ouvert)\n\nPour lâ€™identifiant du site:\n\ncliquer sur Site Settings dans les onglets en haut\nGarder lâ€™onglet ouvert pour copier la valeur quand nÃ©cessaire\n\nIl est maintenant nÃ©cessaire dâ€™aller dans le dÃ©pÃ´t Github et de crÃ©er les secrets (Settings > Secrets > Actions):\n\nCrÃ©er le secret NETLIFY_AUTH_TOKEN en collant la valeur du jeton dâ€™authentification Netlify\nCrÃ©er le secret NETLIFY_SITE_ID en collant lâ€™identifiant du site\n\n\n4ï¸âƒ£ Nous avons effectuÃ© toutes les configurations nÃ©cessaires. On va maintenant mettre Ã  jour lâ€™intÃ©gration continue afin de mettre Ã  disposition sur le web notre rapport. On va utiliser lâ€™interface en ligne de commande (CLI) de Netlify. Celle-ci attend que le site web se trouve dans un dossier public et que la page dâ€™accueil soit nommÃ©e index.html:\n\n{{% panel name=â€œVision dâ€™ensembleâ€ %}}\n\nune installation de npm\nune Ã©tape de dÃ©ploiement via la CLI de netlify\n\n- name: Install npm\n  uses: actions/setup-node@v2\n  with:\n    node-version: '14'\n- name: Deploy to Netlify\n  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets\n  env:\n    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n  run: |\n    mkdir -p public\n    mv report.html public/index.html\n    mv report.ipynb public/report.ipynb\n    npm install --unsafe-perm=true netlify-cli -g\n    netlify init\n    netlify deploy --prod --dir=\"public\" --message \"Deploy master\"\n{{% /panel %}}\n{{% panel name=â€œDÃ©tails npmâ€ %}}\n\n\nname: Install npm uses: actions/setup-node@v2 with: node-version: â€˜14â€™\n\n\nnpm est le gestionnaire de paquet de JS. Il est nÃ©cessaire de le configurer, ce qui est fait automatiquement grÃ¢ce Ã  lâ€™action actions/setup-node@v2\n{{% /panel %}}\n{{% panel name=â€œDÃ©tails Netlify CLIâ€ %}}\n\nOn rappelle Ã  Github Actions nos paramÃ¨tres dâ€™authentification sous forme de variables dâ€™environnement. Cela permet de les garder secrÃ¨tes\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repoâ€™s secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install â€“unsafe-perm=true netlify-cli -g netlify init netlify deploy â€“prod â€“dir=â€œpublicâ€ â€“message â€œDeploy masterâ€\n\n\n\nOn dÃ©place les rapports de la racine vers le dossier public\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repoâ€™s secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install â€“unsafe-perm=true netlify-cli -g netlify init netlify deploy â€“prod â€“dir=â€œpublicâ€ â€“message â€œDeploy masterâ€\n\n\n\nOn installe et initialise Netlify\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repoâ€™s secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install â€“unsafe-perm=true netlify-cli -g netlify init netlify deploy â€“prod â€“dir=â€œpublicâ€ â€“message â€œDeploy masterâ€\n\n\n\nOn dÃ©ploie sur lâ€™url par dÃ©faut (-- prod) depuis le dossier public\n\n\n\nname: Deploy to Netlify # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repoâ€™s secrets env: NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }} NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }} run: | mkdir -p public mv report.html public/index.html mv report.ipynb public/report.ipynb npm install â€“unsafe-perm=true netlify-cli -g netlify init netlify deploy â€“prod â€“dir=â€œpublicâ€ â€“message â€œDeploy masterâ€\n\n\n{{% /panel %}}\n\nAu bout de quelques minutes, le rapport est disponible en ligne sur lâ€™URL Netlify (par exemple https://spiffy-florentine-c913b9.netlify.app)"
  },
  {
    "objectID": "chapters/code-quality.html",
    "href": "chapters/code-quality.html",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "",
    "text": "â€œThe code is read much more often than it is written.â€\nGuido Van Rossum1\n\nLorsque lâ€™on sâ€™initie Ã  la pratique de la data science, il est assez naturel de voir le code dâ€™une maniÃ¨re trÃ¨s fonctionnelle : je veux rÃ©aliser une tÃ¢che donnÃ©e â€” par exemple un algorithme de classification â€” et je vais donc assembler dans un notebook des bouts de code, souvent trouvÃ©s sur internet, jusquâ€™Ã  obtenir un projet qui rÃ©alise la tÃ¢che voulue. La structure du projet importe assez peu, tant quâ€™elle permet dâ€™importer correctement les donnÃ©es nÃ©cessaires Ã  la tÃ¢che en question.\nSi cette approche flexible et minimaliste fonctionne trÃ¨s bien lors de la phase dâ€™apprentissage, il est malgrÃ© tout indispensable de sâ€™en dÃ©tacher progressivement Ã  mesure que lâ€™on progresse et que lâ€™on peut Ãªtre amenÃ© Ã  rÃ©aliser des projets plus professionnels ou bien Ã  intÃ©grer des projets collaboratifs.\nEn particulier, il est important de proposer, parmi les multiples maniÃ¨res de rÃ©soudre un problÃ¨me informatique, une solution qui soit intelligible par dâ€™autres personnes parlant le langage. Le code est en effet lu bien plus souvent quâ€™il nâ€™est Ã©crit, câ€™est donc avant tout un outil de communication. De mÃªme, la maintenance dâ€™un code demande gÃ©nÃ©ralement beaucoup plus de moyens que sa phase de dÃ©veloppement initial, il est donc important de penser en amont la qualitÃ© de son code et la structure de son projet de sorte Ã  le rendre au maximum maintenable dans le temps.\nAfin de faciliter la communication et rÃ©duire la douleur dâ€™avoir Ã  faire Ã©voluer un code obscur, des tentatives plus ou moins institutionnalisÃ©es de dÃ©finir des conventions ont Ã©mergÃ©. Ces conventions dÃ©pendent naturellement du langage utilisÃ©, mais les principes sous-jacents sâ€™appliquent de maniÃ¨re universelle Ã  tout projet basÃ© sur du code.\n\n\n\nPython est un langage trÃ¨s lisible. Avec un peu dâ€™effort sur le nom des objets, sur la gestion des dÃ©pendances et sur la structure du programme, on peut trÃ¨s bien comprendre un script sans avoir besoin de lâ€™exÃ©cuter. Câ€™est lâ€™une des principales forces du langage Python qui permet ainsi une acquisition rapide des bases et facilite lâ€™appropriation dâ€™un script.\nLa communautÃ© Python a abouti Ã  un certain nombre de normes, dites PEP (Python Enhancement Proposal), qui constituent un standard dans lâ€™Ã©cosystÃ¨me Python. Les deux normes les plus connues sont :\n\nla norme PEP8 qui dÃ©finit un certain nombre de conventions relatives au code\nla norme PEP257 consacrÃ©e Ã  la documentation (docstrings).\n\n\n\n\n\n\n\nNote\n\n\n\nDans lâ€™univers R, la formalisation a Ã©tÃ© moins organisÃ©e. Ce langage est plus permissif que Python sur certains aspects2. NÃ©anmoins, des standards ont Ã©mergÃ©, Ã  travers un certain nombre de style guides dont les plus connus sont le tidyverse style guide et le google style guide3 (voir ce post qui pointe vers un certain nombre de ressources sur le sujet).\n\n\nCes conventions sont arbitraires, dans une certaine mesure. Il est tout Ã  fait possible de trouver certaines conventions moins esthÃ©tiques que dâ€™autres.\nCes conventions ne sont pas non plus immuables: les langages et leurs usages Ã©voluent, ce qui nÃ©cessite de mettre Ã  jour les conventions. Cependant, adopter dans la mesure du possible certains des rÃ©flexes prÃ©conisÃ©s par ces conventions devrait amÃ©liorer la capacitÃ© Ã  Ãªtre compris par la communautÃ©, augmenter les chances de bÃ©nÃ©ficier dâ€™apport de celle-ci pour adapter le code mais aussi rÃ©duire la difficultÃ© Ã  faire Ã©voluer un code.\nIl existe beaucoup de philosophies diffÃ©rentes sur le style de codage et, en fait, le plus important est la cohÃ©rence : si on choisit une convention, par exemple snake case plutÃ´t que camel case, le mieux est de sâ€™y tenir.\nLes conventions vont au-delÃ  de la syntaxe. Un certain nombre de standards dâ€™organisation dâ€™un projet ont Ã©mergÃ©, qui seront abordÃ©es dans le prochain chapitre."
  },
  {
    "objectID": "chapters/code-quality.html#lisibilite",
    "href": "chapters/code-quality.html#lisibilite",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "LisibilitÃ©",
    "text": "LisibilitÃ©\nUn code Ã©crit avec des noms de variables et de fonctions explicites est autant, voire plus, informatif que les commentaires qui lâ€™accompagnent4. Câ€™est pourquoi il est essentiel de respecter des conventions pour le choix des noms des objets afin dâ€™assurer la lisibilitÃ© des programmes.\nUn certain nombre de conseils sont prÃ©sents dans le Hitchhikerâ€™s Guide to Python qui vise Ã  faire connaÃ®tre les prÃ©ceptes du â€œZen of Pythonâ€ (PEP 20). Ce post de blog illustre quelques uns de ces principes avec des exemples. Vous pouvez retrouver ces conseils dans Python en tapant le code suivant:\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\nIl est conseillÃ© de suivre ces deux principes lorsquâ€™on commence Ã  programmer des fonctions (ce qui, comme cela est Ã©voquÃ© par la suite, est toujours recommandÃ©).\n\nFaire attention au type dâ€™objet renvoyÃ© par Python. Ce langage ne propose pas de typage fort, il est donc possible quâ€™une fonction renvoie des objets de nature diffÃ©rente selon les cas5. Cela peut amener Ã  des surprises lorsquâ€™on utilise une telle fonction dans un code. Il est recommandÃ© dâ€™Ã©viter ce comportement en proposant des fonctions diffÃ©rentes si lâ€™output dâ€™une fonction est de nature diffÃ©rente. Ce principe de prÃ©caution (mais aussi dâ€™information) renvoie au paradigme de la programmation dÃ©fensive.\n\n\nPrivilÃ©gier la programmation orientÃ©e objet lorsquâ€™une fonction doit sâ€™adapter au type dâ€™objet en entrÃ©e (par exemple aller chercher des Ã©lÃ©ments diffÃ©rents pour un objet lm ou un objet glm). Cela Ã©vite les codes spaghetti ğŸ inutilement complexes qui sont impossibles Ã  dÃ©bugger.\n\n\n\n\n\n\n\nType hinting\n\n\n\nPython propose une fonctionalitÃ© assez plaisante qui est le type hinting (doc officielle et tutoriel sur realpython.com).\nCette approche permet dâ€™indiquer le type dâ€™argument attendu par une fonction et celui qui sera renvoyÃ© par la fonction. Par exemple, la personne ayant Ã©crit la fonction suivante\ndef calcul_moyenne(df: pd.DataFrame, col : str = \"y\") -> pd.DataFrame:\n    return df[col].mean()\npropose dâ€™utiliser deux types dâ€™inputs (un DataFrame Pandas et une chaine de caractÃ¨re) et indique quâ€™elle renverra un DataFrame Pandas. A noter que câ€™est indicatif, non contraignant. En effet, le code ci-dessus fonctionnera si on fournit en argument col une liste puisque Pandas sait gÃ©rer cela Ã  lâ€™Ã©tape df[col].mean().\nLe type hinting est un Ã©lÃ©ment dâ€™autodocumentation puisque grÃ¢ce Ã  ces hints le code suffit Ã  faire comprendre la volontÃ© de la personne lâ€™ayant Ã©crit.\n\n\n\n\n\n\n\n\nLe code spaghetti\n\n\n\nLe code spaghetti est un style dâ€™Ã©criture qui favorise lâ€™apparition du syndrome du plat de spaghettis : un code impossible Ã  dÃ©mÃ©ler parce quâ€™il fait un usage excessif de conditions, dâ€™exceptions en tous sens, de gestion des Ã©vÃ©nements complexes. Il devient quasi-impossible de savoir quelles ont Ã©tÃ© les conditions Ã  lâ€™origine de telle ou telle erreur sans exÃ©cuter ligne Ã  ligne (et celles-ci sont excessivement nombreuses du fait de mauvaises pratiques de programmation) le programme.\nEn fait, la programmation spaghetti qualifie tout ce qui ne permet pas de dÃ©terminer le qui, le quoi et le comment. Le code est donc plus long Ã  mettre Ã  jour car cela nÃ©cessite de remonter un Ã  un le fil des renvois."
  },
  {
    "objectID": "chapters/code-quality.html#concision",
    "href": "chapters/code-quality.html#concision",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "Concision",
    "text": "Concision\nUn code reproductible doit pouvoir sâ€™exÃ©cuter de maniÃ¨re linÃ©aire. Sâ€™il provoque une erreur, il est important de pouvoir identifier lâ€™instruction responsable pour pouvoir debugger. Comme une dÃ©monstration mathÃ©matique, un code intelligible doit viser la concision et la simplicitÃ©. Les codes trÃ¨s longs sont souvent signes de rÃ©pÃ©titions et sont difficiles Ã  dÃ©bugger.\nLes scripts trop longs ne sont pas une bonne pratique. Il est prÃ©fÃ©rable de diviser lâ€™ensemble des scripts exÃ©cutant une chaÃ®ne de production en â€œmonadesâ€, câ€™est-Ã -dire en petites unitÃ©s cohÃ©rentes. Les fonctions sont un outil privilÃ©giÃ© pour cela (en plus de limiter la redondance, et dâ€™Ãªtre un outil privilÃ©giÃ© pour documenter un code).\n\n\n\n\n\n\nExemple: privilÃ©gier les list comprehensions\n\n\n\n\n\nEn Python, il est recommandÃ© de privilÃ©gier les list comprehensions Ã  lâ€™utilisation de boucles for indentÃ©es. Ces derniÃ¨res sont en gÃ©nÃ©ral moins efficaces et surtout impliquent un nombre important de ligne de codes lÃ  oÃ¹ les comprÃ©hensions de listes sont beaucoup plus concises\nliste_nombres = range(10)\n\n# trÃ¨s mauvais\ny = []\nfor x in liste_nombres:\n    if x % 2 == 0:\n        y.append(x*x)\n\n# mieux\ny = [x*x for x in liste_nombres if x % 2 == 0]\n\n\n\n\n\n\n\n\n\nRÃ¨gle dâ€™or\n\n\n\nIl faut utiliser une fonction dÃ¨s quâ€™on utilise une mÃªme portion de code plus de deux fois (donâ€™t repeat yourself (DRY))\n\n\nLes fonctions ont de nombreux avantages par rapport Ã  de longs scripts:\n\nLimite les risques dâ€™erreurs liÃ©s aux copier/coller\nRend le code plus lisible et plus compact\nUn seul endroit du code Ã  modifier lorsquâ€™on souhaite modifier le traitement\nFacilite la rÃ©utilisation et la documentation du code !\n\n\n\n\n\n\n\nRÃ¨gles pour Ã©crire des fonctions pertinentes\n\n\n\n\nUne tÃ¢che = une fonction\nUne tÃ¢che complexe = un enchaÃ®nement de fonctions rÃ©alisant chacune une tÃ¢che simple\nLimiter lâ€™utilisation de variables globales."
  },
  {
    "objectID": "chapters/code-quality.html#coherence",
    "href": "chapters/code-quality.html#coherence",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "CohÃ©rence du script",
    "text": "CohÃ©rence du script\nLister les dÃ©pendances est important:\n\npour des raisons techniques: le logiciel doit savoir oÃ¹ aller chercher les fonctions utilisÃ©es dans un script pour avoir un code fonctionnel ;\npour des raisons conventionnelles: les utilisateurs doivent comprendre les dÃ©pendances Ã  installer pour Ãªtre en mesure de rÃ©utiliser le code.\n\nLes imports se mettent conventionnellement en dÃ©but de script, quâ€™il sâ€™agisse dâ€™import de packages dans leur ensemble ou seulement de certaines fonctions:\nimport pandas as pd\nfrom sklearn.model_selection import cross_val_score\nDans le premier cas, on fait ensuite rÃ©fÃ©rence aux fonctions en les faisant prÃ©cÃ©der du nom du package :\npd.DataFrame([0,1])\nCela permet de dire Ã  Python dâ€™aller chercher dans le namespace pd (alias pour pandas qui est lui-mÃªme un ensemble de scripts enregistrÃ©s sur le disque) la fonction DataFrame.\n\n\n\n\n\n\nQuelques conseils complÃ©mentaires.\n\n\n\n\n\nEn premier lieu, il convient dâ€™adopter les mÃªmes standards que la communautÃ© pour les noms de package.\n# bien\nimport numpy as np\n\n# trompeur\nimport numpy as pd\nIl faut Ã©galement faire attention aux namespaces pour Ã©viter les conflits entre fonctions. Cela implique de ne pas importer lâ€™ensemble des fonctions dâ€™un package de la maniÃ¨re suivante:\nfrom numpy import *\nfrom math import *\nDans ce cas, on va se retrouver avec des conflits potentiels entre les fonctions du package numpy et du package math qui portent le mÃªme nom (floor par exemple).\n\n\n\nEn ce qui concerne lâ€™installation des packages, nous allons voir dans les parties Structure de code et PortabilitÃ© quâ€™il ne faut pas gÃ©rer ceci dans le script mais dans un Ã©lÃ©ment Ã  part, relatif Ã  lâ€™environnement dâ€™exÃ©cution du projet6."
  },
  {
    "objectID": "chapters/code-quality.html#redondance",
    "href": "chapters/code-quality.html#redondance",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "Limiter la redondance",
    "text": "Limiter la redondance\nUn bon principe Ã  suivre est â€œdonâ€™t repeat yourself !â€ (DRY). Celui-ci rÃ©duit la charge de code Ã  Ã©crire, Ã  comprendre et Ã  tenir Ã  jour.\n\nCe post donne quelques bonnes pratiques pour rÃ©duire la redondance des codes.\n\n\n\n\n\n\nUn exemple progressif pour comprendre\n\n\n\n\n\nğŸ’¡ Supposons quâ€™on dispose dâ€™une table de donnÃ©es qui utilise le code âˆ’99 pour reprÃ©senter les valeurs manquantes. On dÃ©sire remplacer lâ€™ensemble des âˆ’99 par des NA.\nVoici un code Python qui permet de se placer dans ce cas qui, malheureusement, arrive frÃ©quemment.\n# On fixe la racine pour Ãªtre sÃ»r de tous avoir le mÃªme dataset\nnp.random.seed(1234)\n\n# On crÃ©Ã© un dataframe\na = np.random.randint(1, 10, size = (5,6))\ndf = np.insert(\n    a,\n    np.random.choice(len(a), size=6),\n    -99,\n)\ndf = pd.DataFrame(df.reshape((6,6)), columns=[chr(x) for x in range(97, 103)])\nUn premier jet de code pourrait prendre la forme suivante:\n# Dupliquer les donnÃ©es\ndf2 = df.copy()\n# Remplacer les -99 par des NA\ndf2.loc[df2['a'] == -99,'a'] = np.nan\ndf2.loc[df2['b'] == -99,'b'] = np.nan\ndf2.loc[df2['c'] == -99,'c'] = np.nan\ndf2.loc[df2['d'] == -99,'d'] = np.nan\ndf2.loc[df2['e'] == -98,'e'] = np.nan\ndf2.loc[df2['f'] == -99,'e'] = np.nan\nQuelles sont les choses qui vous dÃ©rangent dans le code ci-dessus?\n\n\nIndice ğŸ’¡ Regardez prÃ©cisÃ©ment le code et le DataFrame, notamment les colonnes e et g.\n\nIl y a deux erreurs, difficiles Ã  dÃ©tecter:\n\ndf2.loc[df2['e'] == -98,'e'] = np.nan: une erreur de copier-coller sur la valeur de lâ€™erreur ;\ndf2.loc[df2['f'] == -99,'e'] = np.nan: une erreur de copier-coller sur les colonnes en question\n\n\nOn peut noter au moins deux trois :\n\nLe code est long et rÃ©pÃ©titif, ce qui nuit Ã  sa lisibilitÃ©;\nLe code est trÃ¨s dÃ©pendant de la structure des donnÃ©es (nom et nombre de colonnes) et doit Ãªtre adaptÃ© dÃ¨s que celle-ci Ã©volue;\nOn a introduit des erreurs humaines dans le code, difficiles Ã  dÃ©tecter.\n\nOn voit dans la premiÃ¨re version de notre code quâ€™il y a une structure commune Ã  toutes nos lignes de la forme .[. == -99] = np.nan. Cette structure va servir de base Ã  notre fonction, en vue de gÃ©nÃ©raliser le traitement que nous voulons faire.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2['a'] = fix_missing(df['a'])\ndf2['b'] = fix_missing(df['b'])\ndf2['c'] = fix_missing(df['c'])\ndf2['d'] = fix_missing(df['d'])\ndf2['e'] = fix_missing(df['e'])\ndf2['f'] = fix_missing(df['f'])\nCette seconde version du code est meilleure que la premiÃ¨re version, car on a rÃ©glÃ© le problÃ¨me dâ€™erreur humaine (il nâ€™est plus possible de taper -98 au lieu de -99).\n\n\nMais voyez-vous le problÃ¨me qui persiste ?\n\nLe code reste long et rÃ©pÃ©titif, et nâ€™Ã©limine pas encore toute possibilitÃ© dâ€™erreur, car il est toujours possible de se tromper dans le nom des variables.\n\nLa prochaine Ã©tape consiste Ã  Ã©liminer ce risque dâ€™erreur en combinant deux fonctions (ce quâ€™on appelle la combinaison de fonctions).\nLa premiÃ¨re fonction fix_missing() sert Ã  rÃ©gler le problÃ¨me sur un vecteur. La seconde gÃ©nÃ©ralisera ce procÃ©dÃ© Ã  toutes les colonnes. Comme Pandas permet une approche vectorielle, il est frÃ©quent de construire des fonctions sur des vecteurs et les appliquer ensuite Ã  plusieurs colonnes.\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2 = df2.apply(fix_missing)\nCette troisiÃ¨me version du code a plusieurs avantages sur les deux autres versions:\n\nElle est plus concise et plus lisible;\nSi on a un changement de code pour les valeurs manquantes, il suffit de le mettre Ã  un seul endroit;\nElle fonctionne quels que soient le nombre de colonnes et le nom des colonnes;\nOn ne peut pas traiter une colonne diffÃ©remment des autres par erreur.\n\nDe plus, le code est facilement gÃ©nÃ©ralisable.\nPar exemple, Ã  partir de la mÃªme structure, Ã©crire le code qui permet de ne traiter que les colonnes a,b et e ne demande pas beaucoup dâ€™Ã©nergie.\ndf2 = df.copy()\ndf2[['a','b','e']] = df2[['a','b','e']].apply(fix_missing)"
  },
  {
    "objectID": "chapters/code-quality.html#documentation",
    "href": "chapters/code-quality.html#documentation",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "(Auto)documentation",
    "text": "(Auto)documentation\nUn code sans aucun commentaire est trÃ¨s difficile Ã  sâ€™approprier (y compris pour la personne qui lâ€™a rÃ©digÃ© et qui y revient quelques semaines plus tard). Cependant, un code prÃ©sentant trop de commentaires est Ã©galement illisible et reflÃ¨te gÃ©nÃ©ralement un dÃ©faut de conception du code qui nâ€™est pas assez explicite.\nLa documentation vise Ã  prÃ©senter la dÃ©marche gÃ©nÃ©rale, Ã©ventuellement Ã  travers des exemples, mais aussi Ã  expliciter certains Ã©lÃ©ments du code (une opÃ©ration qui nâ€™est pas Ã©vidente, des arguments de fonction, etc.). La documentation se mÃ©lange donc aux instructions visant Ã  Ãªtre exÃ©cutÃ©es mais sâ€™en distingue. Ces principes sont hÃ©ritÃ©s du paradigme de la â€œprogrammation lettrÃ©eâ€ (Literate programming) dont lâ€™un des avocats Ã©tait Donald Knuth.\n\nâ€œJe crois que le temps est venu pour une amÃ©lioration significative de la documentation des programmes, et que le meilleur moyen dâ€™y arriver est de considÃ©rer les programmes comme des Å“uvres littÃ©raires. Dâ€™oÃ¹ mon titre, Â« programmation lettrÃ©eÂ« .\nNous devons changer notre attitude traditionnelle envers la construction des programmes : au lieu de considÃ©rer que notre tÃ¢che principale est de dire Ã  un ordinateur ce quâ€™il doit faire, appliquons-nous plutÃ´t Ã  expliquer Ã  des Ãªtres humains ce que nous voulons que lâ€™ordinateur fasse.\nCelui qui pratique la programmation lettrÃ©e peut Ãªtre vu comme un essayiste, qui sâ€™attache principalement Ã  exposer son sujet dans un style visant Ã  lâ€™excellence. Tel un auteur, il choisit , avec soin, le dictionnaire Ã  la main, les noms de ses variables et en explique la signification pour chacune dâ€™elles. Il cherche donc Ã  obtenir un programme comprÃ©hensible parce que ses concepts sont prÃ©sentÃ©s dans le meilleur ordre possible. Pour cela, il utilise un mÃ©lange de mÃ©thodes formelles et informelles qui se complÃ¨tentâ€\nDonald Knuth, Literate Programming (source)\n\nCela peut amener Ã  distinguer deux types de documentation:\n\nUne documentation gÃ©nÃ©rale de type Jupyter Notebook ou Quarto Markdown qui prÃ©sente certes du code exÃ©cutÃ© mais dont lâ€™objet principal est de prÃ©senter une dÃ©marche ou des rÃ©sultats ;\nUne documentation de la dÃ©marche plus proche du code dont lâ€™un des exemples sont les docstrings Python (ou son Ã©quivalent R, la documentation Roxygen).\n\nLes deux grands principes de la documentation au sein dâ€™un script sont les suivants:\n\nIl est prÃ©fÃ©rable de documenter le pourquoi plutÃ´t que le comment. Le â€œcommentâ€ devrait Ãªtre comprÃ©hensible Ã  la lecture du code ;\nPrivilÃ©gier lâ€™autodocumentation via des nommages pertinents\n\n\n\n\n\n\n\nComment bien documenter un script ?\n\n\n\n\nMinimum ğŸš¦ : commentaire au dÃ©but du script pour dÃ©crire ce quâ€™il fait\nBien ğŸ‘ : commenter les parties â€œdÃ©licatesâ€ du code\nIdÃ©al ğŸ’ª : documenter ses fonctions avec la syntaxe des docstrings."
  },
  {
    "objectID": "chapters/code-quality.html#outils-et-mÃ©thodes-pour-amÃ©liorer-un-code",
    "href": "chapters/code-quality.html#outils-et-mÃ©thodes-pour-amÃ©liorer-un-code",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "Outils et mÃ©thodes pour amÃ©liorer un code",
    "text": "Outils et mÃ©thodes pour amÃ©liorer un code\nLâ€™apprentissage par coeur de ces rÃ¨gles ou faire des aller-retour en continu entre le code et les manuels de rÃ¨gles serait quelques peu rÃ©barbatif.\nPour faire le parallÃ¨le avec le langage naturel, on nâ€™a pas toujours le bÃ©cherelle ou le dictionnaire sous les yeux. Les Ã©diteurs de texte ou les smartphones embarquent des correcteurs orthographiques qui identifient voire corrigent directement le texte Ã©crit.\nIl existe le mÃªme type dâ€™outils pour les langages de programmation. Python Ã©tant lâ€™outil de travail principal de milliers de data-scientists, un certain nombre dâ€™outils ont vu le jour pour rÃ©duire le temps nÃ©cessaire pour crÃ©er un projet ou disposer dâ€™un code fonctionnel. Ces outils permettent un gros gain de productivitÃ©, rÃ©duisent le temps passÃ© Ã  effectuer des tÃ¢ches rÃ©barbatives et amÃ©liorent la qualitÃ© dâ€™un projet en offrant des diagnostics voire des correctifs Ã  des codes perfectibles.\nLes principaux outils sont les suivants:\n\nlinter : programme qui vÃ©rifie que le code est formellement conforme Ã  un certain guidestyle\n\nsignale problÃ¨mes formels, sans corriger\n\nformatter : programme qui reformate un code pour le rendre conforme Ã  un certain guidestyle\n\nmodifie directement le code\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nExemples dâ€™erreurs repÃ©rÃ©es par un linter :\n\nlignes de code trop longues ou mal indentÃ©es, parenthÃ¨ses non Ã©quilibrÃ©es, noms de fonctions mal construitsâ€¦\n\nExemples dâ€™erreurs non repÃ©rÃ©es par un linter :\n\nfonctions mal utilisÃ©es, arguments mal spÃ©cifiÃ©s, structure du code incohÃ©rente, code insuffisamment documentÃ©â€¦"
  },
  {
    "objectID": "chapters/code-quality.html#linters",
    "href": "chapters/code-quality.html#linters",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "Linters",
    "text": "Linters\nLes linters sont des outils qui permettent dâ€™Ã©valuer la qualitÃ© du code et son risque de provoquer une erreur (explicite ou silencieuse).\nVoici quelques exemples de problÃ¨mes que peuvent rencontrer les linters:\n\nles variables sont utilisÃ©es mais nâ€™existent pas (erreur)\nles variables inutilisÃ©es (inutiles)\nla mauvaise organisation du code (risque dâ€™erreur)\nle non respect des bonnes pratiques dâ€™Ã©criture de code\nles erreurs de syntaxe (par exemple les coquilles)\n\nLa plupart des logiciels de dÃ©veloppement embarquent des fonctionalitÃ©s de diagnostic (voire de suggestion de correctif). Il faut parfois les paramÃ©trer dans les options (ils sont dÃ©sactivÃ©s pour ne pas effrayer lâ€™utilisateur avec des croix rouges partout).\nEn Python, les deux principaux linters sont PyLint et Flake8. Dans les exercices, nous proposons dâ€™utiliser PyLint qui est pratique.\n\n\n\n\n\n\nTip\n\n\n\nLâ€™un des intÃ©rÃªts dâ€™utiliser PyLint est quâ€™on obtient une note, ce qui est assez instructif. Nous lâ€™utiliserons dans lâ€™application fil rouge pour comprendre la maniÃ¨re dont chaque Ã©tape amÃ©liore la qualitÃ© du code.\nIl est possible de mettre en oeuvre des pre commit hook qui empÃªchent un commit nâ€™ayant pas une note minimale."
  },
  {
    "objectID": "chapters/code-quality.html#formaters",
    "href": "chapters/code-quality.html#formaters",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "Formaters",
    "text": "Formaters\nLe formater modifie directement le code. On peut faire un parallÃ¨le avec le correcteur orthographique.\nCet outil peut donc induire un changement substantiel du script afin de le rendre plus lisible.\nLe formater le plus utilisÃ© est Black.\n\n\n\n\n\n\nNote\n\n\n\nPour signaler sur Github la qualitÃ© dâ€™un projet utilisant Black, il est possible dâ€™ajouter un badge dans le README:\n\n\n\nCode style: black"
  },
  {
    "objectID": "chapters/code-quality.html#lopensource-comme-moyen-pour-amÃ©liorer-la-qualitÃ©",
    "href": "chapters/code-quality.html#lopensource-comme-moyen-pour-amÃ©liorer-la-qualitÃ©",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "Lâ€™opensource comme moyen pour amÃ©liorer la qualitÃ©",
    "text": "Lâ€™opensource comme moyen pour amÃ©liorer la qualitÃ©\nEn ouvrant son code, il est possible de recevoir des suggestions voire des contributions de rÃ©utilisateurs du code. Cependant, les vertus de lâ€™ouverture vont au-delÃ .\nEn effet, lâ€™ouverture se traduit gÃ©nÃ©ralement par des codes de meilleur qualitÃ©, mieux documentÃ©s pour pouvoir Ãªtre rÃ©utilisÃ©s ou ayant simplement bÃ©nÃ©ficiÃ© dâ€™une attention accrue sur la qualitÃ© pour ne pas paraÃ®tre ridicule. MÃªme en lâ€™absence de retour de (rÃ©)utilisateurs du code, le partage de code amÃ©liore la qualitÃ© des projets."
  },
  {
    "objectID": "chapters/code-quality.html#revue-de-code",
    "href": "chapters/code-quality.html#revue-de-code",
    "title": "AmÃ©liorer la qualitÃ© de son code",
    "section": "Revue de code",
    "text": "Revue de code\nLa revue de code sâ€™inspire de la mÃ©thode du peer reviewing du monde acadÃ©mique pour amÃ©liorer la qualitÃ© du code Python. Dans une revue de code, le code Ã©crit par une personne est relu et Ã©valuÃ© par un ou plusieurs autres dÃ©veloppeurs afin dâ€™identifier les erreurs et les amÃ©liorations possibles. Cette pratique permet de dÃ©tecter les erreurs avant quâ€™elles ne deviennent des problÃ¨mes majeurs, dâ€™assurer une cohÃ©rence dans le code, de garantir le respect des bonnes pratiques mais aussi dâ€™amÃ©liorer la qualitÃ© du code en identifiant les parties du code qui peuvent Ãªtre simplifiÃ©es, optimisÃ©es ou refactorisÃ©es pour en amÃ©liorer la lisibilitÃ© et la maintenabilitÃ©.\nUn autre avantage de cette approche est quâ€™elle permet le partage de connaissances entre des personnes expÃ©rimentÃ©es et des personnes plus dÃ©butantes ce qui permet Ã  ces derniÃ¨res de monter en compÃ©tence. Github et Gitlab proposent des fonctionnalitÃ©s trÃ¨s pratiques pour la revue de code: discussions, suggestions de modificationsâ€¦"
  },
  {
    "objectID": "chapters/deployment.html",
    "href": "chapters/deployment.html",
    "title": "DÃ©ployer et valoriser son projet de data science",
    "section": "",
    "text": "Les chapitres prÃ©cÃ©dents nous ont permis de construire un projet de data science Ã  la fois conforme aux bonnes pratiques de dÃ©veloppement et portable. Ce faisant, nous avons dÃ©jÃ  largement rÃ©duit le coÃ»t de transition entre notre environnement de dÃ©veloppement et un environnement de production. La notion de mise en production dÃ©signe prÃ©cisÃ©ment cette transition vers un environnement de production, câ€™est Ã  dire un environnement dans lequel lâ€™application sera dÃ©ployÃ©e et donc accessible Ã  ses utilisateurs potentiels, avec une haute disponibilitÃ© â€” idÃ©alement, 24h sur 24.\nRÃ©aliser une mise en production nÃ©cessite dâ€™effectuer un certain nombre de choix, plus ou moins contraints. Dâ€™abord, il est nÃ©cessaire de bien comprendre le besoin mÃ©tier â€” qui seront les utilisateurs potentiels de notre application et quels sont leurs besoins â€” pour dÃ©terminer le format de valorisation pertinent pour celle-ci. Par exemple, le format adaptÃ© pour mettre Ã  disposition un modÃ¨le entraÃ®nÃ© de machine learning ne sera pas le mÃªme que lâ€™on sâ€™adresse au grand public â€” auquel cas on sâ€™orientera certainement vers une application interactive simple dâ€™utilisation1 â€” ou Ã  une Ã©quipe interne de notre organisation qui voudrait effectuer de la prÃ©diction Ã  partir du modÃ¨le â€” cas dans lequel une API requÃªtable via un langage de programmation sâ€™avÃ¨rera sans doute plus pertinente. Un second choix Ã  effectuer concerne lâ€™infrastructure de production sur laquelle va Ãªtre dÃ©ployÃ©e lâ€™application. Si des considÃ©rations techniques entrent en compte pour dÃ©terminer lâ€™infrastructure pertinente, ce choix est en gÃ©nÃ©ral contraint pour le data scientist par ce quâ€™il a Ã  sa disposition au sein de son organisation. Enfin, il est nÃ©cessaire de choisir les outils qui vont permettre lâ€™industrialisation du projet : automatisation du dÃ©ploiement, orchestration des traitements, monitoring de la solution en production, etc.\nLa diversitÃ© des rÃ©ponses qui peuvent Ãªtre apportÃ©es Ã  chacun de ces choix illustre la complexitÃ© du sujet de la mise en production. Il serait bien illusoire de prÃ©tendre traiter ici ce sujet avec exhaustivitÃ©. Afin de restreindre cette complexitÃ©, nous allons effectuer Ã  chaque Ã©tape des choix raisonnables, câ€™est Ã  dire qui reprÃ©sentent des pratiques courantes dans le domaine de la data science. Ainsi, nous allons nous restreindre au cas du dÃ©ploiement dâ€™un modÃ¨le de machine learning, exposÃ© via une API, car il sâ€™agit dâ€™une tÃ¢che frÃ©quemment effectuÃ©e dans le monde de la data science et sur laquelle il est donc bon dâ€™avoir quelques notions. Comme nous allons effectuer le dÃ©ploiement sur le SSP Cloud, lâ€™infrastructure de production sera un cluster Kubernetes. LÃ  encore, cette solution constitue un standard actuel de lâ€™industrie. Enfin, pour ce qui est de lâ€™industrialisation de notre projet, nous choisirons des solutions standards et open-source qui permettent dâ€™implÃ©menter lâ€™approche CI/CD (intÃ©gration continue / dÃ©ploiement continu), qui fait rÃ©fÃ©rence en la matiÃ¨re. Dans tous les cas, lâ€™enjeu de ce cours est de transmettre les concepts sous-jacents Ã  la mise en production plus que de former Ã  telle ou telle technologie.\nPar rapport au prÃ©cÃ©dent, ce chapitre est assez technique et repose sur une vaste palette dâ€™outils. Notre propos nâ€™est cependant pas de dire que le data scientist doit maÃ®triser lâ€™ensemble des concepts et outils prÃ©sentÃ©s. Au contraire : avec la complexification des projets et des infrastructures, un ensemble de professions ont Ã©mergÃ© â€” date engineer, dev ops, machine learning engineer, etc. â€” dont les rÃ´les sont plus spÃ©cifiquement centrÃ©s sur lâ€™industrialisation des projets. NÃ©anmoins, nous sommes convaincus quâ€™il est essentiel que le data scientist possÃ¨de des notions sur le sujet afin de pouvoir jouer le rÃ´le dâ€™interface entre le mÃ©tier et les Ã©quipes techniques. Ce chapitre est une tentative de condenser ces diffÃ©rentes notions sous une forme la plus accessible possible."
  },
  {
    "objectID": "chapters/deployment.html#intÃ©gration-continue",
    "href": "chapters/deployment.html#intÃ©gration-continue",
    "title": "DÃ©ployer et valoriser son projet de data science",
    "section": "IntÃ©gration continue",
    "text": "IntÃ©gration continue\n\nAutomatisation des phases de build et de test (data validation, model validation..)\nMise Ã  disposition des artifacts (forme â€œmorteâ€)\nImplÃ©mentation : GitHub Actions"
  },
  {
    "objectID": "chapters/deployment.html#dÃ©ploiement",
    "href": "chapters/deployment.html#dÃ©ploiement",
    "title": "DÃ©ployer et valoriser son projet de data science",
    "section": "DÃ©ploiement",
    "text": "DÃ©ploiement\n\nAvantages de dÃ©ployer sur une infra Cloud\nFonctionnement basique de Kubernetes\nGitOps\nImplÃ©mentation : Kubernetes"
  },
  {
    "objectID": "chapters/deployment.html#orchestration",
    "href": "chapters/deployment.html#orchestration",
    "title": "DÃ©ployer et valoriser son projet de data science",
    "section": "Orchestration",
    "text": "Orchestration\n\nModÃ©lisation dâ€™un pipeline data sous forme de DAG\nImplÃ©mentation : argo-workflow"
  },
  {
    "objectID": "chapters/deployment.html#spÃ©cificitÃ©s-liÃ©es-Ã -la-mise-en-prod-de-modÃ¨les-de-ml",
    "href": "chapters/deployment.html#spÃ©cificitÃ©s-liÃ©es-Ã -la-mise-en-prod-de-modÃ¨les-de-ml",
    "title": "DÃ©ployer et valoriser son projet de data science",
    "section": "SpÃ©cificitÃ©s liÃ©es Ã  la mise en prod de modÃ¨les de ML",
    "text": "SpÃ©cificitÃ©s liÃ©es Ã  la mise en prod de modÃ¨les de ML\n\nEntrainement : batch ou online\nBatch vs real-time prediction\nFeedback loops expÃ©rimentation/dÃ©ploiement/monitoring\nExposer le modÃ¨le via une API"
  },
  {
    "objectID": "chapters/deployment.html#notions-dobservabilitÃ©",
    "href": "chapters/deployment.html#notions-dobservabilitÃ©",
    "title": "DÃ©ployer et valoriser son projet de data science",
    "section": "Notions dâ€™observabilitÃ©",
    "text": "Notions dâ€™observabilitÃ©\n\nDrifts\nQuelles mÃ©triques monitorer"
  },
  {
    "objectID": "chapters/deployment.html#amÃ©lioration-continue",
    "href": "chapters/deployment.html#amÃ©lioration-continue",
    "title": "DÃ©ployer et valoriser son projet de data science",
    "section": "AmÃ©lioration continue",
    "text": "AmÃ©lioration continue\n\nRÃ©entraÃ®nement : pÃ©riodique vs continu"
  },
  {
    "objectID": "chapters/evaluation.html",
    "href": "chapters/evaluation.html",
    "title": "ModalitÃ©s dâ€™Ã©valuation du cours",
    "section": "",
    "text": "Pas dâ€™Ã©valuation pour lâ€™annÃ©e 2022-2023."
  },
  {
    "objectID": "chapters/git.html",
    "href": "chapters/git.html",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "",
    "text": "Le dÃ©veloppement rapide de la data science au cours de ces derniÃ¨res annÃ©es sâ€™est accompagnÃ©e dâ€™une complexification substantielle des projets. Par ailleurs, les projets sont de plus en plus collaboratifs, que ce soit dans le cadre dâ€™Ã©quipes dans un contexte professionnel ou bien pour des contributions Ã  des projets open-source. Naturellement, ces Ã©volutions doivent nous amener Ã  modifier nos maniÃ¨res de travailler pour gÃ©rer cette complexitÃ© croissante et continuer Ã  produire de la valeur Ã  partir des projets de data science.\nPourtant, tout data scientist sâ€™est parfois demandÃ© :\n\nquelle Ã©tait la bonne version dâ€™un programme\nqui Ã©tait lâ€™auteur dâ€™un bout de code en particulier\nsi un changement Ã©tait important ou juste un essai\ncomment fusionner des programmes\netc.\n\nEt il nâ€™est pas rare de perdre le fil des versions de son projet lorsque lâ€™on garde trace de celles-ci de faÃ§on manuelle.\nExemple de contrÃ´le de version fait â€œÃ  la mainâ€\n\nPourtant, il existe un outil informatique puissant afin de rÃ©pondre Ã  tous ces besoins : la gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer lâ€™historique des modifications dâ€™un ensemble de fichiers\nrevenir Ã  des versions prÃ©cÃ©dentes dâ€™un ou plusieurs fichiers\nrechercher les modifications qui ont pu crÃ©er des erreurs\ntravailler simultanÃ©ment sur un mÃªme fichier sans risque de perte\npartager ses modifications et rÃ©cupÃ©rer celles des autres\nproposer des modifications, les discuter, sans pour autant modifier la derniÃ¨re version existante\nidentifier les auteurs et la date des modifications\n\nEn outre, ces outils fonctionnent avec tous les langages informatiques car ils reposent sur la comparaison des lignes et des caractÃ¨res des programmes, indÃ©pendamment du langage. En bref, câ€™est la bonne maniÃ¨re pour partager des codes et travailler Ã  plusieurs sur un projet de data science. En rÃ©alitÃ©, il ne serait pas exagÃ©rÃ© de dire que lâ€™utilisation du contrÃ´le de version est la bonne pratique la plus fondamentale de tout projet faisant intervenir du code, et quâ€™elle conditionne largement toutes les autres.\n\n\n\nPlusieurs logiciels de contrÃ´le de version existent sur le marchÃ©. En principe, le logiciel Git, dÃ©veloppÃ© initialement pour fournir une solution dÃ©centralisÃ©e et open-source dans le cadre du dÃ©veloppement du noyau Linux, est devenu largement hÃ©gÃ©monique. Aussi, toutes les application de ce cours sâ€™effectueront Ã  lâ€™aide du logiciel Git.\n\n\n\nTravailler de maniÃ¨re collaborative avec Git implique de synchroniser son rÃ©pertoire local avec une copie distante, situÃ©e sur un serveur hÃ©bergeant des projets Git. Ce serveur peut Ãªtre un serveur interne Ã  une organisation, ou bien Ãªtre fourni par un hÃ©bergeur externe. Les deux alternatives les plus populaires en la matiÃ¨re sont GitHub et GitLab. Dans ce cours, nous utiliserons GitHub, qui est devenu au fil des annÃ©es la rÃ©fÃ©rence pour lâ€™hÃ©bergement des projets open-source. En pratique, les deux services sont relativement semblables, et tous les concepts prÃ©sentÃ©s se retrouvent sous une forme similaire sur les deux plateformes."
  },
  {
    "objectID": "chapters/git.html#principes-et-commandes-usuelles",
    "href": "chapters/git.html#principes-et-commandes-usuelles",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Principes et commandes usuelles",
    "text": "Principes et commandes usuelles\nLe graphique suivant illustre les principes fondamentaux de Git.\nGit tout-en-un (Source) \nLorsquâ€™on utilise Git, il est important de bien distinguer ce qui se passe en local (sur son poste, sur le serveur sur lequel on travailleâ€¦) de ce qui se passe en remote, i.e.Â en intÃ©ragissant avec un serveur distant. Comme le montre le graphique, lâ€™essentiel du contrÃ´le de version se passe en rÃ©alitÃ© en local.\nEn thÃ©orie, sur un projet individuel, il est mÃªme possible de rÃ©aliser lâ€™ensemble du contrÃ´le de version en mode hors-ligne. Pour cela, il suffit dâ€™indiquer Ã  Git le projet (dossier) que lâ€™on souhaite versionner en utilisant la commande git init. Cette commande a pour effet de crÃ©er un dossier .git Ã  la racine du projet, dans lequel Git va stocker tout lâ€™historique du projet (commits, branches, etc.) et permettre de naviguer entre les versions. A cause du . qui prÃ©fixe son nom, ce dossier est gÃ©nÃ©ralement cachÃ© par dÃ©faut, ce qui nâ€™est pas problÃ©matique dans la mesure oÃ¹ il nâ€™y a jamais besoin de le parcourir ou de le modifier Ã  la main en pratique. Retenez simplement que câ€™est la prÃ©sence de ce dossier .git qui fait quâ€™un dossier est considÃ©rÃ© comme un projet Git, et donc que vous pouvez utilisez les commandes usuelles de Git dans ce dossier Ã  lâ€™aide dâ€™un terminal : - git status : affiche les modifications du projet par rapport Ã  la version prÃ©cÃ©dente ; - git add chemin_du_fichier : ajoute un fichier nouveau ou modifiÃ© Ã  la zone de staging de Git en vue dâ€™un commit ; - git add -A : ajoute tous les fichiers nouveaux ou modifiÃ©s Ã  la zone de staging ; - git commit -m \"message de commit\" : crÃ©e un commit, i.e.Â une photographie des modifications (ajouts, modifications, suppressions) apportÃ©es au projet depuis la derniÃ¨re version, et lui assigne un message dÃ©crivant ces changements. Les commits sont lâ€™unitÃ© de base de lâ€™historique du projet construit par Git.\nEn pratique, travailler uniquement en local nâ€™est pas trÃ¨s intÃ©ressant. Pour pouvoir travailler de maniÃ¨re collaborative, on va vouloir synchroniser les diffÃ©rentes copies locales du projet Ã  un rÃ©pertoire centralisÃ©, qui maintient de fait la â€œsource de vÃ©ritÃ©â€ (single source of truth). MÃªme sur un projet individuel, il fait sens de synchroniser son rÃ©pertoire local Ã  une copie distante pour assurer lâ€™intÃ©gritÃ© du code de son projet en cas de problÃ¨me matÃ©riel.\nEn gÃ©nÃ©ral, on va donc initialiser le projet dans lâ€™autre sens : - crÃ©er un nouveau projet sur GitHub - gÃ©nÃ©rer un jeton dâ€™accÃ¨s (personal access token) - cloner le projet en local via la mÃ©thode HTTPS : git clone https://github.com/<username>/<project_name>.git\nLe projet clonÃ© est un projet Git â€” il contient le dossier .git â€” synchronisÃ© par dÃ©faut avec le rÃ©pertoire distant. On peut le vÃ©rifier avec la commande remote de Git :\n$ git remote -v\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (fetch)\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (push)\nLe projet local est bien liÃ© au rÃ©pertoire distant sur GitHub, auquel Git donne par dÃ©faut le nom origin. Ce lien permet dâ€™utiliser les commandes de synchronisation usuelles : - git pull : rÃ©cupÃ©rer les changements (fetch) sur le remote et les appliquer au projet local - git push : envoyer les changements locaux sur le remote"
  },
  {
    "objectID": "chapters/git.html#implÃ©mentations",
    "href": "chapters/git.html#implÃ©mentations",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "ImplÃ©mentations",
    "text": "ImplÃ©mentations\nGit est un logiciel, qui peut Ãªtre tÃ©lÃ©chargÃ© sur le site officiel pour diffÃ©rents systÃ¨mes dâ€™exploitation. Il existe cependant diffÃ©rentes maniÃ¨res dâ€™utiliser Git : - le client en ligne de commande : câ€™est lâ€™implÃ©mentation standard, et donc la plus complÃ¨te. Câ€™est celle quâ€™on utilisera dans ce cours. Le client Git est installÃ© par dÃ©faut sur les diffÃ©rents services du SSP Cloud (VSCode, RStudio, Jupyter, etc.) et peut donc Ãªtre utilisÃ© via nâ€™importe quel terminal. La documentation du SSP Cloud dÃ©taille la procÃ©dure ; - des interfaces graphiques : elles facilitent la prise en main de Git via des guides visuels, mais ne permettent pas de rÃ©aliser toutes les opÃ©rations permises par Git - lâ€™interface native de RStudio pour les utilisateurs de R : trÃ¨s complÃ¨te et stable. La formation au travail collaboratif avec Git et RStudio prÃ©sente son utilisation de maniÃ¨re dÃ©taillÃ©e ; - le plugin Jupyter-git pour les utilisateurs de Python : elle implÃ©mente les principales features de Git, mais sâ€™avÃ¨re assez instable Ã  lâ€™usage."
  },
  {
    "objectID": "chapters/git.html#bonnes-pratiques",
    "href": "chapters/git.html#bonnes-pratiques",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\nLe contrÃ´le de version est une bonne pratique de dÃ©veloppement en soiâ€¦ mais son utilisation admet elle mÃªme des bonnes pratiques qui, lorsquâ€™elles sont appliquÃ©es, permettent dâ€™en tirer le plus grand profit.\n\nQue versionne-t-on ?\n\nUniquement des fichiers texte\nA chaque commit, Git calcule les diffÃ©rences entre les versions successives du projet, afin de ne pas avoir Ã  stocker une image complÃ¨te de ce dernier Ã  chaque fois. Câ€™est ce qui permet aux projets Git de rester trÃ¨s lÃ©gers par dÃ©faut, et donc aux diffÃ©rentes opÃ©rations impliquant le remote (clone, push, pull..) dâ€™Ãªtre trÃ¨s rapides.\nLa contrepartie de cette lÃ©gÃ¨retÃ© est une contrainte sur les types dâ€™objets que lâ€™on doit versionner. Les diffÃ©rences sont calculables uniquement sur des fichiers de type texte : codes source, fichiers texte, fichiers de configuration non-sensiblesâ€¦ Voici donc une liste non-exhaustive des extensions de fichier que lâ€™on retrouve frÃ©quemment dans un dÃ©pÃ´t Git dâ€™un projet R ou Python : .py, .R, .Rmd, .txt, .json, .xml, .yaml, .toml, et bien dâ€™autres.\nEn revanche tous les fichiers binaires â€” pour faire simple, tous les fichiers qui ne peuvent pas Ãªtre ouverts dans un Ã©diteur de texte basique sans produire une suite inintelligible de caractÃ¨res â€” nâ€™ont gÃ©nÃ©ralement pas destination Ã  se retrouver sur un dÃ©pÃ´t Git. Du fait de leur formatage (binaire), Git ne peut pas calculer les diffÃ©rences entre versions pour ces fichiers et câ€™est donc le fichier entier qui est sauvegardÃ© dans lâ€™historique Ã  chaque changement, ce qui peut trÃ¨s rapidement faire croÃ®tre la taille du dÃ©pÃ´t. Pour Ã©viter de versionner ces fichiers par erreur, on va les ajouter au fichier .gitignore (cf.Â supra).\n\n\nPas de donnÃ©es\nComme expliquÃ© en introduction, le fil rouge de ce cours sur les bonnes pratiques est lâ€™importance de bien sÃ©parer code, donnÃ©es et environnement dâ€™exÃ©cution afin de favoriser la reproductibilitÃ© des projets de data science. Ce principe doit sâ€™appliquer Ã©galement Ã  lâ€™usage du contrÃ´le de version, et ce pour diffÃ©rentes raisons.\nA priori, inclure ces donnÃ©es dans un dÃ©pÃ´t Git peut sembler une bonne idÃ©e en termes de reproductibilitÃ©. En machine learning par exemple, on est souvent amenÃ© Ã  rÃ©aliser de nombreuses expÃ©rimentations Ã  partir dâ€™un mÃªme modÃ¨le appliquÃ© Ã  diffÃ©rentes transformations des donnÃ©es initiales, transformations que lâ€™on pourrait versionner. En pratique, il est gÃ©nÃ©ralement prÃ©fÃ©rable de versionner le code qui permet de gÃ©nÃ©rer ces transformations et donc les expÃ©rimentations associÃ©es, dans la mesure oÃ¹ le suivi des versions des datasets peut sâ€™avÃ©rer rapidement complexe. Pour de plus gros projets, des alternatives spÃ©cifiques existent : câ€™est le champ du MLOps, domaine en constante expansion qui vise Ã  rendre les pipelines de machine learning plus reproductibles.\nEnfin, la structure mÃªme de Git nâ€™est techniquement pas faite pour le stockage de donnÃ©es. Si des petits datasets dans un format texte ne poseront pas de problÃ¨me, des donnÃ©es volumineuses (Ã  partir de plusieurs Mo) vont faire croÃ®tre la taille du dÃ©pÃ´t et donc ralentir significativement les opÃ©rations de synchronisation avec le remote.\n\n\nPas dâ€™informations locales\nLÃ  encore en vertu du principe de sÃ©paration donnÃ©es / code/ environnement, les donnÃ©es locales, i.e.Â spÃ©cifiques Ã  lâ€™environnement de travail sur lequel le code a Ã©tÃ© exÃ©cutÃ©, nâ€™ont pas vocation Ã  Ãªtre versionnÃ©es. Par exemple, des fichiers de configuration spÃ©cifiques Ã  un poste de travail, des chemins dâ€™accÃ¨s spÃ©cifiques Ã  un ordinateur donnÃ©, etc. Cela demande une plus grande rigueur lors de la construction du projet, mais garantit par lÃ  mÃªme une meilleure reproductiblitÃ© pour les futurs utilisateurs du projet.\n\n\nPas dâ€™outputs\nLes outputs dâ€™un projet (graphiques, publications, modÃ¨le entraÃ®nÃ©â€¦) nâ€™ont pas vocation Ã  Ãªtre versionnÃ©, en vertu des diffÃ©rents arguments prÃ©sentÃ©s ci-dessus : - il ne sâ€™agit gÃ©nÃ©ralement pas de fichiers de type texte ; - le code source du projet doit dans tous les cas permettre des les regÃ©nÃ©rer Ã  lâ€™identique.\n\n\nUtiliser un .gitignore\nOn a listÃ© prÃ©cÃ©demment un large Ã©ventail de fichiers qui nâ€™ont, par nature, pas vocation Ã  Ãªtre versionnÃ©. Bien entendu, faire attention Ã  ne pas ajouter ces diffÃ©rents fichiers au moment de chaque git add serait assez pÃ©nible. Git simplifie largement cette procÃ©dure en nous donnant la possibilitÃ© de remplir un fichier .gitignore, situÃ© Ã  la racine du projet, qui spÃ©cifie lâ€™ensemble des fichiers et types de fichiers que lâ€™on ne souhaite pas versionner dans le cadre du projet courant.\nDe maniÃ¨re gÃ©nÃ©rale, il y a pour chaque langage des fichiers que lâ€™on ne souhaitera jamais versionner. Pour en tenir compte, une premiÃ¨re bonne pratique est de choisir le .gitignore associÃ© au langage du projet lors de la crÃ©ation du dÃ©pÃ´t sur GitHub. Ce faisant, le projet est initialitÃ© avec un gitignore dÃ©jÃ  existant et prÃ©-rempli de chemins et de types de fichiers qui ne sont pas Ã  versionner. Regardons un extrait du gitignore Python pour comprendre sa structure et son fonctionnement.\npip-log.txt\n__pycache__/\n*.log\nChaque ligne du gitignore spÃ©cifie un Ã©lÃ©ment Ã  ignorer du contrÃ´le de version, Ã©lÃ©ment qui peut Ãªtre un ficher/dossier ou bien une rÃ¨gle concernant un ensemble de fichiers/dossiers. Sauf si spÃ©cifiÃ© explicitement, les chemins sont relatifs Ã  la racine du projet. Lâ€™extrait du gitignore Python illustre les diffÃ©rentes possibilitÃ©s :\n\nligne 1 : ignore le fichier pip-log.txt ;\nligne 2 : ignore le dossier __pycache__/ ;\nligne 3 : ignore tous les fichiers dont lâ€™extension est .log.\n\nDe nombreuses autres possiblitÃ©s existent, et sont dÃ©taillÃ©es par exemple dans la documentation de Git.\n\n\n\nMessages des commits\nLe commit est lâ€™unitÃ© de temps de Git, et donc fondamentalement ce qui permet de remonter dans lâ€™historique dâ€™un projet. Afin de pouvoir bÃ©nÃ©ficier Ã  plein de cet avantage de Git, il est capital dâ€™accompagner ses commits de messages pertinents, en se plaÃ§ant dans la perspective que lâ€™on peut Ãªtre amenÃ© plusieurs semaines ou mois plus tard Ã  vouloir retrouver du code dans lâ€™historique de son projet. Les quelques secondes prises Ã  chaque commit pour rÃ©flÃ©chir Ã  une description pertinente du bloc de modifications que lâ€™on apporte au projet peuvent donc faire gagner un temps prÃ©cieux Ã  la longue.\nDe nombreuses conventions existent pour rÃ©diger des messages de commit pertinents. Nous rappelons ici les plus importantes :\n\ncontenu : le message doit dÃ©tailler le pourquoi plutÃ´t que le comment des modifications. Par exemple, plutÃ´t que â€œAjoute le fichier test.pyâ€, on prÃ©fÃ©rera Ã©crire â€œAjout dâ€™une sÃ©rie de tests unitairesâ€ ;\nstyle : le message doit Ãªtre Ã  lâ€™impÃ©ratif et former une phrase (sans point Ã  la fin) ;\nlongueur : le message du commit doit Ãªtre court (< 72 caractÃ¨res). Sâ€™il nâ€™est pas possible de trouver un message de cette taille qui rÃ©sume le commit, câ€™est gÃ©nÃ©ralement un signe que le commit regroupe trop de changements (cf.Â point suivant). Le fait de devoir mettre des + ou des & / et dans un message de commit pour sÃ©parer les changements est Ã©galement un bon indicateur dâ€™un commit trop gros.\n\n\n\nFrÃ©quence des commits\nDe maniÃ¨re gÃ©nÃ©rale, il est conseillÃ© de rÃ©aliser des commits rÃ©guliers lorsque lâ€™on travaille sur un projet. Une rÃ¨gle simple que lâ€™on peut par exemple appliquer est la suivante : dÃ¨s lors quâ€™un ensemble de modifications forment un tout cohÃ©rent et peuvent Ãªtre rÃ©sumÃ©es par un message simple, il est temps dâ€™en faire un commit. Cette approche a de nombreux avantages :\n\nsi lâ€™on fait suivre chaque commit dâ€™un push â€” ce qui est conseillÃ© en pratique â€” on sâ€™assure de disposer rÃ©guliÃ¨reemnt dâ€™une copie de ses travaux, ce qui limite le risque de perte ;\nil est plus facile de revenir en arriÃ¨re en cas dâ€™erreur si les commits portent sur des changements ciblÃ©s et cohÃ©rents ;\nle processus de review dâ€™une pull request est facilitÃ©, car les diffÃ©rents blocs de modification sont plus clairement sÃ©parÃ©s ;\ndans une approche dâ€™intÃ©gration continue â€” concept que lâ€™on verra en dÃ©tail dans le chapitre sur la [mise en production]() â€” faire des commits et des PR rÃ©guliÃ¨rement permet de dÃ©ployer de maniÃ¨re continue les changements en production, et donc dâ€™obtenir les feedbacks des utilisateurs et dâ€™adapter plus rapidement si besoin."
  },
  {
    "objectID": "chapters/git.html#branches",
    "href": "chapters/git.html#branches",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Branches",
    "text": "Branches\n\nConcept\nLa possibilitÃ© de crÃ©er des branches est lâ€™une des fonctionnalitÃ©s majeures de Git. La crÃ©ation dâ€™une branche au sein dâ€™un projet permet de diverger de la ligne principale de dÃ©veloppement (gÃ©nÃ©ralement appelÃ©e master, terme tendant Ã  disparaÃ®tre au profit de celui de main) sans impacter cette ligne. Cela permet de sÃ©parer le nouveau dÃ©veloppement et de faire cohabiter plusieurs versions, pouvant Ã©voluer sÃ©parÃ©ment et pouvant Ãªtre facilement rassemblÃ©es si nÃ©cessaire.\nPour comprendre comment fonctionnent les branches, il nous faut revenir un peu plus en dÃ©tail sur la maniÃ¨re dont Git stocke lâ€™historique du projet. Comme nous lâ€™avons vu, lâ€™unitÃ© temporelle de Git est le commit, qui correspond Ã  une photographie Ã  un instant donnÃ© de lâ€™Ã©tat du projet (snapshot). Chaque commit est uniquement identifiÃ© par un hash, une longue suite de caractÃ¨res. La commande git log, qui liste les diffÃ©rents commits dâ€™un projet, permet dâ€™afficher ce hash ainsi que diverses mÃ©tadonnÃ©es (auteur, date, message) associÃ©es au commit.\n$ git log\ncommit e58b004d3b68bdf28093fe6ad6036b5d13216e55 (HEAD -> master, origin/master, origin/HEAD)\nAuthor: Lino Galiana <xxx@xxx.fr>\nDate:   Tue Mar 22 14:34:04 2022 +0100\n\n    ajoute code Ã©quivalent python\n\n...\nUne branche est simplement un pointeur vers un commit. Dans lâ€™exemple prÃ©cÃ©dent, on a imprimÃ© les informations du dernier commit en date. La branche principale (master) pointe vers ce commit. Si lâ€™on faisait un nouveau commit, le pointeur se dÃ©calerait et la branche master pointerait Ã  prÃ©sent sur le nouveau commit.\n\n\nBranches locales\nDans ce contexte, crÃ©er une nouvelle branche (en local) revient simplement Ã  crÃ©er un nouveau pointeur vers un commit donnÃ©. Supposons que lâ€™on crÃ©e une branche testing Ã  partir du dernier commit.\n$ git branch testing  # CrÃ©e une nouvelle branche\n$ git branch  # Liste les branches existantes\n* master  # La branche sur laquelle on se situe\n  testing  # La nouvelle branche crÃ©Ã©e\nLa figure suivante illustre lâ€™effet de cette crÃ©ation sur lâ€™historique Git.\n\nDÃ©sormais, deux branches (master et testing) pointent vers le mÃªme commit. Si lâ€™on effectue Ã  prÃ©sent des commits sur la branche testing, on va diverger de la branche principale, ce qui permet de dÃ©velopper une nouvelle fonctionnalitÃ© sans risquer dâ€™impacter master.\nPour savoir sur quelle branche on se situe Ã  instant donnÃ© â€” et donc sur quelle branche on va commiter â€” Git utilise un pointeur spÃ©cial, appelÃ© HEAD, qui pointe vers la branche courante. On comprend Ã  prÃ©sent mieux la signification de HEAD -> master dans lâ€™output de la commande git log vu prÃ©cÃ©demment. Cet Ã©lÃ©ment spÃ©cifie la situation locale actuelle et signifie : on se situe actuellement sur la branche master, qui pointe sur le commit e58b004. Pour changer de branche, i.e.Â dÃ©placer le HEAD, on utilise la commande git checkout. Par exemple, pour passer de la branche master sur laquelle on est par dÃ©faut Ã  la branche testing :\n$ git checkout testing  # Changement de branche\nSwitched to branch 'testing'\nOn se situe dÃ©sormais sur la branche testing, sur laquelle on peut laisser libre cours Ã  sa crÃ©ativitÃ© sans risquer dâ€™impacer la branche principale du projet. Mais que se passe-t-il si, pendant que lâ€™on dÃ©veloppe sur testing, un autre membre du projet commit sur master ? On dit que les historiques ont divergÃ©. La figure suivante illustre Ã  quoi ressemble Ã  prÃ©sent lâ€™historique du projet (et suppose que lâ€™on est repassÃ© sur master).\n\nCette divergence nâ€™est pas problÃ©matique en soi : il est normal que les diffÃ©rentes parties et expÃ©rimentations dâ€™un projet avancent Ã  diffÃ©rents rythmes. La difficultÃ© est de savoir comment rÃ©concillier les diffÃ©rents changements si lâ€™on dÃ©cide que la branche testing doit Ãªtre intÃ©grÃ©e dans master. Deux situations peuvent survenir : - les modifications opÃ©rÃ©es en parallÃ¨le sur les deux branches ne concernent pas les mÃªmes fichiers ou les mÃªmes parties des fichiers. Dans ce cas, Git est capable de fusionner (merge) les changements automatiquement et tout se passe sans encombre ; - dans le cas contraire, survient un merge conflict : les branches ont divergÃ© de telle sorte quâ€™il nâ€™est pas possible pour Git de fusionner les changements automatiquement. Il faut alors rÃ©soudre les conflits manuellement.\nLa rÃ©solution des conflits est une Ã©tape souvent douloureuse lors de lâ€™apprentissage de Git. Aussi, nous conseillons dans la mesure du possible de ne pas fusionner des branches manuellement en local avec Git â€” câ€™est dâ€™ailleurs pour cette raison que nous nâ€™avons pas dÃ©taillÃ© les commandes pour le faire. Dans les sections suivantes, nous verrons comment une bonne organisation prÃ©alable du travail en Ã©quipe, combinÃ©e aux outils collaboratifs fournis par GitHub, permet de rendre le processus de fusion des branches largement indolore.\n\n\nBranches remote\nRappellons que toutes les opÃ©rations que nous avons effectuÃ©es sur les branches dans cette section se sont passÃ©s en local, le rÃ©pertoire distant est restÃ© totalement inchangÃ©. Pour pouvoir collaborer sur une branche ou bien en faire une pull request (cf.Â supra), il faut pousser la branche sur le rÃ©pertoire distant. La commande est simple : git push origin <branche>.\n$ git push origin testing\nCounting objects: 24, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (15/15), done.\nWriting objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done.\nTotal 24 (delta 2), reused 0 (delta 0)\nTo https://github.com/linogaliana/ensae-reproductibilite-website\n * [new branch]      testing -> testing"
  },
  {
    "objectID": "chapters/git.html#workflow-collaboratif",
    "href": "chapters/git.html#workflow-collaboratif",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Workflow collaboratif",
    "text": "Workflow collaboratif\nComme on lâ€™a vu prÃ©cÃ©demment, si le modÃ¨le des branches de Git semble idÃ©al pour gÃ©rer le travail collaboratif et asynchrone, il peut Ã©galement sâ€™avÃ©rer rapidement complexe Ã  manipuler en lâ€™absence dâ€™une bonne organisation du travail en Ã©quipe. De nombreux modÃ¨les (â€œworkflowsâ€) existent en la matiÃ¨re, avec des complexitÃ©s plus ou moins grandes selon la nature du projet. Nous conseillons dâ€™adopter dans la plupart des cas un modÃ¨le trÃ¨s simple : le GitHub Flow.\nLe GitHub Flow est une mÃ©thode dâ€™organisation minimaliste du travail collaboratif, qui est progressivement devenue la norme dans les projets open-source. Elle est rÃ©sumÃ©e par la figure suivante, dont nous dÃ©taillons par la suite les diffÃ©rentes Ã©tapes.\n\n\nDÃ©finition des rÃ´les des contributeurs\nDans tout projet collaboratif, une premiÃ¨re Ã©tape essentielle est de bien dÃ©limiter les rÃ´les des diffÃ©rents contributeurs. Les diffÃ©rents participants au projet ont en effet gÃ©nÃ©ralement des rÃ´les diffÃ©rents dans lâ€™organisation, des niveaux diffÃ©rents de pratique de Git, etc. Il est important de reflÃ©ter ces diffÃ©rents rÃ´les dans lâ€™organisation du travail collaboratif.\nSur les diffÃ©rents hÃ©bergeurs de projets Git, cela prend la forme de rÃ´les que lâ€™on attribue aux diffÃ©rents membres du porjet. Les mainteneurs sont les seuls Ã  pouvoir Ã©crire directement sur master. Les contributeurs sont quant Ã  eux tenus de dÃ©velopper sur des branches. Cela permet de protÃ©ger la branche principale, qui doit rester une version propre et jamais compromise du projet.\nNotons que la possibilitÃ© de donner des rÃ´les sur les projets GitHub nâ€™est possible que dans le cadre dâ€™organisations (payantes), donc dans un contexte professionnel ou de projets open-source dâ€™une certaine ampleur. Pour des petits projets, il est nÃ©cessaire de sâ€™astreindre Ã  une certaine rigueur individuelle pour respecter cette organisation.\n\n\nDÃ©veloppement sur des branches de court-terme\nLes contributeurs dÃ©veloppent uniquement sur des branches. Il est dâ€™usage de crÃ©er une branche par fonctionnalitÃ©, en lui donnant un nom reflÃ©tant la fonctionnalitÃ© en cours de dÃ©veloppement (ex : ajout-tests-unitaires). Les diffÃ©rents contributeurs Ã  la fonctionnalitÃ© en cours de dÃ©veloppement font des commits sur la branche, en prenant bien soin de pull rÃ©guliÃ¨rement les Ã©ventuels changements pour ne pas risquer de conflits de version. Pour la mÃªme raison, il est prÃ©fÃ©rable de faire des branches dites de court-terme, câ€™est Ã  dire propres Ã  une petite fonctionnalitÃ©, quite Ã  diviser une fonctionnalitÃ© en sÃ©ries dâ€™implÃ©mentations. Cela permet de limiter les Ã©ventuels conflits Ã  gÃ©rer lors de la fusion finale de la branche avec master.\n\n\nPull Request\nUne fois la sÃ©rie de modifications terminÃ©e, vient le temps de rassembler les diffÃ©rents travaux, par lâ€™intermÃ©diaire de la fusion entre la branche et master. Il faut alors â€œdemanderâ€ de fusionner (pull request) sur GitHub. Cela ouvre une page liÃ©e Ã  la pull request, qui rappelle les diffÃ©rents changements apportÃ©s et leurs auteurs, et permet dâ€™entamer une discussion Ã  propos de ces changements.\n\n\nProcessus de review\nLes diffÃ©rents membres du projet peuvent donc analyser et commenter les changements, poser des questions, suggÃ©rer des modifications, apporter dâ€™autres contributions, etc. Il est par exemple possible de mentionner un membre de lâ€™Ã©quipe par lâ€™intermÃ©diaire de @personne. Il est Ã©galement possible de procÃ©der Ã  une code review, par exemple par un dÃ©veloppeur plus expÃ©rimentÃ©.\n\n\nRÃ©solution des Ã©ventuels conflits\nEn adoptant cette maniÃ¨re de travailler, master ne sera modifiÃ©e que via des pull requests. Il ne devrait donc jamais y avoir le moindre conflit Ã  rÃ©gler sur master, les seuls conflits possibles se passent sur les branches. Par exemple, dans le cas oÃ¹ une autre pull request aurait Ã©tÃ© fusionnÃ©e sur master depuis lâ€™ouverture de la pull request en question.\nDans le cas dâ€™un conflit Ã  gÃ©rer, le conflit doit Ãªtre rÃ©solu dans la branche et pas dans master. Voici la marche Ã  suivre :\n\nappliquez le conseil de survie : faites une copie de sauvegarde de votre clone\ndans votre clone, placez vous sur la branche en question : git checkout nom-de-la-branche\nfusionnez master dans la branche : git merge master\nrÃ©solvez les Ã©ventuels conflits dans les fichiers concernÃ©s\nfinalisez le commit de merge et poussez-le sur la branche remote, ce qui le fera apparaÃ®tre dans la pull request\n\n\n\nFusion de la branche\nSi tout est en ordre, la branche peut Ãªtre fusionnÃ©e. Seuls les mainteneurs, qui ont les droits sur master, peuvent fusionner la pull request. En termes dâ€™historique du projet, deux choix sont possibles : - â€œCreate a merge commitâ€ : tous les commits rÃ©alisÃ©s sur la branche apparaÃ®tront dans lâ€™historique du projet ; - â€œSquash and mergeâ€ : les diffÃ©rents commits rÃ©alisÃ©s sur la branche seront rassemblÃ©s en un commit unique. Cette option est gÃ©nÃ©ralement prÃ©fÃ©rable lorsquâ€™on utilise des branches de court-terme : elles permettent de garder lâ€™historique plus lisible."
  },
  {
    "objectID": "chapters/git.html#utiliser-les-issues",
    "href": "chapters/git.html#utiliser-les-issues",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Utiliser les issues",
    "text": "Utiliser les issues\nLa maniÃ¨re la plus simple de contribuer Ã  un projet open-source est dâ€™ouvrir une issue. Sur GitHub, cela se fait sur la page du projet, sous lâ€™onglet Issue (cf.Â documentation officielle). Les issues peuvent avoir diffÃ©rentes nature : - suggestion dâ€™amÃ©lioration (sans code) - notification de bug - rapports dâ€™expÃ©rience - etc.\nLes issues sont une maniÃ¨re trÃ¨s peu couteuse de contributer Ã  un projet, mais leur importance est capitale, dans la mesure oÃ¹ il est impossible pour les dÃ©veloppeurs dâ€™un projet de penser en amont Ã  toutes les utilisations possibles et donc tous les bugs possibles dâ€™une application."
  },
  {
    "objectID": "chapters/git.html#proposer-une-pull-request",
    "href": "chapters/git.html#proposer-une-pull-request",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Proposer une pull request",
    "text": "Proposer une pull request\nUne autre maniÃ¨re, plus ambitieuse, de contribuer Ã  lâ€™open source est de proposer des pull requests. ConcrÃ¨tement, lâ€™idÃ©e est de proposer une amÃ©lioration ou bien de rÃ©soudre un bug sous forme de code, que les mainteneurs du projet peuvent ensuite dÃ©cider dâ€™intÃ©grer au code existant.\nLa procÃ©dure pour proposer une pull request Ã  un projet sur lequel on nâ€™a aucun droit est trÃ¨s similaire Ã  celle dÃ©crite ci-dessus dans le cas normal. La principale diffÃ©rence est que, du fait de lâ€™absence de droits, il est impossible de pousser une branche locale sur le rÃ©pertoire du projet. On va donc devoir crÃ©er au prÃ©alable un fork, i.e.Â une copie du projet que lâ€™on crÃ©e dans son espace personnel sur GitHub. Câ€™est sur cette copie que lâ€™on va appliquer la procÃ©dure dÃ©crite prÃ©cÃ©demment, en prenant bien soin de travailler sur une branche et non sur master. Une fois les modifications pertinentes effectuÃ©es sur la branche du fork, GitHub propose de crÃ©er une pull request sur le dÃ©pÃ´t original. Cette pull request sera alors visible des mainteneurs du projet, qui pourront lâ€™Ã©valuer et dÃ©cider dâ€™adopter (ou non) les changements proposÃ©s."
  },
  {
    "objectID": "chapters/git.html#respecter-les-rÃ¨gles-de-contribution",
    "href": "chapters/git.html#respecter-les-rÃ¨gles-de-contribution",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Respecter les rÃ¨gles de contribution",
    "text": "Respecter les rÃ¨gles de contribution\nVouloir contribuer Ã  un projet open-source est trÃ¨s louable, mais ne peut pas pour autant se faire nâ€™importe comment. Un projet est constituÃ© de personnes, qui ont dÃ©veloppÃ© ensemble une maniÃ¨re de travailler, des standards de bonnes pratiques, etc. Pour sâ€™assurer que sa contribution ne reste pas lettre morte, il est indispensable de sâ€™imprÃ©gner un minimum de la culture du projet.\nPour faciliter les contributions, les projets open-source spÃ©cifient bien souvent la maniÃ¨re dont les utilisateurs peuvent contribuer ainsi que le format attendu. En gÃ©nÃ©ral, ces rÃ¨gles de contribution sont spÃ©cifiÃ©es dans un fichier CONTRIBUTING.md situÃ© Ã  la racine du projet GitHub, ou a dÃ©faut dans le README du projet. Il est essentiel de bien lire ce document sâ€™il existe afin de sâ€™assurer de proposer des contributions pertinentes."
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Ce cours sâ€™adresse aux praticiens de la data science, entendue ici au sens large comme la combinaison de techniques issues des mathÃ©matiques, de la statistique et de lâ€™informatique pour produire de la connaissance utile Ã  partir de donnÃ©es. Cela inclut donc tout autant les data scientists et statisticiens travaillant dans le privÃ© ou dans des administrations que les chercheurs dont les travaux font intervenir des traitements computationnels Ã  partir de donnÃ©es.\nCe cours part du constat que les formations acadÃ©miques dans ce domaine adoptent souvent une orientation essentiellement technique, visant une comprÃ©hension fine des modÃ¨les manipulÃ©s, mais ne discutent que rarement des problÃ¨mes pratiques qui forment le quotidien du data scientist dans un contexte professionnel. Ce cours vise Ã  combler ce manque en proposant des pistes de solution Ã  diverses questions que peuvent se poser les data scientists lorsquâ€™ils transitionnent du contexte de la formation initiale Ã  des projets rÃ©els :\n\nComment travailler de maniÃ¨re collaborative sur un projet ?\nComment partager du code et sâ€™assurer que celui-ci va tourner sans erreur sur un autre environnement dâ€™exÃ©cution ?\nComment passer dâ€™un environnement de dÃ©veloppement1 Ã  un environnement de production2?\nComment dÃ©ployer un modÃ¨le de data science, et rendre celui-ci accessible Ã  des utilisateurs afin de le valoriser ?\nComment automatiser les diffÃ©rentes Ã©tapes de son projet afin de simplifier sa maintenance ?\n\n\n\n\n\n\n\nRessource complÃ©mentaire: le missing semester du MIT\n\n\n\nBeaucoup de praticiens ont pris conscience de certains manques dans les cursus de formations en statistiques ou informatique. Certaines ressources trÃ¨s utiles ont Ã©tÃ© regroupÃ©es dans le missing semester du MIT dont une partie des contenus prÃ©sente des thÃ¨mes communs avec ce cours.\n\n\nAfin de proposer des rÃ©ponses Ã  ces interrogations, ce cours prÃ©sente un ensemble de bonnes pratiques et dâ€™outils issus de diffÃ©rents domaines de lâ€™informatique, comme le dÃ©veloppement logiciel, lâ€™infrastructure, lâ€™administration de serveurs, le dÃ©ploiement applicatif, etc.. Lâ€™objectif nâ€™est bien entendu pas de dÃ©velopper une expertise dans chacun de ces domaines, dans la mesure oÃ¹ ces compÃ©tences font lâ€™objet de mÃ©tiers Ã  part entiÃ¨re, que sont les dÃ©veloppeurs, les data architects, les sysadmin, ou encore les data engineers.\nEn revanche, face Ã  la taille croissante des projets de data science et donc des Ã©quipes qui les portent, le data scientist tend Ã  se retrouver Ã  lâ€™interface de ces diffÃ©rentes professions, avec lesquelles il doit communiquer de maniÃ¨re efficiente pour mener ces projets Ã  bien. Ce cours vise Ã  fournir, plus que des connaissances techniques pointues, les Ã©lements de langage nÃ©cessaires pour pouvoir jouer ce rÃ´le dâ€™interface en communiquant Ã  la fois avec les Ã©quipes mÃ©tiers et les Ã©quipes techniques qui entourent un projet de data science."
  },
  {
    "objectID": "chapters/introduction.html#origine",
    "href": "chapters/introduction.html#origine",
    "title": "Introduction",
    "section": "Origine",
    "text": "Origine\nLa notion de â€œbonnes pratiquesâ€ qui nous intÃ©resse dans ce cours trouve son origine au sein de la communautÃ© des dÃ©veloppeurs logiciels. Elle constitue une rÃ©ponse Ã  plusieurs constats :\n\nle â€œcode est beaucoup plus souvent lu quâ€™il nâ€™est Ã©critâ€ (Guido Van Rossum) ;\nla maintenance dâ€™un code demande souvent (beaucoup) plus de ressources que son dÃ©veloppement initial ;\nla personne qui maintient une base de code a de fortes chances de ne pas Ãªtre celle qui lâ€™a Ã©crite.\n\nFace Ã  ces constats, un ensemble de rÃ¨gles informelles ont Ã©tÃ© conventionnellement acceptÃ©es par la communautÃ© des dÃ©veloppeurs comme produisant des logiciels plus fiables, Ã©volutifs et maintenables dans le temps. Comme toutes conventions de langue, certaines peuvent paraÃ®tre arbitraires. Ces rÃ¨gles favorisent nÃ©anmoins la capacitÃ© Ã  communiquer du code et rÃ©duisent le coÃ»t pour le faire Ã©voluer.\nRÃ©cemment, dans le contexte dâ€™une Ã©volution des logiciels vers des applications web vivant dans le cloud, un certain nombre de ces bonnes pratiques ont Ã©tÃ© formalisÃ©es dans un manifeste : la 12 Factor App."
  },
  {
    "objectID": "chapters/introduction.html#pourquoi-sintÃ©resser-aux-bonnes-pratiques",
    "href": "chapters/introduction.html#pourquoi-sintÃ©resser-aux-bonnes-pratiques",
    "title": "Introduction",
    "section": "Pourquoi sâ€™intÃ©resser aux bonnes pratiques ?",
    "text": "Pourquoi sâ€™intÃ©resser aux bonnes pratiques ?\n\nEn quoi est-ce pertinent pour le data scientist, dont le rÃ´le nâ€™est pas de dÃ©velopper des applications mais de donner du sens aux donnÃ©es ?\n\nDu fait du dÃ©veloppement rapide de la data science et consÃ©quemment de la croissance de la taille moyenne des projets, lâ€™activitÃ© du data scientist tend Ã  se rapprocher par certains aspects de celle du dÃ©veloppeur :\n\nles projets sur lesquels travaille le data scientist sont intenses en code ;\nil doit travailler de maniÃ¨re collaborative au sein de projets de grande envergure ;\nil est de plus en plus amenÃ© Ã  travailler Ã  partir de donnÃ©es massives, ce qui nÃ©cessite de travailler sur des infrastructures big data informatiquement complexes ;\nil est amenÃ© Ã  interagir avec des profils informatiques pour dÃ©ployer ses modÃ¨les et les rendre accessible Ã  des utilisateurs.\n\nAussi, il fait sens pour le data scientist moderne de sâ€™intÃ©resser aux bonnes pratiques en vigueur dans la communautÃ© des dÃ©veloppeurs. Bien entendu, celles-ci doivent Ãªtre adaptÃ©es aux spÃ©cificitÃ©s des projets basÃ©s sur des donnÃ©es. Faisons Ã  prÃ©sent un tour dâ€™horizon des bonnes pratiques et des outils que nous verrons tout au long ce cours."
  },
  {
    "objectID": "chapters/introduction.html#voir-le-code-comme-un-outil-de-communication",
    "href": "chapters/introduction.html#voir-le-code-comme-un-outil-de-communication",
    "title": "Introduction",
    "section": "Voir le code comme un outil de communication",
    "text": "Voir le code comme un outil de communication\nLa premiÃ¨re bonne pratique Ã  adopter est de considÃ©rer le code comme un outil de communication, et non simplement de maniÃ¨re fonctionnelle. Un code ne sert pas seulement Ã  rÃ©aliser une tÃ¢che donnÃ©e, il a vocation Ã  Ãªtre diffusÃ©, rÃ©utilisÃ©, maintenu, que ce soit dans le contexte dâ€™une Ã©quipe dans une organisation ou bien en open-source.\nPour favoriser cette communication du code, des conventions ont Ã©tÃ© developpÃ©es en matiÃ¨re de qualitÃ© du code et de structuration des projets, quâ€™il est utile dâ€™appliquer dans ses projets. Nous prÃ©sentons ces conventions dans les chapitres QualitÃ© du Code et Architecture des Projets.\nIl est pour les mÃªmes raisons indispensable dâ€™appliquer les principes du contrÃ´le de version, qui permettent une documentation en continu des projets, ce qui accroÃ®t fortement leur rÃ©utilisabilitÃ© et leur maintenabilitÃ© dans le temps. Nous proposons donc un chapitre de rappel sur lâ€™utilisation du logiciel Git dans le chapitre Versionner son code et travailler collaborativement avec Git."
  },
  {
    "objectID": "chapters/introduction.html#travailler-de-maniÃ¨re-collaborative",
    "href": "chapters/introduction.html#travailler-de-maniÃ¨re-collaborative",
    "title": "Introduction",
    "section": "Travailler de maniÃ¨re collaborative",
    "text": "Travailler de maniÃ¨re collaborative\nLe data scientist, quel que soit son contexte de travail, est amenÃ© Ã  travailler dans le cadre de projets en Ã©quipe. Cela implique de dÃ©finir une organisation du travail ainsi que dâ€™utiliser des outils permettant de collaborer sur un projet de maniÃ¨re efficace et sÃ©curisÃ©e.\nNous prÃ©sentons une maniÃ¨re moderne de travailler collaborativement avec Git et GitHub dans le chapitre de rappel Versionner son code et travailler collaborativement avec Git. Les autres chapitres prendront pour acquis cette approche collaborative et la raffineront Ã  travers lâ€™approche DevOps3."
  },
  {
    "objectID": "chapters/introduction.html#maximiser-la-reproductibilitÃ©",
    "href": "chapters/introduction.html#maximiser-la-reproductibilitÃ©",
    "title": "Introduction",
    "section": "Maximiser la reproductibilitÃ©",
    "text": "Maximiser la reproductibilitÃ©\nLe troisiÃ¨me pilier des bonnes pratiques prÃ©sentÃ©es dans ce cours est la reproductibilitÃ©.\nUn projet est dit reproductible lorsque, avec le mÃªme code et les mÃªmes donnÃ©es, il est possible de reproduire les rÃ©sultats obtenus. Notons bien que le problÃ¨me de la reproductibilitÃ© est diffÃ©rent de celui de la rÃ©plicabilitÃ©. La rÃ©plicabilitÃ© est un concept scientifique, qui signifie quâ€™un mÃªme procÃ©dÃ© expÃ©rimental donne des rÃ©sultats analogues sur des jeux de donnÃ©es diffÃ©rents. La reproductibilitÃ© est un concept technique : elle ne signifie pas que le protocole expÃ©rimental est scientifiquement correct, mais quâ€™il a Ã©tÃ© spÃ©cifiÃ© et diffusÃ© dâ€™une maniÃ¨re qui permet Ã  tous de reproduire les rÃ©sultats obtenus.\nLa notion de reproductibilitÃ© est le fil rouge de ce cours : toutes les notions vues dans les diffÃ©rents chapitres y contribuent. Le fait de produire du code et des projets qui respectent les conventions communautaires, comme le fait dâ€™utiliser le contrÃ´le de version, contribuent Ã  rendre le code plus lisible et documentÃ©, et donc reproductible.\nIl faut nÃ©anmoins aller plus loin pour atteindre une vÃ©ritable reproductibilitÃ©, et rÃ©flÃ©chir Ã  la notion dâ€™environnement dâ€™exÃ©cution. Un code nâ€™est pas un objet autonome, il est toujours exÃ©cutÃ© sur un environnement (ordinateur personnel, serveur, etc.), et ces environnements peuvent Ãªtre trÃ¨s diffÃ©rents (systÃ¨me dâ€™exploitation, librairies installÃ©es, contraintes de sÃ©curitÃ©, etc.). Câ€™est pourquoi il faut rÃ©flÃ©chir Ã  la portabilitÃ© de son code, i.e.Â sa capacitÃ© Ã  sâ€™exÃ©cuter de maniÃ¨re attendue sur diffÃ©rents environnements, ce qui sera lâ€™objet dâ€™un chapitre Ã  part entiÃ¨re."
  },
  {
    "objectID": "chapters/introduction.html#faciliter-la-mise-en-production",
    "href": "chapters/introduction.html#faciliter-la-mise-en-production",
    "title": "Introduction",
    "section": "Faciliter la mise en production",
    "text": "Faciliter la mise en production\nPour quâ€™un projet de data science crÃ©e in fine de la valeur, il faut quâ€™il soit dÃ©ployÃ© sous une forme valorisable de sorte Ã  toucher son public. Cela implique deux choses :\n\ntrouver le format de diffusion adaptÃ©, i.e.Â qui valorise au mieux les rÃ©sultas obtenus auprÃ¨s des utilisateurs potentiels ;\nfaire transitionner le projet de lâ€™environnement dans lequel il a Ã©tÃ© dÃ©veloppÃ© vers une infrastructure de production, i.e.Â permettant un dÃ©ploiement robuste de lâ€™output du projet afin que celui-ci soit disponible Ã  la demande.\n\nDans le chapitre DÃ©ployer et valoriser son projet de data science, nous proposons des pistes permettant de rÃ©pondre Ã  ces deux besoins. Nous prÃ©sentons un certain nombre de formats standards (API, application, rapport automatisÃ©, site internet) qui permettent Ã  un projet de data science dâ€™Ãªtre valorisÃ©, ainsi que les outils modernes qui permettent de les produire.\nNous dÃ©taillons ensuite les concepts essentiels du dÃ©ploiement sur une infrastructure de production, et illustrons ces derniers par des exemples de dÃ©ploiements dans un environnement cloud moderne.\nCâ€™est en quelque sorte la rÃ©compense de lâ€™application des bonnes pratiques : dÃ¨s lors que lâ€™on sâ€™est donnÃ© la peine de produire un code et un projet appliquant des standards de qualitÃ©, que lâ€™on a bien versionnÃ© son code, et que lâ€™on a pris des mesures pour le rendre portable, le dÃ©ploiement du projet dans un environnement de production sâ€™en trouve largement facilitÃ©."
  },
  {
    "objectID": "chapters/introduction.html#chapitres-supplÃ©mentaires",
    "href": "chapters/introduction.html#chapitres-supplÃ©mentaires",
    "title": "Introduction",
    "section": "Chapitres supplÃ©mentaires",
    "text": "Chapitres supplÃ©mentaires\nPlusieurs outils prÃ©sentÃ©s tout au long de ce cours, tels que les logiciels Git et Docker, impliquent lâ€™utilisation du terminal ainsi que des connaissances de base du fonctionnement dâ€™un systÃ¨me Linux. Dans le chapitre DÃ©mystifier le terminal Linux pour gagner en autonomie, nous prÃ©sentons les connaissances essentielles des systÃ¨mes Linux quâ€™un data scientist doit possÃ©der pour pouvoir Ãªtre autonome dans ses dÃ©ploiements et dans lâ€™application des bonnes pratiques de dÃ©veloppement.\nLa reproductibilitÃ© Ã©tant une quÃªte sans fin, nous concluons ce cours par un chapitre nommÃ© Des ressources pour aller plus loin dans lâ€™industrialisation de son projet. Comme son nom lâ€™indique, il vise Ã  pointer vers un certain nombre de ressources qui permettent dâ€™amÃ©liorer encore et toujours ses pratiques et de sâ€™intÃ©resser Ã  des sujets qui dÃ©passent le cadre de ce cours, comme la sÃ©curitÃ© ou encore les spÃ©cificitÃ©s liÃ©es au dÃ©ploiement et Ã  la maintenance de modÃ¨les de machine learning."
  },
  {
    "objectID": "chapters/introduction.html#comment-fixer-le-bon-seuil",
    "href": "chapters/introduction.html#comment-fixer-le-bon-seuil",
    "title": "Introduction",
    "section": "Comment fixer le bon seuil ?",
    "text": "Comment fixer le bon seuil ?\nLa dÃ©termination du seuil pertinent doit rÃ©sulter dâ€™un arbitrage entre diffÃ©rents critÃ¨res liÃ©s au projet :\n\nambitions : le projet est-il amenÃ© Ã  Ã©voluer, prendre de lâ€™ampleur ? Est-il destinÃ© Ã  devenir collaboratif, que ce soit dans le cadre dâ€™une Ã©quipe en organisation ou bien en open-source ? Les outputs du projet ont-ils vocation Ã  Ãªtre diffusÃ©s au grand public ?\nressources : quels sont les moyens humain du projet ? Pour un projet open-source, existe-t-il une communautÃ© potentiel de contributeurs ?\ncontraintes : le projet a-t-il une Ã©chÃ©ance proche ? Des exigences de qualitÃ© ont-elles Ã©tÃ© fixÃ©es ? Est-il destinÃ© Ã  la mise en production ? Existe-t-il des enjeux de sÃ©curitÃ© forts ?\n\nIl nâ€™est donc pas question pour nous de suggÃ©rer que tout projet de data science doit respecter toutes les bonnes pratiques prÃ©sentÃ©es dans ce cours."
  },
  {
    "objectID": "chapters/introduction.html#un-socle-minimal-pour-les-projets-de-data-science",
    "href": "chapters/introduction.html#un-socle-minimal-pour-les-projets-de-data-science",
    "title": "Introduction",
    "section": "Un socle minimal pour les projets de data science ?",
    "text": "Un socle minimal pour les projets de data science ?\nCela Ã©tant dit, nous sommes convaincus quâ€™il est important pour tout data scientist de rÃ©flÃ©chir Ã  ces questions pour amÃ©liorer ces pratiques au fil du temps.\nEn particulier, nous pensons quâ€™il est possible de dÃ©finir un socle, i.e.Â un ensemble minimal de bonnes pratiques qui apportent plus dâ€™avantages quâ€™elles ne coÃ»tent Ã  implÃ©menter. Notre suggestion pour un tel socle est la suivante :\n\nContrÃ´ler la qualitÃ© de son code en utilisant des outils dÃ©diÃ©s (cf.Â chapitre QualitÃ© du Code) ;\nAdopter une structure standardisÃ©e de projet en utilisant des templates prÃªts Ã  lâ€™emploi (cf.Â chapitre Architecture des Projets) ;\nutiliser Git pour versionner le code de ses projets, quâ€™ils soient individuels ou collectifs (cf.Â chapitre Versionner son code et travailler collaborativement avec Git) ;\ncontrÃ´ler les dÃ©pendances de son projet en dÃ©veloppant dans des environnements virtuels (cf.Â chapitre PortabilitÃ©)."
  },
  {
    "objectID": "chapters/introduction.html#approche-pÃ©dagogique",
    "href": "chapters/introduction.html#approche-pÃ©dagogique",
    "title": "Introduction",
    "section": "Approche pÃ©dagogique",
    "text": "Approche pÃ©dagogique\nLe parti pris de ce cours est que seule la pratique, et en particulier la confrontation Ã  des problÃ¨mes issus de projets rÃ©els, permet dâ€™acquÃ©rir efficacement des concepts informatiques. Aussi, une large part du cours consistera en lâ€™application des notions Ã©tudiÃ©es Ã  des cas concrets. Chaque chapitre se concluera pas des applications touchant Ã  des sujets rÃ©alistes de data science.\nUn exemple fil rouge illustre les progrÃ¨s dans la conception dâ€™un projet reproductible en appliquant successivement le contenu des chapitres de ce cours.\nPour lâ€™Ã©valuation gÃ©nÃ©rale du cours, lâ€™idÃ©e sera de partir dâ€™un projet personnel, idÃ©alement terminÃ©, et de lui appliquer un maximum de bonnes pratiques prÃ©sentÃ©es dans ce cours."
  },
  {
    "objectID": "chapters/introduction.html#langages",
    "href": "chapters/introduction.html#langages",
    "title": "Introduction",
    "section": "Langages",
    "text": "Langages\nLes principes prÃ©sentÃ©s dans ce cours sont pour la plupart agnostiques du langage de programmation utilisÃ©.\nCe choix nâ€™est pas quâ€™Ã©ditorial, câ€™est selon nous un aspect fondamental du sujet des bonnes pratiques. Trop souvent, des diffÃ©rences de langage entre les phases de dÃ©veloppement (notamment R ou Python) et de mise en production (ex : Java) Ã©rigent des murs artificiels qui rÃ©duisent fortement la capacitÃ© Ã  valoriser des projets de data science.\nA lâ€™inverse, plus les diffÃ©rentes Ã©quipes qui forment le cycle de vie dâ€™un projet sâ€™accordent pour appliquer le mÃªme ensemble de bonnes pratiques, plus ces Ã©quipes dÃ©veloppent un langage commun, et plus les dÃ©ploiements en sont facilitÃ©s.\nUn exemple parlant est lâ€™utilisation de la conteneurisation : si le data scientist met Ã  disposition une image Docker comme output de sa phase de dÃ©veloppement et que le data engineer sâ€™occupe de dÃ©ployer cette image sur une infrastructure dÃ©diÃ©e, le contenu mÃªme de lâ€™application en termes de langage importe finalement assez peu. Cet exemple, certes simpliste, illustre malgrÃ© tout lâ€™enjeu des bonnes pratiques en matiÃ¨re de communication au sein dâ€™un projet.\nLes exemples prÃ©sentÃ©s dans ce cours seront pour lâ€™essentiel en Python. La raison principale est que ce langage, malgrÃ© ses dÃ©fauts, est enseignÃ© dans la majoritÃ© des cursus de data science mais aussi dâ€™informatique. Il peut faciliter la passerelle entre le monde des utilisateurs de donnÃ©es et celui des dÃ©veloppeurs informatiques, passerelle indispensable pour favoriser le dialogue entre ces deux profils, nÃ©cessaires tous deux pour un passage en production. Encore une fois, il est tout Ã  fait possible dâ€™appliquer les mÃªmes principes avec dâ€™autres langages, et nous encourageons dâ€™ailleurs les Ã©tudiants Ã  sâ€™essayer Ã  cet exercice formateur."
  },
  {
    "objectID": "chapters/introduction.html#environnement-dexÃ©cution",
    "href": "chapters/introduction.html#environnement-dexÃ©cution",
    "title": "Introduction",
    "section": "Environnement dâ€™exÃ©cution",
    "text": "Environnement dâ€™exÃ©cution\nA lâ€™instar du langage, les principes appliquÃ©s dans ce cours sont agnostiques Ã  lâ€™infrastructure utilisÃ©e pour faire tourner les exemples proposÃ©s. Il est donc Ã  la fois possible et souhaitable dâ€™appliquer les bonnes pratiques aussi bien Ã  un projet individuel dÃ©veloppÃ© sur un ordinateur personnel quâ€™Ã  un projet collaboratif visant Ã  Ãªtre dÃ©ployÃ© sur une infrastructure de production dÃ©diÃ©e.\nCependant, nous choisissons comme environnement de rÃ©fÃ©rence tout au long de ce cours le SSP Cloud, une plateforme de services pour la data science dÃ©veloppÃ©e Ã  lâ€™Insee et accessible aux Ã©lÃ¨ves des Ã©coles statistiques. Les raisons de ce choix sont multiples :\n\nlâ€™environnement de dÃ©veloppement est normalisÃ© : les serveurs du SSP Cloud ont une configuration homogÃ¨ne â€” notamment, ils se basent sur une mÃªme distribution Linux (Debian) â€” ce qui garantit la reproductibilitÃ© des exemples prÃ©sentÃ©s tout au long du cours ;\nvia un cluster Kubernetes sous-jacent, le SSP Cloud met Ã  disposition une infrastructure robuste permettant le dÃ©ploiement automatisÃ© dâ€™applications potentiellement intensives en donnÃ©es, ce qui permet de simuler un vÃ©ritable environnement de production ;\nle SSP Cloud est construit selon les standards les plus rÃ©cents des infrastructures data science, et permet donc dâ€™acquÃ©rir les bonnes pratiques de maniÃ¨re organique :\n\nles services sont lancÃ©s via des conteneurs, configurÃ©s par des images Docker. Cela permet de garantir une forte reproductibilitÃ© des dÃ©ploiements, au prix dâ€™une phase de dÃ©veloppement un peu plus coÃ»teuse ;\nle SSP Cloud est basÃ© sur une approche dite cloud native : il est construit sur un ensemble modulaire de briques logicielles, qui permettent dâ€™appliquer une sÃ©paration nette du code, des donnÃ©es, de la configuration et de lâ€™environnement dâ€™exÃ©cution, principe majeur des bonnes pratiques qui reviendra tout au long de ce cours.\n\n\nPour en savoir plus sur cette plateforme, vous pouvez consulter cette page."
  },
  {
    "objectID": "chapters/introduction.html#ressources-complÃ©mentaires",
    "href": "chapters/introduction.html#ressources-complÃ©mentaires",
    "title": "Introduction",
    "section": "Ressources complÃ©mentaires",
    "text": "Ressources complÃ©mentaires\n\nMissing semester du MIT"
  },
  {
    "objectID": "chapters/linux-101.html",
    "href": "chapters/linux-101.html",
    "title": "DÃ©mystifier le terminal Linux pour gagner en autonomie",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des systÃ¨mes dâ€™exploitation. Mais comme il a la rÃ©putation dâ€™Ãªtre austÃ¨re et complexe, on utilise plutÃ´t des interfaces graphiques pour effectuer nos opÃ©rations informatiques quotidiennes.\nPourtant, avoir des notions quant Ã  lâ€™utilisation dâ€™un terminal est une vraie source dâ€™autonomie, dans la mesure oÃ¹ celui-ci permet de gÃ©rer bien plus finement les commandes que lâ€™on rÃ©alise. Pour le data scientist qui sâ€™intÃ©resse aux bonnes pratiques et Ã  la mise en production, sa maÃ®trise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont gÃ©nÃ©ralement limitÃ©es par rapport Ã  lâ€™utilisation du programme en ligne de commande. Câ€™est par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de rÃ©aliser toutes les opÃ©rations permises par le logiciel ;\nmettre un projet de data science en production nÃ©cessite dâ€™utiliser un serveur, qui le rend disponible en permanence Ã  son public potentiel. Or lÃ  oÃ¹ Windows domine le monde des ordinateurs personnels, une large majoritÃ© des serveurs et des infrastructures cloud fonctionnent sous Linux.\nplus gÃ©nÃ©ralement, une utilisation rÃ©guliÃ¨re du terminal est source dâ€™une meilleure comprÃ©hension du fonctionnement dâ€™un systÃ¨me de fichiers et de lâ€™exÃ©cution des processus sur un ordinateur. Ces connaissances sâ€™avÃ¨rent trÃ¨s utiles dans la pratique quotidienne du data scientist, qui nÃ©cessite de plus en plus de dÃ©velopper dans diffÃ©rents environnements dâ€™exÃ©cution.\n\nDans le cadre de ce cours, on sâ€™intÃ©ressera donc particuliÃ¨rement au terminal Linux.\n\n\n\nDiffÃ©rents environnements de travail peuvent Ãªtre utilisÃ©s pour apprendre Ã  se servir dâ€™un terminal Linux :\n\nle SSP Cloud. Dans la mesure oÃ¹ les exemples de mise en production du cours seront illustrÃ©es sur cet environnement, nous recommandons de lâ€™utiliser dÃ¨s Ã  prÃ©sent pour se familiariser. Le terminal est accessible Ã  partir de diffÃ©rents services (RStudio, Jupyter, etc.), mais nous recommandons dâ€™utiliser le terminal dâ€™un service VSCode, dans la mesure oÃ¹ se servir dâ€™un IDE pour organiser notre code est en soi dÃ©jÃ  une bonne pratique ;\nKatacoda, un bac Ã  sable dans un systÃ¨me Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (Ã©mulation minimaliste dâ€™un terminal Linux), qui est installÃ©e par dÃ©faut avec Git.\n\n\n\n\nLanÃ§ons un terminal pour prÃ©senter son fonctionnement basique. On prend pour exemple le terminal dâ€™un service VSCode lancÃ© via le SSP Cloud (Application Menu tout en haut Ã  gauche de VSCode -> Terminal -> New Terminal). Voici Ã  quoi ressemble le terminal en question.\n\nDÃ©crivons dâ€™abord les diffÃ©rentes inscriptions qui arrivent Ã  lâ€™initialisation : - (base) : cette inscription nâ€™est pas directement liÃ©e au terminal, elle provient du fait que lâ€™on utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en dÃ©tail dans le chapitre sur la portabilitÃ© ; - coder@vscode-824991-64744dd6d8-zbgv5 : le nom de lâ€™utilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que lâ€™on verra lÃ  encore dans le chapitre sur la portabilitÃ© - ~/work : le chemin du rÃ©pertoire courant, i.e.Â Ã  partir duquel va Ãªtre lancÃ©e toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\nPour Ã©viter la lourdeur des images et permettre de copier/coller facilement les commandes, on reprÃ©sentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans lâ€™exemple suivant. Les lignes commenÃ§ant par un $ sont celles avec lesquelles une commande est lancÃ©e, et les lignes sans $ reprÃ©sentent le rÃ©sultat dâ€™une commande. Attention Ã  ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement Ã  diffÃ©rencier celles-ci des rÃ©sultats.\n$ echo \"une petite illustration\"\nune petite illustration\n\n\n\nLe terme filesystem (systÃ¨me de fichiers) dÃ©signe la maniÃ¨re dont sont organisÃ©s les fichiers au sein dâ€™un systÃ¨me dâ€™exploitation. Cette structure est hiÃ©rarchique, en forme dâ€™arbre : - elle part dâ€™un rÃ©pertoire racine (le dossier qui contient tous les autres) ; - contient des dossiers ; - les dossiers peuvent contenir Ã  leur tout des dossiers (sous-dossiers) ou des fichiers.\nIntÃ©ressons nous Ã  la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations : - la racine (root) sur Linux sâ€™appelle /, lÃ  oÃ¹ elle sâ€™appelle C:\\ par dÃ©faut sur Windows ; - le rÃ©pertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un rÃ´le essentiellement technique. Il est tout de mÃªme utile dâ€™en dÃ©crire les principaux : - /bin : contient les binaires, i.e.Â les programmes exÃ©cutables ; - /etc : contient les fichiers de configuration ; - /home : contient lâ€™ensemble des dossiers et fichiers personnels des diffÃ©rents utilisateurs. Chaque utilisateur a un rÃ©pertoire dit â€œHOMEâ€ qui a pour chemin /home/<username> Ce rÃ©pertoire est souvent reprÃ©sentÃ© par le symbole ~. Câ€™Ã©tait notamment le cas dans lâ€™illustration du terminal VSCode ci-dessus, ce qui signifie quâ€™on se trouvait formellement dans le rÃ©pertoire /home/coder/work, coder Ã©tant lâ€™utilisateur par dÃ©faut du service VSCode sur le SSP Cloud.\nChaque dossier ou fichier est reprÃ©sentÃ© par un chemin dâ€™accÃ¨s, qui correspond simplement Ã  sa position dans le filesystem. Il existe deux moyens de spÃ©cifier un chemin : - en utilisant un chemin absolu, câ€™est Ã  dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconnaÃ®t donc un chemin absolu par le fait quâ€™il commence forcÃ©ment par /. - en utilisant un chemin relatif, câ€™est Ã  dire en indiquant le chemin du dossier ou fichier relativement au rÃ©pertoire courant.\nComme tout ce qui touche de prÃ¨s ou de loin au terminal, la seule maniÃ¨re de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront dâ€™appliquer ces concepts Ã  des cas pratiques.\n\n\n\nLe rÃ´le dâ€™un terminal est de lancer des commandes. Ces commandes peuvent Ãªtre classÃ©es en trois grandes catÃ©gories : - navigation au sein du filesystem - manipulations de fichiers (crÃ©er, lire, modifier des dossiers/fichiers) - lancement de programmes\n\n\nLorsque lâ€™on lance un programme Ã  partir du terminal, celui-ci a pour rÃ©fÃ©rence le rÃ©pertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si lâ€™on exÃ©cute un script Python en se trouvant dans un certain rÃ©pertoire, tous les chemins des fichiers utilisÃ©s dans le script seront relatifs au rÃ©pertoire courant dâ€™exÃ©cution â€” Ã  moins dâ€™utiliser uniquement des chemins absolus, ce qui nâ€™est pas une bonne pratique en termes de reproductibilitÃ© puisque cela lie votre projet Ã  la structure de votre filesystem particulier.\nAinsi, la trÃ¨s grande majoritÃ© des opÃ©rations que lâ€™on est amenÃ© Ã  rÃ©aliser dans un terminal consiste simplement Ã  se dÃ©placer au sein du filesystem. Les commandes principales pour naviguer et se repÃ©rer dans le filesystem sont prÃ©sentÃ©es dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez pÃ©nible de manipuler des chemins absolus, qui peuvent facilement Ãªtre trÃ¨s longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient Ã  se dÃ©placer Ã  partir du rÃ©pertoire courant. Pour se faire, voici quelques utilisations trÃ¨s frÃ©quentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter dâ€™un niveau dans lâ€™arborescence (dossier parent)\n\n\ncd ~\nrevenir dans le rÃ©pertoire HOME de lâ€™utilisateur courant\n\n\n\nLa premiÃ¨re commande est lâ€™occasion de revenir sur une convention dâ€™Ã©criture importante pour les chemins relatifs : - . reprÃ©sente le rÃ©pertoire courant. Ainsi, cd . revient Ã  changer de rÃ©pertoire courantâ€¦ pour le rÃ©pertoire courant, ce qui bien sÃ»r ne change rien. Mais le . est trÃ¨s utile pour la copie de fichiers (cf.Â section suivante) ou encore lorsque lâ€™on doit passer des paramÃ¨tres Ã  un programme (cf.Â section Lancement de programmes) ; - .. reprÃ©sente le rÃ©pertoire parent du rÃ©pertoire courant.\nCes diffÃ©rentes commandes constituent la trÃ¨s grande majoritÃ© des usages dans un terminal. Il est essentiel de les pratiquer jusquâ€™Ã  ce quâ€™elles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup dâ€™autres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\ndÃ©placer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncrÃ©er (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncrÃ©er un fichier vide\n\n\n\nDans la mesure oÃ¹ il est gÃ©nÃ©ralement possible de rÃ©aliser toutes ces opÃ©rations Ã  lâ€™aide dâ€™interfaces graphiques (notamment, lâ€™explorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se dÃ©placer dans le filesystem. Nous vous recommandons malgrÃ© tout de les pratiquer Ã©galement, et ce pour plusieurs raisons : - effectuer un maximum dâ€™opÃ©rations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ; - en devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ; - lorsque lâ€™on est amenÃ© Ã  manipuler un terminal pour interagir avec un serveur, il nâ€™y a souvent pas la moindre interface graphique, auquel cas il nâ€™y a pas dâ€™autre choix que dâ€™opÃ©rer uniquement Ã  partir du terminal.\n\n\n\nLe rÃ´le du terminal est de lancer des programmes. Lancer un programme se fait Ã  partir dâ€™un fichier dit exÃ©cutable, qui peut Ãªtre de deux formes : - un binaire, i.e.Â un programme dont le code nâ€™est pas lisible par lâ€™humain ; - un script, i.e.Â un fichier texte contenant une sÃ©rie dâ€™instructions Ã  exÃ©cuter. Le langage du terminal Linux est le shell, et les scripts associÃ©s ont pour extension .sh.\nDans les deux cas, la syntaxe de lancement dâ€™une commande est : le nom de lâ€™exÃ©cutable, suivi dâ€™Ã©ventuels paramÃ¨tres, sÃ©parÃ©s par des espaces. Par exemple, la commande python monscript.py exÃ©cute le binaire python et lui passe comme unique argument le nom dâ€™un script .py (contenu dans le rÃ©pertoire courant), qui va donc Ãªtre exÃ©cutÃ© via Python. De la mÃªme maniÃ¨re, toutes les commandes vues prÃ©cÃ©demment pour se dÃ©placer dans le filesystem ou manipuler des fichiers sont des exÃ©cutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier Ã  copier et le chemin dâ€™arrivÃ©e.\nDans les exemples de commandes prÃ©cÃ©dents, les paramÃ¨tres Ã©taient passÃ©s en mode positionnel : lâ€™exÃ©cutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments nâ€™est pas toujours fixÃ© Ã  lâ€™avance, du fait de la prÃ©sence de paramÃ¨tres optionnels. Ainsi, la plupart des exÃ©cutables permettent le passage dâ€™arguments optionnels, qui modifient le comportement de lâ€™exÃ©cutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier Ã  un autre endroit du filesystem, mais peut-on copier un dossier et lâ€™ensemble de son contenu avec ? Nativement non, mais lâ€™ajout dâ€™un paramÃ¨tre le permet : cp -R dossierdepart dossierarrivee permet de copier rÃ©cursivement le dossier et tout son contenu. Notons que les flags ont trÃ¨s souvent un Ã©quivalent en toute lettre, qui sâ€™Ã©crit quant Ã  lui avec deux tirers. Par exemple, la commande prÃ©cÃ©dente peut sâ€™Ã©crire de maniÃ¨re Ã©quivalente cp --recursive dossierdepart dossierarrivee. Il est frÃ©quent de voir les deux syntaxes en pratique, parfois mÃªme mÃ©langÃ©es au sein dâ€™une mÃªme commande.\n\n\n\n\nComme tout langage de programmation, le langage shell permet dâ€™assigner et dâ€™utiliser des variables dans des commandes. Pour afficher le contenu dâ€™une variable, on utilise la commande echo, qui est lâ€™Ã©quivalent de la fonction print en Python ou en R.\n$ MY_VAR=\"toto\"\n$ echo $MY_VAR\ntoto\nQuelques remarques importantes : - la syntaxe pour la crÃ©ation de variable est prÃ©cise : aucun espace dâ€™un cÃ´tÃ© comme de lâ€™autre du = ; - en Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu Ã©crire MY_VAR=toto pour le mÃªme rÃ©sultat. Par contre, si lâ€™on veut assigner Ã  une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message dâ€™erreur ; - pour accÃ©der Ã  la valeur dâ€™une variable, on la prÃ©fixe dâ€™un $.\nNotre objectif avec ce tutoriel nâ€™est pas de savoir coder en shell, on ne va donc pas sâ€™attarder sur les propriÃ©tÃ©s des variables. En revanche, introduire ce concept Ã©tait nÃ©cessaire pour en prÃ©senter un autre, essentiel quant Ã  lui dans la pratique quotidienne du data scientist : les variables dâ€™environnement. Pour faire une analogie â€” un peu simpliste â€” avec les langages de programmation, ce sont des sortes de variables â€œglobalesâ€, dans la mesure oÃ¹ elles vont Ãªtre accessibles Ã  tous les programmes lancÃ©s Ã  partir dâ€™un terminal, et vont modifier leur comportement.\nLa liste des variables dâ€™environnement peut Ãªtre affichÃ©e Ã  lâ€™aide de la commande env. Il y a gÃ©nÃ©ralement un grand nombre de variables dâ€™environnement prÃ©Ã©xistantes ; en voici un Ã©chantillon obtenu Ã  partir du terminal du service VSCode.\n$ env\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la variÃ©tÃ© des utilisations des variables dâ€™environnements : - la variable $SHELL prÃ©cise lâ€™exÃ©cutable utilisÃ© pour lancer le terminal ; - la variable $HOME donne lâ€™emplacement du rÃ©pertoire utilisateur. En fait, le symbole ~ que lâ€™on a rencontrÃ© plus haut rÃ©fÃ©rence cette mÃªme variable ; - la variable LANG spÃ©cifie la locale, un concept qui permet de dÃ©finir la langue et lâ€™encodage utilisÃ©s par dÃ©faut par Linux ; - la variable CONDA_PYTHON_EXE existe uniquement parce que lâ€™on a installÃ© conda comme systÃ¨me de gestion de packages Python. Câ€™est lâ€™existance de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous intÃ©resse.\nUne variable dâ€™environnement essentielle, et que lâ€™on est frÃ©quemment amenÃ© Ã  modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concatÃ©nation de chemins absolus, sÃ©parÃ©s par :, qui spÃ©cifie les dossiers dans lesquels Linux va chercher les exÃ©cutables lorsque lâ€™on lance une commande, ainsi que lâ€™ordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n$ echo $PATH\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nLâ€™ordre de recherche est de gauche Ã  droite. Câ€™est donc parce que le dossier /home/coder/local/bin/conda/bin est situÃ© en premier que lâ€™interprÃ©teur Python qui sera choisi lorsque lâ€™on lance un script Python est celui issu de Conda, et non celui contenu par dÃ©faut dans /usr/bin par exemple.\nLâ€™existence et la configuration adÃ©quate des variables dâ€™environnement est essentielle pour le bon fonctionnement de nombreux outils trÃ¨s utilisÃ©s en data science, comme Git ou encore Spark par exemple. Il est donc nÃ©cessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration dâ€™un serveur en cas de bug liÃ© Ã  une variable dâ€™environnement manquante ou mal configurÃ©e.\n\n\n\nLa sÃ©curitÃ© est un enjeu central en Linux, qui permet une gestion trÃ¨s fine des permissions sur les diffÃ©rents fichiers et programmes.\nUne diffÃ©rence majeure par rapport Ã  dâ€™autres systÃ¨mes dâ€™exploitation, notamment Windows, est quâ€™aucun utilisateur nâ€™a par dÃ©faut les droits complets dâ€™administrateur (root). Il nâ€™est donc pas possible nativement dâ€™accÃ©der au parties sensibles du systÃ¨me, ou bien de lancer certains types de programme. Par exemple, si lâ€™on essaie de lister les fichiers du dossier /root, on obtient une erreur.\n$ ls /root\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines opÃ©rations telles que lâ€™installation de binaires ou de packages nÃ©cessitent cependant des droits administrateurs. Dans ce cas, il est dâ€™usage dâ€™utiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de lâ€™exÃ©cution de la commande.\n$ sudo ls /root\nLe dossier /root Ã©tant vide, la commande ls renvoie une chaÃ®ne de caractÃ¨res vide, mais nous nâ€™avons plus de problÃ¨me de permission. Notons quâ€™une bonne pratique de sÃ©curitÃ©, en particulier dans les scripts shell que lâ€™on peut Ãªtre amenÃ©s Ã  Ã©crire ou exÃ©cuter, est de limiter lâ€™utilisation de cette commande aux cas oÃ¹ elle sâ€™avÃ¨re nÃ©cessaire.\nUne autre subtilitÃ© concerne justement lâ€™exÃ©cution de scripts shell. Par dÃ©faut, quâ€™il soit crÃ©Ã© par lâ€™utilisateur ou tÃ©lÃ©chargÃ© dâ€™internet, un script nâ€™est pas exÃ©cutable.\n$ touch test.sh # CrÃ©er le script test.sh (vide)\n$ ./test.sh # ExÃ©cuter le script test.sh\nbash: ./test.sh: Permission denied\nCâ€™est bien entendu une mesure de sÃ©curitÃ© pour Ã©viter lâ€™exÃ©cution automatique de scripts potentiellement malveillants. Pour pouvoir exÃ©cuter un tel script, il faut attribuer des droits dâ€™exÃ©cution au fichier avec la commande chmod. Il devient alors possible dâ€™exÃ©cuter le script classiquement.\n$ chmod +x test.sh # Donner des droits d'exÃ©cution au script test.sh\n$ ./test.sh # ExÃ©cuter le script test.sh\n\n# Le script Ã©tant vide, il ne se passe rien\n\n\n\nMaintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell prÃ©cÃ©demment Ã©voquÃ©s. A lâ€™instar dâ€™un script Python, un script shell permet dâ€™automatiser une sÃ©rie de commandes lancÃ©es dans un terminal. Le but de ce tutoriel nâ€™est pas de savoir Ã©crire des scripts shell complexes, travail gÃ©nÃ©ralement dÃ©volu aux les data engineers ou les sysadmin (administrateurs systÃ¨me), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces compÃ©tences sont essentielles lorsque lâ€™on se prÃ©occupe de mise en production. A titre dâ€™exemple, comme nous le verrons dans le chapitre sur la portabilitÃ©, il est frÃ©quent dâ€™utiliser un script shell comme entrypoint dâ€™une image docker, afin de spÃ©cifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement Ã  lâ€™aide dâ€™un script simple. ConsidÃ©rons les commandes suivantes, que lâ€™on met dans un fichier monscript.sh dans le rÃ©pertoire courant.\n#!/bin/bash\nSECTION=$1\nCHAPTER=$2\nFORMATION_DIR=/home/coder/work/formation\nmkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\ntouch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\nAnalysons la structure de ce script : - la premiÃ¨re ligne est classique, elle se nomme le shebang : elle indique au systÃ¨me quel interprÃ©teur utiliser pour exÃ©cuter ce script. Dans notre cas, et de maniÃ¨re gÃ©nÃ©rale, on utilise bash (Bourne-Again SHell, lâ€™implÃ©mentation moderne du shell) ; - les lignes 2 et 3 assignent Ã  des variables les arguments passÃ©s au script dans la commande. Par dÃ©faut, ceux-ci sont assignÃ©s Ã  des variables n oÃ¹ n est la position de lâ€™argument, en commenÃ§ant Ã  1 ; - la ligne 4 assigne un chemin Ã  une variable - la ligne 5 crÃ©e le chemin complet, dÃ©fini Ã  partir des variables crÃ©Ã©es prÃ©cÃ©demment. Le paramÃ¨tre -p est important : il prÃ©cise Ã  mkdir dâ€™agir de maniÃ¨re rÃ©cursive, câ€™est Ã  dire de crÃ©er les dossiers intermÃ©diaires qui nâ€™existent pas encore ; - la ligne 6 crÃ©e un fichier texte vide dans le dossier crÃ©Ã© avec la commande prÃ©cÃ©dente.\nExÃ©cutons maintenant ce script, en prenant soin de lui donner les permission adÃ©quates au prÃ©alable.\n$ chmod +x monscript.sh\n$ bash monscript.sh section2 chapitre3\n$ ls formation/section1/chapitre2/\ntext.txt\nOpÃ©ration rÃ©ussie : le dossier a bien Ã©tÃ© crÃ©Ã© et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash trÃ¨s bien rÃ©alisÃ©e.\n\n\n\nUne diffÃ©rence fondamentale entre Linux et Windows tient Ã  la maniÃ¨re dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier exÃ©cutable en .exe) sur le site du logiciel, et on lâ€™exÃ©cute. En Linux, on passe gÃ©nÃ©ralement par un gestionnaire de packages qui va chercher les logiciels sur un rÃ©pertoire centralisÃ©, Ã  la maniÃ¨re de pip en Python par exemple.\nPourquoi cette diffÃ©rence ? Une raison importante est que, contrairement Ã  Windows, il existe une multitude de distributions diffÃ©rentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent diffÃ©remment et peuvent avoir diffÃ©rentes versions. En utilisant le package manager (gestionnaire de paquets) propre Ã  la distribution en question, on sâ€™assure de tÃ©lÃ©charger le logiciel adaptÃ© Ã  sa distribution. Dans ce cours, on fait le choix dâ€™utiliser une distribution Debian et son gestionnaire de paquets associÃ© apt. Debian est en effet un choix populaire pour les servers de part sa stabilitÃ© et sa simplicitÃ©, et sera Ã©galement familiÃ¨re aux utilisateurs dâ€™Ubuntu, distribution trÃ¨s populaire pour les ordinateurs personnels et qui est basÃ©e sur Debian.\nLâ€™utilisation dâ€™apt est trÃ¨s simple. La seule difficultÃ© est de savoir le nom du paquet que lâ€™on souhaite installer, ce qui nÃ©cessite en gÃ©nÃ©ral dâ€™utiliser un moteur de recherche. Lâ€™installation de paquets est Ã©galement un cas oÃ¹ il faut utiliser sudo, puisque cela implique souvent lâ€™accÃ¨s Ã  des rÃ©pertoires protÃ©gÃ©s.\n$ sudo apt install tree\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nDÃ©sinstaller un package est Ã©galement simple : câ€™est lâ€™opÃ©ration inverse. Par sÃ©curitÃ©, le terminal vous demande si vous Ãªtes sÃ»r de votre choix en vous demandant de tapper la lettre y ou la lettre n.Â On peut passer automatiquement cette Ã©tape en ajoutant le paramÃ¨tre -y\n$ sudo apt remove -y tree\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant dâ€™installer un package, il est toujours prÃ©fÃ©rable de mettre Ã  jour la base des packages, pour sâ€™assurer quâ€™on obtiendra bien la derniÃ¨re version.\n$ sudo apt update\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date.\n\n\n\nOn lâ€™a dit et redit : devenir Ã  lâ€™aise avec le terminal Linux est essentiel et demande de la pratique. Il existe nÃ©anmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa premiÃ¨re est lâ€™autocomplÃ©tion. DÃ¨s lors que vous Ã©crivez une commande contenant un nom dâ€™exÃ©cutable, un chemin sur le filesystem, ou autre, nâ€™hÃ©sitez pas Ã  utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majoritÃ© des cas, cela va vous faire gagner un temps prÃ©cieux.\nUne seconde astuce, qui nâ€™en est pas vraiment une, est de lire la documentation dâ€™une commande lorsque lâ€™on nâ€™est pas sÃ»r de sa syntaxe ou des paramÃ¨tres admissibles. Via le terminal, la documentation dâ€™une commande peut Ãªtre affichÃ©e en exÃ©cutant man suivie de la commande en question, par exemple : man cp. Comme il nâ€™est pas toujours trÃ¨s pratique de lire de longs textes dans un petit terminal, on peut Ã©galement chercher la documentation dâ€™une commande sur le site man7."
  },
  {
    "objectID": "chapters/portability.html",
    "href": "chapters/portability.html",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "",
    "text": "Dans les chapitres prÃ©cÃ©dents, nous avons vu un ensemble de bonnes pratiques qui permettent de considÃ©rablement amÃ©liorer la qualitÃ© dâ€™un projet : rendre le code plus lisible, adopter une structure du projet normalisÃ©e et Ã©volutive, et versionner proprement son code sur un dÃ©pÃ´t GitHub.\nUne fois ces bonnes pratiques appliquÃ©es Ã  notre projet, ce dernier apparaÃ®t largement partageable. Du moins en thÃ©orie, car la pratique est souvent plus compliquÃ©e : il y a fort Ã  parier que si vous essayez dâ€™exÃ©cuter votre projet sur un autre environnement dâ€™exÃ©cution (un autre ordinateur, un serveur, etc.), les choses ne se passent pas du tout comme attendu. Cela signifie que quâ€™en lâ€™Ã©tat, le projet nâ€™est pas portable : il nâ€™est pas possible, sans modifications coÃ»teuses, de lâ€™exÃ©cuter dans un environnement diffÃ©rent de celui dans lequel il a Ã©tÃ© dÃ©veloppÃ©.\nLa principale raison est quâ€™un code ne vit pas dans une bulle isolÃ©e, il contient en gÃ©nÃ©ral de nombreuses adhÃ©rences, plus ou moins visibles, au langage et Ã  lâ€™environnement dans lesquels il a Ã©tÃ© dÃ©veloppÃ© :\n\ndes dÃ©pendances dans le langage du projet ;\ndes dÃ©pendances dans dâ€™autres langages (ex : NumPy est Ã©crit en C et nÃ©cessite donc un compilateur C) ;\ndes librairies systÃ¨mes nÃ©cessaires pour installer certains packages (par exemple, les librairies de cartographie dynamique comme Leaflet ou Folium nÃ©cessitent la librairie systÃ¨me GDAL), qui ne seront pas les mÃªmes selon le systÃ¨me dâ€™exploitation utilisÃ©.\n\nSi le premier problÃ¨me peut Ãªtre gÃ©rÃ© relativement facilement en adoptant une structure de projet et en spÃ©cifiant bien les diffÃ©rentes dÃ©pendances utilisÃ©es, par exemple avec un fichier requirements.txt, les deux autres nÃ©cessitent en gÃ©nÃ©ral des outils plus avancÃ©s.\nCes outils vont nous permettre de normaliser lâ€™environnement afin de produire un projet portable, i.e.Â exÃ©cutable sur une large variÃ©tÃ© dâ€™environnements dâ€™exÃ©cution. Cette Ã©tape est primordiale lorsque lâ€™on se prÃ©occupe de la mise en production dâ€™un projet, car elle assure une transition relativement indolore entre lâ€™environnement de dÃ©veloppement et celui de production.\n\n\n\n\n\nImage empruntÃ©e Ã  [devrant.com}(https://devrant.com/rants/174386/when-i-say-but-it-works-on-my-machine)"
  },
  {
    "objectID": "chapters/portability.html#introduction",
    "href": "chapters/portability.html#introduction",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Introduction",
    "text": "Introduction\nPour illustrer lâ€™importance de travailler avec des environnements virtuels, mettons-nous Ã  la place dâ€™un.e aspirant.e data scientist qui commencerait ses premiers projets.\nSelon toute vraisemblance, on va commencer par installer une distribution de Python â€” souvent, via Anaconda â€” sur son poste et commencer Ã  dÃ©velopper, projet aprÃ¨s projet. Dans cette approche, les diffÃ©rents packages quâ€™on va Ãªtre amenÃ© Ã  utiliser vont Ãªtre installÃ©s au mÃªme endroit. Cela pose plusieurs problÃ¨mes :\n\nconflits de version : une application A peut dÃ©pendre de la version 1 dâ€™un package lÃ  oÃ¹ une application B peut dÃ©pendre de la version 2 de ce mÃªme package. Une seule application peut donc fonctionner dans cette configuration ;\nversion de Python fixe â€” on ne peut avoir quâ€™une seule installation par systÃ¨me â€” lÃ  oÃ¹ on voudrait pouvoir avoir des versions diffÃ©rentes selon le projet ;\nreproductiblitÃ© limitÃ©e : difficile de dire quel projet repose sur tel package, dans la mesure oÃ¹ ceux-ci sâ€™accumulent en un mÃªme endroit au fil des projets ;\nportabilitÃ© limitÃ©e : consÃ©quence du point prÃ©cÃ©dent, il est difficile de fixer dans un fichier les dÃ©pendances spÃ©cifiques Ã  un projet.\n\nLes environnements virtuels constituent une solution Ã  ces diffÃ©rents problÃ¨mes."
  },
  {
    "objectID": "chapters/portability.html#fonctionnement",
    "href": "chapters/portability.html#fonctionnement",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nLe concept dâ€™environnement virtuel est techniquement trÃ¨s simple. On peut lui donner la dÃ©finition suivante pour Python :\n\nâ€œdossier auto-suffisant qui contient une installation de Python pour une version particuliÃ¨re de Python ainsi que des packages additionnels et qui est isolÃ© des autres environnements existants.â€\n\nOn peut donc simplement voir les environnements virtuels comme un moyen de faire cohabiter sur un mÃªme systÃ¨me diffÃ©rentes installations de Python avec chacune leur propre liste de packages installÃ©s et leurs versions. DÃ©velopper dans des environnements virtuels vierges Ã  chaque dÃ©but de projet est une trÃ¨s bonne pratique pour accroÃ®tre la reproductibilitÃ© des analyses."
  },
  {
    "objectID": "chapters/portability.html#implÃ©mentations",
    "href": "chapters/portability.html#implÃ©mentations",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "ImplÃ©mentations",
    "text": "ImplÃ©mentations\nIl existe diffÃ©rentes implÃ©mentations des environnements virtuels en Python, dont chacune ont leurs spÃ©cificitÃ©s et leur communautÃ© dâ€™utilisateurs :\n\nLâ€™implÃ©mentation standard en Python est venv.\nDans le domaine de la data science, lâ€™implÃ©mentation la plus courante est sans doute conda.\n\nEn pratique, ces implÃ©mentations sont relativement proches. La diffÃ©rence majeure est que conda est Ã  la fois un package manager (comme pip) et un gestionnaire dâ€™environnements virtuels (comme venv).\nPendant longtemps, conda en tant que package manager sâ€™est avÃ©rÃ© trÃ¨s pratique en data science, dans la mesure oÃ¹ il gÃ©rait non seulement les dÃ©pendances Python mais aussi dans dâ€™autres langages â€” comme des dÃ©pendances C. Lâ€™autre diffÃ©rence majeure avec pip est que Conda utilise une mÃ©thode plus avancÃ©e â€” et donc Ã©galement plus coÃ»teuse en temps â€” de rÃ©solution des dÃ©pendances1. En effet, diffÃ©rents packages peuvent spÃ©cifier diffÃ©rentes versions dâ€™un mÃªme package dont ils dÃ©pendent tous les deux, ce qui provoque un conflit de version. Conda va par dÃ©faut appliquer un algorithme qui vise Ã  gÃ©rer au mieux ces conflits, lÃ  oÃ¹ pip va choisir une approche plus minimaliste. Enfin, la distribution Anaconda, qui contient Ã  la fois Python, conda et beaucoup de packages utiles pour la data science, explique Ã©galement cette popularitÃ© auprÃ¨s des data scientists.\nPour toutes ces raisons, nous allons prÃ©senter lâ€™utilisation de conda comme gestionnaire dâ€™environnements virtuels. Les principes prÃ©sentÃ©s restent nÃ©anmoins valides pour les autres implÃ©mentations."
  },
  {
    "objectID": "chapters/portability.html#conda",
    "href": "chapters/portability.html#conda",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Conda",
    "text": "Conda\n\nInstallation\nLes instructions Ã  suivre pour installer conda sont dÃ©taillÃ©es dans la documentation officielle. conda seul Ã©tant peu utile en pratique, il est gÃ©nÃ©ralement installÃ© dans le cadre de distributions. Les deux plus populaires sont :\n\nMiniconda : une distribution minimaliste contenant conda, Python ainsi quâ€™un petit nombre de packages techniques trÃ¨s utiles ;\nAnaconda : une distribution assez volumineuse contenant conda, Python, dâ€™autres logiciels (R, Spyder, etc.) ainsi quâ€™un ensemble de packages utiles pour la data science (SciPy, NumPy, etc.).\n\nLe choix de la distribution importe assez peu en pratique, dans la mesure oÃ¹ nous allons de toute maniÃ¨re utiliser des environnements virtuels vierges pour dÃ©velopper nos projets.\nLâ€™Ã©cosystÃ¨me Conda\n\n\n\nEn pratique\n\nCrÃ©er un environnement\nPour commencer Ã  utiliser conda, commenÃ§ons par crÃ©er un environnement vierge, nommÃ© dev, en spÃ©cifiant la version de Python que lâ€™on souhaite installer pour notre projet.\n$ conda create -n dev python=3.9.7\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/coder/local/bin/conda/envs/dev\n\n  added / updated specs:\n    - python=3.9.7\n\n\nThe following packages will be downloaded:\n...\nThe following NEW packages will be INSTALLED:\n...\nProceed ([y]/n)? y\nDownloading and Extracting Packages\n...\nComme indiquÃ© dans les logs, Conda a crÃ©Ã© notre environnement et nous indique son emplacement sur le filesystem. En rÃ©alitÃ©, lâ€™environnement nâ€™est jamais vraiment vierge : Conda nous demande â€” et il faut rÃ©pondre oui en tapant â€œyâ€ â€” dâ€™installer un certain nombre de packages, qui sont ceux qui viennent avec la distribution Miniconda.\nOn peut vÃ©rifier que lâ€™environnement a bien Ã©tÃ© crÃ©Ã© en listant les environnements installÃ©s sur le systÃ¨me.\nconda info --envs\n# conda environments:\n#\nbase                    * /home/coder/local/bin/conda\nbasesspcloud              /home/coder/local/bin/conda/envs/basesspcloud\ndev                       /home/coder/local/bin/conda/envs/dev\n\n\nActiver un environnement\nComme plusieurs environnements peuvent coexister sur un mÃªme systÃ¨me, il faut spÃ©cifier Ã  Conda que lâ€™on souhaite utiliser cet environnement pour la session courante du terminal.\n$ conda activate dev\nConda nous indique que lâ€™on travaille Ã  partir de maintenant dans lâ€™environnement dev en indiquant son nom entre parenthÃ¨ses au dÃ©but de la ligne de commandes. Autrement dit, dev devient pour un temps notre environnement par dÃ©faut. Pour sâ€™en assurer, vÃ©rifions avec la commande which lâ€™emplacement de lâ€™interprÃ©teur Python qui sera utilisÃ© si on lance une commande du type python mon-script.py.\n(dev) $ which python \n/home/coder/local/bin/conda/envs/dev/bin/python\nOn travaille bien dans lâ€™environnement attendu : lâ€™interprÃ©teur qui se lance nâ€™est pas celui du systÃ¨me global, mais bien celui spÃ©cifique Ã  notre environnement virtuel.\n\n\nLister les packages installÃ©s\nUne fois lâ€™environnement activÃ©, on peut lister les packages installÃ©s et leur version. Cela confirme quâ€™un certain nombre de packages sont installÃ©s par dÃ©faut lors de la crÃ©ation dâ€™un environnement virtuel.\n(dev) $ conda list\n# packages in environment at /home/coder/local/bin/conda/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                        main  \n_openmp_mutex             4.5                       1_gnu  \nca-certificates           2022.3.29            h06a4308_0  \n...\n\n\nInstaller un package\nLa syntaxe pour installer un package avec Conda est trÃ¨s similaire Ã  celle de pip :\nconda install nom_du_package\nLa diffÃ©rence est que lÃ  oÃ¹ pip install va installer un package Ã  partir du rÃ©pertoire PyPI, conda install va chercher le package sur les rÃ©pertoires maintenus par les dÃ©veloppeurs de Conda2. Installons par exemple le package phare de machine learning scikit-learn.\n(dev) $ conda install scikit-learn\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /home/coder/local/bin/conda/envs/dev\n\n  added / updated specs:\n    - scikit-learn\n...\nLÃ  encore, Conda nous demande dâ€™installer dâ€™autres packages, qui sont des dÃ©pendances de scikit-learn. Par exemple, la librairie de calcul scientifique NumPy.\nIl arrive que des packages disponibles sur le rÃ©pertoire PyPI ne soient pas disponible sur les canaux gÃ©rÃ©s par Conda. Dans ce cas, il est possible dâ€™installer un package dans lâ€™environnement via la commande pip install. Il est nÃ©anmonins toujours prÃ©fÃ©rable de privilÃ©gier une installation via Conda si disponible.\n\n\nExporter les spÃ©cifications de lâ€™environnement\nDÃ©velopper Ã  partir dâ€™un environnement vierge est une bonne pratique de reproductibilitÃ© : en partant dâ€™une base minimale, on sâ€™assure que seuls les packages effectivement nÃ©cessaires au bon fonctionnement de notre application ont Ã©tÃ© installÃ©s au fur et Ã  mesure du projet.\nCela rend Ã©galement notre projet plus portable : on peut exporter les spÃ©cifications de lâ€™environnement (version de Python, canaux de tÃ©lÃ©chargement des packages, packages installÃ©s et leurs versions) dans un fichier, appelÃ© par convention environment.yml.\n(dev) $ conda env export > environment.yml\nCe fichier est mis par convention Ã  la racine du dÃ©pÃ´t Git du projet. Ainsi, les personnes souhaitant tester lâ€™application peuvent recrÃ©er le mÃªme environnement Conda que celui qui a servi au dÃ©veloppement via la commande suivante.\n$ conda env create -f environment.yml\n\n\nChanger dâ€™environnement\nPour changer dâ€™environnement, il suffit dâ€™en activer un autre.\n(dev) $ conda base\n(base) $ \nPour sortir de tout environnement Conda, on utilise la commande conda deactivate :\n(base) $ conda deactivate\n$ \n\n\nSupprimer un environnement\nPour supprimer lâ€™environnement dev, on utilise la commande conda env remove -n dev.\n\n\n\nAide-mÃ©moire\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\nconda create -n <env_name> python=<python_version>\nCrÃ©ation dâ€™un environnement nommÃ© <env_name> dont la version de Python est <python_version>\n\n\nconda info --envs\nLister les environnements\n\n\nconda activate <env_name>\nUtiliser lâ€™environnement <env_name> pour la session du terminal\n\n\nconda list\nLister les packages dans lâ€™environnement actif\n\n\nconda install <pkg>\nInstaller le package <pkg> dans lâ€™environnement actif\n\n\nconda env export > environment.yml\nExporter les spÃ©cifications de lâ€™environnement dans un fichier environment.yml"
  },
  {
    "objectID": "chapters/portability.html#limites",
    "href": "chapters/portability.html#limites",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Limites",
    "text": "Limites\nDÃ©velopper dans des environnements virtuels est une bonne pratique, car cela accroÃ®t la portabilitÃ© dâ€™une application. NÃ©anmoins, il y a plusieurs limites Ã  leur utilisation :\n\nles librairies systÃ¨me nÃ©cessaires Ã  lâ€™installation des packages ne sont pas gÃ©rÃ©es ;\nles environnements virtuels ne permettent pas toujours de gÃ©rer des projets faisant intervenir diffÃ©rents langages de programmation ;\ndevoir installer conda, Python, et les packages nÃ©cessaires Ã  chaque changement dâ€™environnement peut Ãªtre assez long et pÃ©nible en pratique ;\ndans un environnement de production, gÃ©rer des environnements virtuels diffÃ©rents pour chaque projet peut sâ€™avÃ©rer rapidement complexe pour les administrateurs systÃ¨me.\n\nLa technologie des conteneurs permet de rÃ©pondre Ã  ces diffÃ©rents problÃ¨mes."
  },
  {
    "objectID": "chapters/portability.html#introduction-1",
    "href": "chapters/portability.html#introduction-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Introduction",
    "text": "Introduction\nAvec les environnements virtuels, lâ€™idÃ©e Ã©tait de permettre Ã  chaque utilisateur potentiel de notre projet dâ€™installer sur son environnement dâ€™exÃ©cution les packages nÃ©cessaires Ã  la bonne exÃ©cution du projet. NÃ©anmoins, comme on lâ€™a vu, cette approche ne garantit pas une reproductibilitÃ© parfaite et a lâ€™inconvÃ©nient de nÃ©cessiter beaucoup de gestion manuelle.\nChangeons de perspective : au lieu de distribuer une recette permettant Ã  lâ€™utilisateur de recrÃ©er lâ€™environnement nÃ©cessaire sur sa machine, ne pourrait-on pas directement distribuer Ã  lâ€™utilisateur une machine contenant lâ€™environnement prÃ©-configurÃ© ?\nBien entendu, on ve pas configurer et envoyer des ordinateurs portables Ã  tous les utilisateurs potentiels dâ€™un projet. Une autre solution serait de distribuer des machines virtuelles, qui tournent sur un serveur et simulent un vÃ©ritable ordinateur. Ces machines ont cependant lâ€™inconvÃ©nient dâ€™Ãªtre assez lourdes, et complexes Ã  rÃ©pliquer et distribuer. Pour pallier ces diffÃ©rentes limites, on va utiliser la technologie des conteneurs.\n\n\n\n\n\nImage trouvÃ©e sur reddit"
  },
  {
    "objectID": "chapters/portability.html#fonctionnement-1",
    "href": "chapters/portability.html#fonctionnement-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Fonctionnement",
    "text": "Fonctionnement\nComme les machines virtuelles, les conteneurs permettent dâ€™empaqueter complÃ¨tement lâ€™environnement (librairies systÃ¨mes, application, configuration) qui permet de faire tourner lâ€™application. Mais Ã  lâ€™inverse dâ€™une machine virtuelle, le conteneur nâ€™inclut pas de systÃ¨me dâ€™exploitation propre, il utilise celui de la machine hÃ´te qui lâ€™exÃ©cute. La technologie des conteneurs permet ainsi de garantir une trÃ¨s forte reproductibilitÃ© tout en restant suffisamment lÃ©gÃ¨re pour permettre une distribution et un dÃ©ploiement simple aux utilisateurs.\nDiffÃ©rences entre lâ€™approche conteneurs (gauche) et lâ€™approche machines virtuelles (droite)\n\nSource : docker.com"
  },
  {
    "objectID": "chapters/portability.html#implÃ©mentations-1",
    "href": "chapters/portability.html#implÃ©mentations-1",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "ImplÃ©mentations",
    "text": "ImplÃ©mentations\nComme pour les environnements virtuels, il existe diffÃ©rentes implÃ©mentations de la technologie des conteneurs. En pratique, lâ€™implÃ©mentation offerte par Docker est devenue largement prÃ©dominante, au point quâ€™il est devenu courant dâ€™utiliser de maniÃ¨re interchangeable les termes â€œconteneuriserâ€ et â€œDockeriserâ€ une application. Câ€™est donc cette implÃ©mentation que nous allons Ã©tudier et utiliser dans ce cours."
  },
  {
    "objectID": "chapters/portability.html#docker",
    "href": "chapters/portability.html#docker",
    "title": "Rendre son projet de data science portable et reproductible",
    "section": "Docker ",
    "text": "Docker \n\nInstallation\nLes instructions Ã  suivre pour installer Docker  selon son systÃ¨me dâ€™exploiration sont dÃ©taillÃ©es dans la documentation officielle. Il existe Ã©galement des environnements bacs Ã  sable en ligne comme Play with Docker.\n\n\nPrincipes\nUn conteneur Docker est mis Ã  disposition sous la forme dâ€™une image, câ€™est Ã  dire dâ€™un fichier binaire qui contient lâ€™environnement nÃ©cessaire Ã  lâ€™exÃ©cution de lâ€™application.\nPour construire (build) lâ€™image, on utilise un Dockerfile, un fichier texte qui contient la recette â€” sous forme de commandes Linux â€” de construction de lâ€™environnement. Lâ€™image va Ãªtre uploadÃ©e (push) sur un dÃ©pÃ´t (registry), public ou privÃ©, depuis lequel les utilisateurs vont pouvoir tÃ©lÃ©charger lâ€™image (pull). Le moteur Docker permet ensuite de lancer (run) un conteneur, câ€™est Ã  dire une instance vivante de lâ€™image.\n\n\n\n\n\n\nNote\n\n\n\nLe rÃ©pertoire dâ€™images publiques le plus connu est DockerHub. Il sâ€™agit dâ€™un rÃ©pertoire oÃ¹ nâ€™importe qui peut proposer une image Docker, associÃ©e ou non Ã  un projet disponible sur Github ou Gitlab. Il est possible de mettre Ã  disposition de maniÃ¨re manuelle des images mais, comme nous le montrerons dans le chapitre sur la mise en production, il est beaucoup plus pratique dâ€™utiliser des fonctionalitÃ©s dâ€™interaction automatique entre DockerHub et un dÃ©pÃ´t GitHub.\n\n\n\n\nEn pratique\n\nApplication\nAfin de prÃ©senter lâ€™utilisation de Docker en pratique, nous allons prÃ©senter les diffÃ©rentes Ã©tapes permettant de â€œdockeriserâ€ une application web minimaliste construite avec le framework Python Flask3.\nLa structure de notre projet est la suivante.\nâ”œâ”€â”€ myflaskapp\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ hello-world.py\nâ”‚   â””â”€â”€ requirements.txt\nLe script hello-world.py contient le code dâ€™une application minimaliste, qui affiche simplement â€œHello, World!â€ sur une page web.\nfrom flask import Flask\n\napp = Flask(__name__)\n\n\n@app.route(\"/\")\ndef hello_world():\n    return \"<p>Hello, World!</p>\"\nPour faire tourner lâ€™application, il nous faut donc Ã  la fois Python et le package Flask. Ces installations doivent Ãªtre spÃ©cifiÃ©es dans le Dockerfile (cf.Â section suivante). Lâ€™installation de Flask se fait via un fichier requirements.txt, qui contient juste la ligne suivante :\nFlask==2.1.1\n\n\nLe Dockerfile\nA lÃ  base de chaque image Docker se trouve un Dockerfile. Câ€™est un fichier texte qui contient une sÃ©rie de commandes qui permettent de construire lâ€™image. Ces fichiers peuvent Ãªtre plus ou moins complexes selon lâ€™application que lâ€™on cherche Ã  conteneuriser, mais leur structure est assez normalisÃ©e. Pour sâ€™en rendre compte, analysons ligne Ã  ligne le Dockerfile nÃ©cessaire pour construire une image Docker de notre application Flask.\nFROM ubuntu:20.04\n\nRUN apt-get update -y && \\\n    apt-get install -y python3-pip python3-dev\n    \nWORKDIR /app\n\nCOPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY . /app\n\nENV FLASK_APP=\"hello-world.py\"\nEXPOSE 5000\n\nCMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n\nFROM : spÃ©cifie lâ€™image de base. Une image Docker hÃ©rite toujours dâ€™une image de base. Ici, on choisit lâ€™image Ubuntu version 20.04, tout va donc se passer comme si lâ€™on dÃ©veloppait sur une machine virtuelle vierge ayant pour systÃ¨me dâ€™exploitation Ubuntu 20.044 ;\nRUN : lance une commande Linux. Ici, on met dâ€™abord Ã  jour la liste des packages tÃ©lÃ©chargeables via apt, puis on installe Python ainsi que des librairies systÃ¨me nÃ©cessaires au bon fonctionnement de notre application ;\nWORKDIR : spÃ©cifie le rÃ©pertoire de travail de lâ€™image. Ainsi, toutes les commandes suivantes seront exÃ©cutÃ©es depuis ce rÃ©pertoire ;\nCOPY : copie un fichier local sur lâ€™image Docker. Ici, on copie dâ€™abord le fichier requirements.txt du projet, qui spÃ©cifie les dÃ©pendances Python de notre application, puis on les installe avec une commande RUN. La seconde instruction COPY copie le rÃ©pertoire du projet sur lâ€™image ;\nENV : crÃ©e une variable dâ€™environnement qui sera accessible Ã  lâ€™application dans le conteneur. Ici, on dÃ©finit une variable dâ€™environnement attendue par Flask, qui spÃ©cifie le nom du script permettant de lancer lâ€™application ;\nEXPOSE : informe Docker que le conteneur â€œÃ©couteâ€ sur le port 5000, qui est le port par dÃ©faut utilisÃ© par le serveur web de Flask ;\nCMD : spÃ©cifie la commande que doit exÃ©cuter le conteneur lors de son lancement. Il sâ€™agit dâ€™une liste, qui contient les diffÃ©rentes parties de la commande sous forme de chaÃ®nes de caractÃ¨res. Ici, on lance Flask, qui sait automatiquement quelle application lancer du fait de la commande ENV spÃ©cifiÃ©e prÃ©cÃ©demment.\n\n\n\n\n\n\n\nTip\n\n\n\nAvec la premiÃ¨re commande RUN du Dockerfile, nous installons Python mais aussi des librairies systÃ¨me nÃ©cessaires au bon fonctionnement de lâ€™application. Mais comment les avons-nous trouvÃ©es ?\nPar essai et erreur. Lors de lâ€™Ã©tape de build que lâ€™on verra juste aprÃ¨s, le moteur Docker va essayer de construire lâ€™image selon les spÃ©cifications du Dockerfile, comme sâ€™il partait dâ€™un ordinateur vide contenant simplement Ubuntu 20.04. Si des librairies manquent, le processus de build devrait renvoyer une erreur, qui sâ€™affichera dans les logs de lâ€™application, affichÃ©s par dÃ©faut dans la console. Quand on a de la chance, les logs dÃ©crivent explicitement les librairies systÃ¨me manquantes. Mais souvent, les messages dâ€™erreur ne sont pas trÃ¨s explicites, et il faut alors les copier dans un moteur de recherche bien connu pour trouver la rÃ©ponse, souvent sur Stackoverflow.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLa recette prÃ©sente dans le Dockerfile peut nÃ©cessiter lâ€™utilisation de fichiers appartenant au dossier de travail. Pour que Docker les trouve dans son contexte, il est nÃ©cessaire dâ€™introduire une commande COPY. Câ€™est un petit peu comme pour la cuisine: pour utiliser un produit dans une recette, il faut le sortir du frigo (fichier local) et le mettre sur la table.\n\n\n\n\n\n\n\n\nNote\n\n\n\nNous nâ€™avons vu que les commandes Docker les plus frÃ©quentes, il en existe beaucoup dâ€™autres en pratique. Nâ€™hÃ©sitez pas Ã  consulter la documentation officielle pour comprendre leur utilisation.\n\n\n\n\nConstruction dâ€™une image Docker\nPour construire une image Ã  partir dâ€™un Dockerfile, il suffit dâ€™utiliser la commande docker build. Il faut ensuite spÃ©cifier deux Ã©lÃ©ments importnats : - le build context. Il faut indiquer Ã  Docker le chemin de notre projet, qui doit contenir le Dockerfile. En pratique, il est plus simple de se mettre dans le dossier du projet via la commande cd, puis de passer . comme build context pour indiquer Ã  Docker de build â€œdâ€™iciâ€ ; - le tag, câ€™est Ã  dire le nom de lâ€™image. Tant que lâ€™on utilisee Docker en local, le tag importe peu. On verra par la suite que la structure du tag a de lâ€™importance lorsque lâ€™on souhaite exporter ou importer une image Docker Ã  partir dâ€™un dÃ©pÃ´t distant.\nRegardons ce qui se passe en pratique lorsque lâ€™on essaie de construire notre image.\n$ docker build -t myflaskapp .\nSending build context to Docker daemon     47MB\nStep 1/8 : FROM ubuntu:20.04\n ---> 825d55fb6340\nStep 2/8 : RUN apt-get update && apt-get install -y python3-pip python3-dev\n ---> Running in 92b42d579cfa\n...\ndone.\nRemoving intermediate container 92b42d579cfa\n ---> 8826d53e3c01\nStep 3/8 : WORKDIR /app\n ---> Running in 153b32893c23\nRemoving intermediate container 153b32893c23\n ---> 7b4d22021986\nStep 4/8 : COPY requirements.txt /app/requirements.txt\n...\nSuccessfully built 125bd8da70ff\nSuccessfully tagged myflaskapp:latest\nLe moteur Docker essaie de construire notre image sÃ©quentiellement Ã  partir des commandes spÃ©cifiÃ©es dans le Dockerfile. Sâ€™il rencontre une erreur, la procÃ©dure sâ€™arrÃªte, et il faut alors trouver la source du problÃ¨me dans les logs et adapter le Dockerfile en consÃ©quence. Si tout se passe bien, Docker nous indique que le build a rÃ©ussi et lâ€™image est prÃªte Ã  Ãªtre utilisÃ©e. On peut vÃ©rifier que lâ€™image est bien disponible Ã  lâ€™aide de la commande docker images.\n$ docker images\nREPOSITORY                               TAG       IMAGE ID       CREATED          SIZE\nmyflaskapp                               latest    57d2f410a631   2 hours ago      433MB\nIntÃ©ressons nous un peu plus en dÃ©tail aux logs de lâ€™Ã©tape de build. Entre les Ã©tapes, Docker affiche des suites de lettres et de chiffres un peu Ã©sotÃ©riques, et nous parle de conteneurs intermÃ©diaires. En fait, il faut voir une image Docker comme un empilement de couches (layers), qui sont elles-mÃªmes des images Docker. Quand on hÃ©rite dâ€™une image avec lâ€™instruction FROM, on spÃ©cifie donc Ã  Docker la couche initiale, sur laquelle il va construire le reste de notre environnement. A chaque Ã©tape sa nouvelle couche, et Ã  chaque couche son hash, un identifiant unique fait de lettres et de chiffres.\nCela peut ressembler Ã  des dÃ©tails techniques, mais câ€™est en fait extrÃªmement utile en pratique car cela permet Ã  Docker de faire du caching. Lorsque lâ€™on dÃ©veloppe un Dockerfile, il est frÃ©quent de devoir modifier ce dernier de nombreuses fois avant de trouver la bonne recette, et on aimerait bien ne pas avoir Ã  rebuild lâ€™environnement complet Ã  chaque fois. Docker gÃ¨re cela trÃ¨s bien : il cache chacune des couches intermÃ©diaires. Par exemple, si lâ€™on modifie la 5Ã¨me commande du Dockerfile, Docker va utiliser le cache pour ne pas avoir Ã  recalculer les Ã©tapes prÃ©cÃ©dentes, qui nâ€™ont pas changÃ©. Cela sâ€™appelle lâ€™â€œinvalidation du cacheâ€ : dÃ¨s lors quâ€™une Ã©tape du Dockerfile est modifiÃ©e, Docker va recalculer toutes les Ã©tapes suivantes, mais seulement celles-ci. ConsÃ©quence directe de cette observation : il faut toujours ordonner les Ã©tapes dâ€™un Dockerfile de sorte Ã  ce qui est le plus susceptible dâ€™Ãªtre souvent modifiÃ© soit Ã  la fin du fichier, et inversement.\nPour illustrer cela, regardons ce qui se passe si lâ€™on modifie le nom du script qui lance lâ€™application, et donc la valeur de la variable dâ€™environnement FLASK_APP dans le Dockerfile.\n$ docker build . -t myflaskapp\nSending build context to Docker daemon  4.096kB\nStep 1/10 : FROM ubuntu:20.04\n ---> 825d55fb6340\nStep 2/10 : ENV DEBIAN_FRONTEND=noninteractive\n ---> Using cache\n ---> ea1c7c083ac9\nStep 3/10 : RUN apt-get update -y &&     apt-get install -y python3-pip python3-dev\n ---> Using cache\n ---> 078b8ac0e1cb\nStep 4/10 : WORKDIR /app\n ---> Using cache\n ---> cd19632825b3\nStep 5/10 : COPY requirements.txt /app/requirements.txt\n ---> Using cache\n ---> 271cd1686899\nStep 6/10 : RUN pip install -r requirements.txt\n ---> Using cache\n ---> 3ea406fdf383\nStep 7/10 : COPY . /app\n ---> 3ce5bd3a9572\nStep 8/10 : ENV FLASK_APP=\"new.py\"\n ---> Running in b378d16bb605\nRemoving intermediate container b378d16bb605\n ---> e1f50490287b\nStep 9/10 : EXPOSE 5000\n ---> Running in ab53c461d3de\nRemoving intermediate container ab53c461d3de\n ---> 0b86eca40a80\nStep 10/10 : CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n ---> Running in 340eec151a51\nRemoving intermediate container 340eec151a51\n ---> 16d7a5b8db28\nSuccessfully built 16d7a5b8db28\nSuccessfully tagged myflaskapp:latest\nLâ€™Ã©tape de build a pris quelques secondes au lieu de plusieurs minutes, et les logs montrent bien lâ€™utilisation du cache faite par Docker : les Ã©tapes prÃ©cÃ©dant le changement rÃ©utilisent les couches cachÃ©es, mais celle dâ€™aprÃ¨s sont recalculÃ©es.\n\n\nExÃ©cuter une image Docker\nLâ€™Ã©tape de build a permis de crÃ©er une image Docker. Une image doit Ãªtre vue comme un template : elle permet dâ€™exÃ©cuter lâ€™application sur nâ€™importe quel environnement dâ€™exÃ©cution sur lequel un moteur Docker est installÃ©. En lâ€™Ã©tat, on a donc juste construit, mais rien lancÃ© : notre application ne tourne pas encore. Pour cela, il faut crÃ©er un conteneur, i.e.Â une instance vivante de lâ€™image qui permet dâ€™accÃ©der Ã  lâ€™application. Cela se fait via la commande docker run.\n$ docker run -d -p 8000:5000 myflaskapp:latest\n6a2ab0d82d051a3829b182ede7b9152f7b692117d63fa013e7dfe6232f1b9e81\nDÃ©taillons la syntaxe de cette commande :\n\ndocker run tag : lance lâ€™image dont on fournit le tag. Le tag est de la forme repository/projet:version. Ici, il nâ€™y a pas de repository puisque tout est fait en local ;\n-d : â€œdÃ©tacheâ€ le conteneur du terminal qui le lance ;\n-p : effectue un mapping entre un port de la machine qui exÃ©cute le conteneur, et le conteneur lui-mÃªme. Notre conteneur Ã©coute sur le port 5000, et lâ€™on veut que notre application soit exposÃ©e sur le port 8000 de notre machine.\n\nLorsque lâ€™on exÃ©cute docker run, Docker nous rÃ©pond simplement un hash qui identifie le conteneur que lâ€™on a lancÃ©. On peut vÃ©rifier quâ€™il tourne bien avec la commande docker ps, qui renvoie toutes les informations associÃ©es au conteneur.\n$ docker ps\nCONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS                                   NAMES\n6a2ab0d82d05   myflaskapp   \"flask run --host=0.â€¦\"   7 seconds ago   Up 6 seconds   0.0.0.0:8000->5000/tcp, :::8000->5000/tcp   vigorous_kalam\nLes conteneurs peuvent Ãªtre utilisÃ©s pour rÃ©aliser des tÃ¢ches trÃ¨s diffÃ©rentes. GrossiÃ¨rement, on peut distinguer deux situations :\n\nle conteneur effectue une tÃ¢che â€œone-shotâ€, câ€™est Ã  dire une opÃ©ration qui a vocation Ã  sâ€™effectuer en un certain temps, suite Ã  quoi le conteneur peut sâ€™arrÃªter ;\nle conteneur exÃ©cute une application. Dans ce cas, on souhaite que le conteneur reste en vie aussi longtemps que lâ€™on souhaite utiliser lâ€™application en question.\n\nDans notre cas dâ€™application, on se situe dans la seconde configuration puisque lâ€™on veut exÃ©cuter une application web. Lorsque lâ€™application tourne, elle expose sur le localhost, accessible depuis un navigateur web â€” en lâ€™occurence, Ã  lâ€™adresse localhost:8000/. Les calculs sont effectuÃ©s sur un serveur local, et le navigateur sert dâ€™interface avec lâ€™utilisateur â€” comme lorsque vous utilisez un notebook Jupyter par exemple.\nFinalement, on a pu dÃ©velopper et exÃ©cuter une application complÃ¨te sur notre environnement local, sans avoir eu Ã  installer quoi que ce soit sur notre machine personnelle, Ã  part Docker.\n\n\nExporter une image Docker\nJusquâ€™Ã  maintenant, toutes les commandes Docker que nous avons exÃ©cutÃ©es se sont passÃ©es en local. Ce mode de fonctionnement peut Ãªtre intÃ©ressant pour la phase de dÃ©veloppement. Mais comme on lâ€™a vu, un des gros avantages de Docker est la facilitÃ© de redistribution des images construites, qui peuvent ensuite Ãªtre utilisÃ©es par de nombreux utilisateurs pour faire tourner notre application. Pour cela, il nous faut uploader notre image sur un dÃ©pÃ´t distant, Ã  partir duquel les utilisateurs pourront la tÃ©lÃ©charger.\nPlusieurs possibilitÃ©s existent selon le contexte de travail : une entreprise peut avoir un dÃ©pÃ´t interne par exemple. Si le projet est open-source, on peut utiliser le DockerHub. Le workflow pour uploader une image est le suivant : - crÃ©er un compte sur le DockerHub ; - crÃ©er un projet (public) sur le DockerHub, qui va hÃ©berger les images Docker du projet ; - sur un terminal, utiliser docker login pour sâ€™authentifier au DockerHub ; - on va modifier le tag que lâ€™on fournit lors du build pour spÃ©cifier le chemin attendu. Dans notre cas : docker build -t compte/projet:version . ; - uploader lâ€™image avec docker push compte/projet:version\n$ docker push avouacr/myflaskapp:1.0.0\nThe push refers to repository [docker.io/avouacr/myflaskapp]\n71db96687fe6: Pushed \n624877ac887b: Pushed \nea4ab6b86e70: Pushed \nb5120a5bc48d: Pushed \n5fa484a3c9d8: Pushed \nc5ec52c98b31: Pushed \n1.0.0: digest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9 size: 1574\n\n\nImporter une image Docker\nEn supposant que le dÃ©pÃ´t utilisÃ© pour uploader lâ€™image est public, la procÃ©dure que doit suivre un utilisateur pour la tÃ©lÃ©charger se rÃ©sume Ã  utiliser la commande docker pull compte/projet:version\n$ docker pull avouacr/myflaskapp:1.0.0\n1.0.0: Pulling from avouacr/myflaskapp\ne0b25ef51634: Pull complete \nc0445e4b247e: Pull complete \n48ba4e71d1c2: Pull complete \nffd728caa80a: Pull complete \n906a95f00510: Pull complete \nd7d49b6e17ab: Pull complete \nDigest: sha256:b75fe53fd1990c3092ec41ab0966a9fbbb762f3047957d99327cc16e27c68cc9\nStatus: Downloaded newer image for avouacr/myflaskapp:1.0.0\ndocker.io/avouacr/myflaskapp:1.0.0\nDocker tÃ©lÃ©charge et extrait chacune des couches qui constituent lâ€™image (ce qui peut parfois Ãªtre long). Lâ€™utilisateur peut alors crÃ©er un conteneur Ã  partir de lâ€™image, en utilisant docker run comme illustrÃ© prÃ©cÃ©demment.\n\n\n\nAide-mÃ©moire\nVoici une premiÃ¨re aide-mÃ©moire sur les principales commandes Ã  intÃ©grer dans un Dockerfile:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\nFROM <image>:<tag>\nUtiliser comme point de dÃ©part lâ€™image <image> ayant le tag <tag>\n\n\nRUN <instructions>\nUtiliser la suite dâ€™instructions <instructions> dans un terminal Linux. Pour passer plusieurs commandes dans un RUN, utiliser &&. Cette suite de commande peut avoir plusieurs lignes, dans ce cas, mettre \\ en fin de ligne\n\n\nCOPY <source> <dest>\nRÃ©cupÃ©rer le fichier prÃ©sent dans le systÃ¨me de fichier local Ã  lâ€™emplacement <source> pour que les instructions ultÃ©rieures puissent le trouver Ã  lâ€™emplacement <source>\n\n\nADD <source> <dest>\nGlobalement, mÃªme rÃ´le que COPY\n\n\nENV MY_NAME=\"John Doe\"\nCrÃ©ation dâ€™une variable dâ€™environnement (qui devient disponible sous lâ€™alias $MY_NAME)\n\n\nWORKDIR <path>\nDÃ©finir le working directory du conteuneur Docker dans le dossier <path>\n\n\nUSER <username>\nCrÃ©ation dâ€™un utilisateur non root nommÃ© <username>\n\n\nEXPOSE <PORT_ID>\nLorsquâ€™elle tournera, lâ€™application sera disponible depuis le port <PORT_ID>\n\n\nCMD [\"executable\",\"param1\",\"param2\"]\nAu lancement de lâ€™instance Docker la commande executable (par exemple python3) sera lancÃ©e avec les paramÃ¨tres additionnels fournis\n\n\n\nUne seconde aide-mÃ©moire pour les principales commandes Linux est disponible ci-dessous:\n\n\n\n\n\n\n\nCommande\nPrincipe\n\n\n\n\ndocker build . -t <tag>\nConstruire lâ€™image Docker Ã  partir des fichiers dans le rÃ©pertoire courant (.) en lâ€™identifiant avec le tag <tag>\n\n\ndocker run -it <tag>\nLancer lâ€™instance docker identifiÃ©e par <tag>\n\n\ndocker images\nLister les images disponibles sur la machine et quelques unes de leurs propriÃ©tÃ©s (tags, volume, etc.)\n\n\ndocker system prune\nFaire un peu de mÃ©nage dans ses images Docker (bien rÃ©flÃ©chir avant de faire tourner cette commande)"
  },
  {
    "objectID": "chapters/projects-architecture.html",
    "href": "chapters/projects-architecture.html",
    "title": "AmÃ©liorer lâ€™architecture de ses projets",
    "section": "",
    "text": "La structuration dâ€™un projet permet dâ€™immÃ©diatement identifier les Ã©lÃ©ments de code et les Ã©lÃ©ments annexes, par exemple les dÃ©pendances Ã  gÃ©rer, la documentation, etc.\nUn certain nombre dâ€™assistants au dÃ©veloppement de projets orientÃ©s donnÃ©es ont Ã©mergÃ© pour gagner en productivitÃ© et faciliter le lancement dâ€™un projet (voir ce post trÃ¨s complet sur les extensions VisualStudio).\nLâ€™idÃ©e gÃ©nÃ©rale est de privilÃ©gier une structure de projet bien plus fiable quâ€™une suite sans structure de scripts ou un Notebook Jupyter (voir ce post de blog sur ce sujet). Lâ€™IDE Jupyter nâ€™est pas le meilleur outil pour le dÃ©veloppement de projets Python ; pour des projets intensifs en code il vaut mieux se tourner vers des IDE classiques comme VSCode."
  },
  {
    "objectID": "chapters/projects-architecture.html#adopter-une-structure-de-projet",
    "href": "chapters/projects-architecture.html#adopter-une-structure-de-projet",
    "title": "AmÃ©liorer lâ€™architecture de ses projets",
    "section": "Adopter une structure de projet",
    "text": "Adopter une structure de projet\nLe principe gÃ©nÃ©ral dâ€™une structure de projet est le suivant :\n\nTous les fichiers nÃ©cessaires au projet dans un mÃªme dossier ;\nLe dossier Ã  la racine du projet sert de working directory ;\nUtilisation de chemins relatifs plutÃ´t quâ€™absolus.\n\nLe principe dâ€™une structure de projet est dâ€™adopter une structure arbitraire, mais lisible et cohÃ©rente.\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ data\nâ”‚   â”œâ”€â”€ raw\nâ”‚   â”‚   â”œâ”€â”€ data.csv\nâ”‚   â”‚   â””â”€â”€ data2.csv\nâ”‚   â””â”€â”€ derived\nâ”‚       â””â”€â”€ partial data.csv\nâ”œâ”€â”€ src\n|   â”œâ”€â”€ script.py\nâ”‚   â”œâ”€â”€ script_final.py\nâ”‚   â””â”€â”€ report.qmd\nâ””â”€â”€ output\n    â”œâ”€â”€ fig1.png\n    â”œâ”€â”€ figure 2 (copy).png\n    â”œâ”€â”€ figure10.png\n    â”œâ”€â”€ correlation.png\n    â””â”€â”€ report.pdf\nLes output sont stockÃ©s dans un dossier sÃ©parÃ©, de mÃªme que les inputs. IdÃ©alement les inputs ne sont mÃªme pas stockÃ©s avec le code, nous reviendrons sur la distinction entre lâ€™espace de stockage du code et des donnÃ©es plus tard.\n\n\n\n\n\n\nNote\n\n\n\nComme Git est un prÃ©-requis, tout projet prÃ©sente un fichier .gitignore (il est trÃ¨s important, surtout quand on manipule des donnÃ©es qui ne devraient pas se retrouver sur Github ou Gitlab).\nUn projet prÃ©sente aussi un fichier README.md Ã  la racine, nous reviendrons dessus.\nUn projet qui utilise lâ€™intÃ©gration continue prÃ©sentera Ã©galement :\n\nsi vous utilisez Gitlab, les instructions sont stockÃ©es dans le fichier gitlab-ci.yml\nsi vous utilisez Github, cela se passe dans le dossier .github/workflows"
  },
  {
    "objectID": "chapters/projects-architecture.html#autodocumenter-le-projet-avec-des-noms-pertinents",
    "href": "chapters/projects-architecture.html#autodocumenter-le-projet-avec-des-noms-pertinents",
    "title": "AmÃ©liorer lâ€™architecture de ses projets",
    "section": "Autodocumenter le projet avec des noms pertinents",
    "text": "Autodocumenter le projet avec des noms pertinents\nRien quâ€™en changeant le nom des fichiers, on rend la structure du projet trÃ¨s lisible:\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ data\nâ”‚   â”œâ”€â”€ raw\nâ”‚   â”‚   â”œâ”€â”€ dpe_logement_202103.csv\nâ”‚   â”‚   â””â”€â”€ dpe_logement_202003.csv\nâ”‚   â””â”€â”€ derived\nâ”‚       â””â”€â”€ dpe_logement_merged_preprocessed.csv\nâ”œâ”€â”€ src\n|   â”œâ”€â”€ preprocessing.py\nâ”‚   â”œâ”€â”€ generate_plots.py\nâ”‚   â””â”€â”€ report.qmd\nâ””â”€â”€ output\n    â”œâ”€â”€ histogram_energy_diagnostic.png\n    â”œâ”€â”€ barplot_consumption_pcs.png\n    â”œâ”€â”€ correlation_matrix.png\n    â””â”€â”€ report.pdf\nMaintenant, le type de donnÃ©es en entrÃ©e de chaine est clair, le lien entre les scripts, les donnÃ©es intermÃ©diaires et les output est assez transparent."
  },
  {
    "objectID": "chapters/projects-architecture.html#documenter-son-projet",
    "href": "chapters/projects-architecture.html#documenter-son-projet",
    "title": "AmÃ©liorer lâ€™architecture de ses projets",
    "section": "Documenter son projet",
    "text": "Documenter son projet\nLe fichier README.md, situÃ© Ã  la racine du projet, est Ã  la fois la carte dâ€™identitÃ© et la vitrine du projet. Sur Github et Gitlab, comme il sâ€™agit de lâ€™Ã©lÃ©ment qui sâ€™affiche en accueil, ce fichier fait office de premiÃ¨re impression, instant trÃ¨s court qui peut Ãªtre dÃ©terminant sur la valeur Ã©valuÃ©e dâ€™un projet.\nIdÃ©alement, le README.md contient :\n\nUne prÃ©sentation du contexte et des objectifs du projet\nUne description de son fonctionnement\nUn guide de contribution si le projet accepte des retours dans le cadre dâ€™une dÃ©marche open-source\n\n\n\n\n\n\n\nNote\n\n\n\nQuelques modÃ¨les de README.md complets, en R :\n\nutilitR\nDoReMIFaSol"
  },
  {
    "objectID": "chapters/projects-architecture.html#tests-unitaires",
    "href": "chapters/projects-architecture.html#tests-unitaires",
    "title": "AmÃ©liorer lâ€™architecture de ses projets",
    "section": "Tests unitaires",
    "text": "Tests unitaires\nLes tests unitaires sont des tests automatisÃ©s qui vÃ©rifient le bon fonctionnement dâ€™une unitÃ© de code, comme une fonction ou une mÃ©thode. Lâ€™objectif est de sâ€™assurer que chaque unitÃ© de code fonctionne correctement avant dâ€™Ãªtre intÃ©grÃ©e dans le reste du programme.\nLes tests unitaires sont utiles lorsquâ€™on travaille sur un code de taille consÃ©quente ou lorsquâ€™on partage son code Ã  dâ€™autres personnes, car ils permettent de sâ€™assurer que les modifications apportÃ©es ne crÃ©ent pas de nouvelles erreurs.\nEn Python, on peut utiliser le package unittest pour Ã©crire des tests unitaires. Voici un exemple tirÃ© de ce site :\n# fichier test_str.py\nimport unittest\n\n\nclass ChaineDeCaractereTest(unittest.TestCase):\n\n    def test_reversed(self):\n        resultat = reversed(\"abcd\")\n        self.assertEqual(\"dcba\", \"\".join(resultat))\n\n    def test_sorted(self):\n        resultat = sorted(\"dbca\")\n        self.assertEqual(['a', 'b', 'c', 'd'], resultat)\n\n    def test_upper(self):\n        resultat = \"hello\".upper()\n        self.assertEqual(\"HELLO\", resultat)\n\n    def test_erreur\n\n\nif __name__ == '__main__':\n    unittest.main()\nPour vÃ©rifier que les tests fonctionnent, on execute ce script depuis la ligne de commande:\npython3 test_str.py\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\n\n\n\n\n\n\nWarning\n\n\n\nSi on Ã©crit des tests unitaires, il est important de les maintenir !\nPrendre du temps pour Ã©crire des tests unitaires qui ne sont pas maintenus et donc ne renvoie plus de diagnostics pertinent est du temps perdu."
  },
  {
    "objectID": "chapters/projects-architecture.html#maintenance",
    "href": "chapters/projects-architecture.html#maintenance",
    "title": "AmÃ©liorer lâ€™architecture de ses projets",
    "section": "Maintenance",
    "text": "Maintenance\nLâ€™objectif des conseils de ce cours est de rÃ©duire le coÃ»t de la maintenance Ã  long terme en adoptant les structures les plus lÃ©gÃ¨res, automatisÃ©es et rÃ©utilisables.\nLes notebooks Jupyter sont trÃ¨s pratiques pour tÃ¢tonner et expÃ©rimenter. Cependant, ils prÃ©sentent un certain nombre dâ€™inconvÃ©nients Ã  long terme qui peuvent rendre impossible Ã  maintenir le code Ã©crit avec dans un notebook:\n\ntous les objets (fonctions, classes et donnÃ©es) sont dÃ©finis et disponibles dans le mÃªme fichier. Le moindre changement Ã  une fonction nÃ©cessite de retrouver lâ€™emplacement dans le code, Ã©crire et faire tourner Ã  nouveau une ou plusieurs cellules.\nquand on tÃ¢tonne, on Ã©crit du code dans des cellules. Dans un cahier, on utiliserait la marge mais cela nâ€™existe pas avec un notebook. On crÃ©Ã© donc de nouvelles cellules, pas nÃ©cessairement dans lâ€™ordre. Quand il est nÃ©cessaire de faire tourner Ã  nouveau le notebook, cela provoque des erreurs difficile Ã  debugger (il est nÃ©cessaire de retrouver lâ€™ordre logique du code, ce qui nâ€™est pas Ã©vident).\nles notebooks incitent Ã  faire des copier-coller de cellules et modifier marginalement le code plutÃ´t quâ€™Ã  utiliser des fonctions.\nil est quasi-impossible dâ€™avoir un versioning avec Git des notebooks qui fonctionne. Les notebooks Ã©tant, en arriÃ¨re plan, de gros fichiers JSON, ils ressemblent plus Ã  des donnÃ©es que des codes sources. Git ne parvient pas Ã  identifier les blocs de code qui ont changÃ©\npassage en production des notebooks coÃ»teux alors quâ€™un script bien fait est beaucoup plus facile Ã  passer en prod (voir suite cours)\nJupyter manque dâ€™extensions pour mettre en oeuvre les bonnes pratiques (linters, etc.). VSCode au contraire est trÃ¨s bien\nRisques de rÃ©vÃ©lation de donnÃ©es confidentielles puisque les outputs des blocs de code, par exemple les head, sont Ã©crits en dur dans le code source.\n\nGlobalement, les notebooks sont un bon outil pour tÃ¢tonner ou pour faire communiquer. Mais pour maintenir un projet Ã  long terme, il vaut mieux privilÃ©gier les scripts. Les recommandations de ce cours visent Ã  rendre le plus lÃ©ger possible la maintenance Ã  long terme de projets data-science en favorisant la reprise par dâ€™autres (ou par soi dans le futur)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mise en production des projets de data science",
    "section": "",
    "text": "Cours dans le parcours data science en derniÃ¨re annÃ©e Ã  lâ€™ENSAE construit par Romain Avouac et Lino Galiana.\nLes slides associÃ©es au cours sont disponibles Ã  cette adresse.\nParcours en construction"
  }
]