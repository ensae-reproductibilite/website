[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Putting into production course",
    "section": "",
    "text": "Un parcours de formation construit par Romain Avouac et Lino Galiana pour le cursus d’ingénieurs de la donnée de l’ENSAE.\nLes slides associées au cours sont disponibles à cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "index.html#cours-de-mise-en-production-de-projets-data-science",
    "href": "index.html#cours-de-mise-en-production-de-projets-data-science",
    "title": "Putting into production course",
    "section": "",
    "text": "Un parcours de formation construit par Romain Avouac et Lino Galiana pour le cursus d’ingénieurs de la donnée de l’ENSAE.\nLes slides associées au cours sont disponibles à cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "chapters/galerie/2024/primePredict.html",
    "href": "chapters/galerie/2024/primePredict.html",
    "title": "PrimePredict",
    "section": "",
    "text": "Avec PrimePredict il est possible de calculer les primes annuelles matérielles d’assurance à partir de données fourni. Le modèle prédit à lafois le coût des dommages matériels et la fréquence des incidents matériels.\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "chapters/yaml101.html",
    "href": "chapters/yaml101.html",
    "title": "YAML 101",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran."
  },
  {
    "objectID": "chapters/yaml101.html#quest-ce-que-le-format-yaml",
    "href": "chapters/yaml101.html#quest-ce-que-le-format-yaml",
    "title": "YAML 101",
    "section": "Qu’est-ce que le format YAML",
    "text": "Qu’est-ce que le format YAML\nYAML est un acronyme récursif signifiant YAML Ain’t Markup Language. YAML est un langage de sérialisation de données au format texte : il permet de structurer de la donnée dans des fichiers textuels, à l’instar d’autres formats populaires (CSV, JSON, XML, etc.).\nSa spécificité par rapport aux autres formats populaires est qu’il est conçu pour être à la fois expressif et facile à lire pour un humain. Expressif dans la mesure où il permet de représenter de l’information hiérarchique, ce que ne permet pas le format CSV par exemple qui représente essentiellement des données tabulaires (“fichier plat”). Lisible car, contrairement aux languages dont la structure est basée sur des balises (markup) comme XML ou bien délimitée par des symboles comme l’accolade en JSON, YAML se démarque par une syntaxe basée sur l’indentation — comme Python.\nVoici un exemple typique d’un fichier YAML, qui compare les principaux langages utilisés pour représenter des données hiérarchiques dans un format textuel1.\n---\ntype: tutorial\ndomain:\n  - devops\nlanguage:\n  - yaml:\n      name: YAML Ain't Markup Language\n      born: 2001\n      legibility: awesome\n  - json:\n      name: JavaScript Object Notation\n      born: 2001\n      legibility: good\n  - xml:\n      name: Extensible Markup Language\n      born: 1996\n      legibility: bad\n---"
  },
  {
    "objectID": "chapters/yaml101.html#yaml-et-lapproche-gitops",
    "href": "chapters/yaml101.html#yaml-et-lapproche-gitops",
    "title": "YAML 101",
    "section": "YAML et l’approche GitOps",
    "text": "YAML et l’approche GitOps\nGrâce à sa lisibilité, le langage YAML est rapidement devenu un standard dans les écosystèmes DevOps et MLOps dans la mesure où il facilite grandement l’interaction entre les humains et les systèmes automatisés. Il permet de définir de manière lisible des règles, basées sur des paramètres, qui peuvent facilement être interprétées à la fois par des humains (du fait de sa lisibilité) et par des machines (du fait de sa structure hiérarchisée).\nEn particulier, le YAML est devenu un élément central des environnements de déploiement modernes basés sur une approche dite déclarative (Note 1). Au lieu de spécifier étape par étape comment déployer une ressource (approche impérative), YAML permet de décrire simplement l’état final souhaité de l’environnement. C’est alors l’outil utilisé (par exemple, Kubernetes dans l’exemple ci-dessous) qui gère les détails d’implémentation de manière la plus adaptée. On parle souvent d’approche Infrastructure as Code : l’infrastructure peut être complètement décrite dans des fichiers de configuration YAML — qu’on appelle manifestes — qui permettent de décrire l’état souhaité de celle-ci. Cette manière de spécifier les environnements favorise la reproductibilité, chaque état de l’infrastructure étant clairement décrit et pouvant être reproduit simplement.\n\n\n\n\n\n\nNote 1: YAML n’est pas le premier langage déclaratif que vous avez appris !\n\n\n\n\n\nA titre d’analogie, l’approche déclarative est également au cœur du langage SQL.\nEn SQL, on décrit le traitement souhaité dans un langage proche du langage naturel (SELECT ce groupe de variables, FILTER ces individus, etc.) et c’est le moteur d’exécution sous-jacent qui choisit comment effectuer les calculs demandés de manière optimale.\nC’est notamment une des raisons de la popularité du langage SQL dans l’éco-système big data : il est possible d’appliquer une même requête à des données de volumétries très différentes dans la mesure où le moteur d’exécution sous-jacent va convertir la requête de manière appropriée. Par exemple, une requête SQL sera convertie par Spark en opérations MapReduce distribuées permettant de traiter des volumes considérables de données (voir chapitre big data pour plus de détails).\n\n\n\nBien entendu, ces fichiers décrivant les environnements souhaités ont vocation à être versionnés sur un dépôt Git . Cette extension directe de l’approche Infrastructure as Code se nomme le GitOps : l’architecture est décrite sous forme de manifestes (YAML) et ces manifestes sont versionnés sur un dépôt Git qui devient ainsi la source de vérité unique de spécification de l’environnement. Cette approche favorise à la fois la traçabilité — tout changement est documenté dans l’historique du dépôt Git — et l’automatisation — l’infrastructure peut être déployée automatiquement à partir du dépôt Git grâce à des outils de déploiement continus orientés GitOps comme ArgoCD (cf. chapitre déploiement).\nVoici un exemple illustrant ce concept dans le contexte de Kubernetes. On déclare dans un manifeste YAML une ressource de type Pod, qui a des métadonnées (un nom) et des spécifications. On y déclare le conteneur que le Pod doit déployer : celui d’une API FastAPI (voir l’application fil rouge pour plus de détails). On passe à ce conteneur une variable d’environnement (cf. chapitre Linux 101) qui spécifie le modèle à déployer via l’API au runtime. Ainsi, on décrit dans ce manifeste l’état souhaité, et Kubernetes se charge du déploiement effectif du conteneur.\n\n\nexample.yaml\n\nkind: Pod\nmetadata:\n  name: my-api-pod\nspec:\n  containers:\n    - name: api\n      image: my_dh_account/my_fast_api:0.0.1\n      env:\n        - name: MODEL\n          value: deepseek-ai/DeepSeek-R1"
  },
  {
    "objectID": "chapters/yaml101.html#yaml-vs.-json",
    "href": "chapters/yaml101.html#yaml-vs.-json",
    "title": "YAML 101",
    "section": "YAML vs. JSON",
    "text": "YAML vs. JSON\nLes langages YAML et JSON sont très proches dans la représentation hiérarchique de l’information qu’ils permettent. Fonctionnellement, le langage YAML est un superset du langage JSON. Cela signifie que tout ce qui est représentable en JSON peut l’être en YAML mais l’inverse n’est pas nécessairement vrai — même s’il est très rare en pratique de ne pas pouvoir faire la conversion dans les deux sens. Plusieurs outils en ligne, comme YAML-to-json et json-to-YAML, permettent de convertir facilement ces formats entre eux.\nComparons à titre d’illustration les représentations YAML et JSON du manifeste Kubernetes précédent.\n\n\n\nkind: Pod\nmetadata:\n  name: my-api-pod\nspec:\n  containers:\n    - name: api\n      image: my_dh_account/my_fast_api:0.0.1\n      env:\n        - name: MODEL\n          value: deepseek-ai/DeepSeek-R1\n\n\n{\n  \"kind\": \"Pod\",\n  \"metadata\": {\n    \"name\": \"my-api-pod\"\n  },\n  \"spec\": {\n    \"containers\": [\n      {\n        \"name\": \"api\",\n        \"image\": \"my_dh_account/my_fast_api:0.0.1\",\n        \"env\": [\n          {\n            \"name\": \"MODEL\",\n            \"value\": \"deepseek-ai/DeepSeek-R1\"\n          }\n        ]\n      }\n    ]\n  }\n}\n\n\n\nLa comparaison entre les deux représentations illustre clairement la différence majeure entre les deux langages : le YAML est facilement lisible par l’humain là où le JSON est plus verbeux et ressemble donc plus à un “langage machine”. En effet, YAML utilise l’indentation pour structurer les données là où JSON utilise des accolades pour structurer ses objets, ce qui limite sa lisibilité mais le rend également moins susceptible aux erreurs d’indentation.\nIl n’est donc pas étonnant que YAML soit devenu le langage de référence dans le monde du GitOps, les fichiers de configuration déclaratifs étant généralement générés et lus par des humains. A l’inverse, JSON reste le langage standard de communication avec une API, dont les réponses sont généralement lues par d’autres applications ou bien par le biais d’un langage de programmation, ce à quoi se prête très bien une structure plus verbeuse mais aussi plus robuste."
  },
  {
    "objectID": "chapters/yaml101.html#caractéristiques-dun-fichier-yaml",
    "href": "chapters/yaml101.html#caractéristiques-dun-fichier-yaml",
    "title": "YAML 101",
    "section": "Caractéristiques d’un fichier YAML",
    "text": "Caractéristiques d’un fichier YAML\nLes fichiers YAML portent généralement les extensions .yaml ou .yml. En pratique — et cela est vrai pour tous les formats de fichier en pratique — ce n’est pas l’extension qui fait qu’un fichier texte est un fichier YAML, mais bien la validité de son contenu au regard de la norme YAML. On doit donc respecter un certains nombres de règles et utiliser les types prévus pour produire un fichier YAML.\nPremière règle : le contenu d’un fichier YAML est organisé en paires clé-valeur imbriquées, à la manière d’un dictionnaire en Python. La structure hiérarchique, c’est à dire l’imbrication des dictionnaires de données, est marquée par l’indentation. Par convention, on indente avec deux espaces (pas de tabulation!) à chaque niveau de hiérarchie.\nSeconde règle : on doit utiliser les types de données supportés par YAML. En pratique, ils sont suffisamment nombreux pour couvrir la plupart des usages :\n\nChaînes de caractères : de la donnée textuelle. On peut choisir de l’entourer ou non de guillemets. Par convention, tout ce qui n’est pas des autres types de cette liste est au format textuel, il n’est donc pas indispensable de mettre des guillemets (à l’inverse du JSON).\nNumériques : entiers ou flottants (exemple : 42, 3.14). Attention à ne pas mettre de guillemets dans ce cas, sinon les nombres seront reconnus comme des chaînes de caractères.\nBooléens : représentés par true ou false (tout en minuscule!). Là encore, pas de guillemets.\nListes : éléments précédés d’un tiret (-). Les éléments d’une liste peuvent être, et sont souvent, à plusieurs niveaux, le YAML permettant une structure imbriquée.\n\nAgrémentons encore un peu le manifeste de déploiement Kubernetes pour faire apparaître l’ensemble de ces types.\nkind: Pod\nmetadata:\n  name: my-api-pod\nspec:\n  containers:\n    - name: api\n      image: my_dh_account/my_fast_api:0.0.1\n      env:\n        - name: MODEL\n          value: deepseek-ai/DeepSeek-R1\n        - name: DEBUG\n          value: true\n      ports:\n        - containerPort: 8000\nOn voit ici que dans notre paramètre env, on a pu mettre plusieurs valeurs. Même si on ne connait pas les détails de l’implémentation sous-jacente, on peut se douter qu’on va injecter deux variables d’environnement à notre API dont les valeurs sont définies dans ce fichier.\nLes clés-valeurs acceptées sont des conventions, elles dépendent de chaque solution logicielle. Mais, globalement, les paramètres d’instruction acceptés par telle ou telle machine se ressemblent. Dans le cadre de l’application fil rouge, nous proposons l’utilisation de Github Actions, qui repose sur la spécification Mustache, d’Argo et de Kubernetes. Ces solutions techniques répondent à des besoins différents mais présentent le point commun de recevoir toutes trois leurs instructions depuis des YAML.\n\n\n\n\n\n\nValidation et erreurs fréquentes\n\n\n\nLe principal point de vigilance lorsqu’on écrit du YAML concerne l’indentation. Celle-ci est essentielle à la structuration des données. L’indentation recommandée est de deux espaces ; jamais de tabulations, qui peuvent générer des erreurs d’interprétation car il s’agit de caractères spéciaux.\nPour prévenir ces erreurs, de nombreux outils de validation existent :\n\nIDE : les IDE standards comme VSCode, PyCharm, etc. supportent nativement le YAML et indiquent donc immédiatement les erreurs d’indentation ou de syntaxe. Il est utile d’avoir dans son IDE une coloration des indentations pour faire moins d’erreur: dans VSCode, vous pouvez installer l’extension indent-rainbow pour cela2.\nLinters : des outils spécialisés pour valider le contenu d’un fichier YAML. YAMLlint en est un choix courant.\n\nSi vous utilisez des actions Github, l’interface de Github depuis votre navigateur effectue un diagnostic de la validité formelle du YAML. Si vous committez un fichier non valide formellement (par exemple à cause d’une erreur d’identation), Github ne lancera pas l’action. Il peut donc être utile de vérifier, si une action ne se lance pas, si cela vient d’un problème de formattage du fichier en l’ouvrant directement depuis Github."
  },
  {
    "objectID": "chapters/yaml101.html#utilisation-programmatique",
    "href": "chapters/yaml101.html#utilisation-programmatique",
    "title": "YAML 101",
    "section": "Utilisation programmatique",
    "text": "Utilisation programmatique\nComme on l’a vu, le langage YAML est pensé avant tout pour être écrit et lus par des humains. Cela dit, on peut vouloir parfois générer par exemple de nombreux fichiers YAML à partir d’un même template, ou bien récupérer de manière automatisée des informations dans un fichier de configuration YAML. De la même manière qu’un fichier JSON par exemple, il est tout à fait possible d’interagir avec un fichier YAML de manière programmatique.\nEn Python, le package de référence pour manipuler du YAML est PyYAML. Illustrons quelques commandes de base.\n\nimport yaml\n\nSupposons que le fichier de déploiement Kubernetes qu’on a utilisé tout a long de ce tutoriel soit écrit dans un fichier api-deployment.yaml. Commençons par l’importer et récupérer quelques informations.\nLa syntaxe pour importer le contenu d’un fichier YAML est semblable à celle pour n’importe quel fichier. On utilise la fonction safe_load du package PyYAML.\n\nwith open(\"api-deployment.yaml\", \"r\") as file_in:\n  manifest = yaml.safe_load(file_in)\n\nprint(manifest)\n\n{'kind': 'Pod', 'metadata': {'name': 'my-api-pod'}, 'spec': {'containers': [{'env': [{'name': 'MODEL', 'value': 'deepseek-ai/DeepSeek-R1'}, {'name': 'DEBUG', 'value': True}], 'image': 'my_dh_account/my_fast_api:0.0.1', 'name': 'api', 'ports': [{'containerPort': 8000}]}]}}\n\n\nLe contenu est chargé en Python sous la forme d’un dictionnaire, qui est la manière naturelle en Python de représenter de l’information hiérarchique. A partir de là, on peut requêter le contenu du fichier YAML à la manière de n’importe quel dictionnaire Python. Récupérons par exemple le nom du modèle déployé.\n\nprint(\n  manifest\n  .get(\"spec\")\n  .get(\"containers\")[0]\n  .get(\"env\")[0]\n  .get(\"value\")\n)\n# ou en version plus succincte:\n# print(manifest[\"spec\"][\"containers\"][0][\"env\"][0][\"value\"])\n\ndeepseek-ai/DeepSeek-R1\n\n\nLe chemin dans le dictionnaire reflète les différents objets qui structurent un fichier YAML. Les clés (“spec”, “containers”) correspondent aux dictionnaires en YAML, et les index numériques (“0” ici) correspondent à la sélection d’un élément par position dans une liste. Ici, on récupère la spécification du premier conteneur — et le seul en l’occurrence, mais le Pod pourrait héberger plusieurs conteneurs — et, pour ce conteneur, on récupère la première variable d’environnement (MODEL) dont on extrait la valeur (le nom du modèle).\nA présent, intéressons-nous au cas où l’on voudrait modifier une information du fichier YAML et exporter un manifeste actualisé. On va changer la valeur de la variable d’environnement DEBUG de true à false. En pratique, on fait la modification sur le dictionnaire en Python, puis on exporte le dictionnaire au format YAML.\n\n# Vérification du contenu actuel\nprint(manifest[\"spec\"][\"containers\"][0][\"env\"][1])\n\n# Modification du contenu\nmanifest[\"spec\"][\"containers\"][0][\"env\"][1][\"value\"] = False\nprint(manifest[\"spec\"][\"containers\"][0][\"env\"][1])\n\n{'name': 'DEBUG', 'value': True}\n{'name': 'DEBUG', 'value': False}\n\n\nL’opération a fonctionné sur l’objet en mémoire dans Python. On exporte finalement le dictionnaire au format YAML.\n\nwith open('api-deployment-modifie.yaml', 'w') as file_out:\n    yaml.dump(manifest, file_out)\n\n# Vérification\nprint(open(\"api-deployment-modifie.yaml\", \"r\").read())\n\nkind: Pod\nmetadata:\n  name: my-api-pod\nspec:\n  containers:\n  - env:\n    - name: MODEL\n      value: deepseek-ai/DeepSeek-R1\n    - name: DEBUG\n      value: false\n    image: my_dh_account/my_fast_api:0.0.1\n    name: api\n    ports:\n    - containerPort: 8000\n\n\n\nLe manifeste exporté a bien été modifié comme attendu."
  },
  {
    "objectID": "chapters/yaml101.html#conclusion",
    "href": "chapters/yaml101.html#conclusion",
    "title": "YAML 101",
    "section": "Conclusion",
    "text": "Conclusion\nDu fait de sa prédominance dans l’éco-système DevOps / MLOps, la maîtrise du langage YAML est aujourd’hui indispensable pour tout data scientist impliqué dans des projets visant la mise en production. Sa syntaxe lisible mais puissante et sa compatibilité avec le paradigme déclaratif en font un langage de choix pour la déclaration de manifestes en mode GitOps. Cette approche est progressivement devenue un standard pour le déploiement industrialisé d’applications et sera centrale dans les chapitres suivants sur le déploiement et le MLOps."
  },
  {
    "objectID": "chapters/yaml101.html#footnotes",
    "href": "chapters/yaml101.html#footnotes",
    "title": "YAML 101",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nL’exemple présenté est inspiré de ce tutoriel.↩︎\nDans le cadre de l’application fil rouge, si vous créez votre VSCode à partir de la configuration proposée, celle-ci est installée par défaut pour vous.↩︎"
  },
  {
    "objectID": "chapters/portability.html",
    "href": "chapters/portability.html",
    "title": "Portabilité",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran."
  },
  {
    "objectID": "chapters/portability.html#introduction-1",
    "href": "chapters/portability.html#introduction-1",
    "title": "Portabilité",
    "section": "Introduction",
    "text": "Introduction\nTo illustrate the importance of working with virtual environments, let’s take the perspective of an aspiring data scientist starting their first projects.\nMost likely, they begin by installing a Python distribution—often via Anaconda—on their machine and start developing project after project. If they need to install an additional library, they do so without much thought. Then they move on to the next project using the same approach. And so on.\nThis natural workflow allows for quick experimentation. However, it becomes problematic when it comes time to share a project or revisit it later.\nIn this setup, all packages used across various projects are installed in the same location. While this might seem trivial—after all, Python’s simplicity is one of its strengths—several issues will eventually arise:\n\nVersion conflicts: Application A may require version 1 of a package, while application B needs version 2. These versions may be incompatible. In such a setup, only one application may work;\nFixed Python version — Only one Python installation is available per system, whereas you’d want different versions for different projects;\nLimited reproducibility: It’s difficult to trace which project relies on which packages, since everything accumulates in the same environment;\nLimited portability: As a result of the above, it’s hard to generate a file listing only the dependencies of a specific project.\n\nVirtual environments provide a solution to these issues."
  },
  {
    "objectID": "chapters/portability.html#how-it-works",
    "href": "chapters/portability.html#how-it-works",
    "title": "Portabilité",
    "section": "How It Works",
    "text": "How It Works\nThe concept of a virtual environment is technically quite simple. In Python, it can be defined as:\n\n“A self-contained directory tree that contains a Python installation for a particular version of Python, plus additional packages.”\n\nYou can think of virtual environments as a way to maintain multiple isolated Python setups on the same system, each with its own package list and versions. Starting every new project in a clean virtual environment is a great practice to improve reproducibility."
  },
  {
    "objectID": "chapters/portability.html#implementations",
    "href": "chapters/portability.html#implementations",
    "title": "Portabilité",
    "section": "Implementations",
    "text": "Implementations\nThere are various implementations of virtual environments in Python, each with its own community and features:\n\nThe standard Python implementation is venv.\nconda provides a more comprehensive implementation.\n\nFrom the user’s perspective, these implementations are fairly similar. The key conceptual difference is that conda acts as both a package manager (like pip) and a virtual environment manager (like venv).\nFor a long time, conda was the go-to tool for data science, as it handled not only Python dependencies but also system-level dependencies (e.g., C libraries) used by many data science packages. However, in recent years, the widespread use of wheels—precompiled packages tailored for each system—has made pip more viable again.\n\n\n\n\n\n\nConceptual difference between pip and conda\n\n\n\n\n\nAnother major difference is how they resolve dependency conflicts.\nMultiple packages might require different versions of a shared dependency. conda uses an advanced (and slower) solver to resolve such conflicts optimally, while pip follows a simpler approach.\n\n\n\npip+venv offers simplicity, while conda offers robustness. Depending on your project’s context, either can be appropriate. If your project runs in an isolated container, venv is usually sufficient since the container already provides isolation.\n\n\n\n\n\n\nIs it different in ?\n\n\n\n\n\nSome  enthusiasts claim Python’s environment management is chaotic. That was true in the early 2010s, but not so much today.\nR’s tools, like renv, are great but have limitations—for example, renv doesn’t let you specify the R version.\nIn contrast, Python’s command-line tools offer stronger portability: venv lets you choose the Python version when creating the environment; conda lets you define the version directly in the environment.yml file.\n\n\n\nSince there’s no absolute reason to choose between pip+venv or conda, we recommend pragmatism. We personally lean toward venv because we mainly work in container-based microservices—an increasingly common practice in modern data science. However, we present both approaches, and the application section includes both, so you can choose what suits your needs best."
  },
  {
    "objectID": "chapters/portability.html#practical-guide-to-using-a-virtual-environment",
    "href": "chapters/portability.html#practical-guide-to-using-a-virtual-environment",
    "title": "Portabilité",
    "section": "Practical Guide to Using a Virtual Environment",
    "text": "Practical Guide to Using a Virtual Environment\n\nInstallation\n\nvenvconda\n\n\nvenv is a module included by default in Python, making it easily accessible for managing virtual environments.\nInstructions for using venv, Python’s built-in tool for creating virtual environments, are available in the official Python documentation.\n\n\n\nIllustration of the concept (Source: dataquest)\n\n\n\n\nInstructions for installing conda are provided in the official documentation. conda alone is not very useful in practice and is generally included in distributions. The two most popular ones are:\n\nMiniconda: a minimalist distribution that includes conda, Python, and a small set of useful technical packages;\nAnaconda: a large distribution that includes conda, Python, other tools (R, Spyder, etc.), and many packages useful for data science (SciPy, NumPy, etc.).\n\n\nIn practice, the choice of distribution matters little, since we will use clean virtual environments to develop our projects.\n\n\n\n\n\nCreating an Environment\n\nvenvconda\n\n\nTo start using venv, we first create a clean environment named dev. This is done from the command line using Python. That means the version of Python used in the environment will be the one active at the time of creation.\n1python -m venv dev\n\n1\n\nOn a Windows system, use python.exe -m venv dev\n\n\nThis command creates a folder named dev/ containing an isolated Python installation.\n\n\nExample on a Linux system\n\n\n\n\nExample on a Linux system\n\n\n\nThe Python version used will be the one set as default in your PATH, e.g., Python 3.11. To create an environment with a different version of Python, specify the path explicitly:\n/path/to/python3.8 -m venv dev-old\n\n\nTo start using conda, we first create a clean environment named dev, specifying the Python version we want to install for the project:\nconda create -n dev python=3.9.7\nRetrieving notices: ...working... done\nChannels:\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/mamba/envs/dev\n\n  added / updated specs:\n    - python=3.9.7\n\n\nThe following packages will be downloaded:\n...\nThe following NEW packages will be INSTALLED:\n...\nProceed ([y]/n)? y\nDownloading and Extracting Packages\n...\nAs noted in the logs, conda created the environment and tells us where it’s located on the file system. In reality, the environment isn’t completely empty: conda asks—requiring confirmation with y—to install a number of packages, those that come bundled with the Miniconda distribution.\nWe can verify that the environment was created by listing all environments on the system:\nconda info --envs\n# conda environments:\n#\nbase                  *  /opt/mamba\ndev                      /opt/mamba/envs/dev\n\n\n\n\n\nActivating an Environment\nSince multiple environments can coexist on the same system, you need to tell your environment manager which one to activate. From then on, it will implicitly be used for commands like python, pip, etc., in the current command-line session2.\n\nvenvconda\n\n\nsource dev/bin/activate\nvenv activates the virtual environment dev, which is confirmed by the name of the environment appearing at the beginning of the command line. Once activated, dev temporarily becomes our default Python environment. To confirm, use the which command to see which Python interpreter will be used:\nwhich python\n/home/onyxia/work/dev/bin/python\n\n\nconda activate dev\nConda indicates that you’re now working in the dev environment by showing its name in parentheses at the beginning of the command line. This means dev is temporarily your default environment. Again, we can verify it using which:\nwhich python\n/opt/mamba/envs/dev/bin/python\n\n\n\nYou’re now working in the correct environment: the Python interpreter being used is not the system one, but the one from your virtual environment.\n\n\nListing Installed Packages\nOnce the environment is activated, you can list the installed packages and their versions. This confirms that some packages are present by default when creating a virtual environment.\n\nvenvconda\n\n\nWe start with a truly bare-bones environment:\npip list\nPackage    Version\n---------- -------\npip        23.3.2\nsetuptools 69.0.3\nwheel      0.42.0\n\n\nThe environment is fairly minimal, though more populated than a fresh venv environment:\nconda list\n# packages in environment at /opt/mamba/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\nca-certificates           2023.11.17           hbcca054_0    conda-forge\n...\n\n\n\nAs a quick check, we can confirm that Numpy is indeed not available in the environment:\npython -c \"import numpy as np\"\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'numpy'\n\n\nInstalling a package\nYour environment can be extended, when needed,\nby installing a package via the command line.\nThe procedure is very similar between pip (for venv environments) and conda.\n\n\n\n\n\n\nMixing pip and conda\n\n\n\n\n\nIt is technically possible to install packages using pip\nwhile working within a conda virtual environment3.\nThis is fine for experimentation and speeds up development.\nHowever, in a production environment, it is a practice to avoid.\n\nEither you initialize a fully self-sufficient conda environment with an env.yml (see below);\nOr you create a venv environment and use only pip install.\n\n\n\n\n\nvenvconda\n\n\n\n\nterminal\n\npip install nom_du_package\n\n\n\n\n\nterminal\n\nconda install nom_du_package\n\n\n\n\nThe difference is that while pip install installs a package from the PyPI repository, conda install fetches the package from repositories maintained by the Conda developers4.\nLet’s install the flagship machine learning package scikit-learn.\n\nvenvconda\n\n\n\n\nterminal\n\npip install scikit-learn\n\npip install scikit-learn\nCollecting scikit-learn\n...\nRequired dependencies (like Numpy) are automatically installed.\nThe environment is thus enriched:\n\n\nterminal\n\npip list\n\nPackage       Version\n------------- -------\njoblib        1.3.2\nnumpy         1.26.3\npip           23.2.1\nscikit-learn  1.4.0\nscipy         1.12.0\nsetuptools    65.5.0\nthreadpoolctl 3.2.0\n\n\n\n\nterminal\n\nconda install scikit-learn\n\n\n\nSee output\n\nChannels:\n - conda-forge\n...\nExecuting transaction: done\n\nAgain, conda requires installing additional packages that are dependencies of scikit-learn. For example, the scientific computing library NumPy.\n(dev) $ conda list\n# packages in environment at /opt/mamba/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda-forge    conda-forge\n...\nzlib                      1.2.13               hd590300_5    conda-forge\n\n\n\n\n\nExporting environment specifications\nStarting from a clean environment is a good reproducibility practice:\nby beginning with a minimal setup, we ensure that only the packages\nstrictly necessary for the application’s functionality are installed as the project evolves.\nThis also makes the project easier to port.\nYou can export the environment’s specifications into a special file\nto recreate a similar setup elsewhere.\n\nvenvconda\n\n\n\n\nterminal\n\npip freeze &gt; requirements.txt\n\n\n\nView the generated requirements.txt file\n\n\n\nrequirements.txt\n\njoblib==1.3.2\nnumpy==1.26.3\nscikit-learn==1.4.0\nscipy==1.12.0\nthreadpoolctl==3.2.0\n\n\n\n\n\n\nterminal\n\nconda env export &gt; environment.yml\n\n\n\n\n\nThis file is conventionally stored at the root of the project’s Git repository.\nThis way, collaborators can recreate the exact same Conda environment used during development via the following command:\n\nvenvconda\n\n\nRepeat the earlier process of creating a clean environment,\nthen run pip install -r requirements.txt.\n\n\nterminal\n\npython -m venv newenv\nsource newenv/bin/activate\n\n\n\nterminal\n\npip install -r requirements.txt\n\n\n\nThis can be done in a single command:\n\n\nterminal\n\nconda env create -f environment.yml\n\n\n\n\n\n\nSwitching environments\n\nvenvconda\n\n\nTo switch environments, simply activate a different one.\n\n\nterminal\n\n(myenv) $ deactivate\n$ source anotherenv/bin/activate\n(anotherenv) $ which python\n/chemin/vers/anotherenv/bin/python\n\nTo exit the active virtual environment, just use deactivate:\n\n\nterminal\n\n(anotherenv) $ deactivate\n$\n\n\n\nTo switch environments, just activate another one.\n\n\nterminal\n\n(dev) $ conda activate base\n(base) $ which python\n/opt/mamba/bin/python\n\nTo exit all conda environments, use conda deactivate:\n\n\nterminal\n\n(base) $ conda deactivate\n$\n\n\n\n\n\n\nView the generated environment.yml file\n\n\n\nenvironment.yml\n\nname: dev\nchannels:\n  - conda-forge\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=2_gnu\n  - ca-certificates=2023.11.17=hbcca054_0\n  - joblib=1.3.2=pyhd8ed1a\n\n\n\n\nCheat sheet\n\n\n\n\n\n\n\n\nvenv\nconda\nDescription\n\n\n\n\npython -m venv &lt;env_name&gt;\nconda create -n &lt;env_name&gt; python=&lt;python_version&gt;\nCreate a new environment named &lt;env_name&gt; with Python version &lt;python_version&gt;\n\n\n\nconda info --envs\nList available environments\n\n\nsource &lt;env_name&gt;/bin/activate\nconda activate &lt;env_name&gt;\nActivate the environment for the current terminal session\n\n\npip list\nconda list\nList packages in the active environment\n\n\npip install &lt;pkg&gt;\nconda install &lt;pkg&gt;\nInstall the &lt;pkg&gt; package in the active environment\n\n\npip freeze &gt; requirements.txt\nconda env export &gt; environment.yml\nExport environment specs to a requirements.txt or environment.yml file"
  },
  {
    "objectID": "chapters/portability.html#limitations",
    "href": "chapters/portability.html#limitations",
    "title": "Portabilité",
    "section": "Limitations",
    "text": "Limitations\nUsing virtual environments is good practice as it improves application portability.\nHowever, there are some limitations:\n\nsystem libraries needed by packages aren’t managed;\nenvironments may not handle multi-language projects well;\ninstalling conda, Python, and required packages every time can be tedious;\nin production, managing separate environments per project can quickly become complex for system administrators.\n\nContainerization technologies help address these limitations."
  },
  {
    "objectID": "chapters/portability.html#introduction-3",
    "href": "chapters/portability.html#introduction-3",
    "title": "Portabilité",
    "section": "Introduction",
    "text": "Introduction\nWith virtual environments,\nthe goal was to allow every potential user of our project to install the required packages on their system for proper execution.\nHowever, as we’ve seen, this approach doesn’t ensure perfect reproducibility and requires significant manual effort.\nLet’s shift perspective: instead of giving users a recipe to recreate the environment on their machine, couldn’t we just give them a pre-configured machine?\nOf course, we’re not going to configure and ship laptops to every potential user.\nSo we aim to deliver a virtual version\nof our machine. There are two main approaches:\n\nVirtual machines: not a new approach. They simulate a full computing environment (hardware + OS) on a server to replicate a real computer’s behavior.\nContainers: a more lightweight solution to bundle a computing environment and mimic a real machine’s behavior."
  },
  {
    "objectID": "chapters/portability.html#how-it-works-1",
    "href": "chapters/portability.html#how-it-works-1",
    "title": "Portabilité",
    "section": "How it works",
    "text": "How it works\nVirtual machines are heavy and difficult to replicate or distribute.\nTo overcome these limitations, containers have emerged in the past decade.\nModern cloud infrastructure has largely moved from virtual machines to containers for the reasons we’ll discuss.\nLike VMs, containers package the full environment (system libraries, app, configs) needed to run an app.\nBut unlike VMs, containers don’t include their own OS. Instead, they use the host machine’s OS.\nThis means to simulate a Windows machine, you don’t need a Windows server — a Linux one will do. Conversely, you can test Linux/Mac setups on a Windows machine.\nThe containerization software handles the translation between software-level instructions and the host OS.\nThis technology guarantees strong reproducibility while remaining lightweight enough for easy distribution and deployment.\nWith VMs, strong coupling between the OS and the app makes scaling harder. You need to spin up new servers matching your app’s needs (OS, configs).\nWith containers, scaling is easier: just add Linux servers with the right tools and you’re good to go.\n\n\n\nDifferences between containers (left) and VMs (right) (Source: docker.com)\n\n\nFrom the user’s point of view, the difference is often invisible for typical usage.\nThey access the app via a browser or tool, and computations happen on the remote server.\nBut for the organization hosting the app, containers bring more freedom and flexibility."
  },
  {
    "objectID": "chapters/portability.html#docker-the-standard-implementation",
    "href": "chapters/portability.html#docker-the-standard-implementation",
    "title": "Portabilité",
    "section": "Docker , the standard implementation",
    "text": "Docker , the standard implementation\nAs mentioned, the containerization software acts as a bridge between applications and the server’s OS.\nAs with virtual environments, there are multiple container technologies.\nIn practice, Docker has become the dominant one — to the point where “containerize” and “Dockerize” are often used interchangeably.\nWe will focus on Docker in this course.\n\nInstallation & sandbox environments\nDocker  can be installed on various operating systems.\nInstallation instructions are in the official documentation.\nYou need admin rights on your computer to install it.\n\n\n\n\n\n\nDisk space requirements\n\n\n\n\n\nIt’s also recommended to have free disk space, as some images (we’ll come back to that) can be large once decompressed and built.\nThey may take up several GBs depending on the libraries included.\nStill, this is small compared to a full OS (15GB for Ubuntu or macOS, 20GB for Windows…).\nThe smallest Linux distribution (Alpine) is only 3MB compressed and 5MB uncompressed.\n\n\n\nIf you can’t install Docker, there are free online sandboxes.\nPlay with Docker lets you test Docker as if it were on your local machine.\nThese services are limited though (2GB max image size, outages under heavy load…).\nAs we’ll see, interactive Docker usage is great for learning.\nBut in practice, Docker is mostly used via CI systems like GitHub Actions or Gitlab CI.\n\n\nConcepts\n\n\n\nSource: k21academy.com\n\n\nA Docker container is delivered as an image: a binary file containing the environment needed to run an app.\nIt’s shared in compressed form on a public image repository (e.g., Dockerhub).\nBefore sharing, you must build the image.\nThat’s done using a Dockerfile, a text file with Linux commands describing how to set up the environment.\nOnce built, you can:\n\nRun it locally to test and debug the app.\nRunning it creates an isolated environment called a container — a live instance of the image5.\nPush it to a public or private repository so others (or yourself) can pull and use it.\n\n\n\n\n\n\n\nPublishing your Docker image\n\n\n\n\n\nThe most well-known image repository is DockerHub.\nAnyone can publish a Docker image there, optionally linked to a GitHub or Gitlab project.\nWhile you can upload images manually, as we’ll see in the deployment chapter, it’s much better to use automated links between DockerHub and your GitHub repo."
  },
  {
    "objectID": "chapters/portability.html#application-1",
    "href": "chapters/portability.html#application-1",
    "title": "Portabilité",
    "section": "Application",
    "text": "Application\nTo demonstrate Docker in practice, we’ll walk through the steps to Dockerize a minimal web application built with the Python web framework Flask6.\nOur project structure is as follows:\n├── myflaskapp\n│   ├── Dockerfile\n│   ├── hello-world.py\n│   └── requirements.txt\nThe hello-world.py script contains the code of a basic app that simply displays “Hello, World!” on a web page. We’ll explore how to build a more interactive application in the running example.\n\n\nhello-world.py\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello_world():\n    return \"&lt;p&gt;Hello, World!&lt;/p&gt;\"\n\nTo run the app, we need both Python and the Flask package. This means we need to control the Python environment:\n\nInstall Python;\nInstall the packages required by our code—in this case, only Flask.\n\nSince we’re not tied to a specific version of Python, using a venv virtual environment is simpler than conda. Conveniently, we already have a requirements.txt file that looks like this:\n\n\nrequirements.txt\n\nFlask==2.1.1\n\nBoth steps (installing Python and its required packages) must be declared in the Dockerfile (see next section)."
  },
  {
    "objectID": "chapters/portability.html#dockerfile",
    "href": "chapters/portability.html#dockerfile",
    "title": "Portabilité",
    "section": "The Dockerfile",
    "text": "The Dockerfile\nJust like a dish needs a recipe, a Docker image needs a Dockerfile.\nThis text file contains a sequence of instructions to build the image. These files can range from simple to complex depending on the application being containerized, but their structure follows some standards.\nThe idea is to start from a base layer (typically a minimal Linux distribution) and layer on the tools and configuration needed by our app.\nLet’s go through the Dockerfile for our Flask application line by line:\n#| filename: Dockerfile\n\n1FROM ubuntu:20.04\n\nRUN apt-get update -y && \\\n2    apt-get install -y python3-pip python3-dev\n\n3WORKDIR /app\n\n4COPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY . /app\n\n5ENV FLASK_APP=\"hello-world.py\"\n6EXPOSE 5000\n\n7CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n\n1\n\nFROM: defines the base image. All Docker images inherit from a base. Here we choose ubuntu:20.04, so our environment will act like a blank virtual machine running Ubuntu 20.04 ;\n\n2\n\nRUN: executes a Linux command. We first update the list of installable packages, then install Python  and any required system libraries;\n\n3\n\nWORKDIR: sets the working directory inside the image. All following commands will run from this path—Docker’s equivalent to the cd command (see Linux 101);\n\n4\n\nCOPY: transfers files from the host to the Docker image. This is important because Docker builds images in isolation—your project files don’t exist inside the image unless explicitly copied. First, we copy requirements.txt to install dependencies, then copy the full project directory;\n\n5\n\nENV: defines an environment variable accessible inside the container. Here, we use FLASK_APP to tell Flask which script to run;\n\n6\n\nEXPOSE: tells Docker the app will listen on port 5000—the default port for Flask’s development server;\n\n7\n\nCMD: defines the command to run when the container starts. Here, it launches the Flask server and binds it to all IPs in the container with --host=0.0.0.0.\n\n\n\n\n\n\n\n\nChoosing the Base Image\n\n\n\n\n\nIdeally, you want the smallest base image possible to reduce final image size. For example, there’s no need to use an image with  if your project only uses .\nMost languages provide lightweight base images with preinstalled and well-configured interpreters. In our case, we could have used python:3.9-slim-buster.\n\n\n\nThe first RUN installs Python and system libraries required by our app. But how did we know which libraries to include?\nTrial and error. During the build phase, Docker attempts to construct the image from the Dockerfile—as if it’s starting from a clean Ubuntu 20.04 machine. If a system dependency is missing, the build will fail and show an error message in the console logs. With luck, the logs will explicitly name the missing library. More often, the messages are vague and require some web searching—StackOverflow is a frequent lifesaver.\nBefore creating a Docker image, it’s helpful to go through an intermediate step: writing a shell script (.sh) to automate setup locally. This approach is outlined in the running example.\n\n\n\n\n\n\nThe COPY Instruction\n\n\n\n\n\nYour Dockerfile recipe might require files from your working folder. To ensure Docker sees them during the build, you need a COPY command. Think of it like cooking: if you want to use an ingredient, you need to take it out of the fridge (your local disk) and place it on the table (Docker build context).\n\n\n\nWe’ve covered only the most common Docker commands. For a full reference, check the official documentation."
  },
  {
    "objectID": "chapters/portability.html#build",
    "href": "chapters/portability.html#build",
    "title": "Portabilité",
    "section": "Building a Docker Image",
    "text": "Building a Docker Image\nTo build an image from a Dockerfile, use the docker build command from the terminal7. Two important arguments must be provided:\n\nthe build context: this tells Docker where the project is located (it should contain the Dockerfile). The simplest approach is to navigate into the project directory via cd and pass . to indicate “build from here”;\nthe tag, i.e., the name of the image. While working locally, the tag doesn’t matter much, but we’ll see later that it becomes important when exporting or importing an image from/to a remote repository.\n\nLet’s see what happens when we try to build our image with the tag myflaskapp:\n\n\nterminal\n\ndocker build -t myflaskapp .\n\nSending build context to Docker daemon     47MB\nStep 1/8 : FROM ubuntu:20.04\n ---&gt; 825d55fb6340\nStep 2/8 : RUN apt-get update && apt-get install -y python3-pip python3-dev\n ---&gt; Running in 92b42d579cfa\n...\ndone.\nRemoving intermediate container 92b42d579cfa\n ---&gt; 8826d53e3c01\n...\nSuccessfully built 125bd8da70ff\nSuccessfully tagged myflaskapp:latest\nDocker’s engine processes the instructions from the Dockerfile one at a time. If there’s an error, the build stops, and you’ll need to debug the problem using the log output and adjust the Dockerfile accordingly.\nIf successful, Docker will indicate that the build was completed and that the image is ready for use. You can confirm its presence with the docker images command:\n\n\nterminal\n\ndocker images\n\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\nmyflaskapp   latest    57d2f410a631   2 hours ago      433MB\nLet’s look more closely at the build logs 👆️.\nBetween each step, Docker prints cryptic hashes and mentions intermediate containers. Think of a Docker image as a stack of layers, each layer being itself a Docker image. The FROM instruction specifies the starting layer. Each command adds a new layer with a unique hash.\nThis design is not just technical trivia—it’s incredibly useful in practice because Docker caches each intermediate layer8.\nFor example, if you modify the 5th instruction in the Dockerfile, Docker will reuse the cache for previous layers and only rebuild from the change onward. This is called cache invalidation: once a step changes, Docker recalculates that step and all that follow, but no more. As a result, you should always place frequently changing steps at the end of the file.\nLet’s illustrate this by changing the FLASK_APP environment variable in the Dockerfile:\n\n\nterminal\n\ndocker build . -t myflaskapp\n\nSending build context to Docker daemon  4.096kB\nStep 1/10 : FROM ubuntu:20.04\n ---&gt; 825d55fb6340\nStep 2/10 : ENV DEBIAN_FRONTEND=noninteractive\n ---&gt; Using cache\n ---&gt; ea1c7c083ac9\nStep 3/10 : RUN apt-get update -y && ...\n ---&gt; Using cache\n ---&gt; 078b8ac0e1cb\n...\nStep 8/10 : ENV FLASK_APP=\"new.py\"\n ---&gt; Running in b378d16bb605\n...\nSuccessfully built 16d7a5b8db28\nSuccessfully tagged myflaskapp:latest\nThe build finishes in seconds instead of minutes. The logs show that previous steps were cached, and only modified or dependent ones were rebuilt."
  },
  {
    "objectID": "chapters/portability.html#execution",
    "href": "chapters/portability.html#execution",
    "title": "Portabilité",
    "section": "Running a Docker Image",
    "text": "Running a Docker Image\nThe build step created a Docker image—essentially a blueprint for your app. It can be executed on any environment with Docker installed.\nSo far, we’ve built the image but haven’t run it. To launch the app, we must create a container, i.e., a live instance of the image. This is done with the docker run command:\n\n\nterminal\n\n$ docker run -d -p 8000:5000 myflaskapp:latest\n6a2ab0d82d051a3829b182ede7b9152f7b692117d63fa013e7dfe6232f1b9e81\n\nHere’s a breakdown of the command:\n\ndocker run tag: runs the image specified by tag. Tags usually follow the format repository/project:version. Since we’re local, there’s no repository;\n-d: runs the container in detached mode (in the background);\n-p: maps a port on the host machine (8000) to a port inside the container (5000). Since Flask listens on port 5000, this makes the app accessible via localhost:8000.\n\nThe command returns a long hash—this is the container ID. You can verify that it’s running with:\n\n\nterminal\n\ndocker ps\n\nCONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS                                   NAMES\n6a2ab0d82d05   myflaskapp   \"flask run --host=0.…\"   7 seconds ago   Up 6 seconds   0.0.0.0:8000-&gt;5000/tcp, :::8000-&gt;5000/tcp   vigorous_kalam\nDocker containers serve different purposes. Broadly, they fall into two categories:\n\nOne-shot jobs: containers that execute a task and terminate;\nRunning apps: containers that persist while serving an application.\n\nIn our case, we’re in the second category. We want to run a web app, so the container must stay alive. Flask launches a server on a local port (5000), and we’ve mapped it to port 8000 on our machine. You can access the app from your browser at localhost:8000, just like a Jupyter notebook.\nIn the end, we’ve built and launched a fully working application on our local machine—without installing anything beyond Docker itself."
  },
  {
    "objectID": "chapters/portability.html#exp-docker",
    "href": "chapters/portability.html#exp-docker",
    "title": "Portabilité",
    "section": "Exporting a Docker Image",
    "text": "Exporting a Docker Image\nSo far, all Docker commands we’ve run were executed locally. This is fine for development and experimentation. But as we’ve seen, one of Docker’s biggest strengths is the ability to easily share images with others. These images can then be used by multiple users to run the same application on their own machines.\nTo do this, we need to upload our image to a remote repository from which others can download it.\nThere are various options depending on your context: a company might have a private registry, while open-source projects can use DockerHub.\nHere is the typical workflow for uploading an image:\n\nCreate an account on DockerHub;\nCreate a (public) project on DockerHub to host your Docker images;\nUse docker login in your terminal to authenticate with DockerHub;\nModify the image tag during the build to include the expected path. For example: docker build -t username/project:version .;\nPush the image with docker push username/project:version.\n\n\n\nterminal\n\ndocker push avouacr/myflaskapp:1.0.0\n\nThe push refers to repository [docker.io/avouacr/myflaskapp]\n71db96687fe6: Pushed\n624877ac887b: Pushed\nea4ab6b86e70: Pushed\n..."
  },
  {
    "objectID": "chapters/portability.html#imp-docker",
    "href": "chapters/portability.html#imp-docker",
    "title": "Portabilité",
    "section": "Importing a Docker Image",
    "text": "Importing a Docker Image\nIf the image repository is public, users can pull the image using a single command:\n\n\nterminal\n\ndocker pull avouacr/myflaskapp:1.0.0\n\n1.0.0: Pulling from avouacr/myflaskapp\ne0b25ef51634: Pull complete\nc0445e4b247e: Pull complete\n...\nStatus: Downloaded newer image for avouacr/myflaskapp:1.0.0\nDocker will download and unpack each layer of the image (which may take time). Once downloaded, users can run the container with the docker run command as shown earlier."
  },
  {
    "objectID": "chapters/portability.html#cheat-sheet-1",
    "href": "chapters/portability.html#cheat-sheet-1",
    "title": "Portabilité",
    "section": "Cheat Sheet",
    "text": "Cheat Sheet\nHere’s a quick reference of common Dockerfile commands:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nFROM &lt;image&gt;:&lt;tag&gt;\nUse &lt;image&gt; with &lt;tag&gt; as the base image\n\n\nRUN &lt;instructions&gt;\nExecute Linux shell instructions. Use && to chain commands. Use \\ to split long commands across multiple lines\n\n\nCOPY &lt;source&gt; &lt;dest&gt;\nCopy a file from the local machine into the image\n\n\nADD &lt;source&gt; &lt;dest&gt;\nSimilar to COPY, but can also handle URLs and tar archives\n\n\nENV MY_NAME=\"John Doe\"\nDefine an environment variable available via $MY_NAME\n\n\nWORKDIR &lt;path&gt;\nSet the working directory inside the container\n\n\nUSER &lt;username&gt;\nSet a non-root user named &lt;username&gt;\n\n\nEXPOSE &lt;PORT_ID&gt;\nIndicate that the application will listen on port &lt;PORT_ID&gt;\n\n\nCMD [\"executable\",\"param1\",\"param2\"]\nDefine the default command to run when the container starts\n\n\n\nAnd a second cheat sheet with basic Docker CLI commands:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndocker build . -t &lt;tag&gt;\nBuild the Docker image from current directory, tagging it with &lt;tag&gt;\n\n\ndocker run -it &lt;tag&gt;\nRun the container interactively from image with &lt;tag&gt;\n\n\ndocker images\nList locally available images and metadata (tags, size, etc.)\n\n\ndocker system prune\nClean up unused images and containers (use with caution)"
  },
  {
    "objectID": "chapters/portability.html#footnotes",
    "href": "chapters/portability.html#footnotes",
    "title": "Portabilité",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe’ll later explain how distributing packages as precompiled wheels addresses this issue.↩︎\nThis means that if you open a new terminal, you’ll need to activate the environment again if you want to use it. To activate an environment by default, you can configure your terminal (e.g., by editing .bashrc on Linux) to automatically activate a specific environment when it starts.↩︎\nIn fact, if you’re using pip on SSPCloud,\nyou’re doing exactly this—without realizing it.↩︎\nThese repositories are known as channels in the conda ecosystem.\nThe default channel is maintained by the developers at Anaconda.\nTo ensure stability, this channel updates more slowly.\nThe conda-forge channel emerged to offer developers more flexibility,\nletting them publish newer versions of their packages, much like PyPI.↩︎\nThe terms “image” and “container” are often used interchangeably.\nTechnically, a container is the live version of an image.↩︎\nFlask is a lightweight framework for deploying Python-based web applications.↩︎\nOn Windows, the default command lines (cmd or PowerShell) are not very convenient. We recommend using the Git Bash terminal, a lightweight Linux command-line emulator, for better compatibility with command-line operations.↩︎\nCaching is very useful for local development. Unfortunately, it’s harder to leverage in CI environments, since each run usually happens on a fresh machine.↩︎"
  },
  {
    "objectID": "chapters/linux101.html",
    "href": "chapters/linux101.html",
    "title": "Linux 101",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des systèmes d’exploitation (y compris avec Windows !). Mais comme il a la réputation d’être austère et complexe, on utilise plutôt des interfaces graphiques pour effectuer nos opérations informatiques quotidiennes.\nPourtant, avoir des notions quant à l’utilisation d’un terminal est une vraie source d’autonomie, dans la mesure où celui-ci permet de gérer bien plus finement les commandes que l’on réalise. Pour les data scientists qui s’intéressent aux bonnes pratiques et à la mise en production, sa maîtrise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont généralement limitées par rapport à l’utilisation du programme en ligne de commande. C’est par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de réaliser toutes les opérations permises par le logiciel ;\nmettre un projet de data science en production nécessite d’utiliser un serveur, qui le rend disponible en permanence à son public potentiel. Or là où Windows domine le monde des ordinateurs personnels, une large majorité des serveurs et des infrastructures cloud fonctionnent sous Linux ;\nles principaux langages de programmation des data scientists (, , etc. ) visent à simplifier l’exécution d’opérations complexes par le biais de la ligne de commande mais héritent néanmoins de sa logique. Utiliser la ligne de commande permet de gagner en aisance dans son travail de programmation ;\nplus généralement, une utilisation régulière du terminal est source d’une meilleure compréhension du fonctionnement d’un système de fichiers et de l’exécution des processus sur un ordinateur. Ces connaissances s’avèrent très utiles dans la pratique quotidienne du data scientist, qui nécessite de plus en plus de développer dans différents environnements d’exécution.\n\nDans le cadre de ce cours, on s’intéressera particulièrement au terminal Linux puisque l’écrasante majorité, si ce n’est l’ensemble, des serveurs de mise en production s’appuient sur un système Linux.\n\n\n\nDifférents environnements de travail peuvent être utilisés pour apprendre à se servir d’un terminal Linux :\n\nle SSP Cloud. Dans la mesure où les exemples de mise en production du cours seront illustrées sur cet environnement, nous recommandons de l’utiliser dès à présent pour se familiariser. Le terminal est accessible à partir de différents services (RStudio, Jupyter, etc.), mais nous recommandons d’utiliser le terminal d’un service VSCode, dans la mesure où se servir d’un IDE pour organiser notre code est en soi déjà une bonne pratique ;\nKatacoda, un bac à sable dans un système Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (émulation minimaliste d’un terminal Linux), qui est installée par défaut avec Git.\n\n\n\n\nLançons un terminal pour présenter son fonctionnement basique. On prend pour exemple le terminal d’un service VSCode lancé via le SSP Cloud (Application Menu tout en haut à gauche de VSCode -&gt; Terminal -&gt; New Terminal). Voici à quoi ressemble le terminal en question.\n\nDécrivons d’abord les différentes inscriptions qui arrivent à l’initialisation :\n\n(base) : cette inscription n’est pas directement liée au terminal, elle provient du fait que l’on utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en détail dans le chapitre sur la portabilité ;\ncoder@vscode-824991-64744dd6d8-zbgv5 : le nom de l’utilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que l’on verra là encore dans le chapitre sur la portabilité\n~/work : le chemin du répertoire courant, i.e. à partir duquel va être lancée toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\n\nPour éviter la lourdeur des images et permettre de copier/coller facilement les commandes, on représentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans l’exemple suivant. Les lignes commençant par un $ sont celles avec lesquelles une commande est lancée, et les lignes sans $ représentent le résultat d’une commande. Attention à ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement à différencier celles-ci des résultats.\n\n\nterminal\n\necho \"une première illustration\"\n\nune première illustration\n\n\n\nLe terme filesystem (système de fichiers) désigne la manière dont sont organisés les fichiers au sein d’un système d’exploitation. Cette structure est hiérarchique, en forme d’arbre :\n\nelle part d’un répertoire racine (le dossier qui contient tous les autres) ;\ncontient des dossiers ;\nles dossiers peuvent contenir à leur tour des dossiers (sous-dossiers) ou des fichiers.\n\nIntéressons nous à la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations :\n\nla racine (root) sur Linux s’appelle /, là où elle s’appelle C:\\ par défaut sur Windows ;\nle répertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un rôle essentiellement technique. Il est tout de même utile d’en décrire les principaux :\n\n/bin : contient les binaires, i.e. les programmes exécutables ;\n/etc : contient les fichiers de configuration ;\n/home : contient l’ensemble des dossiers et fichiers personnels des différents utilisateurs. Chaque utilisateur a un répertoire dit “HOME” qui a pour chemin /home/&lt;username&gt; Ce répertoire est souvent représenté par le symbole ~. C’était notamment le cas dans l’illustration du terminal VSCode ci-dessus, ce qui signifie qu’on se trouvait formellement dans le répertoire /home/coder/work, coder étant l’utilisateur par défaut du service VSCode sur le SSP Cloud.\n\n\nChaque dossier ou fichier est représenté par un chemin d’accès, qui correspond simplement à sa position dans le filesystem. Il existe deux moyens de spécifier un chemin :\n\nen utilisant un chemin absolu, c’est à dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconnaît donc un chemin absolu par le fait qu’il commence forcément par /.\nen utilisant un chemin relatif, c’est à dire en indiquant le chemin du dossier ou fichier relativement au répertoire courant.\n\nComme tout ce qui touche de près ou de loin au terminal, la seule manière de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront d’appliquer ces concepts à des cas pratiques.\n\n\n\nLe rôle d’un terminal est de lancer des commandes. Ces commandes peuvent être classées en trois grandes catégories :\n\nnavigation au sein du filesystem\nmanipulations de fichiers (créer, lire, modifier des dossiers/fichiers)\nlancement de programmes\n\n\n\nLorsque l’on lance un programme à partir du terminal, celui-ci a pour référence le répertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si l’on exécute un script Python en se trouvant dans un certain répertoire, tous les chemins des fichiers utilisés dans le script seront relatifs au répertoire courant d’exécution — à moins d’utiliser uniquement des chemins absolus, ce qui n’est pas une bonne pratique en termes de reproductibilité puisque cela lie votre projet à la structure de votre filesystem particulier.\nAinsi, la très grande majorité des opérations que l’on est amené à réaliser dans un terminal consiste simplement à se déplacer au sein du filesystem. Les commandes principales pour naviguer et se repérer dans le filesystem sont présentées dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez pénible de manipuler des chemins absolus, qui peuvent facilement être très longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient à se déplacer à partir du répertoire courant. Pour se faire, voici quelques utilisations très fréquentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter d’un niveau dans l’arborescence (dossier parent)\n\n\ncd ~\nrevenir dans le répertoire HOME de l’utilisateur courant\n\n\n\nLa première commande est l’occasion de revenir sur une convention d’écriture importante pour les chemins relatifs :\n\n. représente le répertoire courant. Ainsi, cd . revient à changer de répertoire courant… pour le répertoire courant, ce qui bien sûr ne change rien. Mais le . est très utile pour la copie de fichiers (cf. section suivante) ou encore lorsque l’on doit passer des paramètres à un programme (cf. section Lancement de programmes) ;\n.. représente le répertoire parent du répertoire courant.\n\nCes différentes commandes constituent la très grande majorité des usages dans un terminal. Il est essentiel de les pratiquer jusqu’à ce qu’elles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup d’autres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\ndéplacer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncréer (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncréer un fichier vide\n\n\n\nDans la mesure où il est généralement possible de réaliser toutes ces opérations à l’aide d’interfaces graphiques (notamment, l’explorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se déplacer dans le filesystem. Nous vous recommandons malgré tout de les pratiquer également, et ce pour plusieurs raisons :\n\neffectuer un maximum d’opérations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ;\nen devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ;\nlorsque l’on est amené à manipuler un terminal pour interagir avec un serveur, il n’y a souvent pas la moindre interface graphique, auquel cas il n’y a pas d’autre choix que d’opérer uniquement à partir du terminal.\n\n\n\n\nLe rôle du terminal est de lancer des programmes. Lancer un programme se fait à partir d’un fichier dit exécutable, qui peut être de deux formes :\n\nun binaire, i.e. un programme dont le code n’est pas lisible par l’humain ;\nun script, i.e. un fichier texte contenant une série d’instructions à exécuter. Le langage du terminal Linux est le shell, et les scripts associés ont pour extension .sh.\n\nDans les deux cas, la syntaxe de lancement d’une commande est : le nom de l’exécutable, suivi d’éventuels paramètres, séparés par des espaces. Par exemple, la commande python monscript.py exécute le binaire python et lui passe comme unique argument le nom d’un script .py (contenu dans le répertoire courant), qui va donc être exécuté via Python. De la même manière, toutes les commandes vues précédemment pour se déplacer dans le filesystem ou manipuler des fichiers sont des exécutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier à copier et le chemin d’arrivée.\nDans les exemples de commandes précédents, les paramètres étaient passés en mode positionnel : l’exécutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments n’est pas toujours fixé à l’avance, du fait de la présence de paramètres optionnels. Ainsi, la plupart des exécutables permettent le passage d’arguments optionnels, qui modifient le comportement de l’exécutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier à un autre endroit du filesystem, mais peut-on copier un dossier et l’ensemble de son contenu avec ? Nativement non, mais l’ajout d’un paramètre le permet : cp -R dossierdepart dossierarrivee permet de copier récursivement le dossier et tout son contenu. Notons que les flags ont très souvent un équivalent en toute lettre, qui s’écrit quant à lui avec deux tirers. Par exemple, la commande précédente peut s’écrire de manière équivalente cp --recursive dossierdepart dossierarrivee. Il est fréquent de voir les deux syntaxes en pratique, parfois même mélangées au sein d’une même commande.\n\n\n\n\nComme tout langage de programmation, le langage shell permet d’assigner et d’utiliser des variables dans des commandes. Pour afficher le contenu d’une variable, on utilise la commande echo, qui est l’équivalent de la fonction print en Python ou en R.\n\n\nterminal\n\nMY_VAR=\"toto\"\necho $MY_VAR\n\ntoto\nQuelques remarques importantes :\n\nla syntaxe pour la création de variable est précise : aucun espace d’un côté comme de l’autre du = ;\nen Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu écrire MY_VAR=toto pour le même résultat. Par contre, si l’on veut assigner à une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message d’erreur ;\npour accéder à la valeur d’une variable, on la préfixe d’un $.\n\nNotre objectif avec ce tutoriel n’est pas de savoir coder en shell, on ne va donc pas s’attarder sur les propriétés des variables. En revanche, introduire ce concept était nécessaire pour en présenter un autre, essentiel quant à lui dans la pratique quotidienne du data scientist : les variables d’environnement. Pour faire une analogie — un peu simpliste — avec les langages de programmation, ce sont des sortes de variables “globales”, dans la mesure où elles vont être accessibles à tous les programmes lancés à partir d’un terminal, et vont modifier leur comportement.\nLa liste des variables d’environnement peut être affichée à l’aide de la commande env. Il y a généralement un grand nombre de variables d’environnement prééxistantes ; en voici un échantillon obtenu à partir du terminal du service VSCode.\n\n\nterminal\n\nenv\n\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la variété des utilisations des variables d’environnements :\n\nla variable $SHELL précise l’exécutable utilisé pour lancer le terminal ;\nla variable $HOME donne l’emplacement du répertoire utilisateur. En fait, le symbole ~ que l’on a rencontré plus haut référence cette même variable ;\nla variable LANG spécifie la locale, un concept qui permet de définir la langue et l’encodage utilisés par défaut par Linux ;\nla variable CONDA_PYTHON_EXE existe uniquement parce que l’on a installé conda comme système de gestion de packages Python. C’est l’existence de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous intéresse.\n\nUne variable d’environnement essentielle, et que l’on est fréquemment amené à modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concaténation de chemins absolus, séparés par :, qui spécifie les dossiers dans lesquels Linux va chercher les exécutables lorsque l’on lance une commande, ainsi que l’ordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n\n\nterminal\n\necho $PATH\n\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nL’ordre de recherche est de gauche à droite. C’est donc parce que le dossier /home/coder/local/bin/conda/bin est situé en premier que l’interpréteur Python qui sera choisi lorsque l’on lance un script Python est celui issu de Conda, et non celui contenu par défaut dans /usr/bin par exemple.\nL’existence et la configuration adéquate des variables d’environnement est essentielle pour le bon fonctionnement de nombreux outils très utilisés en data science, comme Git ou encore Spark par exemple. Il est donc nécessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration d’un serveur en cas de bug lié à une variable d’environnement manquante ou mal configurée.\n\n\n\nLa sécurité est un enjeu central en Linux, qui permet une gestion très fine des permissions sur les différents fichiers et programmes.\nUne différence majeure par rapport à d’autres systèmes d’exploitation, notamment Windows, est qu’aucun utilisateur n’a par défaut les droits complets d’administrateur (root). Il n’est donc pas possible nativement d’accéder au parties sensibles du système, ou bien de lancer certains types de programme. Par exemple, si l’on essaie de lister les fichiers du dossier /root, on obtient une erreur.\n\n\nterminal\n\nls /root\n\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines opérations telles que l’installation de binaires ou de packages nécessitent cependant des droits administrateurs. Dans ce cas, il est d’usage d’utiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de l’exécution de la commande.\n\n\nterminal\n\nsudo ls /root\n\nLe dossier /root étant vide, la commande ls renvoie une chaîne de caractères vide, mais nous n’avons plus de problème de permission. Notons qu’une bonne pratique de sécurité, en particulier dans les scripts shell que l’on peut être amenés à écrire ou exécuter, est de limiter l’utilisation de cette commande aux cas où elle s’avère nécessaire.\nUne autre subtilité concerne justement l’exécution de scripts shell. Par défaut, qu’il soit créé par l’utilisateur ou téléchargé d’internet, un script n’est pas exécutable.\n\n\nterminal\n\n1touch test.sh\n2./test.sh\n\n\n1\n\nCréer le script test.sh (vide)\n\n2\n\nExécuter le script test.sh\n\n\nbash: ./test.sh: Permission denied\nC’est bien entendu une mesure de sécurité pour éviter l’exécution automatique de scripts potentiellement malveillants. Pour pouvoir exécuter un tel script, il faut attribuer des droits d’exécution au fichier avec la commande chmod. Il devient alors possible d’exécuter le script classiquement.\n\n\nterminal\n\n1chmod +x test.sh\n2./test.sh\n\n# Le script étant vide, il ne se passe rien\n\n\n1\n\nDonner des droits d’exécution au script test.sh\n\n2\n\nExécuter le script test.sh\n\n\n\n\n\nMaintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell précédemment évoqués. A l’instar d’un script Python, un script shell permet d’automatiser une série de commandes lancées dans un terminal. Le but de ce tutoriel n’est pas de savoir écrire des scripts shell complexes, travail généralement dévolu aux les data engineers ou les sysadmin (administrateurs système), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces compétences sont essentielles lorsque l’on se préoccupe de mise en production. A titre d’exemple, comme nous le verrons dans le chapitre sur la portabilité, il est fréquent d’utiliser un script shell comme entrypoint d’une image docker, afin de spécifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement à l’aide d’un script simple. Considérons les commandes suivantes, que l’on met dans un fichier monscript.sh dans le répertoire courant.\n\n\nterminal\n\n#!/bin/bash\nSECTION=$1\nCHAPTER=$2\nFORMATION_DIR=/home/coder/work/formation\nmkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\ntouch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\n\nAnalysons la structure de ce script :\n\nla première ligne est classique, elle se nomme le shebang : elle indique au système quel interpréteur utiliser pour exécuter ce script. Dans notre cas, et de manière générale, on utilise bash (Bourne-Again SHell, l’implémentation moderne du shell) ;\nles lignes 2 et 3 assignent à des variables les arguments passés au script dans la commande. Par défaut, ceux-ci sont assignés à des variables n où n est la position de l’argument, en commençant à 1 ;\nla ligne 4 assigne un chemin à une variable\nla ligne 5 crée le chemin complet, défini à partir des variables créées précédemment. Le paramètre -p est important : il précise à mkdir d’agir de manière récursive, c’est à dire de créer les dossiers intermédiaires qui n’existent pas encore ;\nla ligne 6 crée un fichier texte vide dans le dossier créé avec la commande précédente.\n\nExécutons maintenant ce script, en prenant soin de lui donner les permission adéquates au préalable.\n\n\nterminal\n\nchmod +x monscript.sh\nbash monscript.sh section2 chapitre3\nls formation/section1/chapitre2/\n\ntext.txt\nOpération réussie : le dossier a bien été créé et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash très bien réalisée.\n\n\n\nUne différence fondamentale entre Linux et Windows tient à la manière dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier exécutable en .exe) sur le site du logiciel, et on l’exécute. En Linux, on passe généralement par un gestionnaire de packages qui va chercher les logiciels sur un répertoire centralisé, à la manière de pip en Python par exemple.\nPourquoi cette différence ? Une raison importante est que, contrairement à Windows, il existe une multitude de distributions différentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent différemment et peuvent avoir différentes versions. En utilisant le package manager (gestionnaire de paquets) propre à la distribution en question, on s’assure de télécharger le logiciel adapté à sa distribution. Dans ce cours, on fait le choix d’utiliser une distribution Debian et son gestionnaire de paquets associé apt. Debian est en effet un choix populaire pour les servers de part sa stabilité et sa simplicité, et sera également familière aux utilisateurs d’Ubuntu, distribution très populaire pour les ordinateurs personnels et qui est basée sur Debian.\nL’utilisation d’apt est très simple. La seule difficulté est de savoir le nom du paquet que l’on souhaite installer, ce qui nécessite en général d’utiliser un moteur de recherche. L’installation de paquets est également un cas où il faut utiliser sudo, puisque cela implique souvent l’accès à des répertoires protégés.\n\n\nterminal\n\nsudo apt install tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nDésinstaller un package est également simple : c’est l’opération inverse. Par sécurité, le terminal vous demande si vous êtes sûr de votre choix en vous demandant de tapper la lettre y (yes) ou la lettre n. On peut passer automatiquement cette étape en ajoutant le paramètre -y\n\n\nterminal\n\nsudo apt remove -y tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant d’installer un package, il est toujours préférable de mettre à jour la base des packages, pour s’assurer qu’on obtiendra bien la dernière version.\n\n\nterminal\n\nsudo apt update\n\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date.\n\n\n\nOn l’a dit et redit : devenir à l’aise avec le terminal Linux est essentiel et demande de la pratique. Il existe néanmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa première est l’autocomplétion. Dès lors que vous écrivez une commande contenant un nom d’exécutable, un chemin sur le filesystem, ou autre, n’hésitez pas à utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majorité des cas, cela va vous faire gagner un temps précieux.\nLa deuxième est de parcourir l’historique de commandes : la flèche vers le haut (↑) permet de parcourir l’historique des commandes que vous avez précédemment exécutées. Chaque fois que vous appuyez sur cette touche, le terminal affiche la dernière commande exécutée, en remontant dans l’historique à chaque appui supplémentaire.\nLa troisième, dans le même esprit que la deuxième mais plus élaborée, est la recherche inverse dans l’historique de commandes avec les touches Ctrl+R. Lorsque vous appuyez sur Ctrl+R, une invite de recherche apparaît. Vous pouvez alors commencer à taper des caractères de la commande que vous recherchez. Le terminal cherchera dans l’historique des commandes la dernière commande correspondant à ce que vous avez tapé, et la montrera à l’écran. Si ce n’est pas la commande exacte que vous cherchez, vous pouvez continuer à taper pour affiner la recherche ou appuyer à nouveau sur Ctrl+R pour rechercher la commande précédente correspondant à vos critères.\nUne quatrième astuce, qui n’en est pas vraiment une, est de lire la documentation d’une commande lorsque l’on n’est pas sûr de sa syntaxe ou des paramètres admissibles. Via le terminal, la documentation d’une commande peut être affichée en exécutant man suivie de la commande en question, par exemple : man cp. Comme il n’est pas toujours très pratique de lire de longs textes dans un petit terminal, on peut également chercher la documentation d’une commande sur le site man7."
  },
  {
    "objectID": "chapters/linux101.html#pourquoi-sintéresser-au-terminal-linux",
    "href": "chapters/linux101.html#pourquoi-sintéresser-au-terminal-linux",
    "title": "Linux 101",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des systèmes d’exploitation (y compris avec Windows !). Mais comme il a la réputation d’être austère et complexe, on utilise plutôt des interfaces graphiques pour effectuer nos opérations informatiques quotidiennes.\nPourtant, avoir des notions quant à l’utilisation d’un terminal est une vraie source d’autonomie, dans la mesure où celui-ci permet de gérer bien plus finement les commandes que l’on réalise. Pour les data scientists qui s’intéressent aux bonnes pratiques et à la mise en production, sa maîtrise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont généralement limitées par rapport à l’utilisation du programme en ligne de commande. C’est par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de réaliser toutes les opérations permises par le logiciel ;\nmettre un projet de data science en production nécessite d’utiliser un serveur, qui le rend disponible en permanence à son public potentiel. Or là où Windows domine le monde des ordinateurs personnels, une large majorité des serveurs et des infrastructures cloud fonctionnent sous Linux ;\nles principaux langages de programmation des data scientists (, , etc. ) visent à simplifier l’exécution d’opérations complexes par le biais de la ligne de commande mais héritent néanmoins de sa logique. Utiliser la ligne de commande permet de gagner en aisance dans son travail de programmation ;\nplus généralement, une utilisation régulière du terminal est source d’une meilleure compréhension du fonctionnement d’un système de fichiers et de l’exécution des processus sur un ordinateur. Ces connaissances s’avèrent très utiles dans la pratique quotidienne du data scientist, qui nécessite de plus en plus de développer dans différents environnements d’exécution.\n\nDans le cadre de ce cours, on s’intéressera particulièrement au terminal Linux puisque l’écrasante majorité, si ce n’est l’ensemble, des serveurs de mise en production s’appuient sur un système Linux."
  },
  {
    "objectID": "chapters/linux101.html#environnement-de-travail",
    "href": "chapters/linux101.html#environnement-de-travail",
    "title": "Linux 101",
    "section": "",
    "text": "Différents environnements de travail peuvent être utilisés pour apprendre à se servir d’un terminal Linux :\n\nle SSP Cloud. Dans la mesure où les exemples de mise en production du cours seront illustrées sur cet environnement, nous recommandons de l’utiliser dès à présent pour se familiariser. Le terminal est accessible à partir de différents services (RStudio, Jupyter, etc.), mais nous recommandons d’utiliser le terminal d’un service VSCode, dans la mesure où se servir d’un IDE pour organiser notre code est en soi déjà une bonne pratique ;\nKatacoda, un bac à sable dans un système Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (émulation minimaliste d’un terminal Linux), qui est installée par défaut avec Git."
  },
  {
    "objectID": "chapters/linux101.html#introduction-au-terminal",
    "href": "chapters/linux101.html#introduction-au-terminal",
    "title": "Linux 101",
    "section": "",
    "text": "Lançons un terminal pour présenter son fonctionnement basique. On prend pour exemple le terminal d’un service VSCode lancé via le SSP Cloud (Application Menu tout en haut à gauche de VSCode -&gt; Terminal -&gt; New Terminal). Voici à quoi ressemble le terminal en question.\n\nDécrivons d’abord les différentes inscriptions qui arrivent à l’initialisation :\n\n(base) : cette inscription n’est pas directement liée au terminal, elle provient du fait que l’on utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en détail dans le chapitre sur la portabilité ;\ncoder@vscode-824991-64744dd6d8-zbgv5 : le nom de l’utilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que l’on verra là encore dans le chapitre sur la portabilité\n~/work : le chemin du répertoire courant, i.e. à partir duquel va être lancée toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\n\nPour éviter la lourdeur des images et permettre de copier/coller facilement les commandes, on représentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans l’exemple suivant. Les lignes commençant par un $ sont celles avec lesquelles une commande est lancée, et les lignes sans $ représentent le résultat d’une commande. Attention à ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement à différencier celles-ci des résultats.\n\n\nterminal\n\necho \"une première illustration\"\n\nune première illustration"
  },
  {
    "objectID": "chapters/linux101.html#notions-de-filesystem",
    "href": "chapters/linux101.html#notions-de-filesystem",
    "title": "Linux 101",
    "section": "",
    "text": "Le terme filesystem (système de fichiers) désigne la manière dont sont organisés les fichiers au sein d’un système d’exploitation. Cette structure est hiérarchique, en forme d’arbre :\n\nelle part d’un répertoire racine (le dossier qui contient tous les autres) ;\ncontient des dossiers ;\nles dossiers peuvent contenir à leur tour des dossiers (sous-dossiers) ou des fichiers.\n\nIntéressons nous à la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations :\n\nla racine (root) sur Linux s’appelle /, là où elle s’appelle C:\\ par défaut sur Windows ;\nle répertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un rôle essentiellement technique. Il est tout de même utile d’en décrire les principaux :\n\n/bin : contient les binaires, i.e. les programmes exécutables ;\n/etc : contient les fichiers de configuration ;\n/home : contient l’ensemble des dossiers et fichiers personnels des différents utilisateurs. Chaque utilisateur a un répertoire dit “HOME” qui a pour chemin /home/&lt;username&gt; Ce répertoire est souvent représenté par le symbole ~. C’était notamment le cas dans l’illustration du terminal VSCode ci-dessus, ce qui signifie qu’on se trouvait formellement dans le répertoire /home/coder/work, coder étant l’utilisateur par défaut du service VSCode sur le SSP Cloud.\n\n\nChaque dossier ou fichier est représenté par un chemin d’accès, qui correspond simplement à sa position dans le filesystem. Il existe deux moyens de spécifier un chemin :\n\nen utilisant un chemin absolu, c’est à dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconnaît donc un chemin absolu par le fait qu’il commence forcément par /.\nen utilisant un chemin relatif, c’est à dire en indiquant le chemin du dossier ou fichier relativement au répertoire courant.\n\nComme tout ce qui touche de près ou de loin au terminal, la seule manière de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront d’appliquer ces concepts à des cas pratiques."
  },
  {
    "objectID": "chapters/linux101.html#lancer-des-commandes",
    "href": "chapters/linux101.html#lancer-des-commandes",
    "title": "Linux 101",
    "section": "",
    "text": "Le rôle d’un terminal est de lancer des commandes. Ces commandes peuvent être classées en trois grandes catégories :\n\nnavigation au sein du filesystem\nmanipulations de fichiers (créer, lire, modifier des dossiers/fichiers)\nlancement de programmes\n\n\n\nLorsque l’on lance un programme à partir du terminal, celui-ci a pour référence le répertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si l’on exécute un script Python en se trouvant dans un certain répertoire, tous les chemins des fichiers utilisés dans le script seront relatifs au répertoire courant d’exécution — à moins d’utiliser uniquement des chemins absolus, ce qui n’est pas une bonne pratique en termes de reproductibilité puisque cela lie votre projet à la structure de votre filesystem particulier.\nAinsi, la très grande majorité des opérations que l’on est amené à réaliser dans un terminal consiste simplement à se déplacer au sein du filesystem. Les commandes principales pour naviguer et se repérer dans le filesystem sont présentées dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez pénible de manipuler des chemins absolus, qui peuvent facilement être très longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient à se déplacer à partir du répertoire courant. Pour se faire, voici quelques utilisations très fréquentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter d’un niveau dans l’arborescence (dossier parent)\n\n\ncd ~\nrevenir dans le répertoire HOME de l’utilisateur courant\n\n\n\nLa première commande est l’occasion de revenir sur une convention d’écriture importante pour les chemins relatifs :\n\n. représente le répertoire courant. Ainsi, cd . revient à changer de répertoire courant… pour le répertoire courant, ce qui bien sûr ne change rien. Mais le . est très utile pour la copie de fichiers (cf. section suivante) ou encore lorsque l’on doit passer des paramètres à un programme (cf. section Lancement de programmes) ;\n.. représente le répertoire parent du répertoire courant.\n\nCes différentes commandes constituent la très grande majorité des usages dans un terminal. Il est essentiel de les pratiquer jusqu’à ce qu’elles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup d’autres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\ndéplacer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncréer (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncréer un fichier vide\n\n\n\nDans la mesure où il est généralement possible de réaliser toutes ces opérations à l’aide d’interfaces graphiques (notamment, l’explorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se déplacer dans le filesystem. Nous vous recommandons malgré tout de les pratiquer également, et ce pour plusieurs raisons :\n\neffectuer un maximum d’opérations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ;\nen devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ;\nlorsque l’on est amené à manipuler un terminal pour interagir avec un serveur, il n’y a souvent pas la moindre interface graphique, auquel cas il n’y a pas d’autre choix que d’opérer uniquement à partir du terminal.\n\n\n\n\nLe rôle du terminal est de lancer des programmes. Lancer un programme se fait à partir d’un fichier dit exécutable, qui peut être de deux formes :\n\nun binaire, i.e. un programme dont le code n’est pas lisible par l’humain ;\nun script, i.e. un fichier texte contenant une série d’instructions à exécuter. Le langage du terminal Linux est le shell, et les scripts associés ont pour extension .sh.\n\nDans les deux cas, la syntaxe de lancement d’une commande est : le nom de l’exécutable, suivi d’éventuels paramètres, séparés par des espaces. Par exemple, la commande python monscript.py exécute le binaire python et lui passe comme unique argument le nom d’un script .py (contenu dans le répertoire courant), qui va donc être exécuté via Python. De la même manière, toutes les commandes vues précédemment pour se déplacer dans le filesystem ou manipuler des fichiers sont des exécutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier à copier et le chemin d’arrivée.\nDans les exemples de commandes précédents, les paramètres étaient passés en mode positionnel : l’exécutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments n’est pas toujours fixé à l’avance, du fait de la présence de paramètres optionnels. Ainsi, la plupart des exécutables permettent le passage d’arguments optionnels, qui modifient le comportement de l’exécutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier à un autre endroit du filesystem, mais peut-on copier un dossier et l’ensemble de son contenu avec ? Nativement non, mais l’ajout d’un paramètre le permet : cp -R dossierdepart dossierarrivee permet de copier récursivement le dossier et tout son contenu. Notons que les flags ont très souvent un équivalent en toute lettre, qui s’écrit quant à lui avec deux tirers. Par exemple, la commande précédente peut s’écrire de manière équivalente cp --recursive dossierdepart dossierarrivee. Il est fréquent de voir les deux syntaxes en pratique, parfois même mélangées au sein d’une même commande."
  },
  {
    "objectID": "chapters/linux101.html#variables-denvironnement",
    "href": "chapters/linux101.html#variables-denvironnement",
    "title": "Linux 101",
    "section": "",
    "text": "Comme tout langage de programmation, le langage shell permet d’assigner et d’utiliser des variables dans des commandes. Pour afficher le contenu d’une variable, on utilise la commande echo, qui est l’équivalent de la fonction print en Python ou en R.\n\n\nterminal\n\nMY_VAR=\"toto\"\necho $MY_VAR\n\ntoto\nQuelques remarques importantes :\n\nla syntaxe pour la création de variable est précise : aucun espace d’un côté comme de l’autre du = ;\nen Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu écrire MY_VAR=toto pour le même résultat. Par contre, si l’on veut assigner à une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message d’erreur ;\npour accéder à la valeur d’une variable, on la préfixe d’un $.\n\nNotre objectif avec ce tutoriel n’est pas de savoir coder en shell, on ne va donc pas s’attarder sur les propriétés des variables. En revanche, introduire ce concept était nécessaire pour en présenter un autre, essentiel quant à lui dans la pratique quotidienne du data scientist : les variables d’environnement. Pour faire une analogie — un peu simpliste — avec les langages de programmation, ce sont des sortes de variables “globales”, dans la mesure où elles vont être accessibles à tous les programmes lancés à partir d’un terminal, et vont modifier leur comportement.\nLa liste des variables d’environnement peut être affichée à l’aide de la commande env. Il y a généralement un grand nombre de variables d’environnement prééxistantes ; en voici un échantillon obtenu à partir du terminal du service VSCode.\n\n\nterminal\n\nenv\n\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la variété des utilisations des variables d’environnements :\n\nla variable $SHELL précise l’exécutable utilisé pour lancer le terminal ;\nla variable $HOME donne l’emplacement du répertoire utilisateur. En fait, le symbole ~ que l’on a rencontré plus haut référence cette même variable ;\nla variable LANG spécifie la locale, un concept qui permet de définir la langue et l’encodage utilisés par défaut par Linux ;\nla variable CONDA_PYTHON_EXE existe uniquement parce que l’on a installé conda comme système de gestion de packages Python. C’est l’existence de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous intéresse.\n\nUne variable d’environnement essentielle, et que l’on est fréquemment amené à modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concaténation de chemins absolus, séparés par :, qui spécifie les dossiers dans lesquels Linux va chercher les exécutables lorsque l’on lance une commande, ainsi que l’ordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n\n\nterminal\n\necho $PATH\n\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nL’ordre de recherche est de gauche à droite. C’est donc parce que le dossier /home/coder/local/bin/conda/bin est situé en premier que l’interpréteur Python qui sera choisi lorsque l’on lance un script Python est celui issu de Conda, et non celui contenu par défaut dans /usr/bin par exemple.\nL’existence et la configuration adéquate des variables d’environnement est essentielle pour le bon fonctionnement de nombreux outils très utilisés en data science, comme Git ou encore Spark par exemple. Il est donc nécessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration d’un serveur en cas de bug lié à une variable d’environnement manquante ou mal configurée."
  },
  {
    "objectID": "chapters/linux101.html#permissions",
    "href": "chapters/linux101.html#permissions",
    "title": "Linux 101",
    "section": "",
    "text": "La sécurité est un enjeu central en Linux, qui permet une gestion très fine des permissions sur les différents fichiers et programmes.\nUne différence majeure par rapport à d’autres systèmes d’exploitation, notamment Windows, est qu’aucun utilisateur n’a par défaut les droits complets d’administrateur (root). Il n’est donc pas possible nativement d’accéder au parties sensibles du système, ou bien de lancer certains types de programme. Par exemple, si l’on essaie de lister les fichiers du dossier /root, on obtient une erreur.\n\n\nterminal\n\nls /root\n\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines opérations telles que l’installation de binaires ou de packages nécessitent cependant des droits administrateurs. Dans ce cas, il est d’usage d’utiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de l’exécution de la commande.\n\n\nterminal\n\nsudo ls /root\n\nLe dossier /root étant vide, la commande ls renvoie une chaîne de caractères vide, mais nous n’avons plus de problème de permission. Notons qu’une bonne pratique de sécurité, en particulier dans les scripts shell que l’on peut être amenés à écrire ou exécuter, est de limiter l’utilisation de cette commande aux cas où elle s’avère nécessaire.\nUne autre subtilité concerne justement l’exécution de scripts shell. Par défaut, qu’il soit créé par l’utilisateur ou téléchargé d’internet, un script n’est pas exécutable.\n\n\nterminal\n\n1touch test.sh\n2./test.sh\n\n\n1\n\nCréer le script test.sh (vide)\n\n2\n\nExécuter le script test.sh\n\n\nbash: ./test.sh: Permission denied\nC’est bien entendu une mesure de sécurité pour éviter l’exécution automatique de scripts potentiellement malveillants. Pour pouvoir exécuter un tel script, il faut attribuer des droits d’exécution au fichier avec la commande chmod. Il devient alors possible d’exécuter le script classiquement.\n\n\nterminal\n\n1chmod +x test.sh\n2./test.sh\n\n# Le script étant vide, il ne se passe rien\n\n\n1\n\nDonner des droits d’exécution au script test.sh\n\n2\n\nExécuter le script test.sh"
  },
  {
    "objectID": "chapters/linux101.html#les-scripts-shell",
    "href": "chapters/linux101.html#les-scripts-shell",
    "title": "Linux 101",
    "section": "",
    "text": "Maintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell précédemment évoqués. A l’instar d’un script Python, un script shell permet d’automatiser une série de commandes lancées dans un terminal. Le but de ce tutoriel n’est pas de savoir écrire des scripts shell complexes, travail généralement dévolu aux les data engineers ou les sysadmin (administrateurs système), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces compétences sont essentielles lorsque l’on se préoccupe de mise en production. A titre d’exemple, comme nous le verrons dans le chapitre sur la portabilité, il est fréquent d’utiliser un script shell comme entrypoint d’une image docker, afin de spécifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement à l’aide d’un script simple. Considérons les commandes suivantes, que l’on met dans un fichier monscript.sh dans le répertoire courant.\n\n\nterminal\n\n#!/bin/bash\nSECTION=$1\nCHAPTER=$2\nFORMATION_DIR=/home/coder/work/formation\nmkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\ntouch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\n\nAnalysons la structure de ce script :\n\nla première ligne est classique, elle se nomme le shebang : elle indique au système quel interpréteur utiliser pour exécuter ce script. Dans notre cas, et de manière générale, on utilise bash (Bourne-Again SHell, l’implémentation moderne du shell) ;\nles lignes 2 et 3 assignent à des variables les arguments passés au script dans la commande. Par défaut, ceux-ci sont assignés à des variables n où n est la position de l’argument, en commençant à 1 ;\nla ligne 4 assigne un chemin à une variable\nla ligne 5 crée le chemin complet, défini à partir des variables créées précédemment. Le paramètre -p est important : il précise à mkdir d’agir de manière récursive, c’est à dire de créer les dossiers intermédiaires qui n’existent pas encore ;\nla ligne 6 crée un fichier texte vide dans le dossier créé avec la commande précédente.\n\nExécutons maintenant ce script, en prenant soin de lui donner les permission adéquates au préalable.\n\n\nterminal\n\nchmod +x monscript.sh\nbash monscript.sh section2 chapitre3\nls formation/section1/chapitre2/\n\ntext.txt\nOpération réussie : le dossier a bien été créé et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash très bien réalisée."
  },
  {
    "objectID": "chapters/linux101.html#gestionnaire-de-paquets",
    "href": "chapters/linux101.html#gestionnaire-de-paquets",
    "title": "Linux 101",
    "section": "",
    "text": "Une différence fondamentale entre Linux et Windows tient à la manière dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier exécutable en .exe) sur le site du logiciel, et on l’exécute. En Linux, on passe généralement par un gestionnaire de packages qui va chercher les logiciels sur un répertoire centralisé, à la manière de pip en Python par exemple.\nPourquoi cette différence ? Une raison importante est que, contrairement à Windows, il existe une multitude de distributions différentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent différemment et peuvent avoir différentes versions. En utilisant le package manager (gestionnaire de paquets) propre à la distribution en question, on s’assure de télécharger le logiciel adapté à sa distribution. Dans ce cours, on fait le choix d’utiliser une distribution Debian et son gestionnaire de paquets associé apt. Debian est en effet un choix populaire pour les servers de part sa stabilité et sa simplicité, et sera également familière aux utilisateurs d’Ubuntu, distribution très populaire pour les ordinateurs personnels et qui est basée sur Debian.\nL’utilisation d’apt est très simple. La seule difficulté est de savoir le nom du paquet que l’on souhaite installer, ce qui nécessite en général d’utiliser un moteur de recherche. L’installation de paquets est également un cas où il faut utiliser sudo, puisque cela implique souvent l’accès à des répertoires protégés.\n\n\nterminal\n\nsudo apt install tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nDésinstaller un package est également simple : c’est l’opération inverse. Par sécurité, le terminal vous demande si vous êtes sûr de votre choix en vous demandant de tapper la lettre y (yes) ou la lettre n. On peut passer automatiquement cette étape en ajoutant le paramètre -y\n\n\nterminal\n\nsudo apt remove -y tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant d’installer un package, il est toujours préférable de mettre à jour la base des packages, pour s’assurer qu’on obtiendra bien la dernière version.\n\n\nterminal\n\nsudo apt update\n\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date."
  },
  {
    "objectID": "chapters/linux101.html#tricks",
    "href": "chapters/linux101.html#tricks",
    "title": "Linux 101",
    "section": "",
    "text": "On l’a dit et redit : devenir à l’aise avec le terminal Linux est essentiel et demande de la pratique. Il existe néanmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa première est l’autocomplétion. Dès lors que vous écrivez une commande contenant un nom d’exécutable, un chemin sur le filesystem, ou autre, n’hésitez pas à utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majorité des cas, cela va vous faire gagner un temps précieux.\nLa deuxième est de parcourir l’historique de commandes : la flèche vers le haut (↑) permet de parcourir l’historique des commandes que vous avez précédemment exécutées. Chaque fois que vous appuyez sur cette touche, le terminal affiche la dernière commande exécutée, en remontant dans l’historique à chaque appui supplémentaire.\nLa troisième, dans le même esprit que la deuxième mais plus élaborée, est la recherche inverse dans l’historique de commandes avec les touches Ctrl+R. Lorsque vous appuyez sur Ctrl+R, une invite de recherche apparaît. Vous pouvez alors commencer à taper des caractères de la commande que vous recherchez. Le terminal cherchera dans l’historique des commandes la dernière commande correspondant à ce que vous avez tapé, et la montrera à l’écran. Si ce n’est pas la commande exacte que vous cherchez, vous pouvez continuer à taper pour affiner la recherche ou appuyer à nouveau sur Ctrl+R pour rechercher la commande précédente correspondant à vos critères.\nUne quatrième astuce, qui n’en est pas vraiment une, est de lire la documentation d’une commande lorsque l’on n’est pas sûr de sa syntaxe ou des paramètres admissibles. Via le terminal, la documentation d’une commande peut être affichée en exécutant man suivie de la commande en question, par exemple : man cp. Comme il n’est pas toujours très pratique de lire de longs textes dans un petit terminal, on peut également chercher la documentation d’une commande sur le site man7."
  },
  {
    "objectID": "chapters/git.html",
    "href": "chapters/git.html",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran."
  },
  {
    "objectID": "chapters/git.html#pourquoi-faire",
    "href": "chapters/git.html#pourquoi-faire",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi faire ?",
    "text": "Pourquoi faire ?\nLe développement rapide de la data science au cours de ces dernières années s’est accompagnée d’une complexification substantielle des projets. Par ailleurs, les projets sont de plus en plus collaboratifs, que ce soit dans le cadre d’équipes dans un contexte professionnel ou bien pour des contributions à des projets open-source. Naturellement, ces évolutions doivent nous amener à modifier nos manières de travailler pour gérer cette complexité croissante et continuer à produire de la valeur à partir des projets de data science.\nPourtant, tout data scientist s’est parfois demandé :\n\nquelle était la bonne version d’un programme\nqui était l’auteur d’un bout de code en particulier\nsi un changement était important ou juste un essai\ncomment fusionner des programmes\netc.\n\nEt il n’est pas rare de perdre le fil des versions de son projet lorsque l’on garde trace de celles-ci de façon manuelle.\nExemple de contrôle de version fait “à la main”\n\nPourtant, il existe un outil informatique puissant afin de répondre à tous ces besoins : la gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer l’historique des modifications d’un ensemble de fichiers\nrevenir à des versions précédentes d’un ou plusieurs fichiers\nrechercher les modifications qui ont pu créer des erreurs\ntravailler simultanément sur un même fichier sans risque de perte\npartager ses modifications et récupérer celles des autres\nproposer des modifications, les discuter, sans pour autant modifier la dernière version existante\nidentifier les auteurs et la date des modifications\n\nEn outre, ces outils fonctionnent avec tous les langages informatiques car ils reposent sur la comparaison des lignes et des caractères des programmes, indépendamment du langage. En bref, c’est la bonne manière pour partager des codes et travailler à plusieurs sur un projet de data science. En réalité, il ne serait pas exagéré de dire que l’utilisation du contrôle de version est la bonne pratique la plus fondamentale de tout projet faisant intervenir du code, et qu’elle conditionne largement toutes les autres."
  },
  {
    "objectID": "chapters/git.html#pourquoi-git",
    "href": "chapters/git.html#pourquoi-git",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi Git  ?",
    "text": "Pourquoi Git  ?\nPlusieurs logiciels de contrôle de version existent sur le marché. En principe, le logiciel Git, développé initialement pour fournir une solution décentralisée et open-source dans le cadre du développement du noyau Linux, est devenu largement hégémonique. Aussi, toutes les application de ce cours s’effectueront à l’aide du logiciel Git."
  },
  {
    "objectID": "chapters/git.html#pourquoi-github",
    "href": "chapters/git.html#pourquoi-github",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi GitHub  ?",
    "text": "Pourquoi GitHub  ?\nTravailler de manière collaborative avec Git implique de synchroniser son répertoire local avec une copie distante, située sur un serveur hébergeant des projets Git. Ce serveur peut être un serveur interne à une organisation, ou bien être fourni par un hébergeur externe. Les deux alternatives les plus populaires en la matière sont GitHub et GitLab. Dans ce cours, nous utiliserons GitHub, qui est devenu au fil des années la référence pour l’hébergement des projets open-source. En pratique, les deux services sont relativement semblables, et tous les concepts présentés se retrouvent sous une forme similaire sur les deux plateformes."
  },
  {
    "objectID": "chapters/git.html#principes-et-commandes-usuelles",
    "href": "chapters/git.html#principes-et-commandes-usuelles",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Principes et commandes usuelles",
    "text": "Principes et commandes usuelles\nLe graphique suivant illustre les principes fondamentaux de Git.\nGit tout-en-un (Source) \nLorsqu’on utilise Git, il est important de bien distinguer ce qui se passe en local (sur son poste, sur le serveur sur lequel on travaille…) de ce qui se passe en remote, i.e. en intéragissant avec un serveur distant. Comme le montre le graphique, l’essentiel du contrôle de version se passe en réalité en local.\nEn théorie, sur un projet individuel, il est même possible de réaliser l’ensemble du contrôle de version en mode hors-ligne. Pour cela, il suffit d’indiquer à Git le projet (dossier) que l’on souhaite versionner en utilisant la commande git init. Cette commande a pour effet de créer un dossier .git à la racine du projet, dans lequel Git va stocker tout l’historique du projet (commits, branches, etc.) et permettre de naviguer entre les versions. A cause du . qui préfixe son nom, ce dossier est généralement caché par défaut, ce qui n’est pas problématique dans la mesure où il n’y a jamais besoin de le parcourir ou de le modifier à la main en pratique. Retenez simplement que c’est la présence de ce dossier .git qui fait qu’un dossier est considéré comme un projet Git, et donc que vous pouvez utilisez les commandes usuelles de Git dans ce dossier à l’aide d’un terminal : - git status : affiche les modifications du projet par rapport à la version précédente ; - git add chemin_du_fichier : ajoute un fichier nouveau ou modifié à la zone de staging de Git en vue d’un commit ; - git add -A : ajoute tous les fichiers nouveaux ou modifiés à la zone de staging ; - git commit -m \"message de commit\" : crée un commit, i.e. une photographie des modifications (ajouts, modifications, suppressions) apportées au projet depuis la dernière version, et lui assigne un message décrivant ces changements. Les commits sont l’unité de base de l’historique du projet construit par Git.\nEn pratique, travailler uniquement en local n’est pas très intéressant. Pour pouvoir travailler de manière collaborative, on va vouloir synchroniser les différentes copies locales du projet à un répertoire centralisé, qui maintient de fait la “source de vérité” (single source of truth). Même sur un projet individuel, il fait sens de synchroniser son répertoire local à une copie distante pour assurer l’intégrité du code de son projet en cas de problème matériel.\nEn général, on va donc initialiser le projet dans l’autre sens : - créer un nouveau projet sur GitHub - générer un jeton d’accès (personal access token) - cloner le projet en local via la méthode HTTPS : git clone https://github.com/&lt;username&gt;/&lt;project_name&gt;.git\nLe projet cloné est un projet Git — il contient le dossier .git — synchronisé par défaut avec le répertoire distant. On peut le vérifier avec la commande remote de Git :\n\n\nterminal\n\ngit remote -v\n\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (fetch)\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (push)\nLe projet local est bien lié au répertoire distant sur GitHub, auquel Git donne par défaut le nom origin. Ce lien permet d’utiliser les commandes de synchronisation usuelles : - git pull : récupérer les changements (fetch) sur le remote et les appliquer au projet local - git push : envoyer les changements locaux sur le remote"
  },
  {
    "objectID": "chapters/git.html#implémentations",
    "href": "chapters/git.html#implémentations",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Implémentations",
    "text": "Implémentations\nGit est un logiciel, qui peut être téléchargé sur le site officiel pour différents systèmes d’exploitation. Il existe cependant différentes manières d’utiliser Git : - le client en ligne de commande : c’est l’implémentation standard, et donc la plus complète. C’est celle qu’on utilisera dans ce cours. Le client Git est installé par défaut sur les différents services du SSP Cloud (VSCode, RStudio, Jupyter, etc.) et peut donc être utilisé via n’importe quel terminal. La documentation du SSP Cloud détaille la procédure ; - des interfaces graphiques : elles facilitent la prise en main de Git via des guides visuels, mais ne permettent pas de réaliser toutes les opérations permises par Git - l’interface native de RStudio pour les utilisateurs de R : très complète et stable. La formation au travail collaboratif avec Git et RStudio présente son utilisation de manière détaillée ; - le plugin Jupyter-git pour les utilisateurs de Python : elle implémente les principales features de Git, mais s’avère assez instable à l’usage."
  },
  {
    "objectID": "chapters/git.html#bonnes-pratiques",
    "href": "chapters/git.html#bonnes-pratiques",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\nLe contrôle de version est une bonne pratique de développement en soi… mais son utilisation admet elle même des bonnes pratiques qui, lorsqu’elles sont appliquées, permettent d’en tirer le plus grand profit.\n\nQue versionne-t-on ?\n\nUniquement des fichiers texte\nA chaque commit, Git calcule les différences entre les versions successives du projet, afin de ne pas avoir à stocker une image complète de ce dernier à chaque fois. C’est ce qui permet aux projets Git de rester très légers par défaut, et donc aux différentes opérations impliquant le remote (clone, push, pull..) d’être très rapides.\nLa contrepartie de cette légèreté est une contrainte sur les types d’objets que l’on doit versionner. Les différences sont calculables uniquement sur des fichiers de type texte : codes source, fichiers texte, fichiers de configuration non-sensibles… Voici donc une liste non-exhaustive des extensions de fichier que l’on retrouve fréquemment dans un dépôt Git d’un projet R ou Python : .py, .R, .Rmd, .txt, .json, .xml, .yaml, .toml, et bien d’autres.\nEn revanche tous les fichiers binaires — pour faire simple, tous les fichiers qui ne peuvent pas être ouverts dans un éditeur de texte basique sans produire une suite inintelligible de caractères — n’ont généralement pas destination à se retrouver sur un dépôt Git. Du fait de leur formatage (binaire), Git ne peut pas calculer les différences entre versions pour ces fichiers et c’est donc le fichier entier qui est sauvegardé dans l’historique à chaque changement, ce qui peut très rapidement faire croître la taille du dépôt. Pour éviter de versionner ces fichiers par erreur, on va les ajouter au fichier .gitignore (cf. supra).\n\n\nPas de données\nComme expliqué en introduction, le fil rouge de ce cours sur les bonnes pratiques est l’importance de bien séparer code, données et environnement d’exécution afin de favoriser la reproductibilité des projets de data science. Ce principe doit s’appliquer également à l’usage du contrôle de version, et ce pour différentes raisons.\nA priori, inclure ces données dans un dépôt Git peut sembler une bonne idée en termes de reproductibilité. En machine learning par exemple, on est souvent amené à réaliser de nombreuses expérimentations à partir d’un même modèle appliqué à différentes transformations des données initiales, transformations que l’on pourrait versionner. En pratique, il est généralement préférable de versionner le code qui permet de générer ces transformations et donc les expérimentations associées, dans la mesure où le suivi des versions des datasets peut s’avérer rapidement complexe. Pour de plus gros projets, des alternatives spécifiques existent : c’est le champ du MLOps, domaine en constante expansion qui vise à rendre les pipelines de machine learning plus reproductibles.\nEnfin, la structure même de Git n’est techniquement pas faite pour le stockage de données. Si des petits datasets dans un format texte ne poseront pas de problème, des données volumineuses (à partir de plusieurs Mo) vont faire croître la taille du dépôt et donc ralentir significativement les opérations de synchronisation avec le remote.\n\n\nPas d’informations locales\nLà encore en vertu du principe de séparation données / code/ environnement, les données locales, i.e. spécifiques à l’environnement de travail sur lequel le code a été exécuté, n’ont pas vocation à être versionnées. Par exemple, des fichiers de configuration spécifiques à un poste de travail, des chemins d’accès spécifiques à un ordinateur donné, etc. Cela demande une plus grande rigueur lors de la construction du projet, mais garantit par là même une meilleure reproductiblité pour les futurs utilisateurs du projet.\n\n\nPas d’outputs\nLes outputs d’un projet (graphiques, publications, modèle entraîné…) n’ont pas vocation à être versionné, en vertu des différents arguments présentés ci-dessus : - il ne s’agit généralement pas de fichiers de type texte ; - le code source du projet doit dans tous les cas permettre des les regénérer à l’identique.\n\n\nUtiliser un .gitignore\nOn a listé précédemment un large éventail de fichiers qui n’ont, par nature, pas vocation à être versionné. Bien entendu, faire attention à ne pas ajouter ces différents fichiers au moment de chaque git add serait assez pénible. Git simplifie largement cette procédure en nous donnant la possibilité de remplir un fichier .gitignore, situé à la racine du projet, qui spécifie l’ensemble des fichiers et types de fichiers que l’on ne souhaite pas versionner dans le cadre du projet courant.\nDe manière générale, il y a pour chaque langage des fichiers que l’on ne souhaitera jamais versionner. Pour en tenir compte, une première bonne pratique est de choisir le .gitignore associé au langage du projet lors de la création du dépôt sur GitHub. Ce faisant, le projet est initialité avec un gitignore déjà existant et pré-rempli de chemins et de types de fichiers qui ne sont pas à versionner. Regardons un extrait du gitignore Python pour comprendre sa structure et son fonctionnement.\npip-log.txt\n__pycache__/\n*.log\nChaque ligne du gitignore spécifie un élément à ignorer du contrôle de version, élément qui peut être un ficher/dossier ou bien une règle concernant un ensemble de fichiers/dossiers. Sauf si spécifié explicitement, les chemins sont relatifs à la racine du projet. L’extrait du gitignore Python illustre les différentes possibilités :\n\nligne 1 : ignore le fichier pip-log.txt ;\nligne 2 : ignore le dossier __pycache__/ ;\nligne 3 : ignore tous les fichiers dont l’extension est .log.\n\nDe nombreuses autres possiblités existent, et sont détaillées par exemple dans la documentation de Git.\n\n\n\nMessages des commits\nLe commit est l’unité de temps de Git, et donc fondamentalement ce qui permet de remonter dans l’historique d’un projet. Afin de pouvoir bénéficier à plein de cet avantage de Git, il est capital d’accompagner ses commits de messages pertinents, en se plaçant dans la perspective que l’on peut être amené plusieurs semaines ou mois plus tard à vouloir retrouver du code dans l’historique de son projet. Les quelques secondes prises à chaque commit pour réfléchir à une description pertinente du bloc de modifications que l’on apporte au projet peuvent donc faire gagner un temps précieux à la longue.\nDe nombreuses conventions existent pour rédiger des messages de commit pertinents. Nous rappelons ici les plus importantes :\n\ncontenu : le message doit détailler le pourquoi plutôt que le comment des modifications. Par exemple, plutôt que “Ajoute le fichier test.py”, on préférera écrire “Ajout d’une série de tests unitaires” ;\nstyle : le message doit être à l’impératif et former une phrase (sans point à la fin) ;\nlongueur : le message du commit doit être court (&lt; 72 caractères). S’il n’est pas possible de trouver un message de cette taille qui résume le commit, c’est généralement un signe que le commit regroupe trop de changements (cf. point suivant). Le fait de devoir mettre des + ou des & / et dans un message de commit pour séparer les changements est également un bon indicateur d’un commit trop gros.\n\n\n\nFréquence des commits\nDe manière générale, il est conseillé de réaliser des commits réguliers lorsque l’on travaille sur un projet. Une règle simple que l’on peut par exemple appliquer est la suivante : dès lors qu’un ensemble de modifications forment un tout cohérent et peuvent être résumées par un message simple, il est temps d’en faire un commit. Cette approche a de nombreux avantages :\n\nsi l’on fait suivre chaque commit d’un push — ce qui est conseillé en pratique — on s’assure de disposer régulièreemnt d’une copie de ses travaux, ce qui limite le risque de perte ;\nil est plus facile de revenir en arrière en cas d’erreur si les commits portent sur des changements ciblés et cohérents ;\nle processus de review d’une pull request est facilité, car les différents blocs de modification sont plus clairement séparés ;\ndans une approche d’intégration continue — concept que l’on verra en détail dans le chapitre sur la mise en production — faire des commits et des PR régulièrement permet de déployer de manière continue les changements en production, et donc d’obtenir les feedbacks des utilisateurs et d’adapter plus rapidement si besoin."
  },
  {
    "objectID": "chapters/git.html#branches",
    "href": "chapters/git.html#branches",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Branches",
    "text": "Branches\n\nConcept\nLa possibilité de créer des branches est l’une des fonctionnalités majeures de Git. La création d’une branche au sein d’un projet permet de diverger de la ligne principale de développement (généralement appelée master, terme tendant à disparaître au profit de celui de main) sans impacter cette ligne. Cela permet de séparer le nouveau développement et de faire cohabiter plusieurs versions, pouvant évoluer séparément et pouvant être facilement rassemblées si nécessaire.\nPour comprendre comment fonctionnent les branches, il nous faut revenir un peu plus en détail sur la manière dont Git stocke l’historique du projet. Comme nous l’avons vu, l’unité temporelle de Git est le commit, qui correspond à une photographie à un instant donné de l’état du projet (snapshot). Chaque commit est uniquement identifié par un hash, une longue suite de caractères. La commande git log, qui liste les différents commits d’un projet, permet d’afficher ce hash ainsi que diverses métadonnées (auteur, date, message) associées au commit.\n\n\nterminal\n\ngit log\n\ncommit e58b004d3b68bdf28093fe6ad6036b5d13216e55 (HEAD -&gt; master, origin/master, origin/HEAD)\nAuthor: Lino Galiana &lt;xxx@xxx.fr&gt;\nDate:   Tue Mar 22 14:34:04 2022 +0100\n\n    ajoute code équivalent python\n\n...\nUne branche est simplement un pointeur vers un commit. Dans l’exemple précédent, on a imprimé les informations du dernier commit en date. La branche principale (master) pointe vers ce commit. Si l’on faisait un nouveau commit, le pointeur se décalerait et la branche master pointerait à présent sur le nouveau commit.\n\n\nBranches locales\nDans ce contexte, créer une nouvelle branche (en local) revient simplement à créer un nouveau pointeur vers un commit donné. Supposons que l’on crée une branche testing à partir du dernier commit.\n\n\nterminal\n\n1git branch testing\n2git branch\n\n\n1\n\nCrée une nouvelle branche\n\n2\n\nListe les branches existantes\n\n\n1* master\n2  testing\n\n1\n\nLa branche sur laquelle on se situe\n\n2\n\nLa nouvelle branche créée\n\n\nLa figure suivante illustre l’effet de cette création sur l’historique Git.\n\nDésormais, deux branches (master et testing) pointent vers le même commit. Si l’on effectue à présent des commits sur la branche testing, on va diverger de la branche principale, ce qui permet de développer une nouvelle fonctionnalité sans risquer d’impacter master.\nPour savoir sur quelle branche on se situe à instant donné — et donc sur quelle branche on va commiter — Git utilise un pointeur spécial, appelé HEAD, qui pointe vers la branche courante. On comprend à présent mieux la signification de HEAD -&gt; master dans l’output de la commande git log vu précédemment. Cet élément spécifie la situation locale actuelle et signifie : on se situe actuellement sur la branche master, qui pointe sur le commit e58b004. Pour changer de branche, i.e. déplacer le HEAD, on utilise la commande git checkout. Par exemple, pour passer de la branche master sur laquelle on est par défaut à la branche testing :\n\n\nterminal\n\ngit checkout testing  # Changement de branche\n\nSwitched to branch 'testing'\nOn se situe désormais sur la branche testing, sur laquelle on peut laisser libre cours à sa créativité sans risquer d’impacer la branche principale du projet. Mais que se passe-t-il si, pendant que l’on développe sur testing, un autre membre du projet commit sur master ? On dit que les historiques ont divergé. La figure suivante illustre à quoi ressemble à présent l’historique du projet (et suppose que l’on est repassé sur master).\n\nCette divergence n’est pas problématique en soi : il est normal que les différentes parties et expérimentations d’un projet avancent à différents rythmes. La difficulté est de savoir comment réconcillier les différents changements si l’on décide que la branche testing doit être intégrée dans master. Deux situations peuvent survenir : - les modifications opérées en parallèle sur les deux branches ne concernent pas les mêmes fichiers ou les mêmes parties des fichiers. Dans ce cas, Git est capable de fusionner (merge) les changements automatiquement et tout se passe sans encombre ; - dans le cas contraire, survient un merge conflict : les branches ont divergé de telle sorte qu’il n’est pas possible pour Git de fusionner les changements automatiquement. Il faut alors résoudre les conflits manuellement.\nLa résolution des conflits est une étape souvent douloureuse lors de l’apprentissage de Git. Aussi, nous conseillons dans la mesure du possible de ne pas fusionner des branches manuellement en local avec Git — c’est d’ailleurs pour cette raison que nous n’avons pas détaillé les commandes pour le faire. Dans les sections suivantes, nous verrons comment une bonne organisation préalable du travail en équipe, combinée aux outils collaboratifs fournis par GitHub, permet de rendre le processus de fusion des branches largement indolore.\n\n\nBranches remote\nRappellons que toutes les opérations que nous avons effectuées sur les branches dans cette section se sont passés en local, le répertoire distant est resté totalement inchangé. Pour pouvoir collaborer sur une branche ou bien en faire une pull request (cf. supra), il faut pousser la branche sur le répertoire distant. La commande est simple : git push origin &lt;branche&gt;.\n\n\nterminal\n\ngit push origin testing\n\nCounting objects: 24, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (15/15), done.\nWriting objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done.\nTotal 24 (delta 2), reused 0 (delta 0)\nTo https://github.com/linogaliana/ensae-reproductibilite-website\n * [new branch]      testing -&gt; testing"
  },
  {
    "objectID": "chapters/git.html#workflow-collaboratif",
    "href": "chapters/git.html#workflow-collaboratif",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Workflow collaboratif",
    "text": "Workflow collaboratif\nComme on l’a vu précédemment, si le modèle des branches de Git semble idéal pour gérer le travail collaboratif et asynchrone, il peut également s’avérer rapidement complexe à manipuler en l’absence d’une bonne organisation du travail en équipe. De nombreux modèles (“workflows”) existent en la matière, avec des complexités plus ou moins grandes selon la nature du projet. Nous conseillons d’adopter dans la plupart des cas un modèle très simple : le GitHub Flow.\nLe GitHub Flow est une méthode d’organisation minimaliste du travail collaboratif, qui est progressivement devenue la norme dans les projets open-source. Elle est résumée par la figure suivante, dont nous détaillons par la suite les différentes étapes.\n\n\nDéfinition des rôles des contributeurs\nDans tout projet collaboratif, une première étape essentielle est de bien délimiter les rôles des différents contributeurs. Les différents participants au projet ont en effet généralement des rôles différents dans l’organisation, des niveaux différents de pratique de Git, etc. Il est important de refléter ces différents rôles dans l’organisation du travail collaboratif.\nSur les différents hébergeurs de projets Git, cela prend la forme de rôles que l’on attribue aux différents membres du porjet. Les mainteneurs sont les seuls à pouvoir écrire directement sur master. Les contributeurs sont quant à eux tenus de développer sur des branches. Cela permet de protéger la branche principale, qui doit rester une version propre et jamais compromise du projet.\nNotons que la possibilité de donner des rôles sur les projets GitHub n’est possible que dans le cadre d’organisations (payantes), donc dans un contexte professionnel ou de projets open-source d’une certaine ampleur. Pour des petits projets, il est nécessaire de s’astreindre à une certaine rigueur individuelle pour respecter cette organisation.\n\n\nDéveloppement sur des branches de court-terme\nLes contributeurs développent uniquement sur des branches. Il est d’usage de créer une branche par fonctionnalité, en lui donnant un nom reflétant la fonctionnalité en cours de développement (ex : ajout-tests-unitaires). Les différents contributeurs à la fonctionnalité en cours de développement font des commits sur la branche, en prenant bien soin de pull régulièrement les éventuels changements pour ne pas risquer de conflits de version. Pour la même raison, il est préférable de faire des branches dites de court-terme, c’est à dire propres à une petite fonctionnalité, quite à diviser une fonctionnalité en séries d’implémentations. Cela permet de limiter les éventuels conflits à gérer lors de la fusion finale de la branche avec master.\n\n\nPull Request\nUne fois la série de modifications terminée, vient le temps de rassembler les différents travaux, par l’intermédiaire de la fusion entre la branche et master. Il faut alors “demander” de fusionner (pull request) sur GitHub. Cela ouvre une page liée à la pull request, qui rappelle les différents changements apportés et leurs auteurs, et permet d’entamer une discussion à propos de ces changements.\n\n\nProcessus de review\nLes différents membres du projet peuvent donc analyser et commenter les changements, poser des questions, suggérer des modifications, apporter d’autres contributions, etc. Il est par exemple possible de mentionner un membre de l’équipe par l’intermédiaire de @personne. Il est également possible de procéder à une code review, par exemple par un développeur plus expérimenté.\n\n\nRésolution des éventuels conflits\nEn adoptant cette manière de travailler, master ne sera modifiée que via des pull requests. Il ne devrait donc jamais y avoir le moindre conflit à régler sur master, les seuls conflits possibles se passent sur les branches. Par exemple, dans le cas où une autre pull request aurait été fusionnée sur master depuis l’ouverture de la pull request en question.\nDans le cas d’un conflit à gérer, le conflit doit être résolu dans la branche et pas dans master. Voici la marche à suivre :\n\nappliquez le conseil de survie : faites une copie de sauvegarde de votre clone\ndans votre clone, placez vous sur la branche en question : git checkout nom-de-la-branche\nfusionnez master dans la branche : git merge master\nrésolvez les éventuels conflits dans les fichiers concernés\nfinalisez le commit de merge et poussez-le sur la branche remote, ce qui le fera apparaître dans la pull request\n\n\n\nFusion de la branche\nSi tout est en ordre, la branche peut être fusionnée. Seuls les mainteneurs, qui ont les droits sur master, peuvent fusionner la pull request. En termes d’historique du projet, deux choix sont possibles : - “Create a merge commit” : tous les commits réalisés sur la branche apparaîtront dans l’historique du projet ; - “Squash and merge” : les différents commits réalisés sur la branche seront rassemblés en un commit unique. Cette option est généralement préférable lorsqu’on utilise des branches de court-terme : elles permettent de garder l’historique plus lisible."
  },
  {
    "objectID": "chapters/git.html#utiliser-les-issues",
    "href": "chapters/git.html#utiliser-les-issues",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Utiliser les issues",
    "text": "Utiliser les issues\nLa manière la plus simple de contribuer à un projet open-source est d’ouvrir une issue. Sur GitHub, cela se fait sur la page du projet, sous l’onglet Issue (cf. documentation officielle). Les issues peuvent avoir différentes nature : - suggestion d’amélioration (sans code) - notification de bug - rapports d’expérience - etc.\nLes issues sont une manière très peu couteuse de contributer à un projet, mais leur importance est capitale, dans la mesure où il est impossible pour les développeurs d’un projet de penser en amont à toutes les utilisations possibles et donc tous les bugs possibles d’une application."
  },
  {
    "objectID": "chapters/git.html#proposer-une-pull-request",
    "href": "chapters/git.html#proposer-une-pull-request",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Proposer une pull request",
    "text": "Proposer une pull request\nUne autre manière, plus ambitieuse, de contribuer à l’open source est de proposer des pull requests. Concrètement, l’idée est de proposer une amélioration ou bien de résoudre un bug sous forme de code, que les mainteneurs du projet peuvent ensuite décider d’intégrer au code existant.\nLa procédure pour proposer une pull request à un projet sur lequel on n’a aucun droit est très similaire à celle décrite ci-dessus dans le cas normal. La principale différence est que, du fait de l’absence de droits, il est impossible de pousser une branche locale sur le répertoire du projet. On va donc devoir créer au préalable un fork, i.e. une copie du projet que l’on crée dans son espace personnel sur GitHub. C’est sur cette copie que l’on va appliquer la procédure décrite précédemment, en prenant bien soin de travailler sur une branche et non sur master. Une fois les modifications pertinentes effectuées sur la branche du fork, GitHub propose de créer une pull request sur le dépôt original. Cette pull request sera alors visible des mainteneurs du projet, qui pourront l’évaluer et décider d’adopter (ou non) les changements proposés."
  },
  {
    "objectID": "chapters/git.html#respecter-les-règles-de-contribution",
    "href": "chapters/git.html#respecter-les-règles-de-contribution",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Respecter les règles de contribution",
    "text": "Respecter les règles de contribution\nVouloir contribuer à un projet open-source est très louable, mais ne peut pas pour autant se faire n’importe comment. Un projet est constitué de personnes, qui ont développé ensemble une manière de travailler, des standards de bonnes pratiques, etc. Pour s’assurer que sa contribution ne reste pas lettre morte, il est indispensable de s’imprégner un minimum de la culture du projet.\nPour faciliter les contributions, les projets open-source spécifient bien souvent la manière dont les utilisateurs peuvent contribuer ainsi que le format attendu. En général, ces règles de contribution sont spécifiées dans un fichier CONTRIBUTING.md situé à la racine du projet GitHub, ou a défaut dans le README du projet. Il est essentiel de bien lire ce document s’il existe afin de s’assurer de proposer des contributions pertinentes."
  },
  {
    "objectID": "chapters/evaluation.html",
    "href": "chapters/evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "L’objectif général de l’évaluation de ce cours est de mettre en pratique les notions étudiées (bonnes pratiques de développement et mise en production) de manière appliquée et réaliste, i.e. à travers un projet basé sur une problématique “métier” et des données réelles. Pour cela, l’évaluation sera en deux parties :\n\nPar groupe de 3 : un projet à choisir parmi les 3 parcours (MLOps, app interactive / dashboard, publication reproductible + site web). Idéalement, on choisira un projet réel, effectué par exemple dans le cadre d’un cours précédent et qui génère un output propice à une mise en production.\nSeul : effectuer une revue de code d’un autre projet. Compétence essentielle et souvent attendue d’un data scientist, la revue de code sera l’occasion de bien intégrer les bonnes pratiques de développement (cf. checklist ci-dessous) et de faire un retour bienveillant sur un autre projet que celui de son groupe.\n\n\n\n\n\n\n\nAvertissement\n\n\n\nCe projet doit mobiliser des données publiquement accessibles. La récupération et structuration de ces données peut faire partie des enjeux du projet mais celles-ci ne doivent pas provenir d’un projet antérieur de votre scolarité pour lequel le partage de données n’est pas possible."
  },
  {
    "objectID": "chapters/evaluation.html#modalités",
    "href": "chapters/evaluation.html#modalités",
    "title": "Evaluation",
    "section": "",
    "text": "L’objectif général de l’évaluation de ce cours est de mettre en pratique les notions étudiées (bonnes pratiques de développement et mise en production) de manière appliquée et réaliste, i.e. à travers un projet basé sur une problématique “métier” et des données réelles. Pour cela, l’évaluation sera en deux parties :\n\nPar groupe de 3 : un projet à choisir parmi les 3 parcours (MLOps, app interactive / dashboard, publication reproductible + site web). Idéalement, on choisira un projet réel, effectué par exemple dans le cadre d’un cours précédent et qui génère un output propice à une mise en production.\nSeul : effectuer une revue de code d’un autre projet. Compétence essentielle et souvent attendue d’un data scientist, la revue de code sera l’occasion de bien intégrer les bonnes pratiques de développement (cf. checklist ci-dessous) et de faire un retour bienveillant sur un autre projet que celui de son groupe.\n\n\n\n\n\n\n\nAvertissement\n\n\n\nCe projet doit mobiliser des données publiquement accessibles. La récupération et structuration de ces données peut faire partie des enjeux du projet mais celles-ci ne doivent pas provenir d’un projet antérieur de votre scolarité pour lequel le partage de données n’est pas possible."
  },
  {
    "objectID": "chapters/evaluation.html#checklist-des-bonnes-pratiques-de-développement",
    "href": "chapters/evaluation.html#checklist-des-bonnes-pratiques-de-développement",
    "title": "Evaluation",
    "section": "Checklist des bonnes pratiques de développement",
    "text": "Checklist des bonnes pratiques de développement\nLes bonnes pratiques de développement ci-dessous sont les indispensables de ce cours. Elles doivent être à la fois appliquées dans les projets de groupe, et à la base de la revue de code individuelle.\n\nUtilisation de Git\n\nPrésence d’un fichier .gitignore adapté au langage et avec des règles additionnelles pour respecter les bonnes pratiques de versioning\nTravail collaboratif : utilisation des branches et des pull requests\n\nPrésence d’un fichier README présentant le projet : contexte, objectif, comment l’utiliser ?\nPrésence d’un fichier LICENSE déclarant la licence (open-source) d’exploitation du projet.\nVersioning des packages : présence d’un fichier requirements.txt ou d’un fichier d’environnement environment.yml pour conda\nQualité du code\n\nRespect des standards communautaires : utiliser un linter et/ou un formatter\nModularité : un script principal qui appelle des modules\n\nStructure des projets\n\nRespect des standards communautaires (cookiecutter)\nModularité du projet selon le modèle évoqué dans le cours:\n\nCode sur GitHub\nDonnées sur S3\nFichiers de configuration (secrets, etc.) à part\n\n\n\n\n\n\nProposition de modularité du projet illustrée pour un projet mixte MLOps et dashboard"
  },
  {
    "objectID": "chapters/evaluation.html#projets",
    "href": "chapters/evaluation.html#projets",
    "title": "Evaluation",
    "section": "Projets",
    "text": "Projets\nVoici trois “parcours” possibles afin de mettre en application les concepts et techniques du cours dans le cadre de projets appliqués. Des projets qui sortiraient de ces parcours-types sont tout à fait possibles et appréciés, il suffit d’en discuter avec les auteurs du cours.\n\nParcours MLOps\n\n\n\n\n\n\nObjectif\n\n\n\nA partir d’un projet existant ou d’un projet type contest Kaggle, développer un modèle de ML répondant à une problématique métier, puis la déployer sur une infrastructure de production conformément aux principes du MLOps.\n\n\nÉtapes :\n\nRespecter la checklist des bonnes pratiques de développement ;\nDévelopper un modèle de ML qui répond à un besoin métier ;\nEntraîner le modèle via validation croisée, avec une procédure de fine-tuning des hyperparamètres ;\nFormaliser le processus de fine-tuning de manière reproductible via MLFlow ;\nConstruire une API avec Fastapi pour exposer le meilleur modèle ;\nCréer une image Docker pour mettre à disposition l’API ;\nDéployer l’API sur le SSP Cloud ;\nIndustrialiser le déploiement en mode GitOps avec ArgoCD\nGérer le monitoring de l’application : logs, dashboard de suivi des performances, etc.\n\n\n\nParcours dashboard / application interactive\n\n\n\n\n\n\nObjectif\n\n\n\nA partir d’un projet existant ou d’un projet que vous construirez, développer une application interactive ou un dashboard statique répondant à une problématique métier, puis déployer sur une infrastructure de production.\n\n\nÉtapes :\n\nRespecter la checklist des bonnes pratiques de développement\nDévelopper une application interactive Streamlit ou un dashboard statique avec Quarto répondant à une problématique métier\nCréer une image Docker permettant d’exposer l’application en local\nDéployer l’application sur le SSP Cloud (application interactive) ou sur Github Pages (site statique)\nCustomiser le thème, le CSS etc. pour mettre en valeur au maximum les résultats de la publication et les messages principaux\nAutomatiser l’ingestion des données en entrée pour que le site web se mette à jour régulièrement\nIndustrialiser le déploiement en mode GitOps avec ArgoCD\nGérer le monitoring de l’application : logs, métriques de suivi des performances, etc.\n\n\n\nParcours big data\n\n\n\n\n\n\nObjectif\n\n\n\nL’objectif de ce parcours est de construire un pipeline type ETL (Extract/Transform/Load) prenant en entrée une source de données massives afin de les mettre à disposition dans un système de base de données optimisé pour l’analyse. Ce parcours est intéressant pour les étudiant.e.s souhaitant un projet avec une coloration data engineering plus marquée.\n\n\nÉtapes :\n\nRespecter la checklist des bonnes pratiques de développement\nExtract : identifier une ou plusieurs sources de données massives ouvertes (idées : 1, 2), et réaliser l’ingestion de ces données sur le service de stockage S3 du SSP Cloud (documentation)\nTransform : en utilisant une technologie big data adopté à la volumétrie des données en entrée (données massives : Spark, données volumineuses : Arrow / DuckDB, toutes disponibles sur le SSP Cloud), effectuer des opérations sur les données brutes (filtrages, agrégations, etc.) afin d’en extraire des sous-ensembles de données pertinents pour répondre à une problématique métier\nLoad : charger les tables construites à l’étape précédente dans un système de base de données relationnelle (ex : PostgreSQL, disponible dans le catalogue du SSP Cloud)\nIntégrer l’ensemble des étapes dans un pipeline de données avec un orchestrateur de traitements (ex : Argo Workflows, disponible dans le catalogue du SSP Cloud) afin d’automatiser leur exécution\nConstruire un dashboard minimaliste (par exemple, avec Superset, disponible dans le catalogue du SSP Cloud) afin de valoriser les données produites\n\n\n\nParcours publication reproductible\n\n\n\n\n\n\nObjectif\n\n\n\nA partir d’un projet existant ou d’un projet que vous construirez, rédiger un rapport reproductible à partir de données afin de répondre à une problématique métier, puis le mettre à disposition à travers un site web automatiquement généré et publié.\n\n\nÉtapes :\n\nRespecter la checklist des bonnes pratiques de développement\nRédiger un rapport reproductible avec Quarto qui fasse intervenir des données, du code, de la visualisation de données, du texte, etc.\nExposer le rapport sous la forme d’un site web via GitHub Actions\nCustomiser le thème, le CSS etc. pour mettre en valeur au maximum les résultats de la publication et les messages principaux\nAutomatiser l’ingestion des données en entrée pour que le site web se mette à jour régulièrement\nMettre en place des tests automatisés de vérification des standards de qualité du code (linter), de détection de fautes d’orthographes/de grammaire, etc.\nGénérer des slides au format quarto-revealjs afin de présenter les principaux résultats de la publication, et les exposer comme une page du site"
  },
  {
    "objectID": "chapters/evaluation.html#revue-de-code",
    "href": "chapters/evaluation.html#revue-de-code",
    "title": "Evaluation",
    "section": "Revue de code",
    "text": "Revue de code\nSur le projet d’un groupe différent du sien (attribué aléatoirement au cours du semestre) :\n\nouvrir une pull request de revue de code via un fork (cf. chapitre sur Git pour la procédure)\ndonner une appréciation générale de la conformité du projet à la checklist des bonnes pratiques de développement\nsuggérer des pistes d’amélioration du projet\n\nChaque groupe, ayant reçu des revues de code de son projet, pourra prendre en compte ces pistes d’améliorations dans la mesure du temps disponible, par le biais d’une autre pull request qui devra référencer celle de la revue de code. Cette dernière partie ne sera cependant pas strictement attendue, elle sera valorisée en bonus dans la notation finale."
  },
  {
    "objectID": "chapters/code-quality.html",
    "href": "chapters/code-quality.html",
    "title": "Code quality",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran.\nThis chapter introduces the topic of code quality,\nthe first level in the hierarchy of best practices. It outlines\nwhy code quality matters, general principles for improving it,\nand a few simple tools or practices to enhance code quality.\nThese are explored further in the running example."
  },
  {
    "objectID": "chapters/code-quality.html#why-readable-and-maintainable-code-matters",
    "href": "chapters/code-quality.html#why-readable-and-maintainable-code-matters",
    "title": "Code quality",
    "section": "Why Readable and Maintainable Code Matters",
    "text": "Why Readable and Maintainable Code Matters\n\n“The code is read much more often than it is written.”\nGuido Van Rossum1\n\nWhen getting started with data science, it’s natural to think of code in a purely functional way: “I want to complete a given task—say, run a classification algorithm—so I’ll piece together some code, often found online, in a notebook until the task is done.” The project’s structure doesn’t matter much, as long as it loads the necessary data.\nWhile this minimalist and flexible mindset works well during the learning phase, it’s essential to move past it as you progress—especially if you’re building professional or collaborative projects. Otherwise, you’ll likely end up with code that’s hard to maintain or improve—and that will eventually be abandoned.\nIt’s important to choose, among many ways to solve a problem, a solution that can be understood by others who speak the same programming language. Code is read far more than it’s written—it’s primarily a communication tool. Moreover, maintaining code typically takes more effort than writing it in the first place. That’s why thinking ahead about code quality and project structure is critical to long-term maintainability.\nTo improve communication and reduce the pain of working with unclear code, developers have attempted—sometimes informally, sometimes through institutions—to define conventions. These depend on the language but are based on principles that apply universally to code-based projects."
  },
  {
    "objectID": "chapters/code-quality.html#why-follow-conventions",
    "href": "chapters/code-quality.html#why-follow-conventions",
    "title": "Code quality",
    "section": "Why Follow Conventions?",
    "text": "Why Follow Conventions?\nPython is a very readable language. With a bit of care—naming things well, managing dependencies, and structuring code properly—you can often understand a script without running it. This is one of Python’s biggest strengths, enabling fast learning and easy understanding of other people’s code.\nThe Python community has developed a set of widely accepted standards, called PEPs (Python Enhancement Proposals), that serve as the foundation of the ecosystem. The two most well-known are:\n\nPEP8, which defines code style conventions;\nPEP257, which outlines conventions for documentation (docstrings).\n\nThese conventions go beyond syntax. Several standards for project organization have also emerged, which we’ll explore in the next chapter.\n\n\n\n\n\n\nComparison with \n\n\n\n\n\nIn the  ecosystem, formalization has been less structured. The language is more permissive than Python in some ways2. Still, some style standards have emerged, including:\n\nthe tidyverse style guide,\nthe Google R style guide,\nand the MLR style guide…\n\nFor further learning in :\n\nThe Insee training on best practices with Git and , which aligns closely with this course;\nAdditional guidance in the collaborative utilitR documentation;\nThis blog post that links to various resources on the subject.\n\n\n\n\nThese conventions are somewhat arbitrary—it’s natural to prefer some styles over others.\nThey’re also not set in stone. Languages and practices evolve, which means conventions must adapt. Still, adopting the recommended habits—when possible—will make your code easier for the community to understand, increase your chances of getting help, and make future maintenance easier.\nThere are many coding style philosophies, but the most important principle is consistency: If you choose a convention—say, snake_case (my_function_name) over camelCase (myFunctionName)—then stick with it."
  },
  {
    "objectID": "chapters/code-quality.html#a-good-ide-the-first-step-toward-quality",
    "href": "chapters/code-quality.html#a-good-ide-the-first-step-toward-quality",
    "title": "Code quality",
    "section": "A Good IDE: The First Step Toward Quality",
    "text": "A Good IDE: The First Step Toward Quality\nWithout automated code formatting tools, adopting best practices would be time-consuming and difficult to implement daily. Tools that provide diagnostics or automatically format code are incredibly useful. They allow developers to meet minimum quality standards almost instantly, saving time throughout a data science project. These tools are a prerequisite for production deployment, which we’ll discuss later.\nThe first step toward best practices is choosing a suitable development environment. VSCode is an excellent IDE, as we’ll explore in the practical section. It offers autocompletion, built-in diagnostics (unlike Jupyter), and a wide array of extensions to expand functionality:\n\n\n\nExample of diagnostics and actions in VSCode\n\n\nHowever, IDE-level tools are not enough. They require manual interaction, which can be time-consuming and difficult to apply consistently. Fortunately, we also have automated tools for diagnostics and formatting.\n\nAutomated Tools for Code Diagnostics and Formatting\nSince Python is the primary tool of thousands of data scientists, many tools have been developed to reduce the time needed to create a functional project. These tools boost productivity, reduce repetitive tasks, and improve project quality through diagnostics or even automatic fixes.\nThere are two main types of tools:\n\nLinters: programs that check whether code formally adheres to a given guidestyle\n\nThey report issues but do not fix them\n\nFormatters: programs that automatically rewrite code to follow a specific guidestyle\n\nThey modify the code directly\n\n\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\nIssues that a linter can catch:\n\nlong or poorly indented lines, unbalanced parentheses, poorly named functions…\n\nIssues that a linter typically won’t catch:\n\nincorrect function usage, mis-specified arguments, incoherent structure, insufficient documentation…\n\n\n\n\n\n\n\nLinters to Identify Bad Coding Habits\nLinters assess code quality and its potential to trigger explicit or silent errors.\nExamples of issues linters can catch include:\n\nusage of undefined variables (errors)\nunused variables (unnecessary code)\npoor code organization (risk of bugs)\nviolations of code style guidelines\nsyntax errors (e.g., typos)\n\nMost development tools offer built-in diagnostics (and sometimes suggestions). You may need to enable them in the settings, as they’re often disabled by default to avoid overwhelming beginners.\nHowever, if you don’t fix issues as you go, the backlog of changes can become overwhelming.\nIn Python, the two most common linters are PyLint and Flake8. In this course, we’ll use PyLint for its practicality and pedagogy. It can be run from the command line as follows:\npip install pylint\npylint myscript.py   # for a single file\npylint src           # for all files in the 'src' folder\n\n\n\n\n\n\nTip\n\n\n\n\n\nOne of the nice features of PyLint is that it gives a score, which is quite informative. We’ll use this in the running project to track how each step improves code quality.\nYou can also set up pre-commit hooks to block commits that don’t meet a minimum score.\n\n\n\n\n\nFormatters for Bulk Code Cleanup\nA formatter rewrites code directly—like a spellchecker, but for style. It can make substantial changes to improve readability.\nThe most widely used formatter is Black. More recently, Ruff—a hybrid linter/formatter—has gained popularity. It builds on Black while integrating diagnostics from other packages.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf your project uses Black, you can add a badge to the README on GitHub:\n\n\n\n\nIt’s quite instructive to review code after formatting—it helps identify and correct stylistic habits. You’ll likely notice some rules that contradict your current habits. Try applying these new rules incrementally. Once they become second nature, revisit the guide and tackle the next set of improvements. This step-by-step approach helps raise code quality without getting bogged down in micro-details that distract from the bigger project goals."
  },
  {
    "objectID": "chapters/code-quality.html#sharing-a-path-to-better-code-quality",
    "href": "chapters/code-quality.html#sharing-a-path-to-better-code-quality",
    "title": "Code quality",
    "section": "Sharing: A Path to Better Code Quality",
    "text": "Sharing: A Path to Better Code Quality\n\nOpen Source as a Quality Driver\nBy sharing your code on open-source platforms (see Git chapter), you may receive suggestions or even contributions from other users. But the benefits of openness go further. Public code tends to be better written, better documented, and more thoughtfully structured—often because authors want to avoid public embarrassment! Even without external feedback, sharing code encourages higher quality.\n\n\nCode Review\nCode review borrows from academic peer review to improve the quality of Python code. In a review, one or more developers read and evaluate code written by someone else to identify errors and suggest improvements.\nBenefits include:\n\ncatching bugs before they escalate\nensuring consistent style and structure\nenforcing best practices\nidentifying code that could be refactored for clarity or maintainability\n\nIt’s also a great way to share knowledge: senior developers can help junior ones grow by reviewing their work.\nPlatforms like GitHub  and GitLab  offer convenient code review features: inline discussions, suggestions, etc."
  },
  {
    "objectID": "chapters/code-quality.html#objectives",
    "href": "chapters/code-quality.html#objectives",
    "title": "Code quality",
    "section": "Objectives",
    "text": "Objectives\n\nEncourage conciseness to reduce the risk of error and make the process clearer;\nImprove readability, which is essential to make the process understandable by others but also for yourself when revisiting an old script;\nReduce redundancy, which simplifies code (the don’t repeat yourself paradigm);\nMinimize the risk of errors due to copy/paste."
  },
  {
    "objectID": "chapters/code-quality.html#advantages-of-functions",
    "href": "chapters/code-quality.html#advantages-of-functions",
    "title": "Code quality",
    "section": "Advantages of Functions",
    "text": "Advantages of Functions\nFunctions have many advantages over long scripts:\n\nLimit the risk of errors caused by copy/paste;\nMake the code more readable and compact;\nOnly one place to modify the code if the processing changes;\nFacilitate code reuse and documentation!\n\n\n\n\n\n\n\nGolden Rule\n\n\n\nYou should use a function whenever a piece of code is used more than twice (don’t repeat yourself (DRY)).\n\n\n\n\n\n\n\n\nRules for Writing Effective Functions\n\n\n\n\nOne task = one function;\nA complex task = a sequence of functions, each performing a simple task;\nLimit the use of global variables.\n\n\n\nRegarding package installation, as we will see in the Project Structure and Portability sections, this should not be managed inside the script, but in a separate element related to the project’s execution environment3. Those sections also provide practical advice on handling API or database tokens, which should never be written in the code.\nOverly long scripts are not a best practice. It is better to divide all scripts executing a production chain into “monads”, i.e., small coherent units. Functions are a key tool for this purpose (they help reduce redundancy and are a preferred tool for documenting code).\n\n\n\n\n\n\nExample: Prefer List Comprehensions\n\n\n\n\n\nIn Python, it is recommended to prefer list comprehensions over indented for loops. The latter are generally less efficient and involve a larger number of code lines, whereas list comprehensions are much more concise:\nliste_nombres = range(10)\n\n# very bad\ny = []\nfor x in liste_nombres:\n    if x % 2 == 0:\n        y.append(x*x)\n\n# better\ny = [x*x for x in liste_nombres if x % 2 == 0]\n\n\n\n\nProgramming Advice\nIn the Python programming world, there are two main paradigms:\n\nFunctional programming: builds code by chaining functions, i.e., more or less standardized operations;\nObject-Oriented Programming (OOP): builds code by defining objects of a given class with attributes (intrinsic features) and custom methods to perform class-specific operations.\n\n\n\nExample comparing the two paradigms\n\nThanks ChatGPT for the example:\n\nclass AverageCalculator:\n    def __init__(self, numbers):\n        self.numbers = numbers\n\n    def calculate_average(self):\n        return sum(self.numbers) / len(self.numbers)\n\n# Usage\ncalculator = AverageCalculator([1, 2, 3, 4, 5])\nprint(\"Average (OOP):\", calculator.calculate_average())\n\ndef calculate_average(numbers):\n    return sum(numbers) / len(numbers)\n\n# Usage\nnumbers = [1, 2, 3, 4, 5]\nprint(\"Average (FP):\", calculate_average(numbers))\n\nAverage (OOP): 3.0\nAverage (FP): 3.0\n\n\n\nFunctional programming is more intuitive than OOP and often allows for quicker code development. OOP is a more formalist approach, useful when functions need to adapt to the input object type (e.g., loading different model weights depending on the model type in Pytorch). It avoids 🍝 spaghetti code that’s hard to debug.\nHowever, one should remain pragmatic. OOP can be more complex to implement than functional programming. In many cases, well-written functional code is sufficient. For large projects, adopting a defensive programming approach is helpful — a precautionary strategy in the functional paradigm that anticipates and manages unexpected situations (e.g., wrong argument type or structure).\n\n\n\n\n\n\nSpaghetti Code\n\n\n\n“Spaghetti code” refers to programming style that leads to tangled code due to excessive use of conditions, exceptions, and complex event handling. It becomes almost impossible to trace the cause of errors without stepping through every line of code — and there are many, due to poor practices.\nSpaghetti code prevents determining who, what, and how something happens, making updates time-consuming since one must follow the chain of references line by line.\n\n\n\n\n\n\n\n\nA Progressive Example\n\n\n\n\n\n💡 Suppose we have a dataset that uses −99 to represent missing values. We want to replace all −99 with NA.\nnp.random.seed(1234)\na = np.random.randint(1, 10, size = (5,6))\ndf = np.insert(\n    a,\n    np.random.choice(len(a), size=6),\n    -99,\n)\ndf = pd.DataFrame(df.reshape((6,6)), columns=[chr(x) for x in range(97, 103)])\nFirst attempt:\ndf2 = df.copy()\ndf2.loc[df2['a'] == -99,'a'] = np.nan\ndf2.loc[df2['b'] == -99,'b'] = np.nan\ndf2.loc[df2['c'] == -99,'c'] = np.nan\ndf2.loc[df2['d'] == -99,'d'] = np.nan\ndf2.loc[df2['e'] == -98,'e'] = np.nan\ndf2.loc[df2['f'] == -99,'e'] = np.nan\nWhat’s wrong here?\n\n\nHint 💡 Look at columns e and g.\n\nTwo copy-paste errors: - -98 instead of -99; - 'e' instead of 'f' in the last line.\n\nNext improvement — using a function:\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2['a'] = fix_missing(df['a'])\ndf2['b'] = fix_missing(df['b'])\n...\nStill repetitive and error-prone with column names.\nBest version — apply function across all columns:\ndf2 = df.copy()\ndf2 = df2.apply(fix_missing)\nNow the code is: 1. Concise; 2. Robust to data structure changes; 3. Free from hard-coded mistakes; 4. Easily generalizable — e.g., apply only on selected columns:\ndf2[['a','b','e']] = df2[['a','b','e']].apply(fix_missing)\n\n\n\nResources like the Hitchhiker’s Guide to Python and this blog post illustrate these design principles well.\n\n\n\n\n\n\nThe Zen of Python\n\n\n\n\n\nWritten by Tim Peters in 2004, this set of aphorisms embodies Python’s design philosophy:\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\n...\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "chapters/code-quality.html#footnotes",
    "href": "chapters/code-quality.html#footnotes",
    "title": "Code quality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGuido Van Rossum is the creator of , which makes him a voice worth listening to.↩︎\nFor example, in , you can use &lt;- or = for assignment, and the language won’t complain about poor indentation…↩︎\nWe will present the two main approaches in Python, their similarities and differences: virtual environments (managed by a requirements.txt file) and conda environments (managed by an environment.yml file).↩︎"
  },
  {
    "objectID": "chapters/application.html",
    "href": "chapters/application.html",
    "title": "Application",
    "section": "",
    "text": "L’objectif de cette mise en application est d’illustrer les différentes étapes qui séparent la phase de développement d’un projet de celle de la mise en production. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.\nL’objectif pédagogique principal de cette application est d’adopter un point de vue pragmatique en choisissant des outils et des méthodes de travail qui permettent de réaliser des objectifs ambitieux de valorisation de données. Python sera le trait d’union entre les différentes technologies ou infrastructures que nous utiliserons.\nCette application est un tutoriel pas à pas pour avoir un projet reproductible et disponible sous plusieurs livrables. Toutes les étapes ne sont pas indispensables à tous les projets de data science et il existe des outils alternatifs à ceux présentés. Néanmoins, les outils présentés ont l’avantage d’être très bien intégrés à Python, bien configurés si vous utilisez le SSPCloud comme nous le recommandons, tout en étant agnostiques sur le reste des outils que vous utilisez ; de sorte à ne pas être bloquants si on remplace l’une des briques logicielles par une autre.\nNous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d’un projet de data science. On a un notebook un peu monolithique, qui réalise les étapes classiques d’un pipeline de machine learning :\n\nImport de données ;\nStatistiques descriptives et visualisations ;\nFeature engineering ;\nEntraînement d’un modèle ;\nEvaluation du modèle.\n\n\n\nL’objectif est d’améliorer le projet de manière incrémentale jusqu’à pouvoir le mettre en production, en le valorisant sous une forme adaptée et en adoptant une méthode de travail fluidifiant les évolutions futures.\nLa Figure 1 montre que notre point de départ initial, à savoir un notebook, mélange tout. Ceci rend très complexe la mise à jour de notre modèle ou l’exploitation de notre modèle sur de nouvelles données, ce qui est pourtant la raison d’être du machine learning qui est pensé pour l’extrapolation. Si on vous demande de valoriser votre modèle sur de nouvelles données, vous risquez de devoir refaire tourner tout votre notebook, avec le risque de ne pas retrouver les mêmes résultats que dans la version précédente.\nLa Figure 2 illustre l’horizon auquel nous aboutirons à la fin de cette application. Nous désynchronisons les étapes d’entraînement et de prédiction, en identifiant mieux les pré-requis de chacune et en adoptant des briques technologiques adaptées à celles-ci. Les noms présents sur cette figure sont encore obscurs, c’est normal, mais ils vous deviendrons familiers si vous adoptez une infrastructure et une méthode de travail à l’état de l’art.\n\n\n\n\n\n\nFigure 1: Illustration de notre point de départ\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration de l’horizon vers lequel on se dirige\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIl est important de bien lire les consignes et d’y aller progressivement. Certaines étapes peuvent être rapides, d’autres plus fastidieuses ; certaines être assez guidées, d’autres vous laisser plus de liberté. Si vous n’effectuez pas une étape, vous risquez de ne pas pouvoir passer à l’étape suivante qui en dépend.\nBien que l’exercice soit applicable sur toute configuration bien faite, nous recommandons de privilégier l’utilisation du SSP Cloud, où tous les outils nécessaires sont pré-installés et pré-configurés. Le service VSCode ne sera en effet que le point d’entrée pour l’utilisation d’outils plus exigeants sur le plan de l’infrastructure: Argo, MLFLow, etc.\n\n\n\n\n\nA l’heure actuelle, cette application se concentre sur la mise en oeuvre fiable de l’entraînement de modèles de machine learning. Comme vous pouvez le voir, quand on part d’aussi loin qu’un projet monolithique dans un notebook, c’est un travail conséquent d’en arriver à un pipeline pensé pour la production. Cette application vise à vous sensibiliser au fait qu’avoir la Figure 2 en tête et adopter une organisation de travail et faire des choix techniques adéquats, vous fera économiser des dizaines voire centaines d’heures lorsque votre modèle aura vocation à passer en production.\nA l’heure actuelle, cette application ne se concentre que sur une partie du cycle de vie d’un projet data ; il y a déjà fort à faire. Nous nous concentrons sur l’entraînement et la mise à disposition d’un modèle à des fins opérationnelles. C’est la première partie du cycle de vie d’un modèle. Dans une approche MLOps, il faut également penser la maintenance de ce modèle et les enjeux que représentent l’arrivée continue de nouvelles données, ou le besoin d’en collecter de nouvelles à travers des annotations, sur la qualité prédictive d’un modèle. Toute entreprise qui ne pense pas cet après est vouée à se faire doubler par un nouveau venu. Une prochaine version de cette application permettra certainement d’illustrer certains des enjeux afférants à la vie en production d’un modèle (supervision, annotations…) sur notre cas d’usage.\nIl convient aussi de noter que nous ne faisons que parcourir la surface des sujets que nous évoquons. Ce cours, déjà dense, deviendrait indigeste si nous devions présenter chaque outil dans le détail. Nous laissons donc les curieux approfondir chacun des outils que nous présentons pour découvrir comment en tirer le maximum (et si vous avez l’impression que nous oublions des éléments cruciaux, les issues et pull requests  sont bienvenues).\n\n\n\nPour simplifier la reprise en cours de ce fil rouge, nous proposons un système de checkpoints qui s’appuient sur des tags Git. Ces tags figent le projet tel qu’il est à l’issue d’un exercice donné.\nSi vous faites évoluer votre projet de manière expérimentale mais désirez tout de même utiliser à un moment ces checkpoints, il va falloir faire quelques acrobaties Git. Pour cela, nous mettons à disposition un script qui permet de sauvegarder votre avancée dans un tag donné (au cas où, à un moment, vous vouliez revenir dessus) et écraser la branche main avec le tag en question. Par exemple, si vous désirez reprendre après l’exercice 9, vous devrez faire tourner le code dans cette boite :\n  \n    \n      \n        \n      \n      \n        Checkpoint d'exemple      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli92\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli9\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCelui-ci sauvegarde votre avancée dans un tag nommé dev_before_appli9, le pousse sur votre dépôt Github  puis force votre branche à adopter l’état du tag appli9."
  },
  {
    "objectID": "chapters/application.html#objectif",
    "href": "chapters/application.html#objectif",
    "title": "Application",
    "section": "",
    "text": "L’objectif est d’améliorer le projet de manière incrémentale jusqu’à pouvoir le mettre en production, en le valorisant sous une forme adaptée et en adoptant une méthode de travail fluidifiant les évolutions futures.\nLa Figure 1 montre que notre point de départ initial, à savoir un notebook, mélange tout. Ceci rend très complexe la mise à jour de notre modèle ou l’exploitation de notre modèle sur de nouvelles données, ce qui est pourtant la raison d’être du machine learning qui est pensé pour l’extrapolation. Si on vous demande de valoriser votre modèle sur de nouvelles données, vous risquez de devoir refaire tourner tout votre notebook, avec le risque de ne pas retrouver les mêmes résultats que dans la version précédente.\nLa Figure 2 illustre l’horizon auquel nous aboutirons à la fin de cette application. Nous désynchronisons les étapes d’entraînement et de prédiction, en identifiant mieux les pré-requis de chacune et en adoptant des briques technologiques adaptées à celles-ci. Les noms présents sur cette figure sont encore obscurs, c’est normal, mais ils vous deviendrons familiers si vous adoptez une infrastructure et une méthode de travail à l’état de l’art.\n\n\n\n\n\n\nFigure 1: Illustration de notre point de départ\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration de l’horizon vers lequel on se dirige\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIl est important de bien lire les consignes et d’y aller progressivement. Certaines étapes peuvent être rapides, d’autres plus fastidieuses ; certaines être assez guidées, d’autres vous laisser plus de liberté. Si vous n’effectuez pas une étape, vous risquez de ne pas pouvoir passer à l’étape suivante qui en dépend.\nBien que l’exercice soit applicable sur toute configuration bien faite, nous recommandons de privilégier l’utilisation du SSP Cloud, où tous les outils nécessaires sont pré-installés et pré-configurés. Le service VSCode ne sera en effet que le point d’entrée pour l’utilisation d’outils plus exigeants sur le plan de l’infrastructure: Argo, MLFLow, etc."
  },
  {
    "objectID": "chapters/application.html#ce-que-cette-application-ne-couvre-pas-pour-le-moment",
    "href": "chapters/application.html#ce-que-cette-application-ne-couvre-pas-pour-le-moment",
    "title": "Application",
    "section": "",
    "text": "A l’heure actuelle, cette application se concentre sur la mise en oeuvre fiable de l’entraînement de modèles de machine learning. Comme vous pouvez le voir, quand on part d’aussi loin qu’un projet monolithique dans un notebook, c’est un travail conséquent d’en arriver à un pipeline pensé pour la production. Cette application vise à vous sensibiliser au fait qu’avoir la Figure 2 en tête et adopter une organisation de travail et faire des choix techniques adéquats, vous fera économiser des dizaines voire centaines d’heures lorsque votre modèle aura vocation à passer en production.\nA l’heure actuelle, cette application ne se concentre que sur une partie du cycle de vie d’un projet data ; il y a déjà fort à faire. Nous nous concentrons sur l’entraînement et la mise à disposition d’un modèle à des fins opérationnelles. C’est la première partie du cycle de vie d’un modèle. Dans une approche MLOps, il faut également penser la maintenance de ce modèle et les enjeux que représentent l’arrivée continue de nouvelles données, ou le besoin d’en collecter de nouvelles à travers des annotations, sur la qualité prédictive d’un modèle. Toute entreprise qui ne pense pas cet après est vouée à se faire doubler par un nouveau venu. Une prochaine version de cette application permettra certainement d’illustrer certains des enjeux afférants à la vie en production d’un modèle (supervision, annotations…) sur notre cas d’usage.\nIl convient aussi de noter que nous ne faisons que parcourir la surface des sujets que nous évoquons. Ce cours, déjà dense, deviendrait indigeste si nous devions présenter chaque outil dans le détail. Nous laissons donc les curieux approfondir chacun des outils que nous présentons pour découvrir comment en tirer le maximum (et si vous avez l’impression que nous oublions des éléments cruciaux, les issues et pull requests  sont bienvenues)."
  },
  {
    "objectID": "chapters/application.html#comment-gérer-les-checkpoints",
    "href": "chapters/application.html#comment-gérer-les-checkpoints",
    "title": "Application",
    "section": "",
    "text": "Pour simplifier la reprise en cours de ce fil rouge, nous proposons un système de checkpoints qui s’appuient sur des tags Git. Ces tags figent le projet tel qu’il est à l’issue d’un exercice donné.\nSi vous faites évoluer votre projet de manière expérimentale mais désirez tout de même utiliser à un moment ces checkpoints, il va falloir faire quelques acrobaties Git. Pour cela, nous mettons à disposition un script qui permet de sauvegarder votre avancée dans un tag donné (au cas où, à un moment, vous vouliez revenir dessus) et écraser la branche main avec le tag en question. Par exemple, si vous désirez reprendre après l’exercice 9, vous devrez faire tourner le code dans cette boite :\n  \n    \n      \n        \n      \n      \n        Checkpoint d'exemple      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli92\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli9\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCelui-ci sauvegarde votre avancée dans un tag nommé dev_before_appli9, le pousse sur votre dépôt Github  puis force votre branche à adopter l’état du tag appli9."
  },
  {
    "objectID": "chapters/application.html#étape-1-sassurer-que-le-script-sexécute-correctement",
    "href": "chapters/application.html#étape-1-sassurer-que-le-script-sexécute-correctement",
    "title": "Application",
    "section": "Étape 1 : s’assurer que le script s’exécute correctement",
    "text": "Étape 1 : s’assurer que le script s’exécute correctement\nOn va partir du fichier notebook.py qui reprend le contenu du notebook2 mais dans un script classique. Le travail de nettoyage en sera facilité.\nLa première étape est simple, mais souvent oubliée : vérifier que le code fonctionne correctement. Pour cela, nous recommandons de faire un aller-retour entre le script ouvert dans VSCode et un terminal pour le lancer.\n\n\n\n\n\n\nApplication 1: corriger les erreurs\n\n\n\n\nOuvrir dans VSCode le script titanic.py ;\nExécuter le script en ligne de commande (python titanic.py)3 pour détecter les erreurs ;\nCorriger les deux erreurs qui empêchent la bonne exécution ;\nVérifier le fonctionnement du script en utilisant la ligne de commande:\n\n\n\nterminal\n\npython titanic.py\n\nLe code devrait afficher des sorties.\n\n\nAide sur les erreurs rencontrées\n\nLa première erreur rencontrée est une alerte FileNotFoundError, la seconde est liée à un package.\n\nIl est maintenant temps de commit les changements effectués avec Git4 :\n\n\nterminal\n\ngit add titanic.py\ngit commit -m \"Corrige l'erreur qui empêchait l'exécution\"\ngit push\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli1      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli1\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-2-utiliser-un-linter-puis-un-formatter",
    "href": "chapters/application.html#étape-2-utiliser-un-linter-puis-un-formatter",
    "title": "Application",
    "section": "Étape 2: utiliser un linter puis un formatter",
    "text": "Étape 2: utiliser un linter puis un formatter\nOn va maintenant améliorer la qualité de notre code en appliquant les standards communautaires. Pour cela, on va utiliser le linter classique PyLint et le formatter Black. Si vous désirez un outil deux en un, il est possible d’utiliser Ruff en complément ou substitut.\nCe nettoyage automatique du code permettra, au passage, de restructurer notre script de manière plus naturelle.\n\n\n\n\n\n\nImportant\n\n\n\nPyLint, Black et Ruff sont des packages Python qui s’utilisent principalement en ligne de commande.\nSi vous avez une erreur qui suggère que votre terminal ne connait pas PyLint, Black, ou Ruff, n’oubliez pas d’exécuter la commande pip install pylint, pip install black ou pip install ruff.\n\n\nLe linter PyLint renvoie alors une série d’irrégularités, en précisant à chaque fois la ligne de l’erreur et le message d’erreur associé (ex : mauvaise identation). Il renvoie finalement une note sur 10, qui estime la qualité du code à l’aune des standards communautaires évoqués dans la partie Qualité du code.\n\n\n\n\n\n\nApplication 2: rendre lisible le script\n\n\n\n\nDiagnostiquer et évaluer la qualité de titanic.py avec PyLint. Regarder la note obtenue.\nUtiliser black titanic.py --diff --color pour observer les changements de forme que va induire l’utilisation du formatter Black. Cette étape n’applique pas les modifications, elle ne fait que vous les montrer.\nAppliquer le formatter Black\nRéutiliser PyLint pour diagnostiquer l’amélioration de la qualité du script et le travail qui reste à faire.\nComme la majorité du travail restant est à consacrer aux imports:\n\nMettre tous les imports ensemble en début de script\nRetirer les imports redondants en s’aidant des diagnostics de votre éditeur\nRéordonner les imports si PyLint vous indique de le faire\nCorriger les dernières fautes formelles suggérées par PyLint\n\nDélimiter des parties dans votre code pour rendre sa structure plus lisible. Si des parties vous semblent être dans le désordre, vous pouvez réordonner le script (mais n’oubliez pas de le tester)\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli2      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli22\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli2\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nLe code est maintenant lisible, il obtient à ce stade une note formelle proche de 10. Mais il n’est pas encore totalement intelligible ou fiable. Il y a notamment quelques redondances de code auxquelles nous allons nous attaquer par la suite. Néanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script: jetons d’API et chemin des fichiers."
  },
  {
    "objectID": "chapters/application.html#étape-3-gestion-des-paramètres",
    "href": "chapters/application.html#étape-3-gestion-des-paramètres",
    "title": "Application",
    "section": "Étape 3: gestion des paramètres",
    "text": "Étape 3: gestion des paramètres\n  \n    \n      \n        \n      \n      \n        Reprendre à partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration proposée dans l'application préliminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, après avoir clôné le dépôt\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli22\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli2\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nL’exécution du code et les résultats obtenus dépendent de certains paramètres définis dans le code. L’étude de résultats alternatifs, en jouant sur des variantes des (hyper)paramètres, est à ce stade compliquée car il est nécessaire de parcourir le code pour trouver ces paramètres. De plus, certains paramètres personnels comme des jetons d’API ou des mots de passe n’ont pas vocation à être présents dans le code.\nIl est plus judicieux de considérer ces paramètres comme des variables d’entrée du script. Cela peut être fait de deux manières:\n\nAvec des arguments optionnels appelés depuis la ligne de commande (Application 3a). Cela peut être pratique pour mettre en oeuvre des tests automatisés mais n’est pas forcément pertinent pour toutes les variables. Nous allons montrer cet usage avec le nombre d’arbres de notre random forest ;\nEn utilisant un fichier de configuration dont les valeurs sont importées dans le script principal (Application 3b).\n\n\n\nUn exemple de définition d’un argument pour l’utilisation en ligne de commande\n\n\n\nprenom.py\n\nimport argparse\nparser = argparse.ArgumentParser(description=\"Qui êtes-vous?\")\nparser.add_argument(\n    \"--prenom\", type=str, default=\"Toto\", help=\"Un prénom à afficher\"\n)\nargs = parser.parse_args()\nprint(args.prenom)\n\nExemples d’utilisations en ligne de commande\n\n\nterminal\n\npython prenom.py\npython prenom.py --prenom \"Zinedine\"\n\n\n\n\n\n\n\n\nApplication 3a: Paramétrisation du script\n\n\n\n\nEn s’inspirant de l’exemple ci-dessus 👆️, créer une variable n_trees qui peut éventuellement être paramétrée en ligne de commande et dont la valeur par défaut est 20 ;\nTester cette paramétrisation en ligne de commande avec la valeur par défaut puis 2, 10 et 50 arbres.\n\n\n\nL’exercice suivant permet de mettre en application le fait de paramétriser un script en utilisant des variables définies dans un fichier YAML.\n\n\n\n\n\n\nApplication 3b: La configuration dans un fichier dédié\n\n\n\n\nInstaller le package python-dotenv que nous allons utiliser pour charger notre jeton d’API à partir d’une variable d’environnement.\nA partir de l’exemple de la documentation, utiliser la fonction load_dotenv pour charger dans Python nos variables d’environnement à partir d’un fichier (vous pouvez le créer mais ne pas le remplir encore avec les valeurs voulues, ce sera fait ensuite)\nCréer la variable et vérifier la sortie de Python en faisant tourner titanic.py en ligne de commande\n\n\n\ntitanic.py\n\njeton_api = os.environ.get(\"JETON_API\", \"\")\n\nif jeton_api.startswith(\"$\"):\n    print(\"API token has been configured properly\")\nelse:\n    print(\"API token has not been configured\")\n\n\nMaintenant introduire la valeur voulue pour le jeton d’API dans le fichier d’environnement lu par dotenv\nS’il n’existe pas déjà, créer un fichier .gitignore (cf. Chapitre Git). Ajouter dans ce fichier .env car il ne faut pas committer ce fichier. Au passage ajouter __pycache__/ au .gitignore5, cela évitera d’avoir à le faire ultérieurement ;\nCréer un fichier README.md où vous indiquez qu’il faut créer un fichier .env pour pouvoir utiliser l’API.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli3      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli32\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli3\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-4-privilégier-la-programmation-fonctionnelle",
    "href": "chapters/application.html#étape-4-privilégier-la-programmation-fonctionnelle",
    "title": "Application",
    "section": "Étape 4 : Privilégier la programmation fonctionnelle",
    "text": "Étape 4 : Privilégier la programmation fonctionnelle\nNous allons mettre en fonctions les parties importantes de l’analyse. Ceci facilitera l’étape ultérieure de modularisation de notre projet. Comme cela est évoqué dans les éléments magistraux de ce cours, l’utilisation de fonctions va rendre notre code plus concis, plus traçable, mieux documenté.\nCet exercice étant chronophage, il n’est pas obligatoire de le réaliser en entier. L’important est de comprendre la démarche et d’adopter fréquemment une approche fonctionnelle6. Pour obtenir une chaine entièrement fonctionnalisée, vous pouvez reprendre le checkpoint.\nPour commencer, cet exercice fait un petit pas de côté pour faire comprendre la manière dont les pipelines scikit sont un outil au service des bonnes pratiques.\n\n\n\n\n\n\nApplication 4 (optionnelle): pourquoi utiliser un pipeline Scikit ?\n\n\n\n\nLe pipeline Scikit d’estimation et d’évaluation vous a été donné tel quel. Regardez, ci-dessous, le code équivalent sans utiliser de pipeline Scikit:\n\n\n\nLe code équivalent sans pipeline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nimport pandas as pd\nimport numpy as np\n\n# Définition des variables\nnumeric_features = [\"Age\", \"Fare\"]\ncategorical_features = [\"Embarked\", \"Sex\"]\n\n# PREPROCESSING ----------------------------\n\n# Handling missing values for numerical features\nnum_imputer = SimpleImputer(strategy=\"median\")\nX_train[numeric_features] = num_imputer.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = num_imputer.transform(X_test[numeric_features])\n\n# Scaling numerical features\nscaler = MinMaxScaler()\nX_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = scaler.transform(X_test[numeric_features])\n\n# Handling missing values for categorical features\ncat_imputer = SimpleImputer(strategy=\"most_frequent\")\nX_train[categorical_features] = cat_imputer.fit_transform(X_train[categorical_features])\nX_test[categorical_features] = cat_imputer.transform(X_test[categorical_features])\n\n# One-hot encoding categorical features\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nX_train_encoded = encoder.fit_transform(X_train[categorical_features])\nX_test_encoded = encoder.transform(X_test[categorical_features])\n\n# Convert encoded features into a DataFrame\nX_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_features), index=X_train.index)\nX_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_features), index=X_test.index)\n\n# Drop original categorical columns and concatenate encoded ones\nX_train = X_train.drop(columns=categorical_features).join(X_train_encoded)\nX_test = X_test.drop(columns=categorical_features).join(X_test_encoded)\n\n# MODEL TRAINING ----------------------------\n\n# Defining the model\nmodel = RandomForestClassifier(n_estimators=n_trees)\n\n# Fitting the model\nmodel.fit(X_train, y_train)\n\n# EVALUATION ----------------------------\n\n# Scoring\nrdmf_score = model.score(X_test, y_test)\nprint(f\"{rdmf_score:.1%} de bonnes réponses sur les données de test pour validation\")\n\n# Confusion matrix\nprint(20 * \"-\")\nprint(\"matrice de confusion\")\nprint(confusion_matrix(y_test, model.predict(X_test)))\n\n\nVoyez-vous l’intérêt de l’approche par pipeline en termes de lisibilité, évolutivité et fiabilité ?\nCréer un notebook qui servira de brouillon. Y introduire le code suivant:\n\n\n\nLe code à copier-coller dans un notebook\n\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nX_train, y_train = train.drop(\"Survived\", axis=\"columns\"), train[\"Survived\"]\nX_test, y_test = test.drop(\"Survived\", axis=\"columns\"), train[\"Survived\"]\n\nMAX_DEPTH = None\nMAX_FEATURES = \"sqrt\"\nn_trees=20\n\nnumeric_features = [\"Age\", \"Fare\"]\ncategorical_features = [\"Embarked\", \"Sex\"]\n\n# Variables numériques\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", MinMaxScaler()),\n    ]\n)\n\n# Variables catégorielles\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder()),\n    ]\n)\n\n# Preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"Preprocessing numerical\", numeric_transformer, numeric_features),\n        (\n            \"Preprocessing categorical\",\n            categorical_transformer,\n            categorical_features,\n        ),\n    ]\n)\n\n# Pipeline\npipe = Pipeline(\n    [\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", RandomForestClassifier(\n            n_estimators=n_trees,\n            max_depth=MAX_DEPTH,\n            max_features=MAX_FEATURES\n        )),\n    ]\n)\n\npipe.fit(X_train, y_train)\n\n\nAfficher ce pipeline dans une cellule de votre notebook. Cela vous aide-t-il mieux à comprendre les différentes étapes du pipeline de modélisation ?\nComment pouvez-vous accéder aux étapes de preprocessing ?\n\n\n\nComment pouvez-vous faire pour appliquer le pipeline de preprocessing des variables numériques (et uniquement celui-ci) à ce DataFrame ?\n\n\n\nLe DataFrame à créer pour appliquer un bout de notre pipeline\n\nimport numpy as np\n\nnew_data = {\n    \"Age\": [22, np.nan, 35, 28, np.nan],\n    \"Fare\": [7.25, 8.05, np.nan, 13.00, 15.50]\n}\n\nnew_data = pd.DataFrame(new_data)\n\n\n\nNormalement ce code ne devrait pas prendre plus d’une demie-douzaine de lignes. Sans pipeline le code équivalent, beaucoup plus verbeux et moins fiable, ressemble à celui-ci\n\n\n\nLe code équivalent, sans pipeline\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Définition des nouvelles données\nnew_data = pd.DataFrame({\n    \"Age\": [25, np.nan, 40, 33, np.nan],\n    \"Fare\": [10.50, 7.85, np.nan, 22.00, 12.75]\n})\n\n# Définition des transformations (même que dans le pipeline)\nnum_imputer = SimpleImputer(strategy=\"median\")\nscaler = MinMaxScaler()\n\n# Apprentissage des transformations sur X_train (assumant que vous l'avez déjà)\nX_train_numeric = X_train[[\"Age\", \"Fare\"]]  # Supposons que X_train existe\nnum_imputer.fit(X_train_numeric)\nscaler.fit(num_imputer.transform(X_train_numeric))\n\n# Transformation des nouvelles données\nnew_data_imputed = num_imputer.transform(new_data)\nnew_data_scaled = scaler.transform(new_data_imputed)\n\n# Création du DataFrame final\nnew_data_preprocessed = pd.DataFrame(\n    new_data_scaled,\n    columns=[\"Age_scaled\", \"Fare_scaled\"]  # Générer des noms de colonnes adaptés\n)\n\n# Affichage du DataFrame\nprint(new_data_preprocessed)\n\n\nImaginons que vous ayez déjà des données préprocessées:\n\n\n\nCréer des données préprocessées\n\nimport numpy as np\nimport pandas as pd\n\nnew_data = pd.DataFrame({\n    \"Age\": [25, np.nan, 40, 33, np.nan],\n    \"Fare\": [10.50, 7.85, np.nan, 22.00, 12.75],\n    \"Embarked\": [\"S\", \"C\", np.nan, \"Q\", \"S\"],\n    \"Sex\": [\"male\", \"female\", \"male\", np.nan, \"female\"]\n})\nnew_y = np.random.randint(0, 2, size=len(new_data))\n\npreprocessed_data = pd.DataFrame(\n    pipe[:-1].transform(new_data),\n    columns = preprocessor_numeric.get_feature_names_out()\n)\npreprocessed_data\n\n\nDéterminer le score en prédiction sur ces données\n\n\n\n\nMaintenant, revenons à notre chaine de production et appliquons des fonctions pour la rendre plus lisible, plus fiable et plus modulaire.\n\n\n\n\n\n\nApplication 4: adoption des standards de programmation fonctionnelle\n\n\n\nCette application peut être chronophage, vous pouvez aller plus ou moins loin dans la fonctionalisation de votre script en fonction du temps dont vous disposez.\n\nCréer une fonction qui intègre les différentes étapes du pipeline (preprocessing et définition du modèle). Cette fonction prend en paramètre le nombre d’arbres (argument obligatoire) et des arguments optionnels supplémentaires (les colonnes sur lesquelles s’appliquent les différentes étapes du pipeline, max_depth et max_features).\nCréer une fonction d’évaluation renvoyant le score obtenu et la matrice de confusion, à l’issue d’une estimation (mais cette estimation est faite en amont de la fonction, pas au sein de celle-ci)\nDéplacer toutes les fonctions ensemble, en début de script. Si besoin, ajouter des paramètres à votre fichier d’environnement pour créer de nouvelles variables comme les chemins des données.\nEn profiter pour supprimer le code zombie qu’on a gardé jusqu’à présent mais qui ne correspond pas vraiment à des opérations utiles à notre chaine de production\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli4      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli42\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli4\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCela ne se remarque pas encore vraiment car nous avons de nombreuses définitions de fonctions mais notre chaine de production est beaucoup plus concise (le script fait environ 150 lignes dont une centaine issues de définitions de fonctions génériques). Cette auto-discipline facilitera grandement les étapes ultérieures. Cela aurait été néanmoins beaucoup moins coûteux en temps d’adopter ces bons gestes de manière plus précoce."
  },
  {
    "objectID": "chapters/application.html#étape-1-modularisation",
    "href": "chapters/application.html#étape-1-modularisation",
    "title": "Application",
    "section": "Étape 1 : modularisation",
    "text": "Étape 1 : modularisation\nNous allons profiter de la modularisation pour adopter une structure applicative pour notre code. Celui-ci n’étant en effet plus lancé que depuis la ligne de commande, on peut considérer qu’on construit une application générique où un script principal (main.py) encapsule des éléments issus d’autres scripts Python.\n\n\n\n\n\n\nApplication 5: modularisation\n\n\n\n\nDéplacer les fonctions dans une série de fichiers dédiés:\n\nbuild_pipeline.py: script avec la définition du pipeline\ntrain_evaluate.py: script avec les fonctions d’évaluation du projet\n\nSpécifier les dépendances (i.e. les packages à importer) dans les modules pour que ceux-ci puissent s’exécuter indépendamment ;\nRenommer titanic.py en main.py pour suivre la convention de nommage des projets Python ;\nImporter les fonctions nécessaires à partir des modules.\nVérifier que tout fonctionne bien en exécutant le script main à partir de la ligne de commande :\n\n\n\nterminal\n\npython main.py\n\n\nOptionnel: profitez en pour mettre un petit coup de formatter à votre projet, si vous ne l’avez pas fait régulièrement.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli5      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli52\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli5\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-2-adopter-une-architecture-standardisée-de-projet",
    "href": "chapters/application.html#étape-2-adopter-une-architecture-standardisée-de-projet",
    "title": "Application",
    "section": "Étape 2 : adopter une architecture standardisée de projet",
    "text": "Étape 2 : adopter une architecture standardisée de projet\nOn dispose maintenant d’une application Python fonctionnelle. Néanmoins, le projet est certes plus fiable mais sa structuration laisse à désirer et il serait difficile de rentrer à nouveau dans le projet dans quelques temps.\n\n\nEtat actuel du projet 🙈\n\n├── .gitignore\n├── .env\n├── data.csv\n├── train.csv\n├── test.csv\n├── README.md\n├── build_pipeline.py\n├── train_evaluate.py\n├── titanic.ipynb\n└── main.py\n\nComme cela est expliqué dans la partie Structure des projets, on va adopter une structure certes arbitraire mais qui va faciliter l’autodocumentation de notre projet. De plus, une telle structure va faciliter des évolutions optionnelles comme la packagisation du projet. Passer d’une structure modulaire bien faite à un package est quasi-immédiat en Python.\nOn va donc modifier l’architecture de notre projet pour la rendre plus standardisée. Pour cela, on va s’inspirer des structures cookiecutter qui génèrent des templates de projet. En l’occurrence notre source d’inspiration sera le template datascience issu d’un effort communautaire.\n\n\n\n\n\n\nNote\n\n\n\nL’idée de cookiecutter est de proposer des templates que l’on utilise pour initialiser un projet, afin de bâtir à l’avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante :\n\n\nterminal\n\npip install cookiecutter\ncookiecutter https://github.com/drivendata/cookiecutter-data-science\n\nIci, on a déjà un projet, on va donc faire les choses dans l’autre sens : on va s’inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.\n\n\nEn s’inspirant du cookiecutter data science on va adopter la structure suivante:\n\n\nStructure recommandée\n\napplication\n├── main.py\n├── .env\n├── README.md\n├── data\n│   ├── raw\n│   │   └── data.csv\n│   └── derived\n│       ├── test.csv\n│       └── train.csv\n├── notebooks\n│   └── titanic.ipynb\n└── src\n    ├── pipeline\n    │   └── build_pipeline.py\n    └── models\n        └── train_evaluate.py\n\n\n\n\n\n\n\nApplication 6: adopter une structure lisible\n\n\n\n\n(optionnel) Analyser et comprendre la structure de projet proposée par le template ;\nModifier l’arborescence du projet selon le modèle ;\nMettre à jour l’import des dépendances, le fichier de configuration et main.py avec les nouveaux chemins ;\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli6      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli62\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli6\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-3-mieux-tracer-notre-chaine-de-production",
    "href": "chapters/application.html#étape-3-mieux-tracer-notre-chaine-de-production",
    "title": "Application",
    "section": "Étape 3: mieux tracer notre chaine de production",
    "text": "Étape 3: mieux tracer notre chaine de production\n\nIndiquer l’environnement minimal de reproductibilité\nLe script main.py nécessite un certain nombre de packages pour être fonctionnel. Chez vous les packages nécessaires sont bien sûr installés mais êtes-vous assuré que c’est le cas chez la personne qui testera votre code ?\nAfin de favoriser la portabilité du projet, il est d’usage de “fixer l’environnement”, c’est-à-dire d’indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version. Nous proposons de créer un fichier requirements.txt minimal, sur lequel nous reviendrons dans la partie consacrée aux environnements reproductibles.\nLe fichier requirements.txt est conventionnellement localisé à la racine du projet. Ici on ne va pas fixer les versions, on raffinera ce fichier ultérieurement.\n\n\n\n\n\n\nApplication 7a: création du requirements.txt\n\n\n\n\nCréer un fichier requirements.txt avec la liste des packages nécessaires\nAjouter une indication dans README.md sur l’installation des packages grâce au fichier requirements.txt\n\n\n\n\n\nTracer notre chaîne\nQuand votre projet passera en production, vous aurez un accès limité à celui-ci. Il est donc important de faire remonter, par le biais du logging des informations critiques sur votre projet qui vous permettront de savoir où il en est (si vous avez accès à la console où il tourne) ou là où il s’est arrêté.\nL’utilisation de print montre rapidement ses limites pour cela. Les informations enregistrées ne persistent pas après la session et sont quelques peu rudimentaires.\nPour faire du logging, la librairie consacrée depuis longtemps en Python est… logging. Il existe aussi une librairie nommée loguru qui est un peu plus simple à configurer (l’instanciation du logger est plus aisée) et plus agréable grâce à ses messages en couleurs qui permettent de visuellement trier les informations.\n\nL’exercice suivant peut être fait avec les deux librairies, cela ne change pas grand chose. Les prochaines applications repartiront de la version utilisant la librairie standard logging.\n\n\n\n\n\n\nApplication 7b: remontée de messages par logging\n\n\n\n\nVersion utilisant loggingVersion utilisant loguru\n\n\n\nAller sur la documentation de la librairie ici et sur ce tutoriel pour trouver des sources d’inspiration sur la configuration et l’utilisation de logging.\nPour afficher les messages dans la console et dans un fichier de log, s’inspirer de cette réponse sur stack overflow.\nTester en ligne de commande votre code et observer le fichier de log\n\n\n\n\nInstaller loguru et l’ajouter au requirements.txt\nEn s’aidant du README du projet sur Github, remplacer nos print par différents types de messages (info, success, etc.).\nTester l’exécution du script en ligne de commande et observer vos sorties\nMettre à jour le logger pour enregistrer dans un fichier de log. Ajouter celui-ci au .gitignore puis tester en ligne de commande votre script. Ouvrir le fichier en question, refaites tourner le script et regardez son évolutoin.\nIl est possible avec loguru de capturer les erreurs des fonctions grâce au système de cache décrit ici. Introduire une erreur dans une des fonctions (par exemple dans create_pipeline) avec un code du type raise ValueError(\"Problème ici\")\n\n\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli7      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli72\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli7\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#stockageS3",
    "href": "chapters/application.html#stockageS3",
    "title": "Application",
    "section": "Étape 4 : stocker les données de manière externe",
    "text": "Étape 4 : stocker les données de manière externe\nPour cette partie, il faut avoir un service VSCode dont les jetons d’authentification à S3 sont valides. Pour cela, si vous êtes sur le SSPCloud, le plus simple est de recréer un nouveau service avec le bouton suivant\n\net remplir l’onglet Git comme ça votre VSCode sera pré à l’emploi (cf. application 0).\nUne fois que vous avez un VSCode fonctionnel, il est possible de reprendre cette application fil rouge depuis le checkpoint précédent.\n  \n    \n      \n        \n      \n      \n        Reprendre à partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration proposée dans l'application préliminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, après avoir clôné le dépôt\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli72\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli7\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nEnfin, il vous suffira d’ouvrir un terminal et faire pip install -r requirements.txt && python main.py pour pouvoir démarrer l’application.\nL’étape précédente nous a permis d’isoler la configuration. Nous avons conceptuellement isolé les données du code lors des applications précédentes. Cependant, nous n’avons pas été au bout du chemin car le stockage des données reste conjoint à celui du code. Nous allons maintenant dissocier ces deux éléments.\n\n\n\n\n\n\nPour en savoir plus sur le système de stockage S3\n\n\n\n\n\nPour mettre en oeuvre cette étape, il peut être utile de comprendre un peu comme fonctionne le SSP Cloud. Vous devrez suivre la documentation du SSP Cloud pour la réaliser. Une aide-mémoire est également disponible dans le cours de 2e année de l’ENSAE Python pour la data science.\n\n\n\n\n\n\n\n\n\nPour en savoir plus sur le format Parquet\n\n\n\n\n\nL’objectif de cette application est de montrer comment utiliser le format Parquet dans une chaîne production ; un objectif somme toute modeste.\nSi vous voulez aller plus loin dans la découverte du format Parquet, vous pouvez consulter cette ressource R très similaire à ce cours (oui elle est faite par les mêmes auteurs…) et essayer de faire les exercices avec votre librairie Python de prédilection (PyArrow ou DuckDB)\n\n\n\n\n\n\n\n\n\nEt si vous utilisez une infrastructure cloud qui n’est pas le SSPCloud ? (une idée saugrenue mais sait-on jamais)\n\n\n\n\n\nLes exemples à venir peuvent très bien être répliqués sur n’importe quel cloud provider qui propose une solution de type S3, qu’il s’agisse d’un cloud provider privé (AWS, GCP, Azure, etc.) ou d’une réinstanciation ad hoc du projet Onyxia, le logiciel derrière le SSPCloud.\nPour un système de stockage S3, il suffit de changer les paramètres de connexion de s3fs (endpoint, region, etc.). Pour les stockages sur GCP, les codes sont presque équivalents, il suffit de remplacer la librairie s3fs par gcfs; ces deux librairies sont en fait des briques d’un standard plus général de gestion de systèmes de fichiers en Python ffspec.\n\n\n\nLe chapitre sur la structure des projets développe l’idée qu’il est recommandé de converger vers un modèle où environnements d’exécution, de stockage du code et des données sont conceptuellement séparés. Ce haut niveau d’exigence est un gain de temps important lors de la mise en production car au cours de cette dernière, le projet est amené à être exécuté sur une infrastructure informatique dédiée qu’il est bon d’anticiper. Schématiquement, nous visons la structure de projet suivante:\n\nA l’heure actuelle, les données sont stockées dans le dépôt. C’est une mauvaise pratique. En premier lieu, Git n’est techniquement pas bien adapté au stockage de données. Ici ce n’est pas très grave car il ne s’agit pas de données volumineuses et ces dernières ne sont pas modifiées au cours de notre chaine de traitement.\nLa raison principale est que les données traitées par les data scientists sont généralement soumises à des clauses de confidentialités (RGPD, secret statistique…). Mettre ces données sous contrôle de version c’est prendre le risque de les divulguer à un public non habilité. Il est donc recommandé de privilégier des outils techniques adaptés au stockage de données.\nL’idéal, dans notre cas, est d’utiliser une solution de stockage externe. On va utiliser pour cela MinIO, la solution de stockage de type S3 offerte par le SSP Cloud. Cela nous permettra de supprimer les données de Github tout en maintenant la reproductibilité de notre projet 7.\nPlus concrètement, nous allons adopter le pipeline suivant pour notre projet:\n\nLe scénario type est que nous avons une source brute, reçue sous forme de CSV, dont on ne peut changer le format. Il aurait été idéal d’avoir un format plus adapté au traitement de données pour ce fichier mais ce n’était pas de notre ressort. Notre chaine va aller chercher ce fichier, travailler dessus jusqu’à valoriser celui-ci sous la forme de notre matrice de confusion. Si on imagine que notre chaine prend un certain temps, il n’est pas inutile d’écrire des données intermédiaires. Pour faire cela, puisque nous avons la main, autant choisir un format adapté, à savoir le format Parquet.\nCette application va se dérouler en trois temps:\n\nUpload de notre source brute (CSV) sur S3\nIllustration de l’usage des librairies cloud native pour lire celle-ci\nPartage public de cette donnée pour la rendre accessible de manière plus simple à nos futures applications.\n\n\n\n\n\n\n\nApplication 8a: ajout de données sur le système de stockage S3\n\n\n\nPour commencer, à partir de la ligne de commande, utiliser l’utilitaire MinIO pour copier les données data/raw/data.csv vers votre bucket personnel. Les données intermédiaires peuvent être laissées en local mais doivent être ajoutées au .gitignore.\n\n\nIndice\n\nStructure à adopter:\n\n\nterminal\n\nBUCKET_PERSONNEL=\"nom_utilisateur_sspcloud\"\nmc cp data/raw/data.csv s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.csv\n\nen modifiant la variable BUCKET_PERSONNEL, l’emplacement de votre bucket personnel\n\nPour se simplifier la vie, dans les prochaines applications, on va utiliser des URL de téléchargement des fichiers (comme si ceux-ci étaient sur n’importe quel espace de stockage) plutôt que d’utiliser une librairie S3 compatible comme boto3 ou s3fs.\nNéanmoins, il est utile de les utiliser une fois pour comprendre la logique. Pour aller plus loin sur ces librairies, vous pouvez consulter cette page du cours de 2A de Python pour la data science.\nPour commencer, on va lister les fichiers se trouvant dans un bucket. En ligne de commande, sur notre poste local, on ferait ls (cf. Linux 101). Cela ne va pas beaucoup différer avec les librairies cloud native:\n\nAvec s3fsAvec mc\n\n\nDans un notebook, copier-coller ce code, le modifier et exécuter:\nimport s3fs\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n2CHEMIN = \"ensae-reproductibilite/data/raw\"\nfs.ls(f\"s3://{MY_BUCKET}/{CHEMIN}\")\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\n\n\nDans un terminal, copier-coller ligne à ligne ce code, le modifier et exécuter:\nimport s3fs\n\n1MY_BUCKET=\"mon_nom_utilisateur_sspcloud\"\n2CHEMIN = \"ensae-reproductibilite/data/raw\"\nmc ls s3/${MY_BUCKET}/${CHEMIN}\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\n\n\n\n\n\nOn va maintenant lire directement une donnée stockée sur S3. Pour illustrer le fait que cela change peu notre code d’être sur un système cloud avec les librairies adaptées, on va lire directement un fichier CSV stocké sur le SSPCloud, sans passer par un fichier en local8.\n\n\n\n\n\n\nApplication 8b: importer une donnée depuis un système de stockage S3\n\n\n\nPour illustrer la cohérence avec un système de fichier local, voici trois solutions pour lire le fichier que vous venez de mettre sur S3. Attention, il faut avoir des jetons de connexion à S3 à jour. Si vous avez cette erreur\n\nA client error (InvalidAccessKeyId) occurred when calling the ListBuckets operation: The AWS Access Key Id you provided does not exist in our records.\n\nc’est que vos identifiants de connexion ne sont plus à jour (pour des raisons de sécurité, ils sont régulièrement renouvelés). Dans ce cas, recréez un service VSCode avec le bouton proposé plus haut.\nDans un notebook, copier-coller et mettre à jour ces deux variables qui seront utilisées dans différents exemples:\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n2CHEMIN_FICHIER = \"ensae-reproductibilite/data/raw/data.csv\"\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\n\nAvec Pandas et s3fsAvec Pyarrow et s3fsAvec DuckDB\n\n\nimport s3fs\nimport pandas as pd\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\nwith fs.open(f\"s3://{MY_BUCKET}/{CHEMIN_FICHIER}\") as f:\n    df = pd.read_csv(f)\n\ndf\n\n\nimport s3fs\nfrom pyarrow import csv\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\nwith fs.open(f\"s3://{MY_BUCKET}/{CHEMIN_FICHIER}\") as f:\n    df = csv.read_csv(f)\n\ndf\n\n\nimport os\nimport duckdb\n\ncon = duckdb.connect(database=\":memory:\")\n\ncon.execute(\n    f\"\"\"\nCREATE SECRET secret (\n    TYPE S3,\n    KEY_ID '{os.environ[\"AWS_ACCESS_KEY_ID\"]}',\n    SECRET '{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}',\n    ENDPOINT 'minio.lab.sspcloud.fr',\n    SESSION_TOKEN '{os.environ[\"AWS_SESSION_TOKEN\"]}',\n    REGION 'us-east-1',\n    URL_STYLE 'path',\n    SCOPE 's3://{MY_BUCKET}/'\n);\n\"\"\"\n)\n\nquery_definition = f\"SELECT * FROM read_csv('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')\"\ndf = con.sql(query_definition)\n\ndf\n\n\n\nPour illustrer le fonctionnement encore plus simple de S3 avec les fichiers Parquet, on propose de copier un Parquet mis à disposition dans un bucket collectiv vers votre bucket personnel:\n1BUCKET_PERSONNEL=\"nom_utilisateur_sspcloud\"\n\n2curl -o rp.parquet \"https://minio.lab.sspcloud.fr/projet-formation/bonnes-pratiques/data/REGION=11/part-0.parquet\"\n\nmc cp rp.parquet s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/example/rp.parquet\n\nrm rp.parquet\n\n1\n\nRemplacer par le nom de votre bucket.\n\n2\n\nTélécharger le fichier Parquet mis à dispositoin\n\n\nPour lire ceux-ci, tester les exemples de code suivants:\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\nCHEMIN_FICHIER = \"ensae-reproductibilite/data/example/rp.parquet\"\n\n1\n\nRemplacer ici par la valeur appropriée\n\n\n\nAvec Pandas et s3fsAvec Pyarrow et s3fsAvec DuckDB\n\n\nimport s3fs\nimport pandas as pd\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\ndf = pd.read_parquet(f\"s3://{MY_BUCKET}/{CHEMIN_FICHIER}\", filesystem=fs)\n\ndf\n\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ns3 = pa.fs.S3FileSystem(endpoint_override =\"https://minio.lab.sspcloud.fr\")\n\ndf = pq.read_table(f\"{MY_BUCKET}/{CHEMIN_FICHIER}\", filesystem=s3)\n\ndf\n\n\nimport os\nimport duckdb\n\ncon = duckdb.connect(database=\":memory:\")\n\ncon.execute(\n    f\"\"\"\nCREATE SECRET secret (\n    TYPE S3,\n    KEY_ID '{os.environ[\"AWS_ACCESS_KEY_ID\"]}',\n    SECRET '{os.environ[\"AWS_SECRET_ACCESS_KEY\"]}',\n    ENDPOINT 'minio.lab.sspcloud.fr',\n    SESSION_TOKEN '{os.environ[\"AWS_SESSION_TOKEN\"]}',\n    REGION 'us-east-1',\n    URL_STYLE 'path',\n    SCOPE 's3://{MY_BUCKET}/'\n);\n\"\"\"\n)\n\nquery_definition = f\"SELECT * FROM read_parquet('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')\"\ndf = con.sql(query_definition)\n\ndf\n\n\n\nPour aller plus loin sur le format Parquet, notamment découvrir comment importer des données partitionnées, vous pouvez traduire en Python les exemples issus de la formation aux bonnes pratiques avec R de l’Insee.\n\n\n\n\n\n\n\n\nApplication 8c: privilégier le format Parquet dans notre chaîne\n\n\n\nDans main.py, remplacer le format csv initialement prévu par un format parquet:\ndata_train_path = os.environ.get(\"train_path\", \"data/derived/train.parquet\")\ndata_test_path = os.environ.get(\"test_path\", \"data/derived/test.parquet\")\nEt modifier l’écriture des données pour utiliser to_parquet plutôt que to_csv pour écrire les fichiers intermédiaires:\n\n\nmain.py\n\npd.concat([X_train, y_train], axis = 1).to_parquet(data_train_path)\npd.concat([X_test, y_test], axis = 1).to_parquet(data_test_path)\n\n\n\n\n\n\n\n\n\nApplication 8d: partage de données sur le système de stockage S3\n\n\n\nPar défaut, le contenu de votre bucket est privé, seul vous y avez accès. Pour pouvoir lire votre donnée, vos applications externes devront utiliser des jetons vous identifiant. Ici, comme nous utilisons une donnée publique, vous pouvez rendre accessible celle-ci à tous en lecture. Dans le jargon S3, cela signifie donner un accès anonyme à votre donnée.\nLe modèle de commande à utiliser dans le terminal est le suivant:\n\n\nterminal\n\n1BUCKET_PERSONNEL=\"nom_utilisateur_sspcloud\"\n\nmc anonymous set download s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/\n\n\n1\n\nRemplacer par le nom de votre bucket.\n\n\nLes URL de téléchargement seront de la forme https://minio.lab.sspcloud.fr/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/data/raw/data.csv\n\nRemplacer la définition de data_path pour utiliser, par défaut, directement l’URL dans l’import. Modifier, si cela est pertinent, aussi votre fichier .env.\n\n1URL_RAW = \"\"\ndata_path = os.environ.get(\"data_path\", URL_RAW)\n\n1\n\nModifier avec URL_RAW un lien de la forme \"https://minio.lab.sspcloud.fr/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.csv\" (ne laissez pas ${BUCKET_PERSONNEL}, remplacez par la vraie valeur!).\n\n\n\nAjouter le dossier data/ au .gitignore ainsi que les fichiers *.parquet\nSupprimer le dossier data de votre projet et faites git rm --cached -r data\nVérifier le bon fonctionnement de votre application.\n\n\n\nMaintenant qu’on a arrangé la structure de notre projet, c’est l’occasion de supprimer le code qui n’est plus nécessaire au bon fonctionnement de notre projet (cela réduit la charge de maintenance9).\nPour vous aider, vous pouvez utiliser vulture de manière itérative pour vous assister dans le nettoyage de votre code.\n\n\nterminal\n\npip install vulture\nvulture .\n\n\n\nExemple de sortie\n\n\n\nterminal\n\nvulture .\n\nsrc/data/import_data.py:3: unused function 'split_and_count' (60% confidence)\nsrc/pipeline/build_pipeline.py:12: unused function 'split_train_test' (60% confidence)\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli8      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli82\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli8\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-1-proposer-des-tests-unitaires-optionnel",
    "href": "chapters/application.html#étape-1-proposer-des-tests-unitaires-optionnel",
    "title": "Application",
    "section": "Étape 1 : proposer des tests unitaires (optionnel)",
    "text": "Étape 1 : proposer des tests unitaires (optionnel)\nNotre code comporte un certain nombre de fonctions génériques. On peut vouloir tester leur usage sur des données standardisées, différentes de celles du Titanic.\nMême si la notion de tests unitaires prend plus de sens dans un package, nous pouvons proposer dans le projet des exemples d’utilisation de la fonction, ceci peut être pédagogique.\nNous allons utiliser unittest pour effectuer des tests unitaires. Cette approche nécessite quelques notions de programmation orientée objet ou une bonne discussion avec ChatGPT.\n\n\n\n\n\n\nApplication 9: test unitaire (optionnel)\n\n\n\nDans le dossier tests/, créer avec l’aide de ChatGPT ou de Copilot un test pour la fonction split_and_count.\n\nEffectuer le test unitaire en ligne de commande avec unittest (python -m unittest tests/test_split.py). Corriger le test unitaire en cas d’erreur.\nSi le temps le permet, proposer des variantes ou d’autres tests.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli9      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli92\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli9\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\n\n\n\n\nNote\n\n\n\nLorsqu’on effectue des tests unitaires, on cherche généralement à tester le plus de lignes possibles de son code. On parle de taux de couverture (coverage rate) pour désigner la statistique mesurant cela.\nCela peut s’effectuer de la manière suivante avec le package coverage:\n\n\nterminal\n\ncoverage run -m unittest tests/test_create_variable_title.py\ncoverage report -m\n\nName                                  Stmts   Miss  Cover   Missing\n-------------------------------------------------------------------\nsrc/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113\ntests/test_create_variable_title.py      21      1    95%   54\n-------------------------------------------------------------------\nTOTAL                                    55     22    60%\nLe taux de couverture est souvent mis en avant par les gros projets comme indicateur de leur qualité. Il existe d’ailleurs des badges Github dédiés."
  },
  {
    "objectID": "chapters/application.html#étape-2-transformer-son-projet-en-package-optionnel",
    "href": "chapters/application.html#étape-2-transformer-son-projet-en-package-optionnel",
    "title": "Application",
    "section": "Étape 2 : transformer son projet en package (optionnel)",
    "text": "Étape 2 : transformer son projet en package (optionnel)\nNotre projet est modulaire, ce qui le rend assez simple à transformer en package, en s’inspirant de la structure du cookiecutter adapté, issu de cet ouvrage.\nOn va créer un package nommé titanicml qui encapsule tout notre code et qui sera appelé par notre script main.py. La structure attendue est la suivante:\n\n\nStructure visée\n\nensae-reproductibilite-application\n├── docs                                    ┐\n│   ├── main.py                             │\n│   └── notebooks                           │ Package documentation and examples\n│       └── titanic.ipynb                   │\n├── configuration                           ┐ Configuration (pas à partager avec Git)\n│   └── config.yaml                         ┘\n├── README.md\n├── pyproject.toml                          ┐\n├── requirements.txt                        │\n├── titanicml                               │\n│   ├── __init__.py                         │ Package source code, metadata\n│   ├── data                                │ and build instructions\n│   │   ├── import_data.py                  │\n│   │   └── test_create_variable_title.py   │\n│   ├── features                            │\n│   │   └── build_features.py               │\n│   └── models                              │\n│       └── train_evaluate.py               ┘\n└── tests                                   ┐\n    └── test_create_variable_title.py       ┘ Package tests\n\n\n\nRappel: structure actuelle\n\nensae-reproductibilite-application\n├── notebooks\n│   └── titanic.ipynb\n├── configuration\n│   └── config.yaml\n├── main.py\n├── README.md\n├── requirements.txt\n└── src\n    ├── data\n    │   ├── import_data.py\n    │   └── test_create_variable_title.py\n    ├── features\n    │   └── build_features.py\n    └── models\n        └── train_evaluate.py\n\nIl existe plusieurs frameworks pour construire un package. Nous allons privilégier Poetry à Setuptools.\n\n\n\n\n\n\nNote\n\n\n\nPour créer la structure minimale d’un package, le plus simple est d’utiliser le cookiecutter adapté, issu de cet ouvrage.\nComme on a déjà une structure très modulaire, on va plutôt recréer cette structure dans notre projet déjà existant. En fait, il ne manque qu’un fichier essentiel, le principal distinguant un projet classique d’un package : pyproject.toml.\n\n\nterminal\n\ncookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git\n\n\n\nDérouler pour voir les choix possibles\n\nauthor_name [Monty Python]: Daffy Duck\npackage_name [mypkg]: titanicml\npackage_short_description []: Impressive Titanic survival analysis\npackage_version [0.1.0]:\npython_version [3.9]:\nSelect open_source_license:\n1 - MIT\n2 - Apache License 2.0\n3 - GNU General Public License v3.0\n4 - Creative Commons Attribution 4.0\n5 - BSD 3-Clause\n6 - Proprietary\n7 - None\nChoose from 1, 2, 3, 4, 5, 6 [1]:\nSelect include_github_actions:\n1 - no\n2 - ci\n3 - ci+cd\nChoose from 1, 2, 3 [1]:\n\n\n\n\n\n\n\n\n\nApplication 10: packagisation (optionnel)\n\n\n\n\nRenommer le dossier titanicml pour respecter la nouvelle arborescence ;\nCréer un fichier pyproject.toml sur cette base ;\n\n#| code-summary: \"pyproject.toml\"\n#| filename: \"pyproject.toml\"\n[tool.poetry]\nname = \"titanicml\"\nversion = \"0.0.1\"\ndescription = \"Awesome Machine Learning project\"\nauthors = [\"Daffy Duck &lt;daffy.duck@fauxmail.fr&gt;\", \"Mickey Mouse\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n[tool.pytest.ini_options]\nlog_cli = true\nlog_cli_level = \"WARNING\"\nlog_cli_format = \"%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)\"\nlog_cli_date_format = \"%Y-%m-%d %H:%M:%S\"\n\nCréer le dossier docs et mettre les fichiers indiqués dedans\nDans titanicml/, créer un fichier __init__.py10\n\n#| code-summary: \"__init__.py\"\n#| filename: \"__init__.py\"\nfrom .data.import_data import (\n    split_and_count\n)\nfrom .pipeline.build_pipeline import (\n    split_train_test,\n    create_pipeline\n)\nfrom .models.train_evaluate import (\n    evaluate_model\n)\n__all__ = [\n    \"split_and_count\",\n    \"split_train_test\",\n    \"create_pipeline\",\n    \"evaluate_model\"\n]\n\nInstaller le package en local avec pip install -e .\nModifier le contenu de docs/main.py pour importer les fonctions de notre package titanicml et tester en ligne de commande notre fichier main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli10      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli102\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli10\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#anaconda",
    "href": "chapters/application.html#anaconda",
    "title": "Application",
    "section": "Étape 1 : un environnement pour rendre le projet portable",
    "text": "Étape 1 : un environnement pour rendre le projet portable\nPour qu’un projet soit portable, il doit remplir deux conditions:\n\nNe pas nécessiter de dépendance qui ne soient pas renseignées quelque part ;\nNe pas proposer des dépendances inutiles, qui ne sont pas utilisées dans le cadre du projet.\n\nLe prochain exercice vise à mettre ceci en oeuvre. Comme expliqué dans le chapitre portabilité, le choix du gestionnaire d’environnement est laissé libre. Il est recommandé de privilégier venv si vous découvrez la problématique de la portabilité.\n\nEnvironnement virtuel venvEnvironnement condaEnvironnement virtuel via uv\n\n\nL’approche la plus légère est l’environnement virtuel. Nous avons en fait implicitement déjà commencé à aller vers cette direction en créant un fichier requirements.txt.\n\n\n\n\n\n\nApplication 11a: environnement virtuel venv\n\n\n\n\nExécuter pip freeze en ligne de commande et observer la (très) longue liste de package\nCréer l’environnement virtuel titanic en s’inspirant de la documentation officielle11 ou du chapitre dédié\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin installé\nLe SSPCloud, par défaut, fonctionne sur un environnement conda. Le désactiver en faisant conda deactivate.\nActiver l’environnement et vérifier l’installation de Python maintenant utilisée par votre machine \nVérifier directement depuis la ligne de commande que Python exécute bien une commande12 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la même chose mais avec import pandas as pd\nInstaller les packages à partir du requirements.txt. Tester à nouveau import pandas as pd pour comprendre la différence.\nExécuter pip freeze et comprendre la différence avec la situation précédente.\nVérifier que le script main.py fonctionne bien. Sinon ajouter les packages manquants dans le requirements.txt et reprendre de manière itérative à partir de la question 7.\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier à Git.\n\n\n\nAide pour la question 4\n\nAprès l’activation, vous pouvez vérifier quel python est utilisé de cette manière\n\n\nterminal\n\nwhich python\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11a2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli11a\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\nLes environnements conda sont plus lourds à mettre en oeuvre que les environnements virtuels mais peuvent permettre un contrôle plus formel des dépendances.\n\n\n\n\n\n\nApplication 11b: environnement conda\n\n\n\n\nExécuter conda env export en ligne de commande et observer la (très) longue liste de package\nCréer un environnement titanic avec conda create\nActiver l’environnement et vérifier l’installation de Python maintenant utilisée par votre machine \nVérifier directement depuis la ligne de commande que Python exécute bien une commande13 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la même chose mais avec import pandas as pd\nInstaller les packages qu’on avait listé dans le requirements.txt précédemment. Ne pas faire un pip install -r requirements.txt afin de privilégier conda install\nExécuter à nouveau conda env export et comprendre la différence avec la situation précédente14.\nVérifier que le script main.py fonctionne bien. Sinon installer les packages manquants et reprndre de manière itérative à partir de la question 7.\nQuand main.py fonctionne, faire conda env export &gt; environment.yml pour figer l’environnement de travail.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11b2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli11b\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\nuv est le new kid in the game pour gérer les environnements virtuels avec Python.\n\n\n\n\n\n\nApplication 11c: environnement virtuel venv (via uv)\n\n\n\n\nAprès avoir installé uv, exécuter uv init . et supprimer le fichier hello.py généré. Ouvrir le pyproject.toml et observer sa structure.\nExécuter uv pip freeze en ligne de commande et observer la (très) longue liste de package\nCréer un environnement virtuel titanic par le biais d’uv (documentation) sous le nom titanic\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin installé\nActiver l’environnement et vérifier l’installation de Python maintenant utilisée par votre machine \nVérifier directement depuis la ligne de commande que Python exécute bien une commande15 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la même chose mais avec import pandas as pd. Maintenant, essayer uv run main.py en ligne de commande: comprenez-vous ce qu’il se passe ?\nInstaller de manière itérative les packages à partir d’uv add (documentation) et en testant avec uv run main.py: avez-vous remarqué la vitesse à laquelle cela a été quand vous avez fait uv add pandas ?\nObserver votre pyproject.toml. Regarder le lockfile uv.lock. Générer automatiquement le requirements.txt en faisant pip compile et regarder celui-ci.\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier à Git.\n\n\n\nAide pour la question 5\n\nAprès l’activation, vous pouvez vérifier quel python est utilisé de cette manière\n\n\nterminal\n\nwhich python\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11c      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11c2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli11c\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#shell",
    "href": "chapters/application.html#shell",
    "title": "Application",
    "section": "Étape 2: construire l’environnement de notre application via un script shell",
    "text": "Étape 2: construire l’environnement de notre application via un script shell\nLes environnements virtuels permettent de mieux spécifier les dépendances de notre projet, mais ne permettent pas de garantir une portabilité optimale. Pour cela, il faut recourir à la technologie des conteneurs. L’idée est de construire une machine, en partant d’une base quasi-vierge, qui permette de construire étape par étape l’environnement nécessaire au bon fonctionnement de notre projet. C’est le principe des conteneurs Docker .\nLeur méthode de construction étant un peu difficile à prendre en main au début, nous allons passer par une étape intermédiaire afin de bien comprendre le processus de production.\n\nNous allons d’abord créer un script shell, c’est à dire une suite de commandes Linux permettant de construire l’environnement à partir d’une machine vierge ;\nNous transformerons celui-ci en Dockerfile dans un deuxième temps. C’est l’objet de l’étape suivante.\n\n\nEnvironnement virtuel venvEnvironnement conda\n\n\n\n\n\n\n\n\nApplication 12a : créer un fichier d’installation de A à Z\n\n\n\n\nCréer un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le dépôt\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11a avec git checkout appli11a\nVia l’explorateur de fichiers, créer le fichier install.sh à la racine du projet avec le contenu suivant:\n\n\n\nScript à créer sous le nom install.sh\n\n\n\ninstall.sh\n\n#!/bin/bash\n\n# Install Python\napt-get -y update\napt-get install -y python3-pip python3-venv\n\n# Create empty virtual environment\npython3 -m venv titanic\nsource titanic/bin/activate\n\n# Install project dependencies\npip install -r requirements.txt\n\n\n\nChanger les permissions sur le script pour le rendre exécutable\n\n\n\nterminal\n\nchmod +x install.sh\n\n\nExécuter le script depuis la ligne de commande avec des droits de super-utilisateur (nécessaires pour installer des packages via apt)\n\n\n\nterminal\n\nsudo ./install.sh\n\n\nVérifier que le script main.py fonctionne correctement dans l’environnement virtuel créé\n\n\n\nterminal\n\nsource titanic/bin/activate\npython3 main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli12a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12a2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli12a\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\n\n\n\n\n\n\nApplication 12b : créer un fichier d’installation de A à Z\n\n\n\n\nCréer un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le dépôt\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11b avec git checkout appli11b\nVia l’explorateur de fichiers, créer le fichier install.sh à la racine du projet avec le contenu suivant:\n\n\n\nScript à créer sous le nom install.sh\n\n\n\ninstall.sh\n\napt-get -y update && apt-get -y install wget\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \\\n    rm -f Miniconda3-latest-Linux-x86_64.sh\n\nPATH=\"/miniconda/bin:${PATH}\"\n\n# Create environment\nconda create -n titanic pandas PyYAML scikit-learn -c conda-forge\nconda activate titanic\n\nPATH=\"/miniconda/envs/titanic/bin:${PATH}\"\n\npython main.py\n\n\n\nChanger les permissions sur le script pour le rendre exécutable\n\n\n\nterminal\n\nchmod +x install.sh\n\n\nExécuter le script depuis la ligne de commande avec des droits de super-utilisateur (nécessaires pour installer des packages via apt)\n\n\n\nterminal\n\nsudo ./install.sh\n\n\nVérifier que le script main.py fonctionne correctement dans l’environnement virtuel créé\n\n\n\nterminal\n\nconda activate titanic\npython3 main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli12b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12b2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli12b\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#docker",
    "href": "chapters/application.html#docker",
    "title": "Application",
    "section": "Étape 3: conteneuriser l’application avec Docker",
    "text": "Étape 3: conteneuriser l’application avec Docker\n\n\n\n\n\n\nNote\n\n\n\nCette application nécessite l’accès à une version interactive de Docker. Il n’y a pas beaucoup d’instances en ligne disponibles.\nNous proposons deux solutions:\n\nInstaller Docker sur sa machine ;\nSe rendre sur l’environnement bac à sable Play with Docker\n\nSinon, elle peut être réalisée en essai-erreur par le biais des services d’intégration continue de Github  ou Gitlab . Néanmoins, nous présenterons l’utilisation de ces services plus tard, dans la prochaine partie.\n\n\nMaintenant qu’on sait que ce script préparatoire fonctionne, on va le transformer en Dockerfile pour anticiper la mise en production. Comme la syntaxe Docker est légèrement différente de la syntaxe Linux classique (voir le chapitre portabilité), il va être nécessaire de changer quelques instructions mais ceci sera très léger.\nOn va tester le Dockerfile dans un environnement bac à sable pour ensuite pouvoir plus facilement automatiser la construction de l’image Docker.\n\n\n\n\n\n\nApplication 13: création de l’image Docker\n\n\n\nSe placer dans un environnement avec Docker, par exemple Play with Docker\n\nCréation du Dockerfile\n\nDans le terminal Linux, cloner votre dépôt Github\nRepartir de la dernière version à disposition. Par exemple, si vous avez privilégié l’environnement virtuel venv, ce sera:\n\n\n\nterminal\n\n1git stash\ngit checkout appli12a\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\nCréer via la ligne de commande un fichier texte vierge nommé Dockerfile (la majuscule au début du mot est importante)\n\n\n\nCommande pour créer un Dockerfile vierge depuis la ligne de commande\n\n\n\nterminal\n\ntouch Dockerfile\n\n\n\nOuvrir ce fichier via un éditeur de texte et copier le contenu suivant dedans:\n\n\n\nPremier Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCMD [\"python3\", \"main.py\"]\n\n\n\n\nConstruire (build) l’image\n\nUtiliser docker build pour créer une image avec le tag my-python-app\n\n\n\nterminal\n\ndocker build . -t my-python-app\n\n\nVérifier les images dont vous disposez. Vous devriez avoir un résultat proche de celui-ci :\n\n\n\nterminal\n\ndocker images\n\nREPOSITORY      TAG       IMAGE ID       CREATED              SIZE\nmy-python-app   latest    188957e16594   About a minute ago   879MB\n\n\nTester l’image: découverte du cache\nL’étape de build a fonctionné: une image a été construite.\nMais fait-elle effectivement ce que l’on attend d’elle ?\nPour le savoir, il faut passer à l’étape suivante, l’étape de run.\n\n\nterminal\n\ndocker run -it my-python-app\n\npython3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory\nLe message d’erreur est clair : Docker ne sait pas où trouver le fichier main.py. D’ailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont nécessaires pour faire tourner le code, par exemple le dossier src.\n\nAvant l’étape CMD, copier les fichiers nécessaires sur l’image afin que l’application dispose de tous les éléments nécessaires pour être en mesure de fonctionner.\n\n\n\nNouveau Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY main.py .\nCOPY src ./src\nCMD [\"python3\", \"main.py\"]\n\n\n\nRefaire tourner l’étape de build\nRefaire tourner l’étape de run. A ce stade, la matrice de confusion doit fonctionner 🎉. Vous avez créé votre première application reproductible !\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIci, le cache permet d’économiser beaucoup de temps. Par besoin de refaire tourner toutes les étapes, Docker agit de manière intelligente en faisant tourner uniquement les étapes qui ont changé.\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli13      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli132\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli13\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-1-mise-en-place-de-tests-automatisés",
    "href": "chapters/application.html#étape-1-mise-en-place-de-tests-automatisés",
    "title": "Application",
    "section": "Étape 1: mise en place de tests automatisés",
    "text": "Étape 1: mise en place de tests automatisés\nAvant d’essayer de mettre en oeuvre la création de notre image Docker de manière automatisée, nous allons présenter la logique de l’intégration continue en testant de manière automatisée notre script main.py.\nPour cela, nous allons partir de la structure proposée dans l’action officielle. La documentation associée est ici. Des éléments succincts de présentation de la logique déclarative des actions Github sont disponibles dans le chapitre sur la mise en production. Néanmoins, la meilleure école pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et d’observer les actions Github mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !\n\n\n\n\n\n\nApplication 14: premier script d’intégration continue\n\n\n\nA partir de l’exemple présent dans la documentation officielle de Github , on a déjà une base de départ qui peut être modifiée. Les questions suivantes permettront d’automatiser les tests et le diagnostic qualité de notre code16\n\nCréer un fichier .github/workflows/test.yaml avec le contenu de l’exemple de la documentation\nAvec l’aide de la documentation, introduire une étape d’installation des dépendances. Utiliser le fichier requirements.txt pour installer les dépendances.\nUtiliser pylint pour vérifier la qualité du code. Ajouter l’argument --fail-under=6 pour renvoyer une erreur en cas de note trop basse17\nUtiliser une étape appelant notre application en ligne de commande (python main.py) pour tester que la matrice de confusion s’affiche bien.\nCréer un secret stockant une valeur du JETON_API. Ne le faites pas commencer par un “$” comme ça vous pourrez regarder la log ultérieurement\nAller voir votre test automatisé dans l’onglet Actions de votre dépôt sur Github\n(optionnel): Créer un artefact à partir du fichier de log que vous créez dans main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli14      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli142\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli14\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nMaintenant, nous pouvons observer que l’onglet Actions s’est enrichi. Chaque commit va entraîner une série d’actions automatisées.\nSi l’une des étapes échoue, ou si la note de notre projet est mauvaise, nous aurons une croix rouge (et nous recevrons un mail). On pourra ainsi détecter, en développant son projet, les moments où on dégrade la qualité du script afin de la rétablir immédiatemment."
  },
  {
    "objectID": "chapters/application.html#étape-2-automatisation-de-la-livraison-de-limage-docker",
    "href": "chapters/application.html#étape-2-automatisation-de-la-livraison-de-limage-docker",
    "title": "Application",
    "section": "Étape 2: Automatisation de la livraison de l’image Docker",
    "text": "Étape 2: Automatisation de la livraison de l’image Docker\nMaintenant, nous allons automatiser la mise à disposition de notre image sur DockerHub (le lieu de partage des images Docker). Cela facilitera sa réutilisation mais aussi des valorisations ultérieures.\nLà encore, nous allons utiliser une série d’actions pré-configurées.\nPour que Github puisse s’authentifier auprès de DockerHub, il va falloir d’abord interfacer les deux plateformes. Pour cela, nous allons utiliser un jeton (token) DockerHub que nous allons mettre dans un espace sécurisé associé à votre dépôt Github.\n\n\n\n\n\n\nApplication 15a: configuration\n\n\n\n\nSe rendre sur https://hub.docker.com/ et créer un compte. Il est recommandé d’associer ce compte à votre compte Github.\nCréer un dépôt public application\nAller dans les paramètres de votre compte et cliquer, à gauche, sur Security\nCréer un jeton personnel d’accès, ne fermez pas l’onglet en question, vous ne pouvez voir sa valeur qu’une fois.\nDans le dépôt Github de votre projet, cliquer sur l’onglet Settings et cliquer, à gauche, sur Secrets and variables puis dans le menu déroulant en dessous sur Actions. Sur la page qui s’affiche, aller dans la section Repository secrets\nCréer un jeton DOCKERHUB_TOKEN à partir du jeton que vous aviez créé sur Dockerhub. Valider\nCréer un deuxième secret nommé DOCKERHUB_USERNAME ayant comme valeur le nom d’utilisateur que vous avez créé sur Dockerhub\n\n\n\nEtape optionnelle supplémentaire si on met en production un site web\n\n\nDans le dépôt Github de votre projet, cliquer sur l’onglet Settings et cliquer, à gauche, sur Actions. Donner les droits d’écriture à vos actions sur le dépôt du projet (ce sera nécessaire pour Github Pages)\n\n\n\n\n\nA ce stade, nous avons donné les moyens à Github de s’authentifier avec notre identité sur Dockerhub. Il nous reste à mettre en oeuvre l’action en s’inspirant de la documentation officielle. On ne va modifier que trois éléments dans ce fichier. Effectuer les actions suivantes:\n\n\n\n\n\n\nApplication 15b: automatisation de l’image Docker\n\n\n\n\nEn s’inspirant de ce template, créer le fichier .github/workflows/prod.yml qui va build et push l’image sur le DockerHub. Il va être nécessaire de changer légèrement ce modèle :\n\nRetirer la condition restrictive sur les commits pour lesquels sont lancés cette automatisation. Pour cela, remplacer le contenu de on de sorte à avoir\n\non:\n  push:\n    branches:\n      - main\n      - dev\n\nChanger le tag à la fin pour mettre username/application:latest où username est le nom d’utilisateur sur DockerHub;\nOptionnel: changer le nom de l’action\n\nFaire un commit et un push de ces fichiers\n\nComme on est fier de notre travail, on va afficher ça avec un badge sur le README (partie optionnelle).\n\nSe rendre dans l’onglet Actions et cliquer sur une des actions listées.\nEn haut à droite, cliquer sur ...\nSélectionner Create status badge\nRécupérer le code Markdown proposé\nCopier dans votre README.md le code markdown proposé\n\n\n\nCréer le badge\n\n\n\n\n\nMaintenant, il nous reste à tester notre application dans l’espace bac à sable ou en local, si Docker est installé.\n\n\n\n\n\n\nApplication 15b (partie optionnelle): Tester l’application\n\n\n\n\nSe rendre sur l’environnement bac à sable Play with Docker ou dans votre environnement Docker de prédilection.\nRécupérer et lancer l’image :\n\n\n\nterminal\n\ndocker run -it username/application:latest\n\n🎉 La matrice de confusion doit s’afficher ! Vous avez grandement facilité la réutilisation de votre image.\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli15      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli152\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli15\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-1-développer-une-api-en-local",
    "href": "chapters/application.html#étape-1-développer-une-api-en-local",
    "title": "Application",
    "section": "Étape 1: développer une API en local",
    "text": "Étape 1: développer une API en local\nLe premier livrable devenu classique dans un projet impliquant du machine learning est la mise à disposition d’un modèle par le biais d’une API (voir chapitre sur la mise en production). Le framework FastAPI va permettre de rapidement transformer notre application Python en une API fonctionnelle.\n\n\n\n\n\n\nApplication 16: Mise à disposition sous forme d’API locale\n\n\n\n\nInstaller fastAPI et uvicorn puis les ajouter au requirements.txt\nRenommer le fichier main.py en train.py.\nDans ce script, ajouter une sauvegarde du modèle après l’avoir entraîné, sous le format joblib.\nFaire tourner\n\n\n\nterminal\n\npython train.py\n\npour enregistrer en local votre modèle de production.\n\nModifier les appels à main.py dans votre Dockerfile et vos actions Github sous peine d’essuyer des échecs lors de vos actions Github après le prochain push.\nAjouter model.joblib au .gitignore car Git n’est pas fait pour ce type de fichiers.\n\nNous allons maintenant passer au développement de l’API. Comme découvrir FastAPI n’est pas l’objet de cet enseignement, nous donnons directement le modèle pour créer l’API. Si vous désirez tester de vous-mêmes, vous pouvez créer votre fichier sans vous référer à l’exemple.\n\nCréer le fichier app/api.py permettant d’initialiser l’API:\n\n\n\nFichier app/api.py\n\n\n\napp/api.py\n\n\"\"\"A simple API to expose our trained RandomForest model for Tutanic survival.\"\"\"\nfrom fastapi import FastAPI\nfrom joblib import load\n\nimport pandas as pd\n\nmodel = load('model.joblib')\n\napp = FastAPI(\n    title=\"Prédiction de survie sur le Titanic\",\n    description=\n    \"Application de prédiction de survie sur le Titanic 🚢 &lt;br&gt;Une version par API pour faciliter la réutilisation du modèle 🚀\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\n\n@app.get(\"/\", tags=[\"Welcome\"])\ndef show_welcome_page():\n    \"\"\"\n    Show welcome page with model name and version.\n    \"\"\"\n\n    return {\n        \"Message\": \"API de prédiction de survie sur le Titanic\",\n        \"Model_name\": 'Titanic ML',\n        \"Model_version\": \"0.1\",\n    }\n\n\n@app.get(\"/predict\", tags=[\"Predict\"])\nasync def predict(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\"\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    prediction = \"Survived 🎉\" if int(model.predict(df)) == 1 else \"Dead ⚰️\"\n\n    return prediction\n\n\n\nDéployer l’API en local avec la commande suivante.\n\n\n\nterminal\n\nuvicorn app.api:app\n\n\nObserver l’output dans la console. Notre API est désormais déployée en local, plus précisément sur le localhost, un serveur web local déployé à l’adresse http://127.0.0.1. L’API est déployée sur le port par défaut utilisé par uvicorn, soit le port 8000.\nSans fermer le terminal précédent, ouvrir un nouveau terminal. Tester le bon déploiement de l’API en requêtant son endpoint. Pour cela, on envoie une simple requête GET sur le endpoint via l’utilitaire curl.\n\n\n\nterminal\n\ncurl \"http://127.0.0.1:8000\"\n\n\nSi tout s’est bien passé, on devrait avoir récupéré une réponse (au format JSON) affichant le message d’accueil de notre API. Dans ce cas, on va pouvoir requêter notre modèle via l’API.\nEn vous inspirant du code qui définit le endpoint /predict dans le code de l’API (app/api.py), effectuer sur le même modèle que la requête précédente une requête qui calcule la survie d’une femme de 32 ans qui aurait payé son billet 16 dollars et aurait embarqué au port S.\n\n\n\nSolution\n\n\n\nterminal\n\ncurl \"http://127.0.0.1:8000/predict?sex=female&age=32&fare=16&embarked=S\"\n\n\n\nToujours sans fermer le terminal qui déploie l’API, ouvrir une session Python et tester une requête avec des paramètres différents, avec la librairie requests :\n\n\n\nSolution\n\nimport requests\n\nURL = \"http://127.0.0.1:8000/predict?sex=male&age=25&fare=80&embarked=S\"\nrequests.get(URL).json()\n\n\nUne fois que l’API a été testée, vous pouvez fermer l’application en effectuant CTRL+C depuis le terminal où elle est lancée.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli16      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli162\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli16\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#étape-2-déployer-lapi-de-manière-manuelle",
    "href": "chapters/application.html#étape-2-déployer-lapi-de-manière-manuelle",
    "title": "Application",
    "section": "Étape 2: déployer l’API de manière manuelle",
    "text": "Étape 2: déployer l’API de manière manuelle\nA ce stade, nous avons déployé l’API seulement localement, dans le cadre d’un terminal qui tourne en arrière-plan. C’est une mise en production manuelle, pas franchement pérenne. Ce mode de déploiement est très pratique pour la phase de développement, afin de s’assurer que l’API fonctionne comme attendue. Pour pérenniser la mise en production, on va éliminer l’aspect artisanal de celle-ci.\nIl est temps de passer à l’étape de déploiement, qui permettra à notre API d’être accessible, à tout moment, via une URL sur le web et d’avoir un serveur, en arrière plan, qui effectuera les opérations pour répondre à une requête. Pour se faire, on va utiliser les possibilités offertes par Kubernetes, technologie sur laquelle est basée l’infrastructure SSP Cloud.\n\n\n\n\n\n\nEt si vous n’utilisez pas le SSPCloud ? (une idée saugrenue mais sait-on jamais)\n\n\n\n\n\nLes exemples à venir peuvent très bien être répliqués sur n’importe quel cloud provider qui propose une solution d’ordonnancement type Kubernetes. Il existe également des fournisseurs de services dédiés, généralement associés à une implémentation, par exemple pour Streamlit. Ces services sont pratiques si on n’a pas le choix mais il faut garder à l’esprit qu’ils peuvent constituer un mur de la production car vous ne contrôlez pas l’environnement en question, qui peut se distinguer de votre environnement de développement.\nEt si jamais vous voulez avoir un SSPCloud dans votre entreprise c’est possible: le logiciel Onyxia sur lequel repose cette infrastructure est open source et est, déjà, réimplémenté par de nombreux acteurs. Pour bénéficier d’un accompagnement dans la création d’une telle infrastructure, rdv sur le Slack du projet Onyxia:\n\n\n\n\n\n\n\n\n\n\nApplication 17: Dockeriser l’API (intégration continue)\n\n\n\n\nCréer un script app/run.sh à la racine du projet qui lance le script train.py puis déploie localement l’API. Attention, quand on se place dans le monde des conteneurs et plus généralement des infrastructures cloud, on ne va plus déployer sur le localhost mais sur “l’ensemble des interfaces réseaux”. Lorsqu’on déploie une application web dans un conteneur, on va donc toujours devoir spécifier un host valant 0.0.0.0 (et non plus localhost ou, de manière équivalente, http://127.0.0.1).\n\n\n\nFichier run.sh\n\n\n\napi/run.sh\n\n#/bin/bash\n\npython3 train.py\nuvicorn app.api:app --host \"0.0.0.0\"\n\n\n\nDonner au script api/run.sh des permissions d’exécution : chmod +x api/run.sh\nAjouter COPY app ./app pour avoir les fichiers nécessaires au lancement dans l’API dans l’image\nModifier COPY train.py . pour tenir compte du nouveau nom du fichier\nChanger l’instruction CMD du Dockerfile pour exécuter le script api/run.sh au lancement du conteneur (CMD [\"bash\", \"-c\", \"./app/run.sh\"])\nCommit et push les changements\nUne fois le CI terminé, vérifier que le nouveau tag latest a été pushé sur le DockerHub. Récupérer la nouvelle image dans votre environnement de test de Docker et vérifier que l’API se déploie correctement.\n\n\n\nTester l’image sur le SSP Cloud\n\nLancer dans un terminal la commande suivante pour pull l’application depuis le DockerHub et la déployer en local :\n\n\nterminal\n\nkubectl run -it api-ml --image=votre_compte_docker_hub/application:latest\n\n\n\nSi tout se passe correctement, vous devriez observer dans la console un output similaire au déploiement en local de la partie précédente. Cette fois, l’application est déployée à l’adresse http://0.0.0.0:8000. On ne peut néanmoins pas directement l’exploiter à ce stade : si le conteneur de l’API est déployé, il manque un ensemble de ressources Kubernetes qui permettent de déployer proprement l’API à tout utilisateur. C’est l’objet de l’application suivante !\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli17      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli172\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli17\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nNous avons préparé la mise à disposition de notre API mais à l’heure actuelle elle n’est pas accessible de manière aisée car il est nécessaire de lancer manuellement une image Docker pour pouvoir y accéder. Ce type de travail est la spécialité de Kubernetes que nous allons utiliser pour gérer la mise à disposition de notre API.\n\n\n\n\n\n\nApplication 18: Mettre à disposition l’API (déploiement manuel)\n\n\n\nCette partie nécessite d’avoir à disposition une infrastructure cloud.\n\nCréer un dossier deployment à la racine du projet qui va contenir les fichiers de configuration nécessaires pour déployer sur un cluster Kubernetes\nEn vous inspirant de la documentation, y ajouter un premier fichier deployment.yaml qui va spécifier la configuration du Pod à lancer sur le cluster\n\n\n\nFichier deployment/deployment.yaml\n\n#| filename: \"deployment/deployment.yaml\"\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n        image: votre_compte_docker_hub/application:latest\n        ports:\n        - containerPort: 8000\n\n\nEn vous inspirant de la documentation, y ajouter un second fichier service.yaml qui va créer une ressource Service permettant de donner une identité fixe au Pod précédemment créé au sein du cluster\n\n\n\nFichier deployment/service.yaml\n\n\n\ndeployment/service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: titanic-service\nspec:\n  selector:\n    app: titanic\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8000\n\n\n\nEn vous inspirant de la documentation, y ajouter un troisième fichier ingress.yaml qui va créer une ressource Ingress permettant d’exposer le service via une URL en dehors du cluster\n\n\n\nFichier deployment/ingress.yaml\n\n#| filename: \"deployment/ingress.yaml\"\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: titanic-ingress\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - votre_nom_d_application.lab.sspcloud.fr\n  rules:\n  - host: votre_nom_d_application.lab.sspcloud.fr\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: titanic-service\n            port:\n              number: 80\n\nMettez l’URL auquel vous voulez exposer votre service. Sur le modèle de titanic.lab.sspcloud.fr (mais ne tentez pas celui-là, il est déjà pris 😃)\nMettre cette même URL ici aussi\n\n\n\nAppliquer ces fichiers de configuration sur le cluster : kubectl apply -f deployment/\nVérifier le bon déploiement de l’application (c’est à dire du Pod qui encapsule le conteneur) à l’aide de la commande kubectl get pods\nSi tout a correctement fonctionné, vous devriez pouvoir accéder depuis votre navigateur à l’API à l’URL spécifiée dans le fichier deployment/ingress.yaml. Par exemple https://api-titanic-test.lab.sspcloud.fr/ si vous avez mis celui-ci plus tôt\nExplorer le swagger de votre API à l’adresse https://api-titanic-test.lab.sspcloud.fr/docs. Il s’agit d’une page de documentation standard à la plupart des APIs, bien utiles pour tester des requêtes de manière interactive.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli18      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli182\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli18\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nOn peut remarquer quelques voies d’amélioration de notre approche qui seront ultérieurement traitées:\n\nL’entraînement du modèle est ré-effectué à chaque lancement d’un nouveau conteneur. On relance donc autant de fois un entraînement qu’on déploie de conteneurs pour répondre à nos utilisateurs. Ce sera l’objet de la partie MLOps de fiabiliser et optimiser cette partie du pipeline.\nil est nécessaire de (re)lancer manuellement kubectl apply -f deployment/ à chaque changement de notre code. Autrement dit, lors de cette application, on a amélioré la fiabilité du lancement de notre API mais un lancement manuel est encore indispensable. Comme dans le reste de ce cours, on va essayer d’éviter un geste manuel pouvant être source d’erreur en privilégiant l’automatisation et l’archivage dans des scripts. C’est l’objet de la prochaine étape."
  },
  {
    "objectID": "chapters/application.html#etape-3-automatiser-le-déploiement-déploiement-en-continu",
    "href": "chapters/application.html#etape-3-automatiser-le-déploiement-déploiement-en-continu",
    "title": "Application",
    "section": "Etape 3: automatiser le déploiement (déploiement en continu)",
    "text": "Etape 3: automatiser le déploiement (déploiement en continu)\n\n\n\n\n\n\nClarification sur la branche de travail, les tags et l’image Docker utilisée\n\n\n\n\n\nA partir de maintenant, il est nécessaire de clarifier la branche principale sur laquelle nous travaillons. Toutes les prochaines applications supposeront que vous travaillez depuis la branche main. Si vous avez changé de branche, vous pouvez fusionner celle-ci à main.\nSi vous avez utilisé un tag pour sauter une ou plusieurs étapes, il va être nécessaire de se placer sur une branche car vous êtes en head detached. Si vous avez utilisé les scripts automatisés de checkpoint, cette gymnastique a été faite pour vous.\nLes prochaines applications vont également nécessiter d’utiliser une image Docker. Si vous avez suivi de manière linéaire cette application, votre image Docker devrait exister depuis l’application 15 si vous avez pushé votre dépôt à ce moment là.\nNéanmoins, si vous n’avez pas fait cette application, vous pouvez utiliser le checkpoint de l’application 18 et faire un git push origin main --force (à ne pas reproduire sur vos projets!) qui devrait déclencher les opérations côté Github pour construire et livrer votre image Docker. Cela nécessite quelques opérations de votre côté, notamment la création d’un token Dockerhub à renseigner en secret Github. Pour vous refraîchir la mémoire sur le sujet, vous pouvez retourner consulter l’application 15.\n\n\n\nQu’est-ce qui peut déclencher une évolution nécessitant de mettre à jour l’ensemble de notre processus de production ?\nRegardons à nouveau notre pipeline:\n\nLes inputs de notre pipeline sont donc:\n\nLa configuration. Ici, on peut considérer que notre .env de configuration, les secrets renseignés à Github ou encore le requirements.txt relèvent de cette catégorie ;\nLes données. Nos données sont statiques et n’ont pas vocation à évoluer. Si c’était le cas, il faudrait en tenir compte dans notre automatisation (Note 1). ;\nLe code. C’est l’élément principal qui évolue chez nous. Idéalement, on veut automatiser le processus au maximum en faisant en sorte qu’à chaque mise à jour de notre code (un push sur Github), les étapes ultérieures (production de l’image Docker, etc.) se lancent. Néanmoins, on veut aussi éviter qu’une erreur puisse donner lieu à une mise en production non-fonctionnelle, on va donc maintenir une action manuelle minimale comme garde-fou.\n\n\n\n\n\n\n\nNote 1: Et le versionning des données ?\n\n\n\nIci, nous nous plaçons dans le cas simple où les données brutes reçues sont figées. Ce qui peut changer est la manière dont on constitue nos échantillons train/test. Il sera donc utile de logguer les données en question par le biais de MLFlow. Mais il n’est pas nécessaire de versionner les données brutes.\nSi celles-ci évoluaient, il pourrait être utile de versionner les données, à la manière dont on le fait pour le code. Git n’est pas l’outil approprié pour cela. Parmi les outils populaires de versionning de données, bien intégrés avec S3, il y a, sur le SSPCloud, lakefs.\n\n\nPour automatiser au maximum la mise en production, on va utiliser un nouvel outil : ArgoCD. Ainsi, au lieu de devoir appliquer manuellement la commande kubectl apply à chaque modification des fichiers de déploiement (présents dans le dossier kubernetes/), c’est l’opérateur ArgoCD, déployé sur le cluster, qui va détecter les changements de configuration du déploiement et les appliquer automatiquement.\nC’est l’approche dite GitOps : le dépôt Git du déploiement fait office de source de vérité unique de l’état voulu de l’application, tout changement sur ce dernier doit donc se répercuter immédiatement sur le déploiement effectif.\n\n\n\n\n\n\nApplication 19a: Automatiser la mise à disposition de l’API (déploiement continu)\n\n\n\n\nLancer un service ArgoCD sur le SSPCloud depuis la page Mes services (catalogue Automation). Laisser les configurations par défaut.\nSur GitHub, créer un dépôt application-deployment qui va servir de dépôt GitOps, c’est à dire un dépôt qui spécifie le paramétrage du déploiement de votre application.\nAjouter un dossier deployment à votre dépôt GitOps, dans lequel on mettra les trois fichiers de déploiement qui permettent de déployer notre application sur Kubernetes (deployment.yaml, service.yaml, ingress.yaml).\nA la racine de votre dépôt GitOps, créez un fichier application.yml avec le contenu suivant, en prenant bien soin de modifier les lignes annotées avec des informations pertinentes :\n\n\napplication.yaml\n\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: ensae-mlops\nspec:\n  project: default\n  source:\n1    repoURL: https://github.com/&lt;your_github_username&gt;/application-deployment.git\n2    targetRevision: main\n3    path: deployment\n  destination:\n    server: https://kubernetes.default.svc\n4    namespace: user-&lt;your_sspcloud_username&gt;\n  syncPolicy:\n    automated:\n      selfHeal: true\n\n\n1\n\nL’URL de votre dépôt Github  faisant office de dépôt GitOps.\n\n2\n\nLa branche à partir de laquelle vous déployez.\n\n3\n\nLe nom du dossier contenant vos fichiers de déploiement Kubernetes.\n\n4\n\nVotre namespace Kubernetes. Sur le SSPCloud, cela prend la forme user-${username}.\n\n\nPousser sur Github le dépôt GitOps.\nDans ArgoCD, cliquez sur New App puis Edit as a YAML. Copiez-collez le contenu de application.yml et cliquez sur Create.\nObservez dans l’interface d’ArgoCD le déploiement progressif des ressources nécessaires à votre application sur le cluster. Joli non ?\nVérifiez que votre API est bien déployée en utilisant l’URL définie dans le fichier ingress.yml.\nSupprimer du code applicatif le dossier deployment puisque c’est maintenant votre dépôt de déploiement qui le contrôle.\nIndiquer dans le README.md que le déploiement de votre application (dont vous pouvez mettre l’URL dans le README) est contrôlé par un autre dépôt.\n\n\n\nSi cela a fonctionné, vous devriez maintenant voir votre application dans votre tableau de bord ArgoCD:\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli19a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli19a2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli19a\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nA présent, nous avons tous les outils à notre disposition pour construire un vrai pipeline de CI/CD, automatisé de bout en bout. Il va nous suffire pour cela de mettre à bout les composants :\n\ndans la partie 4 de l’application, nous avons construit un pipeline de CI : on a donc seulement à faire un commit sur le dépôt de l’application pour lancer l’étape de build et de mise à disposition de la nouvelle image sur le DockerHub ;\ndans l’application précédente, nous avons construit un pipeline de CD : ArgoCD suit en permanence l’état du dépôt GitOps, tout commit sur ce dernier lancera donc automatiquement un redéploiement de l’application.\n\nIl y a donc un élément qui fait la liaison entre ces deux pipelines et qui nous sert de garde-fou en cas d’erreur : la version de l’application.\n\n\n\n\n\n\nApplication 19b : Mettre à jour la version en production\n\n\n\nJusqu’à maintenant, on a utilisé le tag latest pour définir la version de notre application. En pratique, lorsqu’on passe de la phase de développement à celle de production, on a plutôt envie de versionner proprement les versions de l’application afin de savoir ce qui est déployé. On va pour cela utiliser les tags avec Git, qui vont se propager au nommage de l’image Docker.\n\nModifier le fichier de CI prod.yml pour assurer la propagation des tags.\n\n\n\nFichier .github/workflows/prod.yml\n\n\n\n.github/workflows/prod.yml\n\nname: Construction image Docker\n\non:\n  push:\n    branches:\n      - main\n      - dev\n    tags:\n      - 'v*.*.*'\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      -\n        name: Docker meta\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n1          images: linogaliana/application\n\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n\n1\n\nModifier ici !\n\n\n\n\nDans le dépôt de l’application, mettre à jour le code dans app/main.py pour changer un élément de l’interface de votre documentation. Par exemple, mettre en gras un titre.\n\n\napp/main.py\n\napp = FastAPI(\n    title=\"Démonstration du modèle de prédiction de survie sur le Titanic\",\n    description=\n    \"&lt;b&gt;Application de prédiction de survie sur le Titanic&lt;/b&gt; 🚢 &lt;br&gt;Une version par API pour faciliter la réutilisation du modèle 🚀\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\nCommit et push les changements.\nTagger le commit effectué précédemment et push le nouveau tag :\n\n\nterminal\n\ngit tag v0.0.1\ngit push --tags\n\nVérifier sur le dépôt GitHub de l’application que ce commit lance bien un pipeline de CI associé au tag v1.0.0. Une fois terminé, vérifier sur le DockerHub que le tag v0.0.1 existe bien parmi les tags disponibles de l’image.\n\nLa partie CI a correctement fonctionné. Intéressons-nous à présent à la partie CD.\n\nSur le dépôt GitOps, mettre à jour la version de l’image à déployer en production dans le fichier deployment/deployment.yaml\n\n\n\nFichier deployment/deployment.yaml\n\n\n\ndeployment/deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n1        image: linogaliana/application:v0.0.1\n        ports:\n        - containerPort: 8000\n\n\n1\n\nRemplacer ici par le dépôt applicatif adéquat\n\n\n\n\nAprès avoir committé et pushé, observer dans ArgoCD le statut de votre application. Normalement, l’opérateur devrait avoir automatiquement identifié le changement, et mettre à jour le déploiement pour en tenir compte.\n\n\n\nVérifier que l’API a bien été mise à jour.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli19b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli19b2\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli19b\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#etape-4-construire-un-site-web",
    "href": "chapters/application.html#etape-4-construire-un-site-web",
    "title": "Application",
    "section": "Etape 4: construire un site web",
    "text": "Etape 4: construire un site web\n\n\n\n\n\n\nSi vous prenez ce projet fil rouge en cours de route\n\n\n\n\n\n\n\nterminal\n\ngit checkout appli19\ngit checkout -b dev\ngit push origin dev\n\n\n\n\n\n\n\n\n\nOn va proposer un nouveau livrable pour parler à un public plus large. Pour faire ce site web, on va utiliser Quarto et déployer sur Github Pages.\n\n\n\n\n\n\nApplication 20: Création d’un site web pour valoriser le projet\n\n\n\nquarto create project website mysite\n\nFaire remonter d’un niveau _quarto.yml\nSupprimer about.qmd, déplacer index.qmd vers la racine de notre projet.\nRemplacer le contenu de index.qmd par celui-ci et retirer about.qmd des fichiers à compiler.\nDéplacer styles.css à la racine du projet\nMettre à jour le .gitignore avec les instructions suivantes\n\n/.quarto/\n*.html\n*_files\n_site/\n\nEn ligne de commande, faire quarto preview\nObserver le site web généré en local\n\nEnfin, on va construire et déployer automatiquement ce site web grâce au combo Github Actions et Github Pages:\n\nCréer une branche gh-pages à partir des lignes suivantes\n\n\n\nterminal\n\ngit checkout --orphan gh-pages\ngit reset --hard # make sure all changes are committed before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\n\n\nRevenir à votre branche principale (main normalement)\nCréer un fichier .github/workflows/website.yaml avec le contenu de ce fichier\nModifier le README pour indiquer l’URL de votre site web et de votre API\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli20      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli202\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli20\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#revenir-sur-le-code-dentraînement-du-modèle-pour-faire-de-la-validation-croisée",
    "href": "chapters/application.html#revenir-sur-le-code-dentraînement-du-modèle-pour-faire-de-la-validation-croisée",
    "title": "Application",
    "section": "Revenir sur le code d’entraînement du modèle pour faire de la validation croisée",
    "text": "Revenir sur le code d’entraînement du modèle pour faire de la validation croisée\nPour pouvoir faire ceci, il va falloir changer un tout petit peu notre code applicatif dans sa phase d’entraînement.\n\n\n\n\n\n\nApplication 21 (optionnelle): restructuration de la chaîne\n\n\n\n\nFaire les modifications suivantes pour restructurer notre pipeline afin de mieux distinguer les étapes d’estimation et d’évaluation\n\n\n\nModification de train.py pour faire une grid search\n\n\n\ntrain.py\n\n\"\"\"\nPrediction de la survie d'un individu sur le Titanic\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\nimport argparse\nfrom loguru import logger\nfrom joblib import dump\n\nimport pathlib\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom src.pipeline.build_pipeline import create_pipeline\nfrom src.models.train_evaluate import evaluate_model\n\n\n# ENVIRONMENT CONFIGURATION ---------------------------\n\nlogger.add(\"recording.log\", rotation=\"500 MB\")\nload_dotenv()\n\nparser = argparse.ArgumentParser(description=\"Paramètres du random forest\")\nparser.add_argument(\n    \"--n_trees\", type=int, default=20, help=\"Nombre d'arbres\"\n)\nargs = parser.parse_args()\n\nURL_RAW = \"https://minio.lab.sspcloud.fr/lgaliana/ensae-reproductibilite/data/raw/data.csv\"\n\nn_trees = args.n_trees\njeton_api = os.environ.get(\"JETON_API\", \"\")\ndata_path = os.environ.get(\"data_path\", URL_RAW)\ndata_train_path = os.environ.get(\"train_path\", \"data/derived/train.parquet\")\ndata_test_path = os.environ.get(\"test_path\", \"data/derived/test.parquet\")\nMAX_DEPTH = None\nMAX_FEATURES = \"sqrt\"\n\nif jeton_api.startswith(\"$\"):\n    logger.info(\"API token has been configured properly\")\nelse:\n    logger.warning(\"API token has not been configured\")\n\n\n# IMPORT ET STRUCTURATION DONNEES --------------------------------\n\np = pathlib.Path(\"data/derived/\")\np.mkdir(parents=True, exist_ok=True)\n\nTrainingData = pd.read_csv(data_path)\n\ny = TrainingData[\"Survived\"]\nX = TrainingData.drop(\"Survived\", axis=\"columns\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1\n)\npd.concat([X_train, y_train], axis = 1).to_parquet(data_train_path)\npd.concat([X_test, y_test], axis = 1).to_parquet(data_test_path)\n\n\n# PIPELINE ----------------------------\n\n\n# Create the pipeline\npipe = create_pipeline(\n    n_trees, max_depth=MAX_DEPTH, max_features=MAX_FEATURES\n)\n\nparam_grid = {\n    \"classifier__n_estimators\": [10, 20, 50],\n    \"classifier__max_leaf_nodes\": [5, 10, 50],\n}\n\npipe_cross_validation = GridSearchCV(\n    pipe,\n    param_grid=param_grid,\n    scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n    refit=\"f1\",\n    cv=5,\n    n_jobs=5,\n    verbose=1,\n)\n\npipe_cross_validation.fit(X_train, y_train)\n\npipe = pipe_cross_validation.best_estimator_\n\n\n# ESTIMATION ET EVALUATION ----------------------\n\npipe.fit(X_train, y_train)\n\nwith open(\"model.joblib\", \"wb\") as f:\n    dump(pipe, f)\n\n# Evaluate the model\nscore, matrix = evaluate_model(pipe, X_test, y_test)\n\nlogger.success(f\"{score:.1%} de bonnes réponses sur les données de test pour validation\")\nlogger.debug(20 * \"-\")\nlogger.info(\"Matrice de confusion\")\nlogger.debug(matrix)\n\n\n\nDans le code de l’API (app/api.py), changer la version du modèle mis en oeuvre en “0.2” (dans la fonction show_welcome_page)\nAprès avoir committé cette nouvelle version du code applicatif, tagguer ce dépôt avec le tag v0.0.2\nModifier deployment/deployment.yaml dans le code GitOps pour utiliser ce tag.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli21      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli212\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli21\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#garder-une-trace-des-entraînements-de-notre-modèle-grâce-au-register-de-mlflow",
    "href": "chapters/application.html#garder-une-trace-des-entraînements-de-notre-modèle-grâce-au-register-de-mlflow",
    "title": "Application",
    "section": "Garder une trace des entraînements de notre modèle grâce au register de MLFlow",
    "text": "Garder une trace des entraînements de notre modèle grâce au register de MLFlow\n  \n    \n      \n        \n      \n      \n        Reprendre à partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration proposée dans l'application préliminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, après avoir clôné le dépôt\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli212\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli21\n          3\n          Nettoyer derrière nous"
  },
  {
    "objectID": "chapters/application.html#enregistrer-nos-premiers-entraînements",
    "href": "chapters/application.html#enregistrer-nos-premiers-entraînements",
    "title": "Application",
    "section": "Enregistrer nos premiers entraînements",
    "text": "Enregistrer nos premiers entraînements\n\n\n\n\n\n\nApplication 22 : archiver nos entraînements avec MLFlow\n\n\n\n\nLancer MLFlow depuis l’onflet Mes services du SSPCloud. Attendre que le service soit bien lancé. Cela créera un service dont l’URL est de la forme https://user-{username}.user.lab.sspcloud.fr. Ce service MLFlow communiquera avec les VSCode que vous ouvrirez ultérieurement à partir de cet URL ainsi qu’avec le système de stockage S318.\nRegarder la page Experiments. Elle ne contient que Default à ce stade, c’est normal.\n\n\nUne fois le service MLFlow fonctionnel, lancer un nouveau VSCode pour bénéficier de la connexion automatique entre les services interactifs du SSPCloud et les services d’automatisation comme MLFlow.\nClôner votre projet, vous situer sur la branche de travail.\nDans la section de passage des paramètres de notre ligne de commande, introduire ce morceau de code:\n\nparser = argparse.ArgumentParser(description=\"Paramètres du random forest\")\nparser.add_argument(\n    \"--n_trees\", type=int, default=20, help=\"Nombre d'arbres\"\n)\nparser.add_argument(\n    \"--experiment_name\", type=str, default=\"titanicml\", help=\"MLFlow experiment name\"\n)\nargs = parser.parse_args()\n\nA la fin du script train.py, ajouter le code suivant\n\n\n\nCode à ajouter\n\n\n\nfin de train.py\n\n# LOGGING IN MLFLOW -----------------\n\nmlflow_server = os.getenv(\"MLFLOW_TRACKING_URI\")\n\nlogger.info(f\"Saving experiment in {mlflow_server}\")\n\nmlflow.set_tracking_uri(mlflow_server)\nmlflow.set_experiment(args.experiment_name)\n\n\ninput_data_mlflow = mlflow.data.from_pandas(\n    TrainingData, source=data_path, name=\"Raw dataset\"\n)\ntraining_data_mlflow = mlflow.data.from_pandas(\n    pd.concat([X_train, y_train], axis=1), source=data_path, name=\"Training data\"\n)\n\n\nwith mlflow.start_run():\n\n    # Log datasets\n    mlflow.log_input(input_data_mlflow, context=\"raw\")\n    mlflow.log_input(training_data_mlflow, context=\"raw\")\n\n    # Log parameters\n    mlflow.log_param(\"n_trees\", n_trees)\n    mlflow.log_param(\"max_depth\", MAX_DEPTH)\n    mlflow.log_param(\"max_features\", MAX_FEATURES)\n\n    # Log best hyperparameters from GridSearchCV\n    best_params = pipe_cross_validation.best_params_\n    for param, value in best_params.items():\n        mlflow.log_param(param, value)\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", score)\n\n    # Log confusion matrix as an artifact\n    matrix_path = \"confusion_matrix.txt\"\n    with open(matrix_path, \"w\") as f:\n        f.write(str(matrix))\n    mlflow.log_artifact(matrix_path)\n\n    # Log model\n    mlflow.sklearn.log_model(pipe, \"model\")\n\n\n\nAjouter mlruns/* dans .gitignore\nTester train.py en ligne de commande\nObserver l’évolution de la page Experiments. Cliquer sur un des run. Observer toutes les métadonnées archivées (hyperparamètres, métriques d’évaluation, requirements.txt dont MLFlow a fait l’inférence, etc.)\nObserver le code proposé par MLFlow pour récupérer le run en question. Tester celui-ci dans un notebook sur le fichier intermédiaire de test au format Parquet\nEn ligne de commande, faites tourner pour une autre valeur de n_trees. Retourner à la liste des runs en cliquant à nouveau sur “titanicml” dans les expérimentations\nDans l’onglet Table, sélectionner plusieurs expérimentations, cliquer sur Columns et ajouter la statistique d’accuracy. Ajuster la taille des colonnes pour la voir et classer les modèles par score décroissants\nCliquer sur Compare après en avoir sélectionné plusieurs. Afficher un scatterplot des performances en fonction du nombre d’estimateurs. Conclure.\nAjouter mlflow au requirements.txt\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli22      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli222\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli22\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCette appplication illustre l’un des premiers apports de MLFlow: on garde une trace de nos expérimentations: le modèle est archivé avec les paramètres et des métriques de performance. On peut donc retrouver de plusieurs manières un modèle qui nous avait tapé dans l’oeil.\nNéanmoins, persistent un certain nombre de voies d’amélioration dans notre pipeline.\n\nOn entraîne le modèle en local, de manière séquentielle, et en lançant nous-mêmes le script train.py.\nPis encore, à l’heure actuelle, cette étape d’estimation n’est pas séparée de la mise à disposition du modèle par le biais de notre API. On archive des modèles mais on les utilise pas ultérieurement.\n\nLes prochaines applications permettront d’améliorer ceci."
  },
  {
    "objectID": "chapters/application.html#consommation-dun-modèle-archivé-sur-mlflow",
    "href": "chapters/application.html#consommation-dun-modèle-archivé-sur-mlflow",
    "title": "Application",
    "section": "Consommation d’un modèle archivé sur MLFlow",
    "text": "Consommation d’un modèle archivé sur MLFlow\nA l’heure actuelle, notre pipeline est linéaire:\n\nCeci nous gêne pour faire évoluer notre modèle: on ne dissocie pas ce qui relève de l’entraînement du modèle de son utilisation. Un pipeline plus cyclique permettra de mieux dissocier l’expérimentation de la production:\n\n\n\n\n\n\n\nApplication 23 : passer en production un modèle avec MLFlow\n\n\n\n\nSi vous avez entraîné plusieurs modèles avec des n_trees différents, utiliser l’interface de MLFlow pour sélectionner le “meilleur”. Cliquer sur le modèle en question et faire l’action “Register Model”. L’enregistrer comme le modèle de “production”\nRendez-vous sur l’onglet Models et observez cet entrepôt de modèles. Cliquez sur le modèle de production. Vous pourrez par ce biais suivre ses différentes versions.\nOuvrir un notebook temporaire et observer le résultat.\n\n\n\nExemple de code à tester\n\nimport mlflow\nimport pandas as pd\n\nmodel_name = \"production\"\nmodel_version = \"latest\"\n\n# Load the model from the Model Registry\nmodel_uri = f\"models:/{model_name}/{model_version}\"\nlogged_model = mlflow.sklearn.load_model(model_uri)\n\n# GENERATE PREDICTION DATA ---------------------\n\ndef create_data(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\",\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    return df\n\n\ndata = pd.concat(\n    [create_data(age=40), create_data(sex=\"male\")]\n)\n\n# PREDICTION ---------------------\n\nlogged_model.predict(pd.DataFrame(data))\n\n\nOn va adapter le code applicatif de notre API pour tenir compte de ce modèle de production.\n\n\n\nVoir le script app/api.py proposé\n\n\"\"\"A simple API to expose our trained RandomForest model for Tutanic survival.\"\"\"\nfrom fastapi import FastAPI\nimport mlflow\n\nimport pandas as pd\n\n# Preload model -------------------\n\nmodel_name = \"production\"\nmodel_version = \"latest\"\n\n# Load the model from the Model Registry\nmodel_uri = f\"models:/{model_name}/{model_version}\"\nmodel = mlflow.sklearn.load_model(model_uri)\n\n# Define app -------------------------\n\n\napp = FastAPI(\n    title=\"Prédiction de survie sur le Titanic\",\n    description=\n    \"Application de prédiction de survie sur le Titanic 🚢 &lt;br&gt;Une version par API pour faciliter la réutilisation du modèle 🚀\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\n\n@app.get(\"/\", tags=[\"Welcome\"])\ndef show_welcome_page():\n    \"\"\"\n    Show welcome page with model name and version.\n    \"\"\"\n\n    return {\n        \"Message\": \"API de prédiction de survie sur le Titanic\",\n        \"Model_name\": 'Titanic ML',\n        \"Model_version\": \"0.3\",\n    }\n\n\n@app.get(\"/predict\", tags=[\"Predict\"])\nasync def predict(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\"\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    prediction = \"Survived 🎉\" if int(model.predict(df)) == 1 else \"Dead ⚰️\"\n\n    return prediction\n\nLes changements principaux de ce code sont:\n\non va chercher le modèle de production\non met à jour la version de notre API pour signaler à nos clients que celle-ci a évolué\n\n\nOn va retirer l’entraînement de la séquence d’opération du api/run.sh. En supprimant la ligne relative à l’entraînement du modèle, vous devriez avoir\n\n#/bin/bash\nuvicorn app.api:app --host \"0.0.0.0\"\nMettons en production cette nouvelle version. Cela implique de faire les gestes suivants:\n\nCommit de ce changement dans main\nPublier un tag v0.0.3 pour le code applicatif\nMettre à jour notre manifeste dans le dépôt GitOps.\n\nEn premier lieu, il faut changer la version de référence pour utiliser le tag v0.0.3.\nDe plus, il faut déclarer la variable d’environnement MLFLOW_TRACKING_URI qui indique à Python l’entrepôt de modèles où aller chercher celui en production. La bonne pratique est de définir ceci hors du code, dans un fichier de configuration donc, ce qui est l’objet de notre manifeste deployment.yaml. On peut donc changer de cette manière ce fichier:\n\n\n\n\nLe modèle deployment.yaml proposé\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: titanic-deployment\nlabels:\n    app: titanic\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: titanic\ntemplate:\n    metadata:\n    labels:\n        app: titanic\n    spec:\n    containers:\n    - name: titanic\n1        image: linogaliana/application:v0.0.3\n        ports:\n        - containerPort: 8000\n        env:\n        - name: MLFLOW_TRACKING_URI\n2            value: https://user-${USERNAME}-mlflow.user.lab.sspcloud.fr\n        resources:\n        limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n\n1\n\nLe tag de notre code applicatif\n\n2\n\nLa variable d’environnement à adapter en fonction de l’adresse du dépôt demodèles utilisé. Remplacer par votre URL MLFlow.\n\n\n\n\nPour s’assurer que l’application fonctionne bien, on peut aller voir les logs de la machine qui fait tourner notre code. Pour ça, faire kubectl get pods et, en supposant que votre service soit nommé titanic dans vos fichiers YAML de configuration, récupérer le nom commençant par titanic-deployment-* et faire kubectl logs titanic-deployment-*\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli23      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli232\nrm -f update.sh\n          \n        \n        \n          1\n          Récupérer le script de checkpoint\n          2\n          Avancer à l’état à l’issue de l’application appli23\n          3\n          Nettoyer derrière nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nA ce stade, nous avons amélioré la fiabilité de notre application car nous utilisons le meilleur modèle. Néanmoins, nos entraînements sont encore manuels. Là encore il y a des gains possibles car cela paraît pénible à la longue de devoir systématiquement relancer des entraînements manuellement pour tester des variations de tel ou tel paramètre. Heureusement, nous allons pouvoir automatiser ceci également."
  },
  {
    "objectID": "chapters/application.html#industrialiser-les-entraînements-de-nos-modèles",
    "href": "chapters/application.html#industrialiser-les-entraînements-de-nos-modèles",
    "title": "Application",
    "section": "Industrialiser les entraînements de nos modèles",
    "text": "Industrialiser les entraînements de nos modèles\nPour industrialiser nos entraînements, nous allons créer des processus parallèles indépendants pour chaque combinaison de nos hyperparamètres.\nCe travail nous amène de l’approche pipeline à mi chemin entre data science et data engineering. Il existe plusieurs outils pour faire ceci, généralement issus de la sphère du data engineering. L’outil le plus complet sur le SSPCloud, bien intégré à l’écosystème Kubernetes, est Argo Workflows19.\nChaque combinaison d’hyperparamètres sera un processus isolé à l’issue duquel sera loggué le résultat dans MLFlow. Ces entraînements auront lieu en parallèle.\nNous allons construire, dans les deux prochaines applications, un pipeline simple prenant cette forme20:\n\n\n\n\n\n\n\n\n\n\n\n(a) Via Argo Workflows\n\n\n\n\n\n\n\n\n\n\n\n(b) Via Github Actions\n\n\n\n\n\n\n\nFigure 3: Pipeline d’entraînement de nos modèles avec deux outils d’automatisation différents\n\n\n\nL’outil permettant une intégration native de notre pipeline dans l’infrastructure cloud (SSPCloud) que nous avons utilisée jusqu’à présent est Argo Workflows. Néanmoins, pour illustrer la modularité de notre chaîne, permise par l’adoption de Docker, nous allons montrer que les serveurs d’intégration continue de Github peuvent très bien servir d’environnement d’exécution, sans rien perdre de ce que nous avons mis en oeuvre précédemment (logging des modèles dans MLFlow, récupération de données depuis S3, etc.)\n\n\n\n\n\n\nApplication 24 : industrialisation des entraînements avec Argo Workflow\n\n\n\nA l’heure actuelle, notre entraînement ne dépend que d’un hyperparamètre fixé à partir de la ligne de commande: n_trees. Nous allons commencer par ajouter un argument à notre chaine de production (code applicatif):\n\nDans train.py, dans la section relative au parsing de nos arguments, ajouter ce bout de code\n\nparser.add_argument(\n    \"--max_features\",\n    type=str, default=\"sqrt\",\n    choices=['sqrt', 'log2'],\n    help=\"Number of features to consider when looking for the best split\"\n)\net remplacer la définition de MAX_FEATURES par l’argument fourni en ligne de commande:\nMAX_FEATURES = args.max_features\n\nFaire un commit, taguer cette version (v0.0.4) et pusher le tag\nMaintenant, dans le dépôt GitOps, créer un fichier argo-workflow/manifest.yaml\n\n\n\nLe modèle proposé\n\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\ngenerateName: titanic-training-workflow-\nnamespace: user-lgaliana\nspec:\nentrypoint: main\nserviceAccountName: workflow\narguments:\n    parameters:\n    # The MLflow tracking server is responsible to log the hyper-parameter and model metrics.\n    - name: mlflow-tracking-uri\n1        value: https://user-${USERNAME}-mlflow.user.lab.sspcloud.fr/\n    - name: mlflow-experiment-name\n2        value: titanicml\n    - name: model-training-conf-list\n        value: |\n        [\n            { \"n_trees\": 10, \"max_features\": \"log2\" },\n            { \"n_trees\": 20, \"max_features\": \"sqrt\" },\n            { \"n_trees\": 20, \"max_features\": \"log2\" },\n            { \"n_trees\": 50, \"max_features\": \"sqrt\" }\n        ]\ntemplates:\n    # Entrypoint DAG template\n    - name: main\n    dag:\n        tasks:\n        # Task 0: Start pipeline\n        - name: start-pipeline\n            template: start-pipeline-wt\n        # Task 1: Train model with given params\n        - name: train-model-with-params\n            dependencies: [ start-pipeline ]\n            template: run-model-training-wt\n            arguments:\n            parameters:\n                - name: max_features\n                value: \"{{item.max_features}}\"\n                - name: n_trees\n                value: \"{{item.n_trees}}\"\n            # Pass the inputs to the task using \"withParam\"\n            withParam: \"{{workflow.parameters.model-training-conf-list}}\"\n    # Now task container templates are defined\n    # Worker template for task 0 : start-pipeline\n    - name: start-pipeline-wt\n    inputs:\n    container:\n        image: busybox\n        command: [ sh, -c ]\n        args: [ \"echo Starting pipeline\" ]\n    # Worker template for task-1 : train model with params\n    - name: run-model-training-wt\n    inputs:\n        parameters:\n        - name: n_trees\n        - name: max_features\n    container:\n3        image: ****/application:v0.0.4\n        imagePullPolicy: Always\n        command: [sh, -c]\n        args: [\n        \"python3 train.py --n_trees={{inputs.parameters.n_trees}} --max_features={{inputs.parameters.max_features}}\"\n        ]\n        env:\n        - name: MLFLOW_TRACKING_URI\n            value: \"{{workflow.parameters.mlflow-tracking-uri}}\"\n        - name: MLFLOW_EXPERIMENT_NAME\n            value: \"{{workflow.parameters.mlflow-experiment-name}}\"\n        - name: AWS_DEFAULT_REGION\n            value: us-east-1\n        - name: AWS_S3_ENDPOINT\n            value: minio.lab.sspcloud.fr\n\n1\n\nChanger pour votre entrepot de modèle\n\n2\n\nLe nom de l’expérimentation MLFLow dont nous allons avoir besoin (on propose de continuer sur titanicml)\n\n3\n\nChanger l’image Docker  ici\n\n\n\n\nObserver l’UI d’Argo Workflow dans vos services ouverts du SSPCloud. Vous devriez retrouver Figure 3 (a) dans celle-ci.\n\n\n\nNous pouvons maintenant passer à la version Github. Celle-ci est optionnelle car elle vient surtout démontrer l’intérêt d’avoir une chaine modulaire et la dissociation que cela permet entre l’environnement d’exécution et les autres environnements nécessaires à notre chaine (notamment le stockage code et le logging).\n\n\n\n\n\n\nApplication 25 (optionnelle) : Github Actions comme ordonnanceur\n\n\n\nPour que Github sache où aller chercher MLFlow et S3 et comment s’y identifier, il va falloir lui donner un certain de variables d’environnement. Il est hors de question de mettre celles-ci dans le code. Heureusement, Github propose la possibilité de renseigner des secrets: nous allons utiliser ceux-ci.\n\nAller dans les paramètres de votre projet GitOps et dans la section Secrets and variables\nVous allez avoir besoin de créer les secrets suivants:\n\nMLFLOW_TRACKING_PASSWORD\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_SESSION_TOKEN\n\n\nLes valeurs à renseigner sont à récupérer à différents endroits:\n\nPour les secrets liés à S3 (AWS_*), ceux-ci sont dans l’espace Mon compte du SSPCloud. Ils ont une durée de validité limitée: si vous devez refaire tourner le code dans quelques jours, il faudra les mettre à jour (ou passer par un compte de service comme indiqué précédemment)\nLe mot de passe de MLFlow est dans le README de votre service, qui s’affiche quand vous cliquez sur le bouton Ouvrir depuis la page Mes services\n\n\nReprendre ce modèle d’action à mettre dans votre dépôt GitOps (.github/workflows/train.yaml par exemple).\n\n\n\nModèle d’action Github\n\nname: Titanic Model Training\n\non:\n  push:\n    branches:\n      - main\n  workflow_dispatch:\n\njobs:\n  start-pipeline:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Start Pipeline\n        run: echo \"Starting pipeline\"\n\n  train-model:\n    needs: start-pipeline\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        model-config:\n          - { n_trees: 10, max_features: \"log2\" }\n          - { n_trees: 20, max_features: \"sqrt\" }\n          - { n_trees: 20, max_features: \"log2\" }\n          - { n_trees: 50, max_features: \"sqrt\" }\n    container:\n1      image: ***/application:v0.0.4\n    env:\n      MLFLOW_TRACKING_URI: \"https://user-lgaliana-mlflow.user.lab.sspcloud.fr/\"\n      MLFLOW_EXPERIMENT_NAME: \"titanicml\"\n      MLFLOW_TRACKING_PASSWORD: \"${{ secrets.MLFLOW_TRACKING_PASSWORD }}\"\n      AWS_DEFAULT_REGION: \"us-east-1\"\n      AWS_S3_ENDPOINT: \"minio.lab.sspcloud.fr\"\n      AWS_ACCESS_KEY_ID: \"${{ secrets.AWS_ACCESS_KEY_ID }}\"\n      AWS_SECRET_ACCESS_KEY: \"${{ secrets.AWS_SECRET_ACCESS_KEY }}\"\n      AWS_SESSION_TOKEN: \"${{ secrets.AWS_SESSION_TOKEN }}\"\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n2            repository: 'ensae-reproductibilite/application'\n            ref: appli24\n\n      - name: Train Model\n        run: |\n          python3 train.py --n_trees=${{ matrix.model-config.n_trees }} --max_features=${{ matrix.model-config.max_features }}\n\n1\n\nMettre votre image ici. Si vous n’en avez pas, vous pouvez mettre linogaliana/application:v0.0.4\n\n2\n\nOn reprend le code applicatif de l’application précédente. Vous pouvez remplacer par votre dépôt et une référence adaptée si vous préférez\n\n\n\n\nPusher et observer l’UI de Github depuis l’onglet Actions. Vous devriez retrouver Figure 3 (b) dans celle-ci."
  },
  {
    "objectID": "chapters/application.html#footnotes",
    "href": "chapters/application.html#footnotes",
    "title": "Application",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIl y a quelques différences entre le VSCode server mis à disposition sur le SSPCloud et la version desktop sur laquelle s’appuient beaucoup de ressources. A quelques extensions prêts (Data Wrangler, Copilot), les différences sont néanmoins minimes.↩︎\nL’export dans un script .py a été fait directement depuis VSCode. Comme cela n’est pas vraiment l’objet du cours, nous passons cette étape et fournissons directement le script expurgé du texte intermédiaire. Mais n’oubliez pas que cette démarche, fréquente quand on a démarré sur un notebook et qu’on désire consolider en faisant la transition vers des scripts, nécessite d’être attentif pour ne pas risquer de faire une erreur.↩︎\nIl est également possible avec VSCode d’exécuter le script ligne à ligne de manière interactive ligne à ligne (MAJ+ENTER). Néanmoins, cela nécessite de s’assurer que le working directory de votre console interactive est le bon. Celle-ci se lance selon les paramètres préconfigurés de VSCode et les votres ne sont peut-être pas les mêmes que les notres. Vous pouvez changer le working directory dans le script en utilisant le package os mais peut-être allez vous découvrir ultérieurement qu’il y a de meilleures pratiques…↩︎\nEssayez de commit vos changements à chaque étape de l’exercice, c’est une bonne habitude à prendre.↩︎\nIl est normal d’avoir des dossiers __pycache__ qui traînent en local : ils se créent automatiquement à l’exécution d’un script en Python. Néanmoins, il ne faut pas associer ces fichiers à Git, voilà pourquoi on les ajoute au .gitignore.↩︎\nNous proposons ici d’adopter le principe de la programmation fonctionnelle. Pour encore fiabiliser un processus, il serait possible d’adopter le paradigme de la programmation orientée objet (POO). Celle-ci est plus rebutante et demande plus de temps au développeur. L’arbitrage coût-avantage est négatif pour notre exemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d’un modèle, il peut être utle de l’adopter car certains frameworks, à commencer par les pipelines scikit, exigeront certaines classes et méthodes si vous désirez brancher des objets ad hoc à ceux-ci.↩︎\nAttention, les données ont été committées au moins une fois. Les supprimer du dépôt ne les efface pas de l’historique. Si cette erreur arrive, le mieux est de supprimer le dépôt en ligne, créer un nouvel historique Git et partir de celui-ci pour des publications ultérieures sur Github. Néanmoins l’idéal serait de ne pas s’exposer à cela. C’est justement l’objet des bonnes pratiques de ce cours: un .gitignore bien construit et une séparation des environnements de stockage du code et des données seront bien plus efficaces pour vous éviter ces problèmes que tout les conseils de vigilance que vous pourrez trouver ailleurs.↩︎\nAlors oui, c’est vrai, s3 se distingue d’un système de fichiers classiques comme on peut le lire dans certains posts énervés sur la question (par exemple sur Reddit). Mais du point de vue de l’utilisateur Python plutôt que de l’architecte cloud, on va avoir assez peu de différence avec un système de fichier local. C’est pour le mieux, cela réduit la difficulté à rentrer dans cette technologie.↩︎\nLorsqu’on développe du code qui finalement ne s’avère plus nécessaire, on a souvent un cas de conscience à le supprimer et on préfère le mettre de côté. Au final, ce syndrôme de Diogène est mauvais pour la pérennité du projet : on se retrouve à devoir maintenir une base de code qui n’est, en pratique, pas utilisée. Ce n’est pas un problème de supprimer un code ; si finalement celui-ci s’avère utile, on peut le retrouver grâce à l’historique Git et les outils de recherche sur Github. Le package vulture est très pratique pour diagnostiquer les morceaux de code inutiles dans un projet.↩︎\nLe fichier __init__.py indique à Python que le dossier est un package. Il permet de proposer certaines configurations lors de l’import du package. Il permet également de contrôler les objets exportés (c’est-à-dire mis à disposition de l’utilisateur) par le package par rapport aux objets internes au package. En le laissant vide, nous allons utiliser ce fichier pour importer l’ensemble des fonctions de nos sous-modules. Ce n’est pas la meilleure pratique mais un contrôle plus fin des objets exportés demanderait un investissement qui ne vaut, ici, pas le coût.↩︎\nSi vous désirez aussi contrôler la version de Python, ce qui peut être important dans une perspective de portabilité, vous pouvez ajouter une option, par exemple -p python3.10. Néanmoins nous n’allons pas nous embarasser de cette nuance pour la suite car nous pourrons contrôler la version de Python plus finement par le biais de Docker.↩︎\nL’option -c passée après la commande python permet d’indiquer à Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu’on va directement lui fournir.↩︎\nL’option -c passée après la commande python permet d’indiquer à Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu’on va directement lui fournir.↩︎\nPour comparer les deux listes, vous pouvez utiliser la fonctionnalité de split du terminal sur VSCode pour comparer les outputs de conda env export en les mettant en face à face.↩︎\nL’option -c passée après la commande python permet d’indiquer à Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu’on va directement lui fournir.↩︎\nIl est tout à fait normal de ne pas parvenir à créer une action fonctionnelle du premier coup. N’hésitez pas à pusher votre code après chaque question pour vérifier que vous parvenez bien à réaliser chaque étape. Sinon vous risquez de devoir corriger bout par bout un fichier plus conséquent.↩︎\nIl existe une approche alternative pour faire des tests réguliers: les hooks Git. Il s’agit de règles qui doivent être satisfaites pour que le fichier puisse être committé. Cela assure que chaque commit remplisse des critères de qualité afin d’éviter le problème de la procrastination.\nLa documentation de pylint offre des explications supplémentaires. Ici, nous allons adopter une approche moins ambitieuse en demandant à notre action de faire ce travail d’évaluation de la qualité de notre code↩︎\nPar conséquent, MLFLow bénéficie de l’injection automatique des tokens pour pouvoir lire/écrire sur S3. Ces jetons ont la même durée avant expiration que ceux de vos services interactifs VSCode. Il faut donc, par défaut, supprimer et rouvrir un service MLFLow régulièrement. La manière d’éviter cela est de créer des service account sur https://minio-console.lab.sspcloud.fr/ et de les renseigner sur la page.↩︎\nIl existe d’autres outils d’ordonnancement de pipelines très utilisés dans l’industrie, notamment Airflow.\nCe dernier est plus utilisé, en pratique, qu’Argo Workflow mais, même s’il est disponible sur le SSPCloud aussi, est moins pensé autour de Kubernetes que l’est Argo.\nPour mieux comprendre la différence entre Argo et Airflow, la philosphie différente de ces deux outils et leurs avantages comparatifs, cette courte vidéo est intéressante:\n\n↩︎\nIl serait bien sûr possible d’aller beaucoup plus loin dans la définition du pipeline.\nPar exemple, il est possible, si le framework utilisé pour la modélisation n’intègre pas la notion de pipeline au niveau de Python de faire ceci au niveau d’Argo. Cela donnerait un pipeline prenant cette forme:\n\nNéanmoins, ici, nous utilisons Scikit qui permet d’intégrer le preprocessing comme une étape de modélisation. Nous n’avons donc pas d’intérêt à définir ceci comme une tâche autonome, raison pour laquelle notre pipeline apparaît plus simple.↩︎"
  },
  {
    "objectID": "chapters/big-data.html",
    "href": "chapters/big-data.html",
    "title": "Traitement des données volumineuses",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran.\nThe big data phenomenon is now well-documented: the generation and collection of data from a multitude of sources (IoT sensors, daily interactions on social media, online transactions, mobile devices, etc.) drastically increases the volume of data available for analysis. There are many reasons to focus on such data in data science projects: high availability, greater granularity of observed phenomena, and large datasets needed for training increasingly data-hungry models (like LLMs), among others.\nBig data is often defined as a situation where the data volume is so large that it can no longer be processed on a single machine. This relative definition may seem reductive but has the advantage of highlighting that a data source, depending on the time and environment, may require different skill sets. Moving to big data is not just a matter of scale—it often involves a fundamental shift in the computing infrastructure, with strong implications for the expertise required and the scalability of the data pipelines.\nProcessing such data introduces new challenges, often summarized as the “three Vs”, a widely accepted way to characterize these data sources (Sagiroglu and Sinanc 2013):\nWhen considering putting a data science project based on large datasets into production, adopting good development practices is not just recommended—it is essential. Massive datasets introduce significant complexity at every stage of the data science project lifecycle, from collection and storage to processing and analysis. Systems must be designed not only to handle the current data load but also to be scalable for future growth. Good practices enable this scalability by promoting modular architectures, reusable code, and technologies suited for large-scale data processing.\nTo address these challenges, technology choices are crucial. In this course, we will focus on three main aspects to guide those choices: computing infrastructure, data formats suited for high volumes, and frameworks (software solutions and their ecosystems) used for data processing."
  },
  {
    "objectID": "chapters/big-data.html#evolution-of-data-infrastructures",
    "href": "chapters/big-data.html#evolution-of-data-infrastructures",
    "title": "Traitement des données volumineuses",
    "section": "Evolution of Data Infrastructures",
    "text": "Evolution of Data Infrastructures\nHistorically, data has been stored in databases, systems designed to store and organize information. These systems emerged in the 1950s and saw significant growth with relational databases in the 1980s. This technology proved especially effective for organizing corporate “business” data and served as the foundation of data warehouses, long considered the standard for data storage infrastructure (Chaudhuri and Dayal 1997). While technical implementations can vary, their core idea is simple: data from various heterogeneous sources is integrated into a relational database system according to business rules via ETL (extract-transform-load) processes, making it accessible for a range of uses (statistical analysis, reporting, etc.) using the standardized SQL language (Figure 1).\n\n\n\n\n\n\nFigure 1: Architecture of a data warehouse. Source: airbyte.com\n\n\n\nIn the early 2000s, the growing adoption of big data practices exposed the limitations of traditional data warehouses. On one hand, data increasingly came in diverse formats (structured, semi-structured, and unstructured), often evolving as new features were added to data collection platforms. These dynamic, heterogeneous formats fit poorly with the ordered nature of data warehouses, which require schemas to be defined a priori. To address this, data lakes were developed—systems that allow for the collection and storage of large volumes of diverse data types (Figure 2).\n\n\n\n\n\n\nFigure 2: Architecture of a data lake. Source: cartelis.com\n\n\n\nAdditionally, the enormous size of these data sets made it increasingly difficult to process them on a single machine. This is when Google introduced the MapReduce paradigm (Ghemawat, Gobioff, and Leung 2003; Dean and Ghemawat 2008), which laid the foundation for a new generation of distributed data processing systems. Traditional infrastructures used vertical scalability—adding more powerful or additional resources to a single machine. However, this quickly became expensive and hit hardware limits. Distributed architectures use horizontal scalability: by using many parallel, lower-powered servers and adapting algorithms to this distributed logic, massive datasets can be processed using commodity hardware. This led to the emergence of the Hadoop ecosystem, combining complementary technologies: a data lake (HDFS - Hadoop Distributed File System), a distributed processing engine (MapReduce), and tools for data integration and transformation (Figure 3). This ecosystem expanded with tools like Hive (which converts SQL queries into distributed MapReduce tasks) and Spark (which overcomes certain technical limitations of MapReduce and provides APIs in multiple languages, including Java, Scala, and Python). Hadoop’s success was profound—it enabled organizations to process petabyte-scale datasets in real-time using widely accessible programming languages.\nThis technological shift fueled the big data revolution, enabling new types of questions to be answered using vast datasets. Philosophically, it marked a shift from collecting only the data needed for known purposes, to storing as much data as possible and evaluating its usefulness later during analysis. This approach is typical of NoSQL environments (“Not only SQL”), where data is stored at each transactional event but in more flexible formats than traditional databases. JSON, derived from web transactions, is especially prominent. Depending on the structure of the data, different tools are used to query it: ElasticSearch or MongoDB for text data, Spark for tabular data, and so on. All these tools share a common trait: they are highly horizontally scalable, making them ideal for server farms.\n\n\n\n\n\n\nFigure 3: Schematic of a Hadoop architecture. Large datasets are split into blocks, and both storage and processing are distributed across multiple compute nodes. Algorithms are adapted to this distributed setup via MapReduce: first, a “map” function is applied to each block (e.g., count word frequencies), then a “reduce” step aggregates these results (e.g., compute total frequencies across blocks). Output data is often much smaller than input data and can be brought back locally for further tasks like visualization. Source: glennklockwood.com\n\n\n\nBy the late 2010s, Hadoop architectures began to decline in popularity. In traditional Hadoop setups, storage and compute are co-located by design: data segments are processed on the servers where they are stored, avoiding network traffic. This architecture scales linearly, increasing both storage and compute capacity—even if only one is needed. In a provocative article titled “Big Data is Dead” (Tigani 2023), Jordan Tigani (one of the founding engineers of Google BigQuery) argues that this model no longer suits modern data workloads. First, he explains, “in practice, data size grows much faster than compute needs.” Most use cases don’t require querying all stored data—just recent subsets or specific columns. Second, “the big data frontier keeps receding”: thanks to more powerful and cheaper servers, fewer workloads require distributed systems (Figure 4). Additionally, new storage formats (see Section 2) make data handling more efficient. As a result, properly decoupling storage from compute often leads to simpler, more efficient infrastructures.\n\n\n\n\n\n\nFigure 4: “The big data frontier keeps receding”: the share of data workloads that cannot be handled by a single machine has steadily declined. Source: motherduck.com"
  },
  {
    "objectID": "chapters/big-data.html#the-role-of-cloud-technologies",
    "href": "chapters/big-data.html#the-role-of-cloud-technologies",
    "title": "Traitement des données volumineuses",
    "section": "The Role of Cloud Technologies",
    "text": "The Role of Cloud Technologies\nBuilding on Tigani’s analysis, we observe a growing shift toward more flexible, loosely coupled architectures. The rise of cloud technologies has been pivotal in this transition, for several reasons. Technically, network latency is no longer the bottleneck it was during Hadoop’s heyday, making the co-location of compute and storage less necessary. In terms of usage, it’s not just that data volumes are growing—it’s also the diversity of data and processing needs that is expanding. Modern infrastructures must support various data formats (from structured tables to unstructured media) and a wide range of compute requirements—from parallel data processing to deep learning on GPUs (Li et al. 2020).\nTwo cloud-native technologies have become central to this modern flexibility: containerization and object storage. Containerization ensures reproducibility and portability—crucial for production environments—and will be discussed in the Portability and Deployment chapters. In this section, we focus on object storage, the default standard in modern data infrastructures.\nSince containers are stateless by nature, a persistent storage layer is needed to store input and output data across computations (Figure 5). In container-based infrastructures, object storage has become dominant—popularized by Amazon’s S3 (Simple Storage Service) (Mesnier, Ganger, and Riedel 2003; Samundiswary and Dongre 2017). To understand its popularity, it’s helpful to contrast object storage with other storage types.\nThere are three main storage models: file systems, block storage, and object storage (Figure 5). File systems organize data in a hierarchical structure—like a traditional desktop environment—but they don’t scale well and require manual access management. Block storage, like that on hard drives, offers fast low-latency access—ideal for databases—but also struggles with scalability and cost. Object storage, on the other hand, breaks data into “objects” stored in a flat namespace and assigned unique IDs and metadata. It removes the need for hierarchical structures, lowering storage costs.\n\n\n\n\n\n\nFigure 5: Comparison of storage types. Source: bytebytego.com\n\n\n\nObject storage’s characteristics make it ideal for containerized data science infrastructures. It’s highly scalable, supports large files, and works well with distributed systems. It also enhances user autonomy by exposing data via APIs like Amazon’s S3, allowing direct interaction from code (R, Python, etc.) and fine-grained access control via tokens. Most importantly, object storage supports decoupled architectures, where compute and storage are independent and remotely accessible. This improves flexibility and efficiency.\n\n\n\n\n\n\nFigure 6: In container-based infrastructure (which is stateless by nature), object storage provides the persistence layer. MinIO is an open-source object storage solution that integrates natively with Kubernetes and supports the S3 API—now the industry standard—ensuring compatibility across environments. Source: lemondeinformatique.fr"
  },
  {
    "objectID": "chapters/big-data.html#application-3-1",
    "href": "chapters/big-data.html#application-3-1",
    "title": "Traitement des données volumineuses",
    "section": "Application 3",
    "text": "Application 3\n\n\n\n\n\n\nPart 3a: What if we filter rows?\n\n\n\n\n\nAdd a row filtering step in our queries:\n\nWith DuckDB, modify the query with WHERE DEPT IN ('18', '28', '36')\nWith Arrow, modify the to_table step as follows: dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n\n\n\nCorrection de cet exercice\nimport pyarrow.dataset as ds\nimport pyarrow.compute as pc\nimport duckdb\n\n@measure_performance\ndef summarize_filter_parquet_arrow(*args, **kwargs):\n\n    dataset = ds.dataset(\"data/RPindividus.parquet\", format=\"parquet\")\n    table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n    grouped_table = (\n        table\n        .group_by([\"AGED\", \"DEPT\"])\n        .aggregate([(\"IPONDI\", \"sum\")])\n        .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n        .to_pandas()\n    )\n\n    return (\n        grouped_table\n    )\n\n@measure_performance\ndef summarize_filter_parquet_duckdb(*args, **kwargs):\n    con = duckdb.connect(\":memory:\")\n    query = \"\"\"\n    FROM read_parquet('data/RPindividus_24.parquet')\n    SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n    WHERE DEPT IN ('11','31','34')\n    GROUP BY AGED, DEPT\n    \"\"\"\n\n    return (con.sql(query).to_df())\n\n\nsummarize_filter_parquet_arrow()\nsummarize_filter_parquet_duckdb()\n\n\n\n\n\n❓️ Why don’t we save time with row filters (or even lose time), unlike with column filters?\nData is not organized in row blocks the way it is in column blocks. Fortunately, there’s a way around this: partitioning!\n\n\n\n\n\n\nPart 3: Partitioned Parquet\n\n\n\n\n\nLazy evaluation and Arrow optimizations already bring considerable performance gains. But we can do even better! When we know that data will regularly be filtered based on a specific variable, it is a good idea to partition the Parquet file by that variable.\n\nBrowse the documentation for pyarrow.parquet.write_to_dataset to understand how to specify a partitioning key when writing a Parquet file. Several methods are possible.\nImport the full census individual table from \"data/RPindividus.parquet\" using pyarrow.dataset.dataset and export it as a partitioned table to \"data/RPindividus_partitionne.parquet\", partitioned by region (REGION) and department (DEPT).\nExamine the file tree structure of the exported table to see how the partitioning was applied.\nModify the import, filtering, and aggregation functions using Arrow or DuckDB to now use the partitioned Parquet file. Compare this to using the non-partitioned file.\n\n\n\nAnswer to question 2 (writing the partitioned Parquet)\nimport pyarrow.parquet as pq\ndataset = ds.dataset(\n    \"data/RPindividus.parquet\", format=\"parquet\"\n).to_table()\n\npq.write_to_dataset(\n    dataset,\n    root_path=\"data/RPindividus_partitionne\",\n    partition_cols=[\"REGION\", \"DEPT\"]\n)\n\n\n\n\nCorrection de la question 4 (lecture du Parquet partitionné)\nimport pyarrow.dataset as ds\nimport pyarrow.compute as pc\nimport duckdb\n\n@measure_performance\ndef summarize_filter_parquet_partitioned_arrow(*args, **kwargs):\n\n    dataset = ds.dataset(\"data/RPindividus_partitionne/\", partitioning=\"hive\")\n    table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n\n    grouped_table = (\n        table\n        .group_by([\"AGED\", \"DEPT\"])\n        .aggregate([(\"IPONDI\", \"sum\")])\n        .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n        .to_pandas()\n    )\n\n    return (\n        grouped_table\n    )\n\n@measure_performance\ndef summarize_filter_parquet_complete_arrow(*args, **kwargs):\n\n    dataset = ds.dataset(\"data/RPindividus.parquet\")\n    table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n\n    grouped_table = (\n        table\n        .group_by([\"AGED\", \"DEPT\"])\n        .aggregate([(\"IPONDI\", \"sum\")])\n        .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n        .to_pandas()\n    )\n\n    return (\n        grouped_table\n    )\n\n\n@measure_performance\ndef summarize_filter_parquet_complete_duckdb(*args, **kwargs):\n    con = duckdb.connect(\":memory:\")\n    query = \"\"\"\n    FROM read_parquet('data/RPindividus.parquet')\n    SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n    WHERE DEPT IN ('11','31','34')\n    GROUP BY AGED, DEPT\n    \"\"\"\n\n    return (con.sql(query).to_df())\n\n\n@measure_performance\ndef summarize_filter_parquet_partitioned_duckdb(*args, **kwargs):\n    con = duckdb.connect(\":memory:\")\n    query = \"\"\"\n    FROM read_parquet('data/RPindividus_partitionne/**/*.parquet', hive_partitioning = True)\n    SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n    WHERE DEPT IN ('11','31','34')\n    GROUP BY AGED, DEPT\n    \"\"\"\n\n    return (con.sql(query).to_df())\n\n\nsummarize_filter_parquet_complete_arrow()\nsummarize_filter_parquet_partitioned_arrow()\nsummarize_filter_parquet_complete_duckdb()\nsummarize_filter_parquet_partitioned_duckdb()\n\n\n\n\n\n❓️ When making data available in Parquet format, how should you choose the partitioning key(s)? What limitation should be kept in mind?"
  },
  {
    "objectID": "chapters/big-data.html#to-go-further",
    "href": "chapters/big-data.html#to-go-further",
    "title": "Traitement des données volumineuses",
    "section": "To go further",
    "text": "To go further\n\nThe training on good practices with R and Git developed by Insee, with content very similar to what’s presented in this chapter.\nA workshop on the Parquet format and DuckDB ecosystem for EHESS, with R and Python examples using the same data source as this application.\nThe getting started guide for census data in Parquet format with examples using DuckDB in WASM (directly in the browser, without R or Python installation)."
  },
  {
    "objectID": "chapters/deployment.html",
    "href": "chapters/deployment.html",
    "title": "Déploiement",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran.\n\n\n\n\n\n\n\nPage en construction.\n\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "chapters/galerie.html",
    "href": "chapters/galerie.html",
    "title": "Galerie d’exemples",
    "section": "",
    "text": "Une galerie d’exemple de projets à venir\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Modèle de carte\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            PrimePredict\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            ResultAthle\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n\n\nNo matching items\n\n\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "See slides (in French) or click here"
  },
  {
    "objectID": "chapters/introduction.html#origin",
    "href": "chapters/introduction.html#origin",
    "title": "Introduction",
    "section": "Origin",
    "text": "Origin\nThe notion of “best practices” as used in this course originates from the software development community. It emerged in response to several observations:\n\n“Code is read much more often than it is written” (Guido Van Rossum);\nMaintaining code often requires (much) more effort than writing it initially;\nThe person maintaining the codebase is likely not the one who wrote it.\n\nIn light of these realities, the developer community has conventionally agreed on an informal set of rules recognized as producing more reliable, scalable, and maintainable software over time. Like language conventions, some may seem arbitrary—but they support a critical goal: enabling code to be shared and communicated effectively. This may seem secondary at first, but it’s a key factor in the success of open source languages, which thrive on shared experience and collaboration.\n\n\n\n\n\n\nThe 12 Factor App\n\n\n\n\n\nRecently, as software has evolved toward cloud-based web applications, many of these best practices were formalized in a manifesto known as the 12 Factor App. The rise of the cloud—i.e., standardized infrastructures external to traditional in-house data systems—makes adopting good practices more crucial than ever."
  },
  {
    "objectID": "chapters/introduction.html#why-care-about-best-practices",
    "href": "chapters/introduction.html#why-care-about-best-practices",
    "title": "Introduction",
    "section": "Why Care About Best Practices?",
    "text": "Why Care About Best Practices?\n\nWhy should this matter to a data scientist, whose job is to derive insights from data—not build applications?\n\nDue to the rapid growth of data science and the increasing size of typical projects, the data scientist’s work is becoming more similar in some ways to that of a developer:\n\nData science projects involve intensive coding;\nCollaboration is required on large-scale projects;\nMassive datasets require working on technically complex big data infrastructures;\nThe data scientist must collaborate with technical roles to deploy models and make them accessible to users.\n\nThus, it makes sense for modern data scientists to take interest in the best practices adopted by developers. Naturally, these need to be tailored to data-centered projects. The upside is significant: projects that adopt best practices are much cheaper to evolve—making them more competitive in the ever-changing data science ecosystem, where tools, data, and user expectations constantly shift."
  },
  {
    "objectID": "chapters/introduction.html#a-continuum-of-best-practices",
    "href": "chapters/introduction.html#a-continuum-of-best-practices",
    "title": "Introduction",
    "section": "A Continuum of Best Practices",
    "text": "A Continuum of Best Practices\nBest practices should not be viewed in a binary way: it’s not that some projects follow them and others don’t. Best practices come with a cost, which should not be overlooked—even though they prevent future costs, especially in maintenance. It’s better to view best practices as a spectrum, and position your project on it based on cost-benefit analysis, particularly in terms of improving reproducibility.\nThe appropriate threshold depends on trade-offs specific to your project:\n\nAmbitions: Will the project grow or evolve? Is it meant to become collaborative—within a team or as open source? Are the outputs intended for public release?\nResources: What human resources are available? For open-source work, is there a potential contributor community?\nConstraints: Are there tight deadlines? Specific quality requirements? Is deployment expected? Are there major security concerns?\nTarget audience: Who will consume the project’s data products? What’s their technical level, and how much time will they spend engaging with your work?\n\nWe are not suggesting that every data science project must follow all the best practices covered in this course. That said, we strongly believe every data scientist should consider these questions and continuously improve their practices.\nIn particular, we believe it’s possible to define a core set—i.e., a minimal set of best practices that provide more value than they cost to implement. Here’s our suggestion for such a baseline:\n\nUse dedicated tools to check code quality (see Code Quality);\nUse a standardized project structure with ready-made templates (see Project Architecture);\nUse Git to version your code, whether or not you’re working with others (see Version Control and Collaboration with Git);\nManage project dependencies with virtual environments (see Portability).\n\nBeyond this minimal baseline, decisions should weigh costs and benefits. But adopting this foundational level of reproducibility will make further progress much easier as your project grows.\nLet’s now look at the core principles promoted by this course and how the content is logically structured."
  },
  {
    "objectID": "chapters/introduction.html#code-as-a-communication-tool",
    "href": "chapters/introduction.html#code-as-a-communication-tool",
    "title": "Introduction",
    "section": "Code as a Communication Tool",
    "text": "Code as a Communication Tool\nThe first best practice to adopt is to view code as a communication tool, not just a functional one. Code doesn’t exist solely to perform a task—it’s meant to be shared, reused, and maintained, whether in a team or an open-source context.\nTo support this communication, conventions have been developed regarding code quality and project structure. These are covered in the chapters Code Quality and Project Architecture.\nFor the same reasons, applying version control principles is essential. These provide continuous documentation of the project, which greatly improves its reusability and maintainability. We revisit the use of Git in the chapter Version Control and Collaborative Work with Git."
  },
  {
    "objectID": "chapters/introduction.html#working-collaboratively",
    "href": "chapters/introduction.html#working-collaboratively",
    "title": "Introduction",
    "section": "Working Collaboratively",
    "text": "Working Collaboratively\nRegardless of context, data scientists typically work in team-based projects. This requires defining a work organization and using tools that enable secure, efficient collaboration.\nWe present a modern way to collaborate using Git and GitHub in the reminder chapter Version Control and Collaborative Work with Git. Later chapters will build on this collaborative approach and refine it using the DevOps methodology4."
  },
  {
    "objectID": "chapters/introduction.html#maximizing-reproducibility",
    "href": "chapters/introduction.html#maximizing-reproducibility",
    "title": "Introduction",
    "section": "Maximizing Reproducibility",
    "text": "Maximizing Reproducibility\nThe third pillar of best practices in this course is reproducibility.\nA project is reproducible when the same code and data can be used to reproduce the same results. It’s important to distinguish this from replicability. Replicability is a scientific concept—meaning the same experimental process yields similar results on different datasets. Reproducibility is a technical concept: it doesn’t guarantee scientific validity but ensures that the protocol is specified and shared in a way that allows others to reproduce the results.\nReproducibility is the guiding theme of this course: all concepts covered in the chapters contribute to it. Producing code and projects that follow community conventions and using version control contribute to making code more readable and documented—and therefore reproducible.\nHowever, achieving full reproducibility requires going further—by considering the concept of an execution environment. Code doesn’t run in a vacuum; it runs in an environment (e.g., personal computer, server), and those environments can differ greatly (OS, installed libraries, security policies, etc.). That’s why we must consider code portability—i.e., its ability to run as expected across different environments, which we explore in the dedicated chapter."
  },
  {
    "objectID": "chapters/introduction.html#facilitating-production-deployment",
    "href": "chapters/introduction.html#facilitating-production-deployment",
    "title": "Introduction",
    "section": "Facilitating Production Deployment",
    "text": "Facilitating Production Deployment\nFor a data science project to ultimately create value, it must be deployed in a usable form that reaches its audience. This implies two things:\n\nChoosing the right distribution format, i.e., one that best highlights the results to the intended users;\nTransitioning the project from its development environment to a production infrastructure, i.e., one that allows the project output to be robustly deployed and accessible on demand.\n\nIn the chapter Deploy and Showcase Your Data Science Project, we propose ways to address both needs. We present common output formats (API, app, automated report, website) that help make data science projects accessible, and the modern tools used to produce them.\nWe then explain the essential concepts of production infrastructure and demonstrate them with examples of deployments in a modern cloud environment.\nThis is, in a way, the reward for following best practices: once you’ve put in the effort to write quality code, properly version it, and make it portable, deploying your project becomes significantly easier."
  },
  {
    "objectID": "chapters/introduction.html#opening-the-door-to-industrialization",
    "href": "chapters/introduction.html#opening-the-door-to-industrialization",
    "title": "Introduction",
    "section": "Opening the Door to Industrialization",
    "text": "Opening the Door to Industrialization\nBy simplifying a project’s structure, you make it easier to scale. In data science, this may take the form of industrializing model training to select the “best” model from a much broader set—far beyond what an ad hoc approach would allow.\nHowever, every model learns from past data, and a model that works today may no longer be valid tomorrow. To account for this ever-changing reality, we will explore key principles of MLOps. Though the term is a buzzword, it represents a meaningful set of practices for data scientists, covered in the dedicated chapter."
  },
  {
    "objectID": "chapters/introduction.html#supplementary-chapters",
    "href": "chapters/introduction.html#supplementary-chapters",
    "title": "Introduction",
    "section": "Supplementary Chapters",
    "text": "Supplementary Chapters\nSeveral tools presented in this course, such as Git and Docker, require terminal usage and a basic understanding of how Linux systems work. In the chapter Demystifying the Linux Terminal for Autonomy, we cover the essential Linux knowledge a data scientist needs to deploy projects independently and apply development best practices."
  },
  {
    "objectID": "chapters/introduction.html#teaching-approach",
    "href": "chapters/introduction.html#teaching-approach",
    "title": "Introduction",
    "section": "Teaching Approach",
    "text": "Teaching Approach\nThe guiding principle of this course is that only practice—especially hands-on experience with real-world problems—can effectively develop understanding of computing concepts. As such, a large part of the course will consist of applying key ideas to concrete use cases. Each chapter will conclude with applications rooted in realistic data science problems.\nA running example illustrates how a reproducible project evolves by progressively applying the practices discussed throughout the course.\nFor the course evaluation, students will be asked to take a personal project—ideally already completed—and apply as many of the best practices introduced here as possible."
  },
  {
    "objectID": "chapters/introduction.html#programming-languages",
    "href": "chapters/introduction.html#programming-languages",
    "title": "Introduction",
    "section": "Programming Languages",
    "text": "Programming Languages\nThe principles presented in this course are mostly language-agnostic.\nThis is not just an editorial decision—we believe it’s central to the topic of best practices. Too often, language differences between development phases (e.g., R or Python) and production phases (e.g., Java) create artificial barriers that limit a data science project’s potential impact.\nBy contrast, when the different teams involved in a project’s lifecycle adopt a shared set of best practices, they also develop a shared vocabulary—greatly easing the deployment process.\nA compelling example is containerization: if the data scientist provides a Docker image as the output of their development work, and a data engineer handles its deployment, then the underlying programming language becomes largely irrelevant. While simplistic, this example captures the essence of how best practices enhance communication within a project.\nExamples in this course will primarily use Python. The main reason is that despite its shortcomings, Python is widely taught in both data science and computer science programs. It serves as a bridge between data users and developers—two essential roles in production workflows.\nThat said, the same principles can be applied with other languages, and we strongly encourage students to practice this transfer of skills."
  },
  {
    "objectID": "chapters/introduction.html#execution-environment",
    "href": "chapters/introduction.html#execution-environment",
    "title": "Introduction",
    "section": "Execution Environment",
    "text": "Execution Environment\nLike programming language, the principles in this course are agnostic to the infrastructure used to run the examples. It is not only possible but desirable to apply best practices to both solo projects on a personal computer and collaborative projects intended for production deployment.\nThat said, we have chosen the SSP Cloud platform as our reference environment throughout the course. Developed at Insee and available to students at statistical schools, it offers several advantages:\n\nStandardized development environment: SSP Cloud servers use a uniform configuration—specifically, the Debian Linux distribution—which ensures reproducibility across course examples;\nBuilt on a Kubernetes cluster, SSP Cloud offers robust infrastructure for automated deployment of potentially data-intensive applications—making it possible to simulate a true production environment;\nSSP Cloud follows modern data science infrastructure standards, enabling learners to internalize best practices organically:\n\nServices are run in containers configured via Docker images, which ensures strong reproducibility of deployments—at the cost of some initial complexity during development;\nThe platform is based on a cloud-native architecture, composed of modular software building blocks. This encourages strict separation of code, data, configuration, and execution environment—a major principle of good practice that will be revisited throughout the course.\n\n\nTo learn more about this platform, see this page."
  },
  {
    "objectID": "chapters/introduction.html#additional-resources",
    "href": "chapters/introduction.html#additional-resources",
    "title": "Introduction",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMIT’s Missing Semester"
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou’re probably most familiar with the Jupyter Notebook. While very convenient for writing exploratory code or sharing annotated code, we’ll see its limitations in collaborative or large-scale projects.↩︎\nWe will define this central concept more formally later. For now, you can think of it as an always-on environment designed to deliver data products—often in the form of a production server or a computing cluster that must remain continuously available.↩︎\nThe strong entanglement of best practices, reproducibility, and deployment actually made it hard for us to settle on a course title. Some names on our shortlist were “Best Practices in Data Science” or “Best Practices for Reproducibility in Data Science”. However, since best practices are a means and deployment is the end, we decided to emphasize the latter.↩︎\nA methodology focused on automating and integrating design and delivery workflows prior to deployment. Like best practices, this approach originated in software development but has become essential for data scientists.↩︎"
  },
  {
    "objectID": "chapters/mlops.html",
    "href": "chapters/mlops.html",
    "title": "Introduction aux enjeux du MLOps",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran.\nDans les chapitres précédents, nous avons vu qu’une majorité des projets data-driven restaient au stade de l’expérimentation, et qu’une des raisons pour expliquer ce phénomène était l’existence de frictions empêchant l’amélioration continue des projets. Dans le cadre des projets basés sur des modèles de machine learning, cette problématique devient encore plus cruciale : en supplément des enjeux sur le cycle de vie de la donnée intervient la dimension supplémentaire du cycle de vie des modèles. Parmi les principaux enjeux, une question souvent éludée dans les enseignements ou les nombreuses ressources en ligne sur le machine learning est la problématique des ré-entraînements périodiques, guidés par l’utilisation faite des modèles et les retours des utilisateurs, afin de maintenir à jour la base de connaissance des modèles et ainsi garantir leur pouvoir prédictif. Ce sujet du ré-entraînement des modèles rend les aller-retours entre les phases d’expérimentation et de production nécessairement fréquents. Pour faciliter la mise en place de pipelines favorisant ces boucles de rétroaction, une nouvelle approche a émergé : le MLOps, qui vise là encore à mobiliser les concepts et outils issus de l’approche DevOps tout en les adaptant au contexte et aux spécificités des projets de machine learning."
  },
  {
    "objectID": "chapters/mlops.html#du-devops-au-mlops",
    "href": "chapters/mlops.html#du-devops-au-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Du DevOps au MLOps",
    "text": "Du DevOps au MLOps\nL’approche MLOps s’est construite sur les bases de l’approche DevOps. En cela, on peut considérer qu’il s’agit simplement d’une extension de l’approche DevOps, développée pour répondre aux défis spécifiques liés à la gestion du cycle de vie des modèles de machine learning. Le MLOps intègre les principes de collaboration et d’automatisation propres au DevOps, mais prend également en compte tous les aspects liés aux données et aux modèles de machine learning.\n\n\n\n\n\n\n\nA mettre en regard à la boucle du DevOps\n\n\n\nLe MLOps implique l’automatisation des tâches telles que la gestion des données, le suivi des versions des modèles, leurs déploiements, ainsi que l’évaluation continue de la performance des modèles en production. De la même manière que le DevOps, le MLOps met l’accent sur la collaboration étroite entre les équipes de développement et d’administration système d’une part, ainsi que les équipes de data science d’autre part. Cette collaboration est clé pour garantir une communication efficace tout au long du cycle de vie du modèle de machine learning et fludifier le passage entre les étapes d’expérimentation et de passage en production."
  },
  {
    "objectID": "chapters/mlops.html#principes-du-mlops",
    "href": "chapters/mlops.html#principes-du-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Principes du MLOps",
    "text": "Principes du MLOps\nPuisque le MLOps est ainsi une extension des principes du DevOps aux enjeux du machine learning, les principes généraux sont les mêmes que ceux évoqués précédemment mais ceux-ci s’adaptent à la problématique de la gestion du cycle de vie d’un modèle:\n\nla reproductibilité : les résultats de chaque expérimentation, fructueuse comme infructueuse, doivent pouvoir être reproduits sans coût. Cela implique d’abord une certaine rigueur dans la gestion des packages, la gestion des environnements, la gestion des librairies système, le contrôle de version du code, etc.\nle contrôle de version: au-delà du simple suivi des versions du code, pour reproduire de manière identique les résultats d’un code c’est l’ensemble des inputs et paramètres influençant l’entraînement d’un modèle (données d’entraînement, hyper-paramètres, etc.) qui doivent être versionnées avec le modèle ;\nl’automatisation : afin de favoriser les boucles rétroactives d’amélioration continue, le cycle de vie du modèle (tests, build, validation, déploiement) doit être automatisé au maximum. Les outils issus de l’approche DevOps, en particulier l’intégration et déploiement continus (CI/CD), doivent être mobilisés ;\nla collaboration : valoriser une culture de travail collaborative autour des projets de ML, dans laquelle la communication au sein des équipes doit permettre de réduire le travail en silos et bénéficier des expertises des différents métiers parti prenantes d’un modèle (analystes, data engineers, devs..). Sur le plan technique, les outils MLOps utilisés doivent favoriser le travail collaboratif sur les données, le modèle et le code utilisés par le projet ;\nl’amélioration continue : une fois déployé, il est essentiel de s’assurer que le modèle fonctionne bien comme attendu en évaluant ses performances sur des données réelles à l’aide d’outils de monitoring en continu. Dans le cas d’une dégradation des performances dans le temps, un ré-entraînement périodique ou un entraînement en continu du modèle doivent être envisagés.\n\nPour plus de détails, voir Kreuzberger, Kühl, and Hirschl (2023)."
  },
  {
    "objectID": "chapters/mlops.html#entraînements-des-modèles",
    "href": "chapters/mlops.html#entraînements-des-modèles",
    "title": "Introduction aux enjeux du MLOps",
    "section": "1️⃣ Entraînements des modèles",
    "text": "1️⃣ Entraînements des modèles\nLa première étape d’un projet de machine learning correspond à tout ce que l’on effectue jusqu’à l’entraînement des premiers modèles. Cette étape est un processus itératif et fastidieux qui ne suit pas un développement linéaire : les méthodes de récupération des données peuvent être changeantes, le preprocessing peut varier, de même que la sélection des features pour le modèle (feature engineering), et les algorithmes testés peuvent être nombreux… On est donc aux antipodes des hypothèses habituelles de stabilité nécessaires à l’entraînement et la validité externe dans les enseignements de machine learning.\nGarder une trace de tous les essais effectués apparaît indispensable afin de savoir ce qui a fonctionné ou non. Le besoin d’archiver ne concerne pas que les métriques de performances associées à un jeu de paramètres. Ceux-ci ne sont qu’une partie des ingrédients nécessaires pour aboutir à une estimation. L’ensemble des inputs d’un processus de production (code, données, configuration logicielle, etc.) est également à conserver pour être en mesure de répliquer une expérimentation.\n\n\n\n\n\n\nLe tracking server de MLFlow, un environnement idéal pour archiver des expérimentations\n\n\n\nLa phase exploratoire est rendue très simple grâce au Tracking Server de MLFlow. Comme cela sera expliqué ultérieurement, lors de l’exécution d’un run, MLflow enregistre tout un tas de métadonnées qui permettent de retrouver toutes les informations relatives à ce run : la date, le hash du commit, les paramètres du modèle, le dataset utilisé, les métriques spécifiées, etc. Cela permet non seulement de comparer les différents essais réalisés, mais aussi d’être capable de reproduire un run passé.\n\n\nDe manière générale, cette phase exploratoire est réalisée par le data scientist ou le ML engineer dans des notebooks. Ces notebooks sont en effet parfaitement adaptés pour cette étape puisqu’ils permettent une grande flexibilité et sont particulièrement commodes pour effectuer des tests. En revanche, lorsque l’on souhaite aller plus loin et que l’on vise une mise en production de son projet, les notebooks ne sont plus adaptés, et cela pour diverses raisons :\n\nla collaboration est grandement limitée à cause d’une compatibilité très faible avec les outils de contrôle de version standard (notamment Git).\nl’automatisation de pipeline est beaucoup plus compliquée et peu lisible. Il existe certes des packages qui permettent d’automatiser des pipelines de notebooks comme Elyra par exemple, mais ce n’est clairement pas l’approche que nous vous recommandons car les scripts sont beaucoup moins usine à gaz.\nLes workflows sont souvent moins clairs, mal organisés (toutes les fonctions définies dans le même fichier affectant la lisibilité du code par exemple) voire peu reproductibles car les cellules sont rarement ordonnées de sorte à exécuter le code de manière linéaire.\nLes notebooks offrent généralement une modularité insuffisante lorsque l’on veut travailler avec des composants de machine learning complexes.\n\nToutes ces raisons nous amènent à vous conseiller de réduire au maximum votre utilisation de notebooks et de restreindre leur utilisation à la phase exploratoire ou à la diffusion de résultats/rapports. Passer le plus tôt possible à des scripts .py vous permettra de réduire le coût de la mise en production. Pour reprendre ce qui a déjà été évoqué dans le chapitre Architecture des projets, nous vous invitons à favoriser une structure modulaire de sorte à pouvoir industrialiser votre projet.\nUne autre spécificité pouvant impacter la mise en production concerne la manière dont l’entraînement est réalisé. Il existe pour cela 2 écoles qui ont chacune leurs avantages et désavantages : le batch training et l’online training.\n\nBatch training\nLe batch training est la manière usuelle d’entraîner un modèle de machine learning. Cette méthode consiste à entraîner son modèle sur un jeu de données fixe d’une seule traite. Le modèle est entraîné sur l’intégralité des données disponibles et les prédictions sont réalisées sur de nouvelles données. Cela signifie que le modèle n’est pas mis à jour une fois qu’il est entraîné, et qu’il est nécessaire de le ré-entraîner si l’on souhaite ajuster ses poids. Cette méthode est relativement simple à mettre en œuvre : il suffit d’entraîner le modèle une seule fois, de le déployer, puis de le ré-entraîner ultérieurement en cas de besoin. Cependant, cette simplicité comporte des inconvénients : le modèle reste statique, nécessitant un ré-entraînement fréquent pour intégrer de nouvelles données. Par exemple, dans le cas de la détection de spams, si un nouveau type de spam apparaît, le modèle entraîné en batch ne sera pas capable de le détecter sans un ré-entraînement complet. De plus, cette méthode peut rapidement exiger une grande quantité de mémoire en fonction de la taille du jeu de données, ce qui peut poser des contraintes sur l’infrastructure et prolonger considérablement le temps d’entraînement.\n\n\nOnline training\nL’online training se présente comme l’antithèse du batch training, car il se déroule de manière incrémentale. Dans ce mode d’entraînement, de petits lots de données sont envoyés séquentiellement à l’algorithme, ce qui permet à celui-ci de mettre à jour ses poids à chaque nouvelle donnée reçue. Cette approche permet au modèle de détecter efficacement les variations dans les données en temps réel. Il est toutefois crucial de bien ajuster le learning rate afin d’éviter que le modèle oublie les informations apprises sur les données précédentes. L’un des principaux avantages de cette méthode est sa capacité à permettre un entraînement continu même lorsque le modèle est en production, ce qui se traduit par une réduction des coûts computationnels. De plus, l’online training est particulièrement adapté aux situations où les données d’entrée évoluent fréquemment, comme dans le cas des prédictions de cours de bourse. Cependant, sa mise en œuvre dans un contexte de production est bien plus complexe que celle du batch training, et les frameworks traditionnels de machine learning tels que Scikit-learn, PyTorch, TensorFlow et Keras ne sont pas compatibles avec cette approche.\n\n\nDistribuer l’optimisation des hyperparamètres\nUne autre spécificité des modèles de machine learning réside dans le nombre important d’hyperparamètres à optimiser, lesquels peuvent sensiblement impacter les performances du modèle. L’approche standard pour réaliser cette optimisation est ce qu’on appelle un Grid Search. Il s’agit simplement de lister toutes les combinaisons d’hyperparamètres à tester et d’entraîner successivement des modèles avec ces combinaisons prédéfinies. Il n’est pas difficile de comprendre que cette technique est très coûteuse en temps de calcul lorsque le nombre d’hyperparamètres à optimiser et leurs modalités à tester sont élevés. Cependant, cette optimisation est indispensable pour entraîner le meilleur modèle pour notre tâche, et si s’inspirer de la littérature est crucial pour limiter le domaine d’optimisation de nos hyperparamètres, réaliser un Grid Search est une étape incontournable.\nAinsi, pour s’inscrire dans l’approche du MLOps, une bonne méthode est d’automatiser cette optimisation des hyperparamètres en la distribuant sur un cluster lorsqu’on dispose de l’infrastructure adéquate. L’idée est de créer des processus indépendants, chacun liés à une combinaison de nos hyperparamètres, et d’entraîner notre modèle sur ceux-ci puis d’enregister les informations à archiver dans un environnement adéquat, par exemple dans MLFlow.\nIl existe un moteur de workflow populaire pour orchestrer des tâches parallèles sur un cluster Kubernetes : Argo Workflow. Le principe est de définir un workflow dans lequel chaque étape correspond à un conteneur contenant uniquement ce qui est strictement nécessaire à l’exécution de cette étape. Ainsi, on s’approche de la perfection en ce qui concerne la reproductibilité, car on maîtrise totalement les installations nécessaires à l’exécution de notre entraînement. Un workflow à plusieurs étapes peut ainsi être modélisé comme un graphe acyclique orienté, et l’exemple ci-dessous représente un cas d’optimisation d’hyperparamètres :\n\n\n\nWorkflow d’optimisation d’hyperparamètres en parallèle\n\n\nCette approche permet d’exécuter facilement en parallèle des tâches intensives en calcul de manière totalement reproductible. Évidemment, l’utilisation de tels workflows ne se limite pas à l’optimisation d’hyperparamètres mais peut également être utilisée pour le preprocessing de données, la création de pipelines d’ETL, etc. D’ailleurs, à l’origine, ces outils ont été pensé pour ces tâches et permettent ainsi de définir un processus de données comme un ensemble de transformations sous la forme de diagramme acyclique dirigé (DAG)."
  },
  {
    "objectID": "chapters/mlops.html#servir-un-modèle-ml-à-des-utilisateurs",
    "href": "chapters/mlops.html#servir-un-modèle-ml-à-des-utilisateurs",
    "title": "Introduction aux enjeux du MLOps",
    "section": "2️⃣ Servir un modèle ML à des utilisateurs",
    "text": "2️⃣ Servir un modèle ML à des utilisateurs\nUne partie très importante, parfois négligée, des projets de machine learning est la mise à disposition des modèles entraînés à d’autres utilisateurs. Puisque vous avez parfaitement suivi les différents chapitres de ce cours, votre projet est en théorie totalement reproductible. Une manière triviale de transmettre le modèle que vous avez sélectionné serait de partager votre code et toutes les informations nécessaires pour qu’une personne tierce ré-entraîne votre modèle de son côté. Évidemment, ce procédé n’est pas optimal, car il suppose que tous les utilisateurs disposent des ressources/infrastructures/connaissances nécessaires pour réaliser l’entraînement.\nL’objectif est donc de mettre à disposition votre modèle de manière simple et efficace. Pour cela, plusieurs possibilités s’offrent à vous en fonction de votre projet, et il est important de se poser quelques questions préalables :\n\nQuel format est le plus pertinent pour mettre à disposition des utilisateurs ?\nLes prédictions du modèle doivent-elles être réalisées par lots (batch) ou en temps réel (online) ?\nQuelle infrastructure utiliser pour déployer notre modèle de machine learning ?\n\nDans le cadre de ce cours, nous avons choisi d’utiliser une API REST pour mettre à disposition un modèle de machine learning. Cela nous semble être la méthode la plus adaptée dans une grande majorité des cas, car elle répond à plusieurs critères :\n\nSimplicité : les API REST permettent de créer une porte d’entrée qui peut cacher la complexité sous-jacente du modèle, facilitant ainsi sa mise à disposition.\nStandardisation : l’un des principaux avantages des API REST est qu’elles reposent sur le standard HTTP. Cela signifie qu’elles sont agnostiques au langage de programmation utilisé et que les requêtes peuvent être réalisées en XML, JSON, HTML, etc.\nModularité : le client et le serveur sont indépendants. En d’autres termes, le stockage des données, l’interface utilisateur ou encore la gestion du modèle sont complètement séparés de la mise à disposition (le serveur).\nPassage à l’échelle : la séparation entre le serveur et le client permet aux API REST d’être très flexibles et facilite le passage à l’échelle (scalability). Elles peuvent ainsi s’adapter à la charge de requêtes concurrentes.\n\nL’exposition d’un modèle de machine learning peut être résumée par le schéma suivant :\n\n\n\nExposer un modèle de ML via une API\n\n\nComme le montre le schéma, l’API est exécutée dans un conteneur afin de garantir un environnement totalement autonome et isolé. Seules les dépendances nécessaires à l’exécution du modèle et au fonctionnement de l’API ne sont intégrées à ce conteneur. Travailler avec des images docker légères présente plusieurs avantages. Tout d’abord, créer une image ne contenant que le strict nécessaire au fonctionnement de votre application permet justement de savoir ce qui est absolument indispensable et ce qui est superflu. De plus, plus votre image est légère, plus son temps de téléchargement depuis votre Hub d’images (e.g. Dockerhub) sera rapide à chaque création de conteneur de votre application. Les conteneurs ont l’avantage d’être totalement portables et offrent la possibilité de mettre à l’échelle votre application de manière simple et efficace. Par exemple, si l’on imagine que vous avez déployé votre modèle et que vous souhaitez le requêter un grand nombre de fois dans un laps de temps court, il est alors préférable de créer plusieurs instances de votre application pour que les calculs puissent être effectués en parallèle. L’avantage de procéder de cette manière est qu’une fois qu’on a créé l’image sous-jacente à notre application, il est ensuite très simple de créer une multitude de conteneurs (replicas) toutes basées sur l’image en question.\nPour tout ce qui concerne le déploiement de votre application, vous pouvez vous référer au chapitre Mise en production. Techniquement, il n’y a aucune difficulté supplémentaire lorsque l’on veut avoir une approche MLOps lors de cette étape. L’unique subtilité à avoir en tête est que l’on souhaite maintenant faire communiquer notre application avec MLflow. En effet, chaque déploiement est basé sur une version particulière du modèle et il est nécessaire de renseigner quelques informations afin de récupérer le bon modèle au sein de notre entrepôt de modèle. Comme pour tout déploiement sous Kubernetes, il faut tout d’abord créer les 3 fichiers YAML : deployment.yaml, service.yaml, ingress.yaml. Ensuite, comme vous pouvez le voir sur le schéma, notre API doit pouvoir être reliée à MLflow qui lui-même a besoin d’être connecté à un espace de stockage (ici s3/MinIO) qui contient l’entrepôt des modèles. Pour cela, dans le fichier deployment.yaml, on rajoute simplement quelques variables d’environnement qui nous permettent de créer de lien à savoir :\n\nMLFLOW_S3_ENDPOINT_URL : L’URL de l’endpoint S3 utilisé par MLflow pour stocker les données (et modèles)\nMLFLOW_TRACKING_URI : L’URI du serveur de suivi MLflow, qui spécifie où les informations concernant les modèles sont stockées.\nAWS_ACCESS_KEY_ID : L’identifiant d’accès utilisé pour authentifier l’accès aux services de stockage s3.\nAWS_SECRET_ACCESS_KEY : La clé secrète utilisée pour authentifier l’accès aux services de stockage s3.\nAWS_DEFAULT_REGION : Identifie la région S3 pour laquelle vous souhaitez envoyer les demandes aux serveurs. \n\nPour faciliter le déploiement continu (voir chapitre Mise en production), il est conseillé de rajouter des variables d’environnement spécifiant la version du modèle à déployer ainsi que le nom du modèle à déployer. En effet, en spécifiant ces valeurs dans le fichier deployment.yaml, cela va permettre de déclencher un nouveau déploiement dès lors que l’on modifiera ces valeurs.\nIl est bon de noter que MLflow permet également de déployer directement un modèle MLflow. Vous pouvez aller regarder la documentation si cela vous intéresse. Cette option est relativement récente et pas encore tout à fait mature mais se base sur les mêmes technologies que celles présentées dans ce cours (Kubernetes, S3, etc.). C’est pour cela que nous avons préféré détailler le développement de notre propre API en utilisant le framework FastAPI, qui est devenu le standard pour le développement d’API en Python.\n\nDéployer sur Kubernetes (plutot dans chap mise en prod ?) \nBatch vs real-time prediction"
  },
  {
    "objectID": "chapters/mlops.html#observabilité-en-temps-réel-dun-modèle-de-ml",
    "href": "chapters/mlops.html#observabilité-en-temps-réel-dun-modèle-de-ml",
    "title": "Introduction aux enjeux du MLOps",
    "section": "3️⃣ Observabilité en temps réel d’un modèle de ML",
    "text": "3️⃣ Observabilité en temps réel d’un modèle de ML\nUne fois la modélisation réalisée, le modèle entraîné, optimisé et mis à disposition des utilisateurs grâce à un déploiement sur un serveur, on peut considérer que le travail est fini. Du point de vue du data-scientist stricto sensu, cela peut être le cas, puisque l’on considère souvent que le domaine du data-scientist s’arrête à la sélection du modèle à déployer, le déploiement étant réalisé par ce qu’on appelle les data-engineers. Pourtant, une fois déployé dans un environnement de production, le modèle n’a pas réalisé l’intégralité de son cycle de vie. En production, le cycle de vie d’un modèle de machine learning suivant l’approche MLOps peut être schématisé de la manière suivante :\n\n\n\n\n\n\nFigure 1: Source : martinfowler.com\n\n\n\nOn retrouve les différentes composantes du MLOps avec les données (DataOps), les modèles (ModelOps) et le code (DevOps). Ces composantes rendent le cycle de vie d’un modèle de machine learning complexe impliquant plusieurs parties prenantes autour du projet. En règle générale, on observe trois parties prenantes principales :\n\nData-scientists/Data-engineers\nIT/DevOps\nÉquipes métiers\n\nQuelques fois, les data-scientists peuvent être intégrés aux équipes métiers et les data-engineers aux équipes IT. Cela peut simplifier les échanges entre les deux équipes, mais cela peut également entraîner un travail en silos et cloisonner les deux équipes aux expertises, attentes et vocabulaires très différents. Or, la communication est primordiale pour permettre une bonne gestion du cycle de vie du modèle de machine learning et notamment pour surveiller le modèle dans son environnement de production.\nIl est extrêmement important de surveiller comment le modèle se comporte une fois déployé pour s’assurer que les résultats renvoyés sont conformes aux attentes. Cela permet d’anticiper des changements dans les données, une baisse des performances ou encore d’améliorer le modèle de manière continue. Il est également nécessaire que notre modèle soit toujours accessible, que notre application soit bien dimensionnée, etc. C’est pour cela que la surveillance (monitoring) d’un modèle de machine learning est un enjeu capital dans l’approche MLOps.\nLe terme surveillance peut renvoyer à plusieurs définitions en fonction de l’équipe dans laquelle l’on se situe. Pour une personne travaillant dans l’équipe informatique, surveiller une application signifie vérifier sa validité technique. Elle va donc s’assurer que la latence n’est pas trop élevée, que la mémoire est suffisante ou encore que le stockage sur le disque est bien proportionné. Pour un data-scientist ou une personne travaillant dans l’équipe métier, ce qui va l’intéresser est la surveillance du modèle d’un point de vue méthodologique. Malheureusement, il n’est pas souvent évident que contrôler la performance en temps réel d’un modèle de machine learning. Il est rare que l’on connaisse la vraie valeur au moment de la prédiction du modèle (sinon on ne s’embêterait pas à construire un modèle !) et on ne peut pas vraiment savoir s’il s’est trompé ou non. Il est donc commun d’utiliser des proxies pour anticiper une potentielle dégradation de la performance de notre modèle. On distingue généralement 2 principaux types de dégradation d’un modèle de machine learning : le data drift et le concept drift.\n\n\n\nSource : whylabs.ai\n\n\n\nData drift\nOn parle de data drift lorsque l’on observe un changement de distribution dans les données utilisées en entrée du modèle. En d’autres termes, il y a data drift lorsque les données utilisées lors de l’entraînement sont sensiblement différentes des données utilisées lors de l’inférence en production. Imaginons que vous souhaitez repérer des habitations à partir d’images satellites. Vous entraînez votre modèle sur des données datant par exemple de février 2022, et une fois en production vous essayer de repérer les habitations tous les mois suivants. Vous constatez finalement durant l’été que votre modèle n’est plus du tout aussi performant puisque les images satellites de juillet diffèrent fortement de celle de février. La distribution des données d’entraînement n’est plus proche de celle d’inférence, \\(P_{train}(X) \\neq P_{inference}(X)\\). Les data drifts apparaissent dès lors que les propriétés statistiques des données changent et cela peut venir de plusieurs facteurs en fonction de votre modèle : changements de comportement, dynamique de marché, nouvelles réglementations politiques, problème de qualité des données, etc. Il n’est pas si simple de détecter rapidement des data drifts, cela suppose de surveiller de manière continue la distribution des données en entrée et en sortie de votre modèle sur un certain laps de temps et d’identifier quand celles-ci diffèrent significativement de la distribution des données d’entraînement. Pour obtenir une idée visuelle, on peut créer des représentations graphiques comme des histogrammes pour comparer les distributions à plusieurs périodes dans le temps, voire des boîtes à moustaches. On peut aussi calculer des métriques, qui seront plus simples d’utilisation si l’on souhaite automatiser un système d’alerte, comme des distances entre distributions (distance de Bhattacharyya, divergence de Kullback-Leibler, distance de Hellinger) ou effectuer des tests statistiques (Test de Kolmogorov-Smirnov, Test du χ²). Pour résumer, la détection d’un data drift peut s’effectuer en plusieurs étapes :\n\nDéfinition d’une référence : on définit la distribution de référence (e.g. celle utilisée lors de l’entraînement).\nDéfinition de seuils : on détermine en dessous de quelles valeurs de nos métriques cela peut être considéré comme un data drift.\nSurveillance continue : soit en temps réel, soit de manière périodique (relativement courte), on compare nos distributions et on calcule les métriques définies préalablement.\nAlerte et correction : on met en place un système d’alerte automatique dès lors que nos métriques indiquent la présence d’un data drift, puis on agit en conséquence (ré-entraînement sur de nouvelles données, ajustement des paramètres du modèle, etc.).\n\n\n\nConcept drift\nOn parle de concept drift lorsque l’on observe un changement dans la relation statistique entre les features (\\(X\\)) et la variable à prédire (\\(Y\\)) au cours du temps. En termes mathématiques, on considère qu’il y a un concept drift dès lors que \\(P_{train}(Y|X) \\neq P_{inference}(Y|X)\\) alors même que \\(P_{train}(X) = P_{inference}(X)\\). Cela peut avoir un impact important sur les performances du modèle si la relation diffère fortement. Par exemple, un modèle de prédiction de la demande de masques chirurgicaux entraîné sur des données avant la pandémie de COVID-19 deviendra totalement inadéquat pour effectuer des prédictions lors de cette pandémie, car il y a eu un changement dans la relation entre la demande de masques chirurgicaux et les features utilisées pour prédire cette demande. Dans le cas d’un concept drift, on sera plus tenté de surveiller des métriques de performance pour repérer une potentielle anomalie. Dans le cas où l’on possède un jeu de test gold standard, alors on sera en capacité de calculer de nombreuses métriques usuelles de machine learning (à savoir l’accuracy, la precision, le recall ou le F1-score pour des problèmes de classification, et toutes les métriques d’erreurs - MSE, RMSE, MAE, … - pour les problèmes de régression) et repérer une baisse tendancielle ou brutale des performances. Dans le cas où l’on n’a pas de jeu de test gold standard, on s’attachera à déterminer des proxys qui peuvent être liés à des métriques de performance ou alors utiliser des algorithmes de détection de changement dans le flux de données (Drift Detection Method, Early Drift Detection Method, Adaptive Windowing)."
  },
  {
    "objectID": "chapters/mlops.html#ré-entraînement-dun-modèle-ml",
    "href": "chapters/mlops.html#ré-entraînement-dun-modèle-ml",
    "title": "Introduction aux enjeux du MLOps",
    "section": "4️⃣ Ré-entraînement d’un modèle ML",
    "text": "4️⃣ Ré-entraînement d’un modèle ML\nDès lors que l’on a constaté une baisse de la performance de notre modèle grâce à notre surveillance fine, il faut ensuite pallier au problème et redéployer un modèle avec des performances satisfaisantes. On est donc à la fin du cycle de vie de notre modèle, ce qui va nous reconduire au début du cycle pour un nouveau modèle comme l’illustre la figure Figure 1. Le ré-entraînement est partie intégrante d’un projet de machine learning dès lors que celui-ci est mis en production. Il existe plusieurs méthodes pour ré-entraîner de la plus basique à la plus MLOps-compatible.\nLa méthode classique est de réaliser un nouvel entraînement from scratch en ajoutant les nouvelles données à notre disposition dans le jeu d’entraînement. Cela permet au modèle de connaître les dernières relations entre les features et la variable à prédire. Cependant, ré-entraîner un modèle peut être particulièrement coûteux lorsque l’on travaille sur de gros modèles dont les ressources computationnelles nécessaires sont importantes. Il est aussi possible de fine-tuner un modèle déjà pré-entraîné. Dans ce cas-là, on n’a pas besoin de repartir de zéro, on repart des poids optimisés lors du premier entraînement et on les ré-optimise en utilisant les nouvelles données à notre disposition. Cette méthode est naturellement beaucoup moins longue à réaliser et est moins coûteuse, notamment lorsque la quantité de nouvelles données est faible par rapport à la quantité des données utilisées lors du premier entraînement.\nL’approche MLOps consiste à automatiser ce ré-entraînement, qu’on appelle également entraînement continu, de sorte à obtenir un cycle de vie totalement automatisé. En effet, le ré-entraînement est fondamental pour s’assurer que le modèle de machine learning est constamment en train de fournir des prédictions cohérentes, tout en minimisant les interventions manuelles. L’objectif est donc de créer un processus qui lance de nouveaux entraînements de manière automatique en prenant en compte les dernières informations disponibles. Les entraînements peuvent être déclenchés soit de manière périodique (tous les lundis à 2h du matin), dès lors qu’une alerte a été déclenchée dans notre système de monitoring, ou bien dès qu’on a une quantité de nouvelles données suffisante pour réaliser un online training par exemple.\nL’utilisation d’outils d’orchestration de workflow comme Argo Workflow ou Airflow est donc indispensable pour réaliser cette automatisation de manière pertinente."
  },
  {
    "objectID": "chapters/mlops.html#défis-organisationnels-du-mlops",
    "href": "chapters/mlops.html#défis-organisationnels-du-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "5️⃣ Défis organisationnels du MLOps",
    "text": "5️⃣ Défis organisationnels du MLOps\nOutre les spécificités techniques précédemment explicitées, le MLOps présente également plusieurs défis en termes organisationnels et managériaux. En effet, dans la plupart des organisations, les équipes data transverses ou intégrées dans différents départements métier sont relativement jeunes et peuvent manquer de ressources qualifiées pour gérer le déploiement et le maintien en condition opérationnelle de systèmes ML complexes. Ces équipes se composent principalement de data scientists qui se concentrent sur le développement des modèles de machine learning, mais n’ont pas les compétences nécessaires pour gérer le déploiement et la maintenance d’applications complètes.\nDe plus, les équipes data évoluent encore trop souvent en silo, sans communiquer avec les différentes équipes techniques avec lesquelles elles devraient interagir pour mettre en production leurs modèles. Or ces équipes techniques, souvent composées d’informaticiens/développeurs, ne connaissent pas forcément les spécificités des modèles de machine learning, accentuant d’autant plus la nécessité d’une communication continue entre ces équipes.\nUne autre difficulté pouvant intervenir lors du déploiement est la différence d’environnements utilisés ainsi que les différents langages connus entre les deux équipes. Il n’est pas rare que les data-scientists développent des modèles en Python tandis que les équipes informatiques gèrent leur serveur de production dans un langage différent, comme Java par exemple.\nAinsi, l’approche MLOps engendre aussi des défis managériaux qui impliquent de faire converger les compétences entre les équipes afin de fluidifier la mise en production de modèles de machine learning.\n\n\n\nGouvernance d’un projet de machine learning"
  },
  {
    "objectID": "chapters/mlops.html#pourquoi-mlflow",
    "href": "chapters/mlops.html#pourquoi-mlflow",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Pourquoi MLflow ?",
    "text": "Pourquoi MLflow ?\nIl existe aujourd’hui de nombreux outils pour orchestrer des tâches et des pipelines de données. Parmi les plus populaires (selon leur ⭐ GitHub), on peut citer Airflow, Luigi, MLflow, Argo Workflow, Prefect ou encore Kubeflow, BentoML… Il est difficile d’affirmer s’il y en a un meilleur qu’un autre ; en réalité, votre choix dépend surtout de votre infrastructure informatique et de votre projet. En l’occurrence ici, nous avons fait le choix d’utiliser MLflow pour sa simplicité d’utilisation grâce à une interface web bien faite, parce qu’il intègre l’ensemble du cycle de vie d’un modèle et également parce qu’il s’intègre très bien avec Kubernetes. De plus, il est présent dans le catalogue du SSP Cloud, ce qui simplifie grandement son installation. Afin d’intégrer les dimensions d’intégration et de déploiement continus, nous utiliserons également Argo CD et Argo Workflow dans la boucle. Ceux-ci sont privilégiés par rapport à Airflow car ils sont optimisés pour les clusters Kubernetes qui représentent aujourd’hui la norme des cloud en ligne ou on premise.\n\n\n\nVue d’ensemble de MLFlow. Source: https://dzlab.github.io\n\n\nMLflow est une plateforme qui permet d’optimiser le développement du cycle de vie d’un modèle de machine learning. Elle permet de suivre en détail les différentes expérimentations, de packager son code pour garantir la reproductibilité, et de servir un modèle à des utilisateurs. MLFlow possède également une API qui permet d’être compatible avec la majorité des librairies de machine learning (PyTorch, Scikit-learn, XGBoost, etc.) mais également différents langages (Python, R et Java)."
  },
  {
    "objectID": "chapters/mlops.html#les-projets-mlflow",
    "href": "chapters/mlops.html#les-projets-mlflow",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Les projets MLflow",
    "text": "Les projets MLflow\nMLflow propose un format pour packager son projet de data science afin de favoriser la réutilisation et la reproductibilité du code. Ce format s’appelle tout simplement MLflow Project. Concrètement, un MLflow project n’est rien d’autre qu’un répertoire contenant le code et les ressources nécessaires (données, fichiers de configuration…) pour l’exécution de votre projet. Il est résumé par un fichier MLproject qui liste les différentes commandes pour exécuter une pipeline ainsi que les dépendances nécessaires. En général, un projet MLflow a la structure suivante :\nProjet_ML/\n├── artifacts/\n│   ├── model.bin\n│   └── train_text.txt\n├── code/\n│   ├── main.py\n│   └── preprocessing.py\n├── MLmodel\n├── conda.yaml\n├── python_env.yaml\n├── python_model.pkl\n└── requirements.txt\nEn plus de packager son projet, MLflow permet également de packager son modèle, quel que soit la librairie de machine learning sous-jacente utilisée (parmi celles compatibles avec MLflow, c’est-à-dire toutes les librairies que vous utilisez !). Ainsi, deux modèles entraînés avec des librairies différentes, disons PyTorch et Keras, peuvent être déployés et requêtés de la même manière grâce à cette surcouche ajoutée par MLflow.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIl est également possible de packager son propre modèle personnalisé ! Pour cela vous pouvez suivre le tutoriel présent dans la documentation.\n\n\nAutrement dit, un projet MLFlow archive l’ensemble des éléments nécessaires pour reproduire un entraînement donné d’un modèle ou pour réutiliser celui-ci à tout moment."
  },
  {
    "objectID": "chapters/mlops.html#le-serveur-de-suivi-tracking-server",
    "href": "chapters/mlops.html#le-serveur-de-suivi-tracking-server",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Le serveur de suivi (tracking server)",
    "text": "Le serveur de suivi (tracking server)\nLe tracking server est le lieu où sont archivés l’ensemble des entraînements d’un modèle. Attention, il ne s’agit pas du serveur sur lequel les modèles sont entraînés mais de celui où les entraînements sont archivés après avoir eu lieu. Au-delà de stocker seulement les poids d’un modèle, c’est l’ensemble de l’environnement nécessaire qui peut être retrouvé dans ce serveur.\n\n\n\n\n\nTechniquement, cela prend la forme d’une API et d’une interface utilisateur pour enregistrer les paramètres, les versions du code, les métriques ou encore les artefacts associés à un entraînement.\n\n\n\nSource: Databricks\n\n\nEn arrière plan, MLFlow va enregistrer tout ceci dans un bucket S3. Néanmoins, l’utilisateur n’aura pas à se soucier de cela puisque c’est MLFLow qui fera l’interface entre l’utilisateur et le système de stockage. Avec son API, MLFLow fournit même une manière simplifiée de récupérer ces objets archivés, par exemple avec un code prenant la forme\nimport mlflow\nmodel = mlflow.pyfunc.load_model(model_uri=\"runs:/d16076a3ec534311817565e6527539c\")\nLe tracking server est très utile pour comparer les différentes expérimentations que vous avez effectuées, pour les stocker et également pour être capable de les reproduire. En effet, chaque run sauvegarde la source des données utilisées, mais également le commit sur lequel le run est basé.\nA la manière de Git qui permet d’identifier chaque moment de l’histoire d’un projet à partir d’un identifiant unique, MLFlow permet de récupérer chaque entraînement d’un modèle à partir d’un SHA. Néanmoins, en pratique, certains modèles ont un statut à part, notamment ceux en production."
  },
  {
    "objectID": "chapters/mlops.html#lentrepôt-de-modèles-model-registry",
    "href": "chapters/mlops.html#lentrepôt-de-modèles-model-registry",
    "title": "Introduction aux enjeux du MLOps",
    "section": "L’entrepôt de modèles (model registry)",
    "text": "L’entrepôt de modèles (model registry)\nUne fois que l’on a effectué différentes expérimentations et pu sélectionner les modèles qui nous satisfont, il est temps de passer à l’étape suivante du cycle de vie d’un modèle. En effet, le modèle choisi doit ensuite pouvoir passer dans un environnement de production ou de pré-production. Or, connaître l’état d’un modèle dans son cycle de vie nécessite une organisation très rigoureuse et n’est pas si aisé. MLflow a développé une fonctionnalité qui permet justement de simplifier cette gestion des versions des modèles grâce à son Model Registry. Cet entrepôt permet d’ajouter des tags et des alias à nos modèles pour définir leur position dans leur cycle de vie et ainsi pouvoir les récupérer de manière efficace.\nDe manière générale, un modèle de machine learning passe par 4 stades qu’il est nécessaire de connaître en tout temps :\n\nExpérimental\nEn évaluation\nEn production\nArchivé"
  },
  {
    "objectID": "chapters/mlops.html#mlflow-en-résumé",
    "href": "chapters/mlops.html#mlflow-en-résumé",
    "title": "Introduction aux enjeux du MLOps",
    "section": "MLflow en résumé",
    "text": "MLflow en résumé\nMLflow est donc un projet open-source qui fournit une plateforme pour suivre le cycle de vie d’un modèle de machine learning de bout en bout. Ce n’est pas le seul outil disponible et il n’est peut-être pas le plus adapté à certains de vos projets précis. En revanche, il présente selon nous plusieurs avantages, en premier lieu sa prise en main très simple et sa capacité à répondre aux besoins de l’approche MLOps. Il faut garder à l’esprit que cet environnement est encore très récent et que de nouveaux projets open-source émergent chaque jour, donc il est nécessaire de rester à jour sur les dernières évolutions.\nPour résumer, MLFlow permet :\n\nde simplifier le suivi de l’entraînement des modèles de machine learning grâce à son API et à son tracking server\nd’intégrer les principaux frameworks de machine learning de manière simple\nd’intégrer son propre framework si besoin\nde standardiser son script d’entraînement et donc de pouvoir l’industrialiser, pour réaliser un fine-tuning des hyperparamètres, par exemple\nde packager ses modèles, de sorte à pouvoir les requêter de manière simple et harmonisée entre les différents frameworks\nde stocker ses modèles de manière pertinente en leur affectant des tags et en favorisant le suivi de leur cycle de vie"
  },
  {
    "objectID": "chapters/projects-architecture.html",
    "href": "chapters/projects-architecture.html",
    "title": "Structure des projets",
    "section": "",
    "text": "Dérouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein écran."
  },
  {
    "objectID": "chapters/projects-architecture.html#demonstration-by-example",
    "href": "chapters/projects-architecture.html#demonstration-by-example",
    "title": "Structure des projets",
    "section": "Demonstration by Example",
    "text": "Demonstration by Example\nHere’s an example of a project structure that might bring back memories:\n├── report.qmd\n├── correlation.png\n├── data.csv\n├── data2.csv\n├── fig1.png\n├── figure 2 (copy).png\n├── report.pdf\n├── partial data.csv\n├── script.R\n└── script_final.py\nSource : eliocamp.github.io\nThe following project structure makes it difficult to understand the project. Some key questions arise:\n\nWhat are the input data to the pipeline?\nIn what order are the intermediate data generated?\nWhat is the purpose of the graphical outputs?\nAre all the scripts actually used in this project?\n\nBy structuring the folder using simple rules — for example, organizing it into inputs and outputs folders — we can significantly improve the project’s readability.\n├── README.md\n├── .gitignore\n├── data\n│   ├── raw\n│   │   ├── data.csv\n│   │   └── data2.csv\n│   └── derived\n│       └── partial data.csv\n├── src\n│   ├── script.py\n│   ├── script_final.py\n│   └── report.qmd\n└── output\n    ├── fig1.png\n    ├── figure 2 (copy).png\n    ├── figure10.png\n    ├── correlation.png\n    └── report.pdf\n\n\n\n\n\n\nNote\n\n\n\nSince Git is a prerequisite, every project includes a .gitignore file (this is especially important when working with data that must not end up on Github or Gitlab).\nA project also includes a README.md file at the root — we will come back to this later.\nA project using continuous integration will also include specific files:\n\nif you’re using Gitlab, the instructions are stored in the gitlab-ci.yml file;\nif you’re using Github, this happens in the .github/workflows directory.\n\n\n\nBy simply changing the file names, the project structure becomes much more readable:\n├── README.md\n├── .gitignore\n├── data\n│   ├── raw\n│   │   ├── dpe_logement_202103.csv\n│   │   └── dpe_logement_202003.csv\n│   └── derived\n│       └── dpe_logement_merged_preprocessed.csv\n├── src\n│   ├── preprocessing.py\n│   ├── generate_plots.py\n│   └── report.qmd\n└── output\n    ├── histogram_energy_diagnostic.png\n    ├── barplot_consumption_pcs.png\n    ├── correlation_matrix.png\n    └── report.pdf\nNow, the type of input data to the pipeline is clear, and the relationship between scripts, intermediate data, and outputs is transparent."
  },
  {
    "objectID": "chapters/projects-architecture.html#separating-code-data-and-execution-environment-storage",
    "href": "chapters/projects-architecture.html#separating-code-data-and-execution-environment-storage",
    "title": "Structure des projets",
    "section": "Separating Code, Data, and Execution Environment Storage",
    "text": "Separating Code, Data, and Execution Environment Storage\nSeparating the storage of code, data, and the execution environment is important for several reasons:\n\nData Security\nBy separating data from code, it’s harder to accidentally access sensitive information.\nConsistency and Portability\nAn isolated environment ensures that the code runs reproducibly, regardless of the host machine.\nModularity and Flexibility\nYou can adapt or update components (code, data, environment) independently.\n\nThe next chapter will focus on dependency management. It will show how to link the environment and code to improve project portability."
  },
  {
    "objectID": "chapters/projects-architecture.html#sensitive-configurations-secrets-and-tokens",
    "href": "chapters/projects-architecture.html#sensitive-configurations-secrets-and-tokens",
    "title": "Structure des projets",
    "section": "Sensitive Configurations: Secrets and Tokens",
    "text": "Sensitive Configurations: Secrets and Tokens\nRunning code may depend on personal parameters (authentication tokens, passwords…). They should never appear in shared source code.\n✅ Best practice: store these configurations in a separate file, not versioned (.gitignore), in YAML format — more readable than JSON.\n\nExample secrets.yaml\ntoken:\n    api_insee: \"toto\"\n    api_github: \"tokengh\"\npwd:\n    base_pg: \"monmotdepasse\"\n\n\nReading in Python\nimport yaml\n\nwith open('secrets.yaml') as f:\n    secrets = yaml.safe_load(f)\n\n# using the secret\njeton_insee = secrets['token']['api_insee']\nThis mechanism turns the file into a Python dictionary that is easy to navigate.\n\n\n\n\n\n\nTests unitaires\n\n\n\n\n\nLes tests unitaires sont des tests automatisés qui vérifient le bon fonctionnement d’une unité de code, comme une fonction ou une méthode. L’objectif est de s’assurer que chaque unité de code fonctionne correctement avant d’être intégrée dans le reste du programme.\nLes tests unitaires sont utiles lorsqu’on travaille sur un code de taille conséquente ou lorsqu’on partage son code à d’autres personnes, car ils permettent de s’assurer que les modifications apportées ne créent pas de nouvelles erreurs.\nEn Python, on peut utiliser le package unittest pour écrire des tests unitaires. Voici un exemple tiré de ce site :\n# fichier test_str.py\nimport unittest\n\nclass ChaineDeCaractereTest(unittest.TestCase):\n\n    def test_reversed(self):\n        resultat = reversed(\"abcd\")\n        self.assertEqual(\"dcba\", \"\".join(resultat))\n\n    def test_sorted(self):\n        resultat = sorted(\"dbca\")\n        self.assertEqual(['a', 'b', 'c', 'd'], resultat)\n\n    def test_upper(self):\n        resultat = \"hello\".upper()\n        self.assertEqual(\"HELLO\", resultat)\n\n    def test_erreur\n\nif __name__ == '__main__':\n    unittest.main()\nPour vérifier que les tests fonctionnent, on exécute ce script depuis la ligne de commande :\npython3 test_str.py\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nSi on écrit des tests unitaires, il est important de les maintenir ! Prendre du temps pour écrire des tests unitaires qui ne sont pas maintenus et donc ne renvoient plus de diagnostics pertinents est du temps perdu."
  },
  {
    "objectID": "chapters/projects-architecture.html#transformer-son-projet-en-package-python",
    "href": "chapters/projects-architecture.html#transformer-son-projet-en-package-python",
    "title": "Structure des projets",
    "section": "Transformer son projet en package Python",
    "text": "Transformer son projet en package Python\nLe package est la structure aboutie d’un projet Python autosuffisant. Il s’agit d’une manière formelle de contrôler la reproductibilité d’un projet car :\n\nle package assure une gestion cohérente des dépendances\nle package offre une certaine structure pour la documentation\nle package facilite la réutilisation du code\nle package permet des économies d’échelle, car on peut réutiliser l’un des packages pour un autre projet\nle package facilite le debuggage car il est plus facile d’identifier une erreur quand elle est dans un package\n…\n\nEn Python, le package est une structure peu contraignante si on a adopté les bonnes pratiques de structuration de projet. À partir de la structure modulaire précédemment évoquée, il n’y a qu’un pas vers le package : l’ajout d’un fichier pyproject.toml qui contrôle la construction du package (voir ici).\nIl existe plusieurs outils pour installer un package dans le système à partir d’une structure de fichiers locale. Les deux principaux sont :\n\nsetuptools\npoetry\n\nLe package fait la transition entre un code modulaire et un code portable, concept sur lequel nous reviendrons dans le prochain chapitre.\n:::"
  },
  {
    "objectID": "chapters/projects-architecture.html#cookiecutters",
    "href": "chapters/projects-architecture.html#cookiecutters",
    "title": "Structure des projets",
    "section": "Cookiecutters",
    "text": "Cookiecutters\nIn Python, there are standardized project structure templates: called cookiecutters. These are community-maintained templates for project directory trees (.py files as well as documentation, config, etc.) that can be used as a starting point.\nThe idea behind cookiecutter is to offer ready-to-use templates to initialize a project with a scalable structure. We’ll follow the structure proposed by the cookiecutter data science community template.\nThe syntax to use is:\n$ pip install cookiecutter\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science\nThe template is customizable, particularly for integrating with remote storage systems. The generated directory tree is large enough to support diverse project types — you typically won’t need every single component included by default.\n\n\nFull structure generated by the cookiecutter data science template\n\n(Identique au bloc ci-dessus – déjà internationalisé)\n\n\n\n\n\n\n\nUnit Tests\n\n\n\n\n\nUnit tests are automated tests that verify the proper functioning of a unit of code, such as a function or a method. The goal is to ensure that each unit of code works correctly before being integrated into the rest of the program.\nUnit tests are helpful when working with large codebases or when sharing code with others, because they ensure that modifications don’t introduce new bugs.\nIn Python, the unittest package can be used to write unit tests. Here’s an example from this site:\n# file test_str.py\nimport unittest\n\nclass StringTest(unittest.TestCase):\n\n    def test_reversed(self):\n        result = reversed(\"abcd\")\n        self.assertEqual(\"dcba\", \"\".join(result))\n\n    def test_sorted(self):\n        result = sorted(\"dbca\")\n        self.assertEqual(['a', 'b', 'c', 'd'], result)\n\n    def test_upper(self):\n        result = \"hello\".upper()\n        self.assertEqual(\"HELLO\", result)\n\n    def test_erreur\n\nif __name__ == '__main__':\n    unittest.main()\nTo verify that the tests work, run this script from the command line:\npython3 test_str.py\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nIf you write unit tests, it’s important to maintain them! Spending time writing unit tests that are no longer maintained and no longer provide useful diagnostics is time wasted."
  },
  {
    "objectID": "chapters/projects-architecture.html#turning-your-project-into-a-python-package",
    "href": "chapters/projects-architecture.html#turning-your-project-into-a-python-package",
    "title": "Structure des projets",
    "section": "Turning Your Project Into a Python Package",
    "text": "Turning Your Project Into a Python Package\nA package is the finalized structure of a self-contained Python project. It provides a formal way to ensure the reproducibility of a project because:\n\nthe package handles dependencies consistently\nthe package offers built-in documentation structure\nthe package facilitates code reuse\nthe package enables scalability—you can reuse a package across projects\nthe package simplifies debugging since it’s easier to pinpoint errors in a package\n…\n\nIn Python, packages are relatively easy to set up if you follow good project structuring practices. From the previously discussed modular structure, it’s a short step to a package: simply add a pyproject.toml file to control how the package is built (see here).\nThere are several tools for installing a package locally from a file structure. The two most common are:\n\nsetuptools\npoetry\n\nThe package bridges the gap between modular and portable code, a topic we’ll revisit in the next chapter."
  },
  {
    "objectID": "chapters/projects-architecture.html#footnotes",
    "href": "chapters/projects-architecture.html#footnotes",
    "title": "Structure des projets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this regard, Python is much more reliable than R. In R, if two scripts use functions with the same name but from different packages, there will be a conflict. In Python, each module is imported as its own package.↩︎"
  },
  {
    "objectID": "chapters/galerie/2024/model.html",
    "href": "chapters/galerie/2024/model.html",
    "title": "Modèle de carte",
    "section": "",
    "text": "Une description en quelques mots du projet\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "chapters/galerie/2024/resultAthle.html",
    "href": "chapters/galerie/2024/resultAthle.html",
    "title": "ResultAthle",
    "section": "",
    "text": "ResultAthle est un projet visant à rendre les outils statistiques d’analyse de performance plus accessibles au niveau amateur en athlétisme. Il aborde les défis de la collecte de résultats et le manque de statistiques descriptives accessibles pour les clubs.\n\n\n\nReuseCC BY-NC 4.0"
  }
]