[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Putting into production course",
    "section": "",
    "text": "Un parcours de formation construit par Romain Avouac et Lino Galiana pour le cursus d‚Äôing√©nieurs de la donn√©e de l‚ÄôENSAE.\nLes slides associ√©es au cours sont disponibles √† cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "index.html#cours-de-mise-en-production-de-projets-data-science",
    "href": "index.html#cours-de-mise-en-production-de-projets-data-science",
    "title": "Putting into production course",
    "section": "",
    "text": "Un parcours de formation construit par Romain Avouac et Lino Galiana pour le cursus d‚Äôing√©nieurs de la donn√©e de l‚ÄôENSAE.\nLes slides associ√©es au cours sont disponibles √† cette adresse et les codes sources sont sur Github ."
  },
  {
    "objectID": "chapters/galerie/2024/primePredict.html",
    "href": "chapters/galerie/2024/primePredict.html",
    "title": "PrimePredict",
    "section": "",
    "text": "Avec PrimePredict il est possible de calculer les primes annuelles mat√©rielles d‚Äôassurance √† partir de donn√©es fourni. Le mod√®le pr√©dit √† lafois le co√ªt des dommages mat√©riels et la fr√©quence des incidents mat√©riels.\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "chapters/yaml101.html",
    "href": "chapters/yaml101.html",
    "title": "YAML 101",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/yaml101.html#quest-ce-que-le-format-yaml",
    "href": "chapters/yaml101.html#quest-ce-que-le-format-yaml",
    "title": "YAML 101",
    "section": "Qu‚Äôest-ce que le format YAML",
    "text": "Qu‚Äôest-ce que le format YAML\nYAML est un acronyme r√©cursif signifiant YAML Ain‚Äôt Markup Language. YAML est un langage de s√©rialisation de donn√©es au format texte : il permet de structurer de la donn√©e dans des fichiers textuels, √† l‚Äôinstar d‚Äôautres formats populaires (CSV, JSON, XML, etc.).\nSa sp√©cificit√© par rapport aux autres formats populaires est qu‚Äôil est con√ßu pour √™tre √† la fois expressif et facile √† lire pour un humain. Expressif dans la mesure o√π il permet de repr√©senter de l‚Äôinformation hi√©rarchique, ce que ne permet pas le format CSV par exemple qui repr√©sente essentiellement des donn√©es tabulaires (‚Äúfichier plat‚Äù). Lisible car, contrairement aux languages dont la structure est bas√©e sur des balises (markup) comme XML ou bien d√©limit√©e par des symboles comme l‚Äôaccolade en JSON, YAML se d√©marque par une syntaxe bas√©e sur l‚Äôindentation ‚Äî comme Python.\nVoici un exemple typique d‚Äôun fichier YAML, qui compare les principaux langages utilis√©s pour repr√©senter des donn√©es hi√©rarchiques dans un format textuel1.\n---\ntype: tutorial\ndomain:\n  - devops\nlanguage:\n  - yaml:\n      name: YAML Ain't Markup Language\n      born: 2001\n      legibility: awesome\n  - json:\n      name: JavaScript Object Notation\n      born: 2001\n      legibility: good\n  - xml:\n      name: Extensible Markup Language\n      born: 1996\n      legibility: bad\n---"
  },
  {
    "objectID": "chapters/yaml101.html#yaml-et-lapproche-gitops",
    "href": "chapters/yaml101.html#yaml-et-lapproche-gitops",
    "title": "YAML 101",
    "section": "YAML et l‚Äôapproche GitOps",
    "text": "YAML et l‚Äôapproche GitOps\nGr√¢ce √† sa lisibilit√©, le langage YAML est rapidement devenu un standard dans les √©cosyst√®mes DevOps et MLOps dans la mesure o√π il facilite grandement l‚Äôinteraction entre les humains et les syst√®mes automatis√©s. Il permet de d√©finir de mani√®re lisible des r√®gles, bas√©es sur des param√®tres, qui peuvent facilement √™tre interpr√©t√©es √† la fois par des humains (du fait de sa lisibilit√©) et par des machines (du fait de sa structure hi√©rarchis√©e).\nEn particulier, le YAML est devenu un √©l√©ment central des environnements de d√©ploiement modernes bas√©s sur une approche dite d√©clarative (Note¬†1). Au lieu de sp√©cifier √©tape par √©tape comment d√©ployer une ressource (approche imp√©rative), YAML permet de d√©crire simplement l‚Äô√©tat final souhait√© de l‚Äôenvironnement. C‚Äôest alors l‚Äôoutil utilis√© (par exemple, Kubernetes dans l‚Äôexemple ci-dessous) qui g√®re les d√©tails d‚Äôimpl√©mentation de mani√®re la plus adapt√©e. On parle souvent d‚Äôapproche Infrastructure as Code : l‚Äôinfrastructure peut √™tre compl√®tement d√©crite dans des fichiers de configuration YAML ‚Äî qu‚Äôon appelle manifestes ‚Äî qui permettent de d√©crire l‚Äô√©tat souhait√© de celle-ci. Cette mani√®re de sp√©cifier les environnements favorise la reproductibilit√©, chaque √©tat de l‚Äôinfrastructure √©tant clairement d√©crit et pouvant √™tre reproduit simplement.\n\n\n\n\n\n\nNote¬†1: YAML n‚Äôest pas le premier langage d√©claratif que vous avez appris !\n\n\n\n\n\nA titre d‚Äôanalogie, l‚Äôapproche d√©clarative est √©galement au c≈ìur du langage SQL.\nEn SQL, on d√©crit le traitement souhait√© dans un langage proche du langage naturel (SELECT ce groupe de variables, FILTER ces individus, etc.) et c‚Äôest le moteur d‚Äôex√©cution sous-jacent qui choisit comment effectuer les calculs demand√©s de mani√®re optimale.\nC‚Äôest notamment une des raisons de la popularit√© du langage SQL dans l‚Äô√©co-syst√®me big data : il est possible d‚Äôappliquer une m√™me requ√™te √† des donn√©es de volum√©tries tr√®s diff√©rentes dans la mesure o√π le moteur d‚Äôex√©cution sous-jacent va convertir la requ√™te de mani√®re appropri√©e. Par exemple, une requ√™te SQL sera convertie par Spark en op√©rations MapReduce distribu√©es permettant de traiter des volumes consid√©rables de donn√©es (voir chapitre big data pour plus de d√©tails).\n\n\n\nBien entendu, ces fichiers d√©crivant les environnements souhait√©s ont vocation √† √™tre versionn√©s sur un d√©p√¥t Git . Cette extension directe de l‚Äôapproche Infrastructure as Code se nomme le GitOps : l‚Äôarchitecture est d√©crite sous forme de manifestes (YAML) et ces manifestes sont versionn√©s sur un d√©p√¥t Git qui devient ainsi la source de v√©rit√© unique de sp√©cification de l‚Äôenvironnement. Cette approche favorise √† la fois la tra√ßabilit√© ‚Äî tout changement est document√© dans l‚Äôhistorique du d√©p√¥t Git ‚Äî et l‚Äôautomatisation ‚Äî l‚Äôinfrastructure peut √™tre d√©ploy√©e automatiquement √† partir du d√©p√¥t Git gr√¢ce √† des outils de d√©ploiement continus orient√©s GitOps comme ArgoCD (cf.¬†chapitre d√©ploiement).\nVoici un exemple illustrant ce concept dans le contexte de Kubernetes. On d√©clare dans un manifeste YAML une ressource de type Pod, qui a des m√©tadonn√©es (un nom) et des sp√©cifications. On y d√©clare le conteneur que le Pod doit d√©ployer : celui d‚Äôune API FastAPI (voir l‚Äôapplication fil rouge pour plus de d√©tails). On passe √† ce conteneur une variable d‚Äôenvironnement (cf.¬†chapitre Linux 101) qui sp√©cifie le mod√®le √† d√©ployer via l‚ÄôAPI au runtime. Ainsi, on d√©crit dans ce manifeste l‚Äô√©tat souhait√©, et Kubernetes se charge du d√©ploiement effectif du conteneur.\n\n\nexample.yaml\n\nkind: Pod\nmetadata:\n  name: my-api-pod\nspec:\n  containers:\n    - name: api\n      image: my_dh_account/my_fast_api:0.0.1\n      env:\n        - name: MODEL\n          value: deepseek-ai/DeepSeek-R1"
  },
  {
    "objectID": "chapters/yaml101.html#yaml-vs.-json",
    "href": "chapters/yaml101.html#yaml-vs.-json",
    "title": "YAML 101",
    "section": "YAML vs. JSON",
    "text": "YAML vs. JSON\nLes langages YAML et JSON sont tr√®s proches dans la repr√©sentation hi√©rarchique de l‚Äôinformation qu‚Äôils permettent. Fonctionnellement, le langage YAML est un superset du langage JSON. Cela signifie que tout ce qui est repr√©sentable en JSON peut l‚Äô√™tre en YAML mais l‚Äôinverse n‚Äôest pas n√©cessairement vrai ‚Äî m√™me s‚Äôil est tr√®s rare en pratique de ne pas pouvoir faire la conversion dans les deux sens. Plusieurs outils en ligne, comme YAML-to-json et json-to-YAML, permettent de convertir facilement ces formats entre eux.\nComparons √† titre d‚Äôillustration les repr√©sentations YAML et JSON du manifeste Kubernetes pr√©c√©dent.\n\n\n\nkind: Pod\nmetadata:\n  name: my-api-pod\nspec:\n  containers:\n    - name: api\n      image: my_dh_account/my_fast_api:0.0.1\n      env:\n        - name: MODEL\n          value: deepseek-ai/DeepSeek-R1\n\n\n{\n  \"kind\": \"Pod\",\n  \"metadata\": {\n    \"name\": \"my-api-pod\"\n  },\n  \"spec\": {\n    \"containers\": [\n      {\n        \"name\": \"api\",\n        \"image\": \"my_dh_account/my_fast_api:0.0.1\",\n        \"env\": [\n          {\n            \"name\": \"MODEL\",\n            \"value\": \"deepseek-ai/DeepSeek-R1\"\n          }\n        ]\n      }\n    ]\n  }\n}\n\n\n\nLa comparaison entre les deux repr√©sentations illustre clairement la diff√©rence majeure entre les deux langages : le YAML est facilement lisible par l‚Äôhumain l√† o√π le JSON est plus verbeux et ressemble donc plus √† un ‚Äúlangage machine‚Äù. En effet, YAML utilise l‚Äôindentation pour structurer les donn√©es l√† o√π JSON utilise des accolades pour structurer ses objets, ce qui limite sa lisibilit√© mais le rend √©galement moins susceptible aux erreurs d‚Äôindentation.\nIl n‚Äôest donc pas √©tonnant que YAML soit devenu le langage de r√©f√©rence dans le monde du GitOps, les fichiers de configuration d√©claratifs √©tant g√©n√©ralement g√©n√©r√©s et lus par des humains. A l‚Äôinverse, JSON reste le langage standard de communication avec une API, dont les r√©ponses sont g√©n√©ralement lues par d‚Äôautres applications ou bien par le biais d‚Äôun langage de programmation, ce √† quoi se pr√™te tr√®s bien une structure plus verbeuse mais aussi plus robuste."
  },
  {
    "objectID": "chapters/yaml101.html#caract√©ristiques-dun-fichier-yaml",
    "href": "chapters/yaml101.html#caract√©ristiques-dun-fichier-yaml",
    "title": "YAML 101",
    "section": "Caract√©ristiques d‚Äôun fichier YAML",
    "text": "Caract√©ristiques d‚Äôun fichier YAML\nLes fichiers YAML portent g√©n√©ralement les extensions .yaml ou .yml. En pratique ‚Äî et cela est vrai pour tous les formats de fichier en pratique ‚Äî ce n‚Äôest pas l‚Äôextension qui fait qu‚Äôun fichier texte est un fichier YAML, mais bien la validit√© de son contenu au regard de la norme YAML. On doit donc respecter un certains nombres de r√®gles et utiliser les types pr√©vus pour produire un fichier YAML.\nPremi√®re r√®gle : le contenu d‚Äôun fichier YAML est organis√© en paires cl√©-valeur imbriqu√©es, √† la mani√®re d‚Äôun dictionnaire en Python. La structure hi√©rarchique, c‚Äôest √† dire l‚Äôimbrication des dictionnaires de donn√©es, est marqu√©e par l‚Äôindentation. Par convention, on indente avec deux espaces (pas de tabulation!) √† chaque niveau de hi√©rarchie.\nSeconde r√®gle : on doit utiliser les types de donn√©es support√©s par YAML. En pratique, ils sont suffisamment nombreux pour couvrir la plupart des usages :\n\nCha√Ænes de caract√®res : de la donn√©e textuelle. On peut choisir de l‚Äôentourer ou non de guillemets. Par convention, tout ce qui n‚Äôest pas des autres types de cette liste est au format textuel, il n‚Äôest donc pas indispensable de mettre des guillemets (√† l‚Äôinverse du JSON).\nNum√©riques : entiers ou flottants (exemple : 42, 3.14). Attention √† ne pas mettre de guillemets dans ce cas, sinon les nombres seront reconnus comme des cha√Ænes de caract√®res.\nBool√©ens : repr√©sent√©s par true ou false (tout en minuscule!). L√† encore, pas de guillemets.\nListes : √©l√©ments pr√©c√©d√©s d‚Äôun tiret (-). Les √©l√©ments d‚Äôune liste peuvent √™tre, et sont souvent, √† plusieurs niveaux, le YAML permettant une structure imbriqu√©e.\n\nAgr√©mentons encore un peu le manifeste de d√©ploiement Kubernetes pour faire appara√Ætre l‚Äôensemble de ces types.\nkind: Pod\nmetadata:\n  name: my-api-pod\nspec:\n  containers:\n    - name: api\n      image: my_dh_account/my_fast_api:0.0.1\n      env:\n        - name: MODEL\n          value: deepseek-ai/DeepSeek-R1\n        - name: DEBUG\n          value: true\n      ports:\n        - containerPort: 8000\nOn voit ici que dans notre param√®tre env, on a pu mettre plusieurs valeurs. M√™me si on ne connait pas les d√©tails de l‚Äôimpl√©mentation sous-jacente, on peut se douter qu‚Äôon va injecter deux variables d‚Äôenvironnement √† notre API dont les valeurs sont d√©finies dans ce fichier.\nLes cl√©s-valeurs accept√©es sont des conventions, elles d√©pendent de chaque solution logicielle. Mais, globalement, les param√®tres d‚Äôinstruction accept√©s par telle ou telle machine se ressemblent. Dans le cadre de l‚Äôapplication fil rouge, nous proposons l‚Äôutilisation de Github Actions, qui repose sur la sp√©cification Mustache, d‚ÄôArgo et de Kubernetes. Ces solutions techniques r√©pondent √† des besoins diff√©rents mais pr√©sentent le point commun de recevoir toutes trois leurs instructions depuis des YAML.\n\n\n\n\n\n\nImportantValidation et erreurs fr√©quentes\n\n\n\nLe principal point de vigilance lorsqu‚Äôon √©crit du YAML concerne l‚Äôindentation. Celle-ci est essentielle √† la structuration des donn√©es. L‚Äôindentation recommand√©e est de deux espaces ; jamais de tabulations, qui peuvent g√©n√©rer des erreurs d‚Äôinterpr√©tation car il s‚Äôagit de caract√®res sp√©ciaux.\nPour pr√©venir ces erreurs, de nombreux outils de validation existent :\n\nIDE : les IDE standards comme VSCode, PyCharm, etc. supportent nativement le YAML et indiquent donc imm√©diatement les erreurs d‚Äôindentation ou de syntaxe. Il est utile d‚Äôavoir dans son IDE une coloration des indentations pour faire moins d‚Äôerreur: dans VSCode, vous pouvez installer l‚Äôextension indent-rainbow pour cela2.\nLinters : des outils sp√©cialis√©s pour valider le contenu d‚Äôun fichier YAML. YAMLlint en est un choix courant.\n\nSi vous utilisez des actions Github, l‚Äôinterface de Github depuis votre navigateur effectue un diagnostic de la validit√© formelle du YAML. Si vous committez un fichier non valide formellement (par exemple √† cause d‚Äôune erreur d‚Äôidentation), Github ne lancera pas l‚Äôaction. Il peut donc √™tre utile de v√©rifier, si une action ne se lance pas, si cela vient d‚Äôun probl√®me de formattage du fichier en l‚Äôouvrant directement depuis Github."
  },
  {
    "objectID": "chapters/yaml101.html#utilisation-programmatique",
    "href": "chapters/yaml101.html#utilisation-programmatique",
    "title": "YAML 101",
    "section": "Utilisation programmatique",
    "text": "Utilisation programmatique\nComme on l‚Äôa vu, le langage YAML est pens√© avant tout pour √™tre √©crit et lus par des humains. Cela dit, on peut vouloir parfois g√©n√©rer par exemple de nombreux fichiers YAML √† partir d‚Äôun m√™me template, ou bien r√©cup√©rer de mani√®re automatis√©e des informations dans un fichier de configuration YAML. De la m√™me mani√®re qu‚Äôun fichier JSON par exemple, il est tout √† fait possible d‚Äôinteragir avec un fichier YAML de mani√®re programmatique.\nEn Python, le package de r√©f√©rence pour manipuler du YAML est PyYAML. Illustrons quelques commandes de base.\n\nimport yaml\n\nSupposons que le fichier de d√©ploiement Kubernetes qu‚Äôon a utilis√© tout a long de ce tutoriel soit √©crit dans un fichier api-deployment.yaml. Commen√ßons par l‚Äôimporter et r√©cup√©rer quelques informations.\nLa syntaxe pour importer le contenu d‚Äôun fichier YAML est semblable √† celle pour n‚Äôimporte quel fichier. On utilise la fonction safe_load du package PyYAML.\n\nwith open(\"api-deployment.yaml\", \"r\") as file_in:\n  manifest = yaml.safe_load(file_in)\n\nprint(manifest)\n\n{'kind': 'Pod', 'metadata': {'name': 'my-api-pod'}, 'spec': {'containers': [{'env': [{'name': 'MODEL', 'value': 'deepseek-ai/DeepSeek-R1'}, {'name': 'DEBUG', 'value': True}], 'image': 'my_dh_account/my_fast_api:0.0.1', 'name': 'api', 'ports': [{'containerPort': 8000}]}]}}\n\n\nLe contenu est charg√© en Python sous la forme d‚Äôun dictionnaire, qui est la mani√®re naturelle en Python de repr√©senter de l‚Äôinformation hi√©rarchique. A partir de l√†, on peut requ√™ter le contenu du fichier YAML √† la mani√®re de n‚Äôimporte quel dictionnaire Python. R√©cup√©rons par exemple le nom du mod√®le d√©ploy√©.\n\nprint(\n  manifest\n  .get(\"spec\")\n  .get(\"containers\")[0]\n  .get(\"env\")[0]\n  .get(\"value\")\n)\n# ou en version plus succincte:\n# print(manifest[\"spec\"][\"containers\"][0][\"env\"][0][\"value\"])\n\ndeepseek-ai/DeepSeek-R1\n\n\nLe chemin dans le dictionnaire refl√®te les diff√©rents objets qui structurent un fichier YAML. Les cl√©s (‚Äúspec‚Äù, ‚Äúcontainers‚Äù) correspondent aux dictionnaires en YAML, et les index num√©riques (‚Äú0‚Äù ici) correspondent √† la s√©lection d‚Äôun √©l√©ment par position dans une liste. Ici, on r√©cup√®re la sp√©cification du premier conteneur ‚Äî et le seul en l‚Äôoccurrence, mais le Pod pourrait h√©berger plusieurs conteneurs ‚Äî et, pour ce conteneur, on r√©cup√®re la premi√®re variable d‚Äôenvironnement (MODEL) dont on extrait la valeur (le nom du mod√®le).\nA pr√©sent, int√©ressons-nous au cas o√π l‚Äôon voudrait modifier une information du fichier YAML et exporter un manifeste actualis√©. On va changer la valeur de la variable d‚Äôenvironnement DEBUG de true √† false. En pratique, on fait la modification sur le dictionnaire en Python, puis on exporte le dictionnaire au format YAML.\n\n# V√©rification du contenu actuel\nprint(manifest[\"spec\"][\"containers\"][0][\"env\"][1])\n\n# Modification du contenu\nmanifest[\"spec\"][\"containers\"][0][\"env\"][1][\"value\"] = False\nprint(manifest[\"spec\"][\"containers\"][0][\"env\"][1])\n\n{'name': 'DEBUG', 'value': True}\n{'name': 'DEBUG', 'value': False}\n\n\nL‚Äôop√©ration a fonctionn√© sur l‚Äôobjet en m√©moire dans Python. On exporte finalement le dictionnaire au format YAML.\n\nwith open('api-deployment-modifie.yaml', 'w') as file_out:\n    yaml.dump(manifest, file_out)\n\n# V√©rification\nprint(open(\"api-deployment-modifie.yaml\", \"r\").read())\n\nkind: Pod\nmetadata:\n  name: my-api-pod\nspec:\n  containers:\n  - env:\n    - name: MODEL\n      value: deepseek-ai/DeepSeek-R1\n    - name: DEBUG\n      value: false\n    image: my_dh_account/my_fast_api:0.0.1\n    name: api\n    ports:\n    - containerPort: 8000\n\n\n\nLe manifeste export√© a bien √©t√© modifi√© comme attendu."
  },
  {
    "objectID": "chapters/yaml101.html#conclusion",
    "href": "chapters/yaml101.html#conclusion",
    "title": "YAML 101",
    "section": "Conclusion",
    "text": "Conclusion\nDu fait de sa pr√©dominance dans l‚Äô√©co-syst√®me DevOps / MLOps, la ma√Ætrise du langage YAML est aujourd‚Äôhui indispensable pour tout data scientist impliqu√© dans des projets visant la mise en production. Sa syntaxe lisible mais puissante et sa compatibilit√© avec le paradigme d√©claratif en font un langage de choix pour la d√©claration de manifestes en mode GitOps. Cette approche est progressivement devenue un standard pour le d√©ploiement industrialis√© d‚Äôapplications et sera centrale dans les chapitres suivants sur le d√©ploiement et le MLOps."
  },
  {
    "objectID": "chapters/yaml101.html#footnotes",
    "href": "chapters/yaml101.html#footnotes",
    "title": "YAML 101",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nL‚Äôexemple pr√©sent√© est inspir√© de ce tutoriel.‚Ü©Ô∏é\nDans le cadre de l‚Äôapplication fil rouge, si vous cr√©ez votre VSCode √† partir de la configuration propos√©e, celle-ci est install√©e par d√©faut pour vous.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/portability.html",
    "href": "chapters/portability.html",
    "title": "Portabilit√©",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/portability.html#introduction-1",
    "href": "chapters/portability.html#introduction-1",
    "title": "Portabilit√©",
    "section": "Introduction",
    "text": "Introduction\nTo illustrate the importance of working with virtual environments, let‚Äôs take the perspective of an aspiring data scientist starting their first projects.\nMost likely, they begin by installing a Python distribution‚Äîoften via Anaconda‚Äîon their machine and start developing project after project. If they need to install an additional library, they do so without much thought. Then they move on to the next project using the same approach. And so on.\nThis natural workflow allows for quick experimentation. However, it becomes problematic when it comes time to share a project or revisit it later.\nIn this setup, all packages used across various projects are installed in the same location. While this might seem trivial‚Äîafter all, Python‚Äôs simplicity is one of its strengths‚Äîseveral issues will eventually arise:\n\nVersion conflicts: Application A may require version 1 of a package, while application B needs version 2. These versions may be incompatible. In such a setup, only one application may work;\nFixed Python version ‚Äî Only one Python installation is available per system, whereas you‚Äôd want different versions for different projects;\nLimited reproducibility: It‚Äôs difficult to trace which project relies on which packages, since everything accumulates in the same environment;\nLimited portability: As a result of the above, it‚Äôs hard to generate a file listing only the dependencies of a specific project.\n\nVirtual environments provide a solution to these issues."
  },
  {
    "objectID": "chapters/portability.html#how-it-works",
    "href": "chapters/portability.html#how-it-works",
    "title": "Portabilit√©",
    "section": "How It Works",
    "text": "How It Works\nThe concept of a virtual environment is technically quite simple. In Python, it can be defined as:\n\n‚ÄúA self-contained directory tree that contains a Python installation for a particular version of Python, plus additional packages.‚Äù\n\nYou can think of virtual environments as a way to maintain multiple isolated Python setups on the same system, each with its own package list and versions. Starting every new project in a clean virtual environment is a great practice to improve reproducibility."
  },
  {
    "objectID": "chapters/portability.html#implementations",
    "href": "chapters/portability.html#implementations",
    "title": "Portabilit√©",
    "section": "Implementations",
    "text": "Implementations\nThere are various implementations of virtual environments in Python, each with its own community and features:\n\nThe standard Python implementation is venv.\nconda provides a more comprehensive implementation.\n\nFrom the user‚Äôs perspective, these implementations are fairly similar. The key conceptual difference is that conda acts as both a package manager (like pip) and a virtual environment manager (like venv).\nFor a long time, conda was the go-to tool for data science, as it handled not only Python dependencies but also system-level dependencies (e.g., C libraries) used by many data science packages. However, in recent years, the widespread use of wheels‚Äîprecompiled packages tailored for each system‚Äîhas made pip more viable again.\n\n\n\n\n\n\nNoteConceptual difference between pip and conda\n\n\n\n\n\nAnother major difference is how they resolve dependency conflicts.\nMultiple packages might require different versions of a shared dependency. conda uses an advanced (and slower) solver to resolve such conflicts optimally, while pip follows a simpler approach.\n\n\n\npip+venv offers simplicity, while conda offers robustness. Depending on your project‚Äôs context, either can be appropriate. If your project runs in an isolated container, venv is usually sufficient since the container already provides isolation.\n\n\n\n\n\n\nNoteIs it different in ?\n\n\n\n\n\nSome  enthusiasts claim Python‚Äôs environment management is chaotic. That was true in the early 2010s, but not so much today.\nR‚Äôs tools, like renv, are great but have limitations‚Äîfor example, renv doesn‚Äôt let you specify the R version.\nIn contrast, Python‚Äôs command-line tools offer stronger portability: venv lets you choose the Python version when creating the environment; conda lets you define the version directly in the environment.yml file.\n\n\n\nSince there‚Äôs no absolute reason to choose between pip+venv or conda, we recommend pragmatism. We personally lean toward venv because we mainly work in container-based microservices‚Äîan increasingly common practice in modern data science. However, we present both approaches, and the application section includes both, so you can choose what suits your needs best."
  },
  {
    "objectID": "chapters/portability.html#practical-guide-to-using-a-virtual-environment",
    "href": "chapters/portability.html#practical-guide-to-using-a-virtual-environment",
    "title": "Portabilit√©",
    "section": "Practical Guide to Using a Virtual Environment",
    "text": "Practical Guide to Using a Virtual Environment\n\nInstallation\n\nvenvconda\n\n\nvenv is a module included by default in Python, making it easily accessible for managing virtual environments.\nInstructions for using venv, Python‚Äôs built-in tool for creating virtual environments, are available in the official Python documentation.\n\n\n\nIllustration of the concept (Source: dataquest)\n\n\n\n\nInstructions for installing conda are provided in the official documentation. conda alone is not very useful in practice and is generally included in distributions. The two most popular ones are:\n\nMiniconda: a minimalist distribution that includes conda, Python, and a small set of useful technical packages;\nAnaconda: a large distribution that includes conda, Python, other tools (R, Spyder, etc.), and many packages useful for data science (SciPy, NumPy, etc.).\n\n\nIn practice, the choice of distribution matters little, since we will use clean virtual environments to develop our projects.\n\n\n\n\n\nCreating an Environment\n\nvenvconda\n\n\nTo start using venv, we first create a clean environment named dev. This is done from the command line using Python. That means the version of Python used in the environment will be the one active at the time of creation.\n1python -m venv dev\n\n1\n\nOn a Windows system, use python.exe -m venv dev\n\n\nThis command creates a folder named dev/ containing an isolated Python installation.\n\n\nExample on a Linux system\n\n\n\n\nExample on a Linux system\n\n\n\nThe Python version used will be the one set as default in your PATH, e.g., Python 3.11. To create an environment with a different version of Python, specify the path explicitly:\n/path/to/python3.8 -m venv dev-old\n\n\nTo start using conda, we first create a clean environment named dev, specifying the Python version we want to install for the project:\nconda create -n dev python=3.9.7\nRetrieving notices: ...working... done\nChannels:\n - conda-forge\nPlatform: linux-64\nCollecting package metadata (repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/mamba/envs/dev\n\n  added / updated specs:\n    - python=3.9.7\n\n\nThe following packages will be downloaded:\n...\nThe following NEW packages will be INSTALLED:\n...\nProceed ([y]/n)? y\nDownloading and Extracting Packages\n...\nAs noted in the logs, conda created the environment and tells us where it‚Äôs located on the file system. In reality, the environment isn‚Äôt completely empty: conda asks‚Äîrequiring confirmation with y‚Äîto install a number of packages, those that come bundled with the Miniconda distribution.\nWe can verify that the environment was created by listing all environments on the system:\nconda info --envs\n# conda environments:\n#\nbase                  *  /opt/mamba\ndev                      /opt/mamba/envs/dev\n\n\n\n\n\nActivating an Environment\nSince multiple environments can coexist on the same system, you need to tell your environment manager which one to activate. From then on, it will implicitly be used for commands like python, pip, etc., in the current command-line session2.\n\nvenvconda\n\n\nsource dev/bin/activate\nvenv activates the virtual environment dev, which is confirmed by the name of the environment appearing at the beginning of the command line. Once activated, dev temporarily becomes our default Python environment. To confirm, use the which command to see which Python interpreter will be used:\nwhich python\n/home/onyxia/work/dev/bin/python\n\n\nconda activate dev\nConda indicates that you‚Äôre now working in the dev environment by showing its name in parentheses at the beginning of the command line. This means dev is temporarily your default environment. Again, we can verify it using which:\nwhich python\n/opt/mamba/envs/dev/bin/python\n\n\n\nYou‚Äôre now working in the correct environment: the Python interpreter being used is not the system one, but the one from your virtual environment.\n\n\nListing Installed Packages\nOnce the environment is activated, you can list the installed packages and their versions. This confirms that some packages are present by default when creating a virtual environment.\n\nvenvconda\n\n\nWe start with a truly bare-bones environment:\npip list\nPackage    Version\n---------- -------\npip        23.3.2\nsetuptools 69.0.3\nwheel      0.42.0\n\n\nThe environment is fairly minimal, though more populated than a fresh venv environment:\nconda list\n# packages in environment at /opt/mamba/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda_forge    conda-forge\n_openmp_mutex             4.5                       2_gnu    conda-forge\nca-certificates           2023.11.17           hbcca054_0    conda-forge\n...\n\n\n\nAs a quick check, we can confirm that Numpy is indeed not available in the environment:\npython -c \"import numpy as np\"\nTraceback (most recent call last):\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\nModuleNotFoundError: No module named 'numpy'\n\n\nInstalling a package\nYour environment can be extended, when needed,\nby installing a package via the command line.\nThe procedure is very similar between pip (for venv environments) and conda.\n\n\n\n\n\n\nCautionMixing pip and conda\n\n\n\n\n\nIt is technically possible to install packages using pip\nwhile working within a conda virtual environment3.\nThis is fine for experimentation and speeds up development.\nHowever, in a production environment, it is a practice to avoid.\n\nEither you initialize a fully self-sufficient conda environment with an env.yml (see below);\nOr you create a venv environment and use only pip install.\n\n\n\n\n\nvenvconda\n\n\n\n\nterminal\n\npip install nom_du_package\n\n\n\n\n\nterminal\n\nconda install nom_du_package\n\n\n\n\nThe difference is that while pip install installs a package from the PyPI repository, conda install fetches the package from repositories maintained by the Conda developers4.\nLet‚Äôs install the flagship machine learning package scikit-learn.\n\nvenvconda\n\n\n\n\nterminal\n\npip install scikit-learn\n\npip install scikit-learn\nCollecting scikit-learn\n...\nRequired dependencies (like Numpy) are automatically installed.\nThe environment is thus enriched:\n\n\nterminal\n\npip list\n\nPackage       Version\n------------- -------\njoblib        1.3.2\nnumpy         1.26.3\npip           23.2.1\nscikit-learn  1.4.0\nscipy         1.12.0\nsetuptools    65.5.0\nthreadpoolctl 3.2.0\n\n\n\n\nterminal\n\nconda install scikit-learn\n\n\n\nSee output\n\nChannels:\n - conda-forge\n...\nExecuting transaction: done\n\nAgain, conda requires installing additional packages that are dependencies of scikit-learn. For example, the scientific computing library NumPy.\n(dev) $ conda list\n# packages in environment at /opt/mamba/envs/dev:\n#\n# Name                    Version                   Build  Channel\n_libgcc_mutex             0.1                 conda-forge    conda-forge\n...\nzlib                      1.2.13               hd590300_5    conda-forge\n\n\n\n\n\nExporting environment specifications\nStarting from a clean environment is a good reproducibility practice:\nby beginning with a minimal setup, we ensure that only the packages\nstrictly necessary for the application‚Äôs functionality are installed as the project evolves.\nThis also makes the project easier to port.\nYou can export the environment‚Äôs specifications into a special file\nto recreate a similar setup elsewhere.\n\nvenvconda\n\n\n\n\nterminal\n\npip freeze &gt; requirements.txt\n\n\n\nView the generated requirements.txt file\n\n\n\nrequirements.txt\n\njoblib==1.3.2\nnumpy==1.26.3\nscikit-learn==1.4.0\nscipy==1.12.0\nthreadpoolctl==3.2.0\n\n\n\n\n\n\nterminal\n\nconda env export &gt; environment.yml\n\n\n\n\n\nThis file is conventionally stored at the root of the project‚Äôs Git repository.\nThis way, collaborators can recreate the exact same Conda environment used during development via the following command:\n\nvenvconda\n\n\nRepeat the earlier process of creating a clean environment,\nthen run pip install -r requirements.txt.\n\n\nterminal\n\npython -m venv newenv\nsource newenv/bin/activate\n\n\n\nterminal\n\npip install -r requirements.txt\n\n\n\nThis can be done in a single command:\n\n\nterminal\n\nconda env create -f environment.yml\n\n\n\n\n\n\nSwitching environments\n\nvenvconda\n\n\nTo switch environments, simply activate a different one.\n\n\nterminal\n\n(myenv) $ deactivate\n$ source anotherenv/bin/activate\n(anotherenv) $ which python\n/chemin/vers/anotherenv/bin/python\n\nTo exit the active virtual environment, just use deactivate:\n\n\nterminal\n\n(anotherenv) $ deactivate\n$\n\n\n\nTo switch environments, just activate another one.\n\n\nterminal\n\n(dev) $ conda activate base\n(base) $ which python\n/opt/mamba/bin/python\n\nTo exit all conda environments, use conda deactivate:\n\n\nterminal\n\n(base) $ conda deactivate\n$\n\n\n\n\n\n\nView the generated environment.yml file\n\n\n\nenvironment.yml\n\nname: dev\nchannels:\n  - conda-forge\ndependencies:\n  - _libgcc_mutex=0.1=conda_forge\n  - _openmp_mutex=4.5=2_gnu\n  - ca-certificates=2023.11.17=hbcca054_0\n  - joblib=1.3.2=pyhd8ed1a\n\n\n\n\nCheat sheet\n\n\n\n\n\n\n\n\nvenv\nconda\nDescription\n\n\n\n\npython -m venv &lt;env_name&gt;\nconda create -n &lt;env_name&gt; python=&lt;python_version&gt;\nCreate a new environment named &lt;env_name&gt; with Python version &lt;python_version&gt;\n\n\n\nconda info --envs\nList available environments\n\n\nsource &lt;env_name&gt;/bin/activate\nconda activate &lt;env_name&gt;\nActivate the environment for the current terminal session\n\n\npip list\nconda list\nList packages in the active environment\n\n\npip install &lt;pkg&gt;\nconda install &lt;pkg&gt;\nInstall the &lt;pkg&gt; package in the active environment\n\n\npip freeze &gt; requirements.txt\nconda env export &gt; environment.yml\nExport environment specs to a requirements.txt or environment.yml file"
  },
  {
    "objectID": "chapters/portability.html#limitations",
    "href": "chapters/portability.html#limitations",
    "title": "Portabilit√©",
    "section": "Limitations",
    "text": "Limitations\nUsing virtual environments is good practice as it improves application portability.\nHowever, there are some limitations:\n\nsystem libraries needed by packages aren‚Äôt managed;\nenvironments may not handle multi-language projects well;\ninstalling conda, Python, and required packages every time can be tedious;\nin production, managing separate environments per project can quickly become complex for system administrators.\n\nContainerization technologies help address these limitations."
  },
  {
    "objectID": "chapters/portability.html#introduction-3",
    "href": "chapters/portability.html#introduction-3",
    "title": "Portabilit√©",
    "section": "Introduction",
    "text": "Introduction\nWith virtual environments,\nthe goal was to allow every potential user of our project to install the required packages on their system for proper execution.\nHowever, as we‚Äôve seen, this approach doesn‚Äôt ensure perfect reproducibility and requires significant manual effort.\nLet‚Äôs shift perspective: instead of giving users a recipe to recreate the environment on their machine, couldn‚Äôt we just give them a pre-configured machine?\nOf course, we‚Äôre not going to configure and ship laptops to every potential user.\nSo we aim to deliver a virtual version\nof our machine. There are two main approaches:\n\nVirtual machines: not a new approach. They simulate a full computing environment (hardware + OS) on a server to replicate a real computer‚Äôs behavior.\nContainers: a more lightweight solution to bundle a computing environment and mimic a real machine‚Äôs behavior."
  },
  {
    "objectID": "chapters/portability.html#how-it-works-1",
    "href": "chapters/portability.html#how-it-works-1",
    "title": "Portabilit√©",
    "section": "How it works",
    "text": "How it works\nVirtual machines are heavy and difficult to replicate or distribute.\nTo overcome these limitations, containers have emerged in the past decade.\nModern cloud infrastructure has largely moved from virtual machines to containers for the reasons we‚Äôll discuss.\nLike VMs, containers package the full environment (system libraries, app, configs) needed to run an app.\nBut unlike VMs, containers don‚Äôt include their own OS. Instead, they use the host machine‚Äôs OS.\nThis means to simulate a Windows machine, you don‚Äôt need a Windows server ‚Äî a Linux one will do. Conversely, you can test Linux/Mac setups on a Windows machine.\nThe containerization software handles the translation between software-level instructions and the host OS.\nThis technology guarantees strong reproducibility while remaining lightweight enough for easy distribution and deployment.\nWith VMs, strong coupling between the OS and the app makes scaling harder. You need to spin up new servers matching your app‚Äôs needs (OS, configs).\nWith containers, scaling is easier: just add Linux servers with the right tools and you‚Äôre good to go.\n\n\n\nDifferences between containers (left) and VMs (right) (Source: docker.com)\n\n\nFrom the user‚Äôs point of view, the difference is often invisible for typical usage.\nThey access the app via a browser or tool, and computations happen on the remote server.\nBut for the organization hosting the app, containers bring more freedom and flexibility."
  },
  {
    "objectID": "chapters/portability.html#docker-the-standard-implementation",
    "href": "chapters/portability.html#docker-the-standard-implementation",
    "title": "Portabilit√©",
    "section": "Docker , the standard implementation",
    "text": "Docker , the standard implementation\nAs mentioned, the containerization software acts as a bridge between applications and the server‚Äôs OS.\nAs with virtual environments, there are multiple container technologies.\nIn practice, Docker has become the dominant one ‚Äî to the point where ‚Äúcontainerize‚Äù and ‚ÄúDockerize‚Äù are often used interchangeably.\nWe will focus on Docker in this course.\n\nInstallation & sandbox environments\nDocker  can be installed on various operating systems.\nInstallation instructions are in the official documentation.\nYou need admin rights on your computer to install it.\n\n\n\n\n\n\nWarningDisk space requirements\n\n\n\n\n\nIt‚Äôs also recommended to have free disk space, as some images (we‚Äôll come back to that) can be large once decompressed and built.\nThey may take up several GBs depending on the libraries included.\nStill, this is small compared to a full OS (15GB for Ubuntu or macOS, 20GB for Windows‚Ä¶).\nThe smallest Linux distribution (Alpine) is only 3MB compressed and 5MB uncompressed.\n\n\n\nIf you can‚Äôt install Docker, there are free online sandboxes.\nPlay with Docker lets you test Docker as if it were on your local machine.\nThese services are limited though (2GB max image size, outages under heavy load‚Ä¶).\nAs we‚Äôll see, interactive Docker usage is great for learning.\nBut in practice, Docker is mostly used via CI systems like GitHub Actions or Gitlab CI.\n\n\nConcepts\n\n\n\nSource: k21academy.com\n\n\nA Docker container is delivered as an image: a binary file containing the environment needed to run an app.\nIt‚Äôs shared in compressed form on a public image repository (e.g., Dockerhub).\nBefore sharing, you must build the image.\nThat‚Äôs done using a Dockerfile, a text file with Linux commands describing how to set up the environment.\nOnce built, you can:\n\nRun it locally to test and debug the app.\nRunning it creates an isolated environment called a container ‚Äî a live instance of the image5.\nPush it to a public or private repository so others (or yourself) can pull and use it.\n\n\n\n\n\n\n\nNotePublishing your Docker image\n\n\n\n\n\nThe most well-known image repository is DockerHub.\nAnyone can publish a Docker image there, optionally linked to a GitHub or Gitlab project.\nWhile you can upload images manually, as we‚Äôll see in the deployment chapter, it‚Äôs much better to use automated links between DockerHub and your GitHub repo."
  },
  {
    "objectID": "chapters/portability.html#application-1",
    "href": "chapters/portability.html#application-1",
    "title": "Portabilit√©",
    "section": "Application",
    "text": "Application\nTo demonstrate Docker in practice, we‚Äôll walk through the steps to Dockerize a minimal web application built with the Python web framework Flask6.\nOur project structure is as follows:\n‚îú‚îÄ‚îÄ myflaskapp\n‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile\n‚îÇ   ‚îú‚îÄ‚îÄ hello-world.py\n‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt\nThe hello-world.py script contains the code of a basic app that simply displays ‚ÄúHello, World!‚Äù on a web page. We‚Äôll explore how to build a more interactive application in the running example.\n\n\nhello-world.py\n\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello_world():\n    return \"&lt;p&gt;Hello, World!&lt;/p&gt;\"\n\nTo run the app, we need both Python and the Flask package. This means we need to control the Python environment:\n\nInstall Python;\nInstall the packages required by our code‚Äîin this case, only Flask.\n\nSince we‚Äôre not tied to a specific version of Python, using a venv virtual environment is simpler than conda. Conveniently, we already have a requirements.txt file that looks like this:\n\n\nrequirements.txt\n\nFlask==2.1.1\n\nBoth steps (installing Python and its required packages) must be declared in the Dockerfile (see next section)."
  },
  {
    "objectID": "chapters/portability.html#dockerfile",
    "href": "chapters/portability.html#dockerfile",
    "title": "Portabilit√©",
    "section": "The Dockerfile",
    "text": "The Dockerfile\nJust like a dish needs a recipe, a Docker image needs a Dockerfile.\nThis text file contains a sequence of instructions to build the image. These files can range from simple to complex depending on the application being containerized, but their structure follows some standards.\nThe idea is to start from a base layer (typically a minimal Linux distribution) and layer on the tools and configuration needed by our app.\nLet‚Äôs go through the Dockerfile for our Flask application line by line:\n#| filename: Dockerfile\n\n1FROM ubuntu:20.04\n\nRUN apt-get update -y && \\\n2    apt-get install -y python3-pip python3-dev\n\n3WORKDIR /app\n\n4COPY requirements.txt /app/requirements.txt\nRUN pip install -r requirements.txt\n\nCOPY . /app\n\n5ENV FLASK_APP=\"hello-world.py\"\n6EXPOSE 5000\n\n7CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]\n\n1\n\nFROM: defines the base image. All Docker images inherit from a base. Here we choose ubuntu:20.04, so our environment will act like a blank virtual machine running Ubuntu 20.04 ;\n\n2\n\nRUN: executes a Linux command. We first update the list of installable packages, then install Python  and any required system libraries;\n\n3\n\nWORKDIR: sets the working directory inside the image. All following commands will run from this path‚ÄîDocker‚Äôs equivalent to the cd command (see Linux 101);\n\n4\n\nCOPY: transfers files from the host to the Docker image. This is important because Docker builds images in isolation‚Äîyour project files don‚Äôt exist inside the image unless explicitly copied. First, we copy requirements.txt to install dependencies, then copy the full project directory;\n\n5\n\nENV: defines an environment variable accessible inside the container. Here, we use FLASK_APP to tell Flask which script to run;\n\n6\n\nEXPOSE: tells Docker the app will listen on port 5000‚Äîthe default port for Flask‚Äôs development server;\n\n7\n\nCMD: defines the command to run when the container starts. Here, it launches the Flask server and binds it to all IPs in the container with --host=0.0.0.0.\n\n\n\n\n\n\n\n\nTipChoosing the Base Image\n\n\n\n\n\nIdeally, you want the smallest base image possible to reduce final image size. For example, there‚Äôs no need to use an image with  if your project only uses .\nMost languages provide lightweight base images with preinstalled and well-configured interpreters. In our case, we could have used python:3.9-slim-buster.\n\n\n\nThe first RUN installs Python and system libraries required by our app. But how did we know which libraries to include?\nTrial and error. During the build phase, Docker attempts to construct the image from the Dockerfile‚Äîas if it‚Äôs starting from a clean Ubuntu 20.04 machine. If a system dependency is missing, the build will fail and show an error message in the console logs. With luck, the logs will explicitly name the missing library. More often, the messages are vague and require some web searching‚ÄîStackOverflow is a frequent lifesaver.\nBefore creating a Docker image, it‚Äôs helpful to go through an intermediate step: writing a shell script (.sh) to automate setup locally. This approach is outlined in the running example.\n\n\n\n\n\n\nNoteThe COPY Instruction\n\n\n\n\n\nYour Dockerfile recipe might require files from your working folder. To ensure Docker sees them during the build, you need a COPY command. Think of it like cooking: if you want to use an ingredient, you need to take it out of the fridge (your local disk) and place it on the table (Docker build context).\n\n\n\nWe‚Äôve covered only the most common Docker commands. For a full reference, check the official documentation."
  },
  {
    "objectID": "chapters/portability.html#build",
    "href": "chapters/portability.html#build",
    "title": "Portabilit√©",
    "section": "Building a Docker Image",
    "text": "Building a Docker Image\nTo build an image from a Dockerfile, use the docker build command from the terminal7. Two important arguments must be provided:\n\nthe build context: this tells Docker where the project is located (it should contain the Dockerfile). The simplest approach is to navigate into the project directory via cd and pass . to indicate ‚Äúbuild from here‚Äù;\nthe tag, i.e., the name of the image. While working locally, the tag doesn‚Äôt matter much, but we‚Äôll see later that it becomes important when exporting or importing an image from/to a remote repository.\n\nLet‚Äôs see what happens when we try to build our image with the tag myflaskapp:\n\n\nterminal\n\ndocker build -t myflaskapp .\n\nSending build context to Docker daemon     47MB\nStep 1/8 : FROM ubuntu:20.04\n ---&gt; 825d55fb6340\nStep 2/8 : RUN apt-get update && apt-get install -y python3-pip python3-dev\n ---&gt; Running in 92b42d579cfa\n...\ndone.\nRemoving intermediate container 92b42d579cfa\n ---&gt; 8826d53e3c01\n...\nSuccessfully built 125bd8da70ff\nSuccessfully tagged myflaskapp:latest\nDocker‚Äôs engine processes the instructions from the Dockerfile one at a time. If there‚Äôs an error, the build stops, and you‚Äôll need to debug the problem using the log output and adjust the Dockerfile accordingly.\nIf successful, Docker will indicate that the build was completed and that the image is ready for use. You can confirm its presence with the docker images command:\n\n\nterminal\n\ndocker images\n\nREPOSITORY   TAG       IMAGE ID       CREATED          SIZE\nmyflaskapp   latest    57d2f410a631   2 hours ago      433MB\nLet‚Äôs look more closely at the build logs üëÜÔ∏è.\nBetween each step, Docker prints cryptic hashes and mentions intermediate containers. Think of a Docker image as a stack of layers, each layer being itself a Docker image. The FROM instruction specifies the starting layer. Each command adds a new layer with a unique hash.\nThis design is not just technical trivia‚Äîit‚Äôs incredibly useful in practice because Docker caches each intermediate layer8.\nFor example, if you modify the 5th instruction in the Dockerfile, Docker will reuse the cache for previous layers and only rebuild from the change onward. This is called cache invalidation: once a step changes, Docker recalculates that step and all that follow, but no more. As a result, you should always place frequently changing steps at the end of the file.\nLet‚Äôs illustrate this by changing the FLASK_APP environment variable in the Dockerfile:\n\n\nterminal\n\ndocker build . -t myflaskapp\n\nSending build context to Docker daemon  4.096kB\nStep 1/10 : FROM ubuntu:20.04\n ---&gt; 825d55fb6340\nStep 2/10 : ENV DEBIAN_FRONTEND=noninteractive\n ---&gt; Using cache\n ---&gt; ea1c7c083ac9\nStep 3/10 : RUN apt-get update -y && ...\n ---&gt; Using cache\n ---&gt; 078b8ac0e1cb\n...\nStep 8/10 : ENV FLASK_APP=\"new.py\"\n ---&gt; Running in b378d16bb605\n...\nSuccessfully built 16d7a5b8db28\nSuccessfully tagged myflaskapp:latest\nThe build finishes in seconds instead of minutes. The logs show that previous steps were cached, and only modified or dependent ones were rebuilt."
  },
  {
    "objectID": "chapters/portability.html#execution",
    "href": "chapters/portability.html#execution",
    "title": "Portabilit√©",
    "section": "Running a Docker Image",
    "text": "Running a Docker Image\nThe build step created a Docker image‚Äîessentially a blueprint for your app. It can be executed on any environment with Docker installed.\nSo far, we‚Äôve built the image but haven‚Äôt run it. To launch the app, we must create a container, i.e., a live instance of the image. This is done with the docker run command:\n\n\nterminal\n\n$ docker run -d -p 8000:5000 myflaskapp:latest\n6a2ab0d82d051a3829b182ede7b9152f7b692117d63fa013e7dfe6232f1b9e81\n\nHere‚Äôs a breakdown of the command:\n\ndocker run tag: runs the image specified by tag. Tags usually follow the format repository/project:version. Since we‚Äôre local, there‚Äôs no repository;\n-d: runs the container in detached mode (in the background);\n-p: maps a port on the host machine (8000) to a port inside the container (5000). Since Flask listens on port 5000, this makes the app accessible via localhost:8000.\n\nThe command returns a long hash‚Äîthis is the container ID. You can verify that it‚Äôs running with:\n\n\nterminal\n\ndocker ps\n\nCONTAINER ID   IMAGE        COMMAND                  CREATED         STATUS         PORTS                                   NAMES\n6a2ab0d82d05   myflaskapp   \"flask run --host=0.‚Ä¶\"   7 seconds ago   Up 6 seconds   0.0.0.0:8000-&gt;5000/tcp, :::8000-&gt;5000/tcp   vigorous_kalam\nDocker containers serve different purposes. Broadly, they fall into two categories:\n\nOne-shot jobs: containers that execute a task and terminate;\nRunning apps: containers that persist while serving an application.\n\nIn our case, we‚Äôre in the second category. We want to run a web app, so the container must stay alive. Flask launches a server on a local port (5000), and we‚Äôve mapped it to port 8000 on our machine. You can access the app from your browser at localhost:8000, just like a Jupyter notebook.\nIn the end, we‚Äôve built and launched a fully working application on our local machine‚Äîwithout installing anything beyond Docker itself."
  },
  {
    "objectID": "chapters/portability.html#exp-docker",
    "href": "chapters/portability.html#exp-docker",
    "title": "Portabilit√©",
    "section": "Exporting a Docker Image",
    "text": "Exporting a Docker Image\nSo far, all Docker commands we‚Äôve run were executed locally. This is fine for development and experimentation. But as we‚Äôve seen, one of Docker‚Äôs biggest strengths is the ability to easily share images with others. These images can then be used by multiple users to run the same application on their own machines.\nTo do this, we need to upload our image to a remote repository from which others can download it.\nThere are various options depending on your context: a company might have a private registry, while open-source projects can use DockerHub.\nHere is the typical workflow for uploading an image:\n\nCreate an account on DockerHub;\nCreate a (public) project on DockerHub to host your Docker images;\nUse docker login in your terminal to authenticate with DockerHub;\nModify the image tag during the build to include the expected path. For example: docker build -t username/project:version .;\nPush the image with docker push username/project:version.\n\n\n\nterminal\n\ndocker push avouacr/myflaskapp:1.0.0\n\nThe push refers to repository [docker.io/avouacr/myflaskapp]\n71db96687fe6: Pushed\n624877ac887b: Pushed\nea4ab6b86e70: Pushed\n..."
  },
  {
    "objectID": "chapters/portability.html#imp-docker",
    "href": "chapters/portability.html#imp-docker",
    "title": "Portabilit√©",
    "section": "Importing a Docker Image",
    "text": "Importing a Docker Image\nIf the image repository is public, users can pull the image using a single command:\n\n\nterminal\n\ndocker pull avouacr/myflaskapp:1.0.0\n\n1.0.0: Pulling from avouacr/myflaskapp\ne0b25ef51634: Pull complete\nc0445e4b247e: Pull complete\n...\nStatus: Downloaded newer image for avouacr/myflaskapp:1.0.0\nDocker will download and unpack each layer of the image (which may take time). Once downloaded, users can run the container with the docker run command as shown earlier."
  },
  {
    "objectID": "chapters/portability.html#cheat-sheet-1",
    "href": "chapters/portability.html#cheat-sheet-1",
    "title": "Portabilit√©",
    "section": "Cheat Sheet",
    "text": "Cheat Sheet\nHere‚Äôs a quick reference of common Dockerfile commands:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nFROM &lt;image&gt;:&lt;tag&gt;\nUse &lt;image&gt; with &lt;tag&gt; as the base image\n\n\nRUN &lt;instructions&gt;\nExecute Linux shell instructions. Use && to chain commands. Use \\ to split long commands across multiple lines\n\n\nCOPY &lt;source&gt; &lt;dest&gt;\nCopy a file from the local machine into the image\n\n\nADD &lt;source&gt; &lt;dest&gt;\nSimilar to COPY, but can also handle URLs and tar archives\n\n\nENV MY_NAME=\"John Doe\"\nDefine an environment variable available via $MY_NAME\n\n\nWORKDIR &lt;path&gt;\nSet the working directory inside the container\n\n\nUSER &lt;username&gt;\nSet a non-root user named &lt;username&gt;\n\n\nEXPOSE &lt;PORT_ID&gt;\nIndicate that the application will listen on port &lt;PORT_ID&gt;\n\n\nCMD [\"executable\",\"param1\",\"param2\"]\nDefine the default command to run when the container starts\n\n\n\nAnd a second cheat sheet with basic Docker CLI commands:\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndocker build . -t &lt;tag&gt;\nBuild the Docker image from current directory, tagging it with &lt;tag&gt;\n\n\ndocker run -it &lt;tag&gt;\nRun the container interactively from image with &lt;tag&gt;\n\n\ndocker images\nList locally available images and metadata (tags, size, etc.)\n\n\ndocker system prune\nClean up unused images and containers (use with caution)"
  },
  {
    "objectID": "chapters/portability.html#footnotes",
    "href": "chapters/portability.html#footnotes",
    "title": "Portabilit√©",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe‚Äôll later explain how distributing packages as precompiled wheels addresses this issue.‚Ü©Ô∏é\nThis means that if you open a new terminal, you‚Äôll need to activate the environment again if you want to use it. To activate an environment by default, you can configure your terminal (e.g., by editing .bashrc on Linux) to automatically activate a specific environment when it starts.‚Ü©Ô∏é\nIn fact, if you‚Äôre using pip on SSPCloud,\nyou‚Äôre doing exactly this‚Äîwithout realizing it.‚Ü©Ô∏é\nThese repositories are known as channels in the conda ecosystem.\nThe default channel is maintained by the developers at Anaconda.\nTo ensure stability, this channel updates more slowly.\nThe conda-forge channel emerged to offer developers more flexibility,\nletting them publish newer versions of their packages, much like PyPI.‚Ü©Ô∏é\nThe terms ‚Äúimage‚Äù and ‚Äúcontainer‚Äù are often used interchangeably.\nTechnically, a container is the live version of an image.‚Ü©Ô∏é\nFlask is a lightweight framework for deploying Python-based web applications.‚Ü©Ô∏é\nOn Windows, the default command lines (cmd or PowerShell) are not very convenient. We recommend using the Git Bash terminal, a lightweight Linux command-line emulator, for better compatibility with command-line operations.‚Ü©Ô∏é\nCaching is very useful for local development. Unfortunately, it‚Äôs harder to leverage in CI environments, since each run usually happens on a fresh machine.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/linux101.html",
    "href": "chapters/linux101.html",
    "title": "Linux 101",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des syst√®mes d‚Äôexploitation (y compris avec Windows !). Mais comme il a la r√©putation d‚Äô√™tre aust√®re et complexe, on utilise plut√¥t des interfaces graphiques pour effectuer nos op√©rations informatiques quotidiennes.\nPourtant, avoir des notions quant √† l‚Äôutilisation d‚Äôun terminal est une vraie source d‚Äôautonomie, dans la mesure o√π celui-ci permet de g√©rer bien plus finement les commandes que l‚Äôon r√©alise. Pour les data scientists qui s‚Äôint√©ressent aux bonnes pratiques et √† la mise en production, sa ma√Ætrise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont g√©n√©ralement limit√©es par rapport √† l‚Äôutilisation du programme en ligne de commande. C‚Äôest par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de r√©aliser toutes les op√©rations permises par le logiciel ;\nmettre un projet de data science en production n√©cessite d‚Äôutiliser un serveur, qui le rend disponible en permanence √† son public potentiel. Or l√† o√π Windows domine le monde des ordinateurs personnels, une large majorit√© des serveurs et des infrastructures cloud fonctionnent sous Linux ;\nles principaux langages de programmation des data scientists (, , etc. ) visent √† simplifier l‚Äôex√©cution d‚Äôop√©rations complexes par le biais de la ligne de commande mais h√©ritent n√©anmoins de sa logique. Utiliser la ligne de commande permet de gagner en aisance dans son travail de programmation ;\nplus g√©n√©ralement, une utilisation r√©guli√®re du terminal est source d‚Äôune meilleure compr√©hension du fonctionnement d‚Äôun syst√®me de fichiers et de l‚Äôex√©cution des processus sur un ordinateur. Ces connaissances s‚Äôav√®rent tr√®s utiles dans la pratique quotidienne du data scientist, qui n√©cessite de plus en plus de d√©velopper dans diff√©rents environnements d‚Äôex√©cution.\n\nDans le cadre de ce cours, on s‚Äôint√©ressera particuli√®rement au terminal Linux puisque l‚Äô√©crasante majorit√©, si ce n‚Äôest l‚Äôensemble, des serveurs de mise en production s‚Äôappuient sur un syst√®me Linux.\n\n\n\nDiff√©rents environnements de travail peuvent √™tre utilis√©s pour apprendre √† se servir d‚Äôun terminal Linux :\n\nle SSP Cloud. Dans la mesure o√π les exemples de mise en production du cours seront illustr√©es sur cet environnement, nous recommandons de l‚Äôutiliser d√®s √† pr√©sent pour se familiariser. Le terminal est accessible √† partir de diff√©rents services (RStudio, Jupyter, etc.), mais nous recommandons d‚Äôutiliser le terminal d‚Äôun service VSCode, dans la mesure o√π se servir d‚Äôun IDE pour organiser notre code est en soi d√©j√† une bonne pratique ;\nKatacoda, un bac √† sable dans un syst√®me Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (√©mulation minimaliste d‚Äôun terminal Linux), qui est install√©e par d√©faut avec Git.\n\n\n\n\nLan√ßons un terminal pour pr√©senter son fonctionnement basique. On prend pour exemple le terminal d‚Äôun service VSCode lanc√© via le SSP Cloud (Application Menu tout en haut √† gauche de VSCode -&gt; Terminal -&gt; New Terminal). Voici √† quoi ressemble le terminal en question.\n\nD√©crivons d‚Äôabord les diff√©rentes inscriptions qui arrivent √† l‚Äôinitialisation :\n\n(base) : cette inscription n‚Äôest pas directement li√©e au terminal, elle provient du fait que l‚Äôon utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en d√©tail dans le chapitre sur la portabilit√© ;\ncoder@vscode-824991-64744dd6d8-zbgv5 : le nom de l‚Äôutilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que l‚Äôon verra l√† encore dans le chapitre sur la portabilit√©\n~/work : le chemin du r√©pertoire courant, i.e.¬†√† partir duquel va √™tre lanc√©e toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\n\nPour √©viter la lourdeur des images et permettre de copier/coller facilement les commandes, on repr√©sentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans l‚Äôexemple suivant. Les lignes commen√ßant par un $ sont celles avec lesquelles une commande est lanc√©e, et les lignes sans $ repr√©sentent le r√©sultat d‚Äôune commande. Attention √† ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement √† diff√©rencier celles-ci des r√©sultats.\n\n\nterminal\n\necho \"une premi√®re illustration\"\n\nune premi√®re illustration\n\n\n\nLe terme filesystem (syst√®me de fichiers) d√©signe la mani√®re dont sont organis√©s les fichiers au sein d‚Äôun syst√®me d‚Äôexploitation. Cette structure est hi√©rarchique, en forme d‚Äôarbre :\n\nelle part d‚Äôun r√©pertoire racine (le dossier qui contient tous les autres) ;\ncontient des dossiers ;\nles dossiers peuvent contenir √† leur tour des dossiers (sous-dossiers) ou des fichiers.\n\nInt√©ressons nous √† la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations :\n\nla racine (root) sur Linux s‚Äôappelle /, l√† o√π elle s‚Äôappelle C:\\ par d√©faut sur Windows ;\nle r√©pertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un r√¥le essentiellement technique. Il est tout de m√™me utile d‚Äôen d√©crire les principaux :\n\n/bin : contient les binaires, i.e.¬†les programmes ex√©cutables ;\n/etc : contient les fichiers de configuration ;\n/home : contient l‚Äôensemble des dossiers et fichiers personnels des diff√©rents utilisateurs. Chaque utilisateur a un r√©pertoire dit ‚ÄúHOME‚Äù qui a pour chemin /home/&lt;username&gt; Ce r√©pertoire est souvent repr√©sent√© par le symbole ~. C‚Äô√©tait notamment le cas dans l‚Äôillustration du terminal VSCode ci-dessus, ce qui signifie qu‚Äôon se trouvait formellement dans le r√©pertoire /home/coder/work, coder √©tant l‚Äôutilisateur par d√©faut du service VSCode sur le SSP Cloud.\n\n\nChaque dossier ou fichier est repr√©sent√© par un chemin d‚Äôacc√®s, qui correspond simplement √† sa position dans le filesystem. Il existe deux moyens de sp√©cifier un chemin :\n\nen utilisant un chemin absolu, c‚Äôest √† dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconna√Æt donc un chemin absolu par le fait qu‚Äôil commence forc√©ment par /.\nen utilisant un chemin relatif, c‚Äôest √† dire en indiquant le chemin du dossier ou fichier relativement au r√©pertoire courant.\n\nComme tout ce qui touche de pr√®s ou de loin au terminal, la seule mani√®re de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront d‚Äôappliquer ces concepts √† des cas pratiques.\n\n\n\nLe r√¥le d‚Äôun terminal est de lancer des commandes. Ces commandes peuvent √™tre class√©es en trois grandes cat√©gories :\n\nnavigation au sein du filesystem\nmanipulations de fichiers (cr√©er, lire, modifier des dossiers/fichiers)\nlancement de programmes\n\n\n\nLorsque l‚Äôon lance un programme √† partir du terminal, celui-ci a pour r√©f√©rence le r√©pertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si l‚Äôon ex√©cute un script Python en se trouvant dans un certain r√©pertoire, tous les chemins des fichiers utilis√©s dans le script seront relatifs au r√©pertoire courant d‚Äôex√©cution ‚Äî √† moins d‚Äôutiliser uniquement des chemins absolus, ce qui n‚Äôest pas une bonne pratique en termes de reproductibilit√© puisque cela lie votre projet √† la structure de votre filesystem particulier.\nAinsi, la tr√®s grande majorit√© des op√©rations que l‚Äôon est amen√© √† r√©aliser dans un terminal consiste simplement √† se d√©placer au sein du filesystem. Les commandes principales pour naviguer et se rep√©rer dans le filesystem sont pr√©sent√©es dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez p√©nible de manipuler des chemins absolus, qui peuvent facilement √™tre tr√®s longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient √† se d√©placer √† partir du r√©pertoire courant. Pour se faire, voici quelques utilisations tr√®s fr√©quentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter d‚Äôun niveau dans l‚Äôarborescence (dossier parent)\n\n\ncd ~\nrevenir dans le r√©pertoire HOME de l‚Äôutilisateur courant\n\n\n\nLa premi√®re commande est l‚Äôoccasion de revenir sur une convention d‚Äô√©criture importante pour les chemins relatifs :\n\n. repr√©sente le r√©pertoire courant. Ainsi, cd . revient √† changer de r√©pertoire courant‚Ä¶ pour le r√©pertoire courant, ce qui bien s√ªr ne change rien. Mais le . est tr√®s utile pour la copie de fichiers (cf.¬†section suivante) ou encore lorsque l‚Äôon doit passer des param√®tres √† un programme (cf.¬†section Lancement de programmes) ;\n.. repr√©sente le r√©pertoire parent du r√©pertoire courant.\n\nCes diff√©rentes commandes constituent la tr√®s grande majorit√© des usages dans un terminal. Il est essentiel de les pratiquer jusqu‚Äô√† ce qu‚Äôelles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup d‚Äôautres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\nd√©placer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncr√©er (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncr√©er un fichier vide\n\n\n\nDans la mesure o√π il est g√©n√©ralement possible de r√©aliser toutes ces op√©rations √† l‚Äôaide d‚Äôinterfaces graphiques (notamment, l‚Äôexplorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se d√©placer dans le filesystem. Nous vous recommandons malgr√© tout de les pratiquer √©galement, et ce pour plusieurs raisons :\n\neffectuer un maximum d‚Äôop√©rations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ;\nen devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ;\nlorsque l‚Äôon est amen√© √† manipuler un terminal pour interagir avec un serveur, il n‚Äôy a souvent pas la moindre interface graphique, auquel cas il n‚Äôy a pas d‚Äôautre choix que d‚Äôop√©rer uniquement √† partir du terminal.\n\n\n\n\nLe r√¥le du terminal est de lancer des programmes. Lancer un programme se fait √† partir d‚Äôun fichier dit ex√©cutable, qui peut √™tre de deux formes :\n\nun binaire, i.e.¬†un programme dont le code n‚Äôest pas lisible par l‚Äôhumain ;\nun script, i.e.¬†un fichier texte contenant une s√©rie d‚Äôinstructions √† ex√©cuter. Le langage du terminal Linux est le shell, et les scripts associ√©s ont pour extension .sh.\n\nDans les deux cas, la syntaxe de lancement d‚Äôune commande est : le nom de l‚Äôex√©cutable, suivi d‚Äô√©ventuels param√®tres, s√©par√©s par des espaces. Par exemple, la commande python monscript.py ex√©cute le binaire python et lui passe comme unique argument le nom d‚Äôun script .py (contenu dans le r√©pertoire courant), qui va donc √™tre ex√©cut√© via Python. De la m√™me mani√®re, toutes les commandes vues pr√©c√©demment pour se d√©placer dans le filesystem ou manipuler des fichiers sont des ex√©cutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier √† copier et le chemin d‚Äôarriv√©e.\nDans les exemples de commandes pr√©c√©dents, les param√®tres √©taient pass√©s en mode positionnel : l‚Äôex√©cutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments n‚Äôest pas toujours fix√© √† l‚Äôavance, du fait de la pr√©sence de param√®tres optionnels. Ainsi, la plupart des ex√©cutables permettent le passage d‚Äôarguments optionnels, qui modifient le comportement de l‚Äôex√©cutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier √† un autre endroit du filesystem, mais peut-on copier un dossier et l‚Äôensemble de son contenu avec ? Nativement non, mais l‚Äôajout d‚Äôun param√®tre le permet : cp -R dossierdepart dossierarrivee permet de copier r√©cursivement le dossier et tout son contenu. Notons que les flags ont tr√®s souvent un √©quivalent en toute lettre, qui s‚Äô√©crit quant √† lui avec deux tirers. Par exemple, la commande pr√©c√©dente peut s‚Äô√©crire de mani√®re √©quivalente cp --recursive dossierdepart dossierarrivee. Il est fr√©quent de voir les deux syntaxes en pratique, parfois m√™me m√©lang√©es au sein d‚Äôune m√™me commande.\n\n\n\n\nComme tout langage de programmation, le langage shell permet d‚Äôassigner et d‚Äôutiliser des variables dans des commandes. Pour afficher le contenu d‚Äôune variable, on utilise la commande echo, qui est l‚Äô√©quivalent de la fonction print en Python ou en R.\n\n\nterminal\n\nMY_VAR=\"toto\"\necho $MY_VAR\n\ntoto\nQuelques remarques importantes :\n\nla syntaxe pour la cr√©ation de variable est pr√©cise : aucun espace d‚Äôun c√¥t√© comme de l‚Äôautre du = ;\nen Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu √©crire MY_VAR=toto pour le m√™me r√©sultat. Par contre, si l‚Äôon veut assigner √† une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message d‚Äôerreur ;\npour acc√©der √† la valeur d‚Äôune variable, on la pr√©fixe d‚Äôun $.\n\nNotre objectif avec ce tutoriel n‚Äôest pas de savoir coder en shell, on ne va donc pas s‚Äôattarder sur les propri√©t√©s des variables. En revanche, introduire ce concept √©tait n√©cessaire pour en pr√©senter un autre, essentiel quant √† lui dans la pratique quotidienne du data scientist : les variables d‚Äôenvironnement. Pour faire une analogie ‚Äî un peu simpliste ‚Äî avec les langages de programmation, ce sont des sortes de variables ‚Äúglobales‚Äù, dans la mesure o√π elles vont √™tre accessibles √† tous les programmes lanc√©s √† partir d‚Äôun terminal, et vont modifier leur comportement.\nLa liste des variables d‚Äôenvironnement peut √™tre affich√©e √† l‚Äôaide de la commande env. Il y a g√©n√©ralement un grand nombre de variables d‚Äôenvironnement pr√©√©xistantes ; en voici un √©chantillon obtenu √† partir du terminal du service VSCode.\n\n\nterminal\n\nenv\n\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la vari√©t√© des utilisations des variables d‚Äôenvironnements :\n\nla variable $SHELL pr√©cise l‚Äôex√©cutable utilis√© pour lancer le terminal ;\nla variable $HOME donne l‚Äôemplacement du r√©pertoire utilisateur. En fait, le symbole ~ que l‚Äôon a rencontr√© plus haut r√©f√©rence cette m√™me variable ;\nla variable LANG sp√©cifie la locale, un concept qui permet de d√©finir la langue et l‚Äôencodage utilis√©s par d√©faut par Linux ;\nla variable CONDA_PYTHON_EXE existe uniquement parce que l‚Äôon a install√© conda comme syst√®me de gestion de packages Python. C‚Äôest l‚Äôexistence de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous int√©resse.\n\nUne variable d‚Äôenvironnement essentielle, et que l‚Äôon est fr√©quemment amen√© √† modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concat√©nation de chemins absolus, s√©par√©s par :, qui sp√©cifie les dossiers dans lesquels Linux va chercher les ex√©cutables lorsque l‚Äôon lance une commande, ainsi que l‚Äôordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n\n\nterminal\n\necho $PATH\n\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nL‚Äôordre de recherche est de gauche √† droite. C‚Äôest donc parce que le dossier /home/coder/local/bin/conda/bin est situ√© en premier que l‚Äôinterpr√©teur Python qui sera choisi lorsque l‚Äôon lance un script Python est celui issu de Conda, et non celui contenu par d√©faut dans /usr/bin par exemple.\nL‚Äôexistence et la configuration ad√©quate des variables d‚Äôenvironnement est essentielle pour le bon fonctionnement de nombreux outils tr√®s utilis√©s en data science, comme Git ou encore Spark par exemple. Il est donc n√©cessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration d‚Äôun serveur en cas de bug li√© √† une variable d‚Äôenvironnement manquante ou mal configur√©e.\n\n\n\nLa s√©curit√© est un enjeu central en Linux, qui permet une gestion tr√®s fine des permissions sur les diff√©rents fichiers et programmes.\nUne diff√©rence majeure par rapport √† d‚Äôautres syst√®mes d‚Äôexploitation, notamment Windows, est qu‚Äôaucun utilisateur n‚Äôa par d√©faut les droits complets d‚Äôadministrateur (root). Il n‚Äôest donc pas possible nativement d‚Äôacc√©der au parties sensibles du syst√®me, ou bien de lancer certains types de programme. Par exemple, si l‚Äôon essaie de lister les fichiers du dossier /root, on obtient une erreur.\n\n\nterminal\n\nls /root\n\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines op√©rations telles que l‚Äôinstallation de binaires ou de packages n√©cessitent cependant des droits administrateurs. Dans ce cas, il est d‚Äôusage d‚Äôutiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de l‚Äôex√©cution de la commande.\n\n\nterminal\n\nsudo ls /root\n\nLe dossier /root √©tant vide, la commande ls renvoie une cha√Æne de caract√®res vide, mais nous n‚Äôavons plus de probl√®me de permission. Notons qu‚Äôune bonne pratique de s√©curit√©, en particulier dans les scripts shell que l‚Äôon peut √™tre amen√©s √† √©crire ou ex√©cuter, est de limiter l‚Äôutilisation de cette commande aux cas o√π elle s‚Äôav√®re n√©cessaire.\nUne autre subtilit√© concerne justement l‚Äôex√©cution de scripts shell. Par d√©faut, qu‚Äôil soit cr√©√© par l‚Äôutilisateur ou t√©l√©charg√© d‚Äôinternet, un script n‚Äôest pas ex√©cutable.\n\n\nterminal\n\n1touch test.sh\n2./test.sh\n\n\n1\n\nCr√©er le script test.sh (vide)\n\n2\n\nEx√©cuter le script test.sh\n\n\nbash: ./test.sh: Permission denied\nC‚Äôest bien entendu une mesure de s√©curit√© pour √©viter l‚Äôex√©cution automatique de scripts potentiellement malveillants. Pour pouvoir ex√©cuter un tel script, il faut attribuer des droits d‚Äôex√©cution au fichier avec la commande chmod. Il devient alors possible d‚Äôex√©cuter le script classiquement.\n\n\nterminal\n\n1chmod +x test.sh\n2./test.sh\n\n# Le script √©tant vide, il ne se passe rien\n\n\n1\n\nDonner des droits d‚Äôex√©cution au script test.sh\n\n2\n\nEx√©cuter le script test.sh\n\n\n\n\n\nMaintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell pr√©c√©demment √©voqu√©s. A l‚Äôinstar d‚Äôun script Python, un script shell permet d‚Äôautomatiser une s√©rie de commandes lanc√©es dans un terminal. Le but de ce tutoriel n‚Äôest pas de savoir √©crire des scripts shell complexes, travail g√©n√©ralement d√©volu aux les data engineers ou les sysadmin (administrateurs syst√®me), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces comp√©tences sont essentielles lorsque l‚Äôon se pr√©occupe de mise en production. A titre d‚Äôexemple, comme nous le verrons dans le chapitre sur la portabilit√©, il est fr√©quent d‚Äôutiliser un script shell comme entrypoint d‚Äôune image docker, afin de sp√©cifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement √† l‚Äôaide d‚Äôun script simple. Consid√©rons les commandes suivantes, que l‚Äôon met dans un fichier monscript.sh dans le r√©pertoire courant.\n\n\nterminal\n\n#!/bin/bash\nSECTION=$1\nCHAPTER=$2\nFORMATION_DIR=/home/coder/work/formation\nmkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\ntouch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\n\nAnalysons la structure de ce script :\n\nla premi√®re ligne est classique, elle se nomme le shebang : elle indique au syst√®me quel interpr√©teur utiliser pour ex√©cuter ce script. Dans notre cas, et de mani√®re g√©n√©rale, on utilise bash (Bourne-Again SHell, l‚Äôimpl√©mentation moderne du shell) ;\nles lignes 2 et 3 assignent √† des variables les arguments pass√©s au script dans la commande. Par d√©faut, ceux-ci sont assign√©s √† des variables n o√π n est la position de l‚Äôargument, en commen√ßant √† 1 ;\nla ligne 4 assigne un chemin √† une variable\nla ligne 5 cr√©e le chemin complet, d√©fini √† partir des variables cr√©√©es pr√©c√©demment. Le param√®tre -p est important : il pr√©cise √† mkdir d‚Äôagir de mani√®re r√©cursive, c‚Äôest √† dire de cr√©er les dossiers interm√©diaires qui n‚Äôexistent pas encore ;\nla ligne 6 cr√©e un fichier texte vide dans le dossier cr√©√© avec la commande pr√©c√©dente.\n\nEx√©cutons maintenant ce script, en prenant soin de lui donner les permission ad√©quates au pr√©alable.\n\n\nterminal\n\nchmod +x monscript.sh\nbash monscript.sh section2 chapitre3\nls formation/section1/chapitre2/\n\ntext.txt\nOp√©ration r√©ussie : le dossier a bien √©t√© cr√©√© et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash tr√®s bien r√©alis√©e.\n\n\n\nUne diff√©rence fondamentale entre Linux et Windows tient √† la mani√®re dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier ex√©cutable en .exe) sur le site du logiciel, et on l‚Äôex√©cute. En Linux, on passe g√©n√©ralement par un gestionnaire de packages qui va chercher les logiciels sur un r√©pertoire centralis√©, √† la mani√®re de pip en Python par exemple.\nPourquoi cette diff√©rence ? Une raison importante est que, contrairement √† Windows, il existe une multitude de distributions diff√©rentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent diff√©remment et peuvent avoir diff√©rentes versions. En utilisant le package manager (gestionnaire de paquets) propre √† la distribution en question, on s‚Äôassure de t√©l√©charger le logiciel adapt√© √† sa distribution. Dans ce cours, on fait le choix d‚Äôutiliser une distribution Debian et son gestionnaire de paquets associ√© apt. Debian est en effet un choix populaire pour les servers de part sa stabilit√© et sa simplicit√©, et sera √©galement famili√®re aux utilisateurs d‚ÄôUbuntu, distribution tr√®s populaire pour les ordinateurs personnels et qui est bas√©e sur Debian.\nL‚Äôutilisation d‚Äôapt est tr√®s simple. La seule difficult√© est de savoir le nom du paquet que l‚Äôon souhaite installer, ce qui n√©cessite en g√©n√©ral d‚Äôutiliser un moteur de recherche. L‚Äôinstallation de paquets est √©galement un cas o√π il faut utiliser sudo, puisque cela implique souvent l‚Äôacc√®s √† des r√©pertoires prot√©g√©s.\n\n\nterminal\n\nsudo apt install tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nD√©sinstaller un package est √©galement simple : c‚Äôest l‚Äôop√©ration inverse. Par s√©curit√©, le terminal vous demande si vous √™tes s√ªr de votre choix en vous demandant de tapper la lettre y (yes) ou la lettre n.¬†On peut passer automatiquement cette √©tape en ajoutant le param√®tre -y\n\n\nterminal\n\nsudo apt remove -y tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant d‚Äôinstaller un package, il est toujours pr√©f√©rable de mettre √† jour la base des packages, pour s‚Äôassurer qu‚Äôon obtiendra bien la derni√®re version.\n\n\nterminal\n\nsudo apt update\n\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date.\n\n\n\nOn l‚Äôa dit et redit : devenir √† l‚Äôaise avec le terminal Linux est essentiel et demande de la pratique. Il existe n√©anmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa premi√®re est l‚Äôautocompl√©tion. D√®s lors que vous √©crivez une commande contenant un nom d‚Äôex√©cutable, un chemin sur le filesystem, ou autre, n‚Äôh√©sitez pas √† utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majorit√© des cas, cela va vous faire gagner un temps pr√©cieux.\nLa deuxi√®me est de parcourir l‚Äôhistorique de commandes : la fl√®che vers le haut (‚Üë) permet de parcourir l‚Äôhistorique des commandes que vous avez pr√©c√©demment ex√©cut√©es. Chaque fois que vous appuyez sur cette touche, le terminal affiche la derni√®re commande ex√©cut√©e, en remontant dans l‚Äôhistorique √† chaque appui suppl√©mentaire.\nLa troisi√®me, dans le m√™me esprit que la deuxi√®me mais plus √©labor√©e, est la recherche inverse dans l‚Äôhistorique de commandes avec les touches Ctrl+R. Lorsque vous appuyez sur Ctrl+R, une invite de recherche appara√Æt. Vous pouvez alors commencer √† taper des caract√®res de la commande que vous recherchez. Le terminal cherchera dans l‚Äôhistorique des commandes la derni√®re commande correspondant √† ce que vous avez tap√©, et la montrera √† l‚Äô√©cran. Si ce n‚Äôest pas la commande exacte que vous cherchez, vous pouvez continuer √† taper pour affiner la recherche ou appuyer √† nouveau sur Ctrl+R pour rechercher la commande pr√©c√©dente correspondant √† vos crit√®res.\nUne quatri√®me astuce, qui n‚Äôen est pas vraiment une, est de lire la documentation d‚Äôune commande lorsque l‚Äôon n‚Äôest pas s√ªr de sa syntaxe ou des param√®tres admissibles. Via le terminal, la documentation d‚Äôune commande peut √™tre affich√©e en ex√©cutant man suivie de la commande en question, par exemple : man cp. Comme il n‚Äôest pas toujours tr√®s pratique de lire de longs textes dans un petit terminal, on peut √©galement chercher la documentation d‚Äôune commande sur le site man7."
  },
  {
    "objectID": "chapters/linux101.html#pourquoi-sint√©resser-au-terminal-linux",
    "href": "chapters/linux101.html#pourquoi-sint√©resser-au-terminal-linux",
    "title": "Linux 101",
    "section": "",
    "text": "Le terminal (ou ligne de commande) est une console interactive qui permet de lancer des commandes. Il existe dans la plupart des syst√®mes d‚Äôexploitation (y compris avec Windows !). Mais comme il a la r√©putation d‚Äô√™tre aust√®re et complexe, on utilise plut√¥t des interfaces graphiques pour effectuer nos op√©rations informatiques quotidiennes.\nPourtant, avoir des notions quant √† l‚Äôutilisation d‚Äôun terminal est une vraie source d‚Äôautonomie, dans la mesure o√π celui-ci permet de g√©rer bien plus finement les commandes que l‚Äôon r√©alise. Pour les data scientists qui s‚Äôint√©ressent aux bonnes pratiques et √† la mise en production, sa ma√Ætrise est essentielle. Les raisons sont multiples :\n\nles interfaces graphiques des logiciels sont g√©n√©ralement limit√©es par rapport √† l‚Äôutilisation du programme en ligne de commande. C‚Äôest par exemple le cas de Git et de Docker. Dans les deux cas, seul le client en ligne de commande permet de r√©aliser toutes les op√©rations permises par le logiciel ;\nmettre un projet de data science en production n√©cessite d‚Äôutiliser un serveur, qui le rend disponible en permanence √† son public potentiel. Or l√† o√π Windows domine le monde des ordinateurs personnels, une large majorit√© des serveurs et des infrastructures cloud fonctionnent sous Linux ;\nles principaux langages de programmation des data scientists (, , etc. ) visent √† simplifier l‚Äôex√©cution d‚Äôop√©rations complexes par le biais de la ligne de commande mais h√©ritent n√©anmoins de sa logique. Utiliser la ligne de commande permet de gagner en aisance dans son travail de programmation ;\nplus g√©n√©ralement, une utilisation r√©guli√®re du terminal est source d‚Äôune meilleure compr√©hension du fonctionnement d‚Äôun syst√®me de fichiers et de l‚Äôex√©cution des processus sur un ordinateur. Ces connaissances s‚Äôav√®rent tr√®s utiles dans la pratique quotidienne du data scientist, qui n√©cessite de plus en plus de d√©velopper dans diff√©rents environnements d‚Äôex√©cution.\n\nDans le cadre de ce cours, on s‚Äôint√©ressera particuli√®rement au terminal Linux puisque l‚Äô√©crasante majorit√©, si ce n‚Äôest l‚Äôensemble, des serveurs de mise en production s‚Äôappuient sur un syst√®me Linux."
  },
  {
    "objectID": "chapters/linux101.html#environnement-de-travail",
    "href": "chapters/linux101.html#environnement-de-travail",
    "title": "Linux 101",
    "section": "",
    "text": "Diff√©rents environnements de travail peuvent √™tre utilis√©s pour apprendre √† se servir d‚Äôun terminal Linux :\n\nle SSP Cloud. Dans la mesure o√π les exemples de mise en production du cours seront illustr√©es sur cet environnement, nous recommandons de l‚Äôutiliser d√®s √† pr√©sent pour se familiariser. Le terminal est accessible √† partir de diff√©rents services (RStudio, Jupyter, etc.), mais nous recommandons d‚Äôutiliser le terminal d‚Äôun service VSCode, dans la mesure o√π se servir d‚Äôun IDE pour organiser notre code est en soi d√©j√† une bonne pratique ;\nKatacoda, un bac √† sable dans un syst√®me Ubuntu, la distribution Linux la plus populaire ;\nsur Windows : Git Bash (√©mulation minimaliste d‚Äôun terminal Linux), qui est install√©e par d√©faut avec Git."
  },
  {
    "objectID": "chapters/linux101.html#introduction-au-terminal",
    "href": "chapters/linux101.html#introduction-au-terminal",
    "title": "Linux 101",
    "section": "",
    "text": "Lan√ßons un terminal pour pr√©senter son fonctionnement basique. On prend pour exemple le terminal d‚Äôun service VSCode lanc√© via le SSP Cloud (Application Menu tout en haut √† gauche de VSCode -&gt; Terminal -&gt; New Terminal). Voici √† quoi ressemble le terminal en question.\n\nD√©crivons d‚Äôabord les diff√©rentes inscriptions qui arrivent √† l‚Äôinitialisation :\n\n(base) : cette inscription n‚Äôest pas directement li√©e au terminal, elle provient du fait que l‚Äôon utilise un environnement conda. Nous verrons le fonctionnement des environnements virtuels en d√©tail dans le chapitre sur la portabilit√© ;\ncoder@vscode-824991-64744dd6d8-zbgv5 : le nom de l‚Äôutilisateur (ici coder) et le nom de la machine (ici, un conteneur, notion que l‚Äôon verra l√† encore dans le chapitre sur la portabilit√©\n~/work : le chemin du r√©pertoire courant, i.e.¬†√† partir duquel va √™tre lanc√©e toute commande. On comprendra mieux la signification de ce chemin dans la section suivante.\n\nPour √©viter la lourdeur des images et permettre de copier/coller facilement les commandes, on repr√©sentera dans la suite du tutoriel (et du cours) le terminal du service VSCode par des bandes de texte sur fond noir, comme dans l‚Äôexemple suivant. Les lignes commen√ßant par un $ sont celles avec lesquelles une commande est lanc√©e, et les lignes sans $ repr√©sentent le r√©sultat d‚Äôune commande. Attention √† ne pas inclure le $ lorsque vous lancez les commandes, il sert simplement √† diff√©rencier celles-ci des r√©sultats.\n\n\nterminal\n\necho \"une premi√®re illustration\"\n\nune premi√®re illustration"
  },
  {
    "objectID": "chapters/linux101.html#notions-de-filesystem",
    "href": "chapters/linux101.html#notions-de-filesystem",
    "title": "Linux 101",
    "section": "",
    "text": "Le terme filesystem (syst√®me de fichiers) d√©signe la mani√®re dont sont organis√©s les fichiers au sein d‚Äôun syst√®me d‚Äôexploitation. Cette structure est hi√©rarchique, en forme d‚Äôarbre :\n\nelle part d‚Äôun r√©pertoire racine (le dossier qui contient tous les autres) ;\ncontient des dossiers ;\nles dossiers peuvent contenir √† leur tour des dossiers (sous-dossiers) ou des fichiers.\n\nInt√©ressons nous √† la structure du filesystem Linux standard.\n\nSource : commons.wikimedia.org\nQuelques observations :\n\nla racine (root) sur Linux s‚Äôappelle /, l√† o√π elle s‚Äôappelle C:\\ par d√©faut sur Windows ;\nle r√©pertoire racine contient un ensemble de sous-dossiers, dont la plupart ont un r√¥le essentiellement technique. Il est tout de m√™me utile d‚Äôen d√©crire les principaux :\n\n/bin : contient les binaires, i.e.¬†les programmes ex√©cutables ;\n/etc : contient les fichiers de configuration ;\n/home : contient l‚Äôensemble des dossiers et fichiers personnels des diff√©rents utilisateurs. Chaque utilisateur a un r√©pertoire dit ‚ÄúHOME‚Äù qui a pour chemin /home/&lt;username&gt; Ce r√©pertoire est souvent repr√©sent√© par le symbole ~. C‚Äô√©tait notamment le cas dans l‚Äôillustration du terminal VSCode ci-dessus, ce qui signifie qu‚Äôon se trouvait formellement dans le r√©pertoire /home/coder/work, coder √©tant l‚Äôutilisateur par d√©faut du service VSCode sur le SSP Cloud.\n\n\nChaque dossier ou fichier est repr√©sent√© par un chemin d‚Äôacc√®s, qui correspond simplement √† sa position dans le filesystem. Il existe deux moyens de sp√©cifier un chemin :\n\nen utilisant un chemin absolu, c‚Äôest √† dire en indiquant le chemin complet du dossier ou fichier depuis la racine. En Linux, on reconna√Æt donc un chemin absolu par le fait qu‚Äôil commence forc√©ment par /.\nen utilisant un chemin relatif, c‚Äôest √† dire en indiquant le chemin du dossier ou fichier relativement au r√©pertoire courant.\n\nComme tout ce qui touche de pr√®s ou de loin au terminal, la seule mani√®re de bien comprendre ces notions est de les appliquer. Les exercices de fin de chapitre vous permettront d‚Äôappliquer ces concepts √† des cas pratiques."
  },
  {
    "objectID": "chapters/linux101.html#lancer-des-commandes",
    "href": "chapters/linux101.html#lancer-des-commandes",
    "title": "Linux 101",
    "section": "",
    "text": "Le r√¥le d‚Äôun terminal est de lancer des commandes. Ces commandes peuvent √™tre class√©es en trois grandes cat√©gories :\n\nnavigation au sein du filesystem\nmanipulations de fichiers (cr√©er, lire, modifier des dossiers/fichiers)\nlancement de programmes\n\n\n\nLorsque l‚Äôon lance un programme √† partir du terminal, celui-ci a pour r√©f√©rence le r√©pertoire courant dans lequel on se trouve au moment du lancement. Par exemple, si l‚Äôon ex√©cute un script Python en se trouvant dans un certain r√©pertoire, tous les chemins des fichiers utilis√©s dans le script seront relatifs au r√©pertoire courant d‚Äôex√©cution ‚Äî √† moins d‚Äôutiliser uniquement des chemins absolus, ce qui n‚Äôest pas une bonne pratique en termes de reproductibilit√© puisque cela lie votre projet √† la structure de votre filesystem particulier.\nAinsi, la tr√®s grande majorit√© des op√©rations que l‚Äôon est amen√© √† r√©aliser dans un terminal consiste simplement √† se d√©placer au sein du filesystem. Les commandes principales pour naviguer et se rep√©rer dans le filesystem sont pr√©sent√©es dans la table suivante.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\npwd\nafficher (Print Working Directory) le chemin (absolu) du dossier courant\n\n\ncd chemin\nchanger (Change Directory) de dossier courant\n\n\nls\nlister les fichiers dans le dossier courant\n\n\n\nLa commande cd accepte aussi bien des chemins absolus que des chemins relatifs. En pratique, il est assez p√©nible de manipuler des chemins absolus, qui peuvent facilement √™tre tr√®s longs. On utilisera donc essentiellement des chemins relatifs, ce qui revient √† se d√©placer √† partir du r√©pertoire courant. Pour se faire, voici quelques utilisations tr√®s fr√©quentes de la commande cd.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncd ..\nremonter d‚Äôun niveau dans l‚Äôarborescence (dossier parent)\n\n\ncd ~\nrevenir dans le r√©pertoire HOME de l‚Äôutilisateur courant\n\n\n\nLa premi√®re commande est l‚Äôoccasion de revenir sur une convention d‚Äô√©criture importante pour les chemins relatifs :\n\n. repr√©sente le r√©pertoire courant. Ainsi, cd . revient √† changer de r√©pertoire courant‚Ä¶ pour le r√©pertoire courant, ce qui bien s√ªr ne change rien. Mais le . est tr√®s utile pour la copie de fichiers (cf.¬†section suivante) ou encore lorsque l‚Äôon doit passer des param√®tres √† un programme (cf.¬†section Lancement de programmes) ;\n.. repr√©sente le r√©pertoire parent du r√©pertoire courant.\n\nCes diff√©rentes commandes constituent la tr√®s grande majorit√© des usages dans un terminal. Il est essentiel de les pratiquer jusqu‚Äô√† ce qu‚Äôelles deviennent une seconde nature.\n\n\n\nLes commandes suivantes permettent de manipuler le filesystem. Il en existe beaucoup d‚Äôautres, mais elles couvrent la plupart des besoins.\n\n\n\n\n\n\n\nCommande\nDescription\n\n\n\n\ncp fichierdepart fichierarrivee\ncopier (CoPy) un fichier\n\n\nmv fichierdepart fichierarrivee\nd√©placer (MoVe) un fichier\n\n\nrm nomdufichier\nsupprimer (ReMove) un fichier\n\n\ncat nomdufichier\nafficher le contenu du fichier\n\n\nmkdir nomdudossier\ncr√©er (MaKe DIRectory) un dossier\n\n\ntouch nomdufichier\ncr√©er un fichier vide\n\n\n\nDans la mesure o√π il est g√©n√©ralement possible de r√©aliser toutes ces op√©rations √† l‚Äôaide d‚Äôinterfaces graphiques (notamment, l‚Äôexplorateur de fichiers), celles-ci sont moins essentielles que celles permettant de se d√©placer dans le filesystem. Nous vous recommandons malgr√© tout de les pratiquer √©galement, et ce pour plusieurs raisons :\n\neffectuer un maximum d‚Äôop√©rations via le terminal permet de bien comprendre son fonctionnement et donc de gagner en autonomie ;\nen devenant efficient sur ces commandes, vous vous rendrez compte que manipuler le filesystem via le terminal est en fait plus rapide que via une interface graphique ;\nlorsque l‚Äôon est amen√© √† manipuler un terminal pour interagir avec un serveur, il n‚Äôy a souvent pas la moindre interface graphique, auquel cas il n‚Äôy a pas d‚Äôautre choix que d‚Äôop√©rer uniquement √† partir du terminal.\n\n\n\n\nLe r√¥le du terminal est de lancer des programmes. Lancer un programme se fait √† partir d‚Äôun fichier dit ex√©cutable, qui peut √™tre de deux formes :\n\nun binaire, i.e.¬†un programme dont le code n‚Äôest pas lisible par l‚Äôhumain ;\nun script, i.e.¬†un fichier texte contenant une s√©rie d‚Äôinstructions √† ex√©cuter. Le langage du terminal Linux est le shell, et les scripts associ√©s ont pour extension .sh.\n\nDans les deux cas, la syntaxe de lancement d‚Äôune commande est : le nom de l‚Äôex√©cutable, suivi d‚Äô√©ventuels param√®tres, s√©par√©s par des espaces. Par exemple, la commande python monscript.py ex√©cute le binaire python et lui passe comme unique argument le nom d‚Äôun script .py (contenu dans le r√©pertoire courant), qui va donc √™tre ex√©cut√© via Python. De la m√™me mani√®re, toutes les commandes vues pr√©c√©demment pour se d√©placer dans le filesystem ou manipuler des fichiers sont des ex√©cutables et fonctionnent donc selon ce principe. Par exemple, cp fichierdepart fichierarrivee lance le binaire cp en lui passant deux arguments : le chemin du fichier √† copier et le chemin d‚Äôarriv√©e.\nDans les exemples de commandes pr√©c√©dents, les param√®tres √©taient pass√©s en mode positionnel : l‚Äôex√©cutable attend des arguments dans un certain ordre, ce qui est clair dans le cas de cp par exemple. Mais le nombre des arguments n‚Äôest pas toujours fix√© √† l‚Äôavance, du fait de la pr√©sence de param√®tres optionnels. Ainsi, la plupart des ex√©cutables permettent le passage d‚Äôarguments optionnels, qui modifient le comportement de l‚Äôex√©cutable, via des flags. Par exemple, on a vu que cp permettait de copier un fichier √† un autre endroit du filesystem, mais peut-on copier un dossier et l‚Äôensemble de son contenu avec ? Nativement non, mais l‚Äôajout d‚Äôun param√®tre le permet : cp -R dossierdepart dossierarrivee permet de copier r√©cursivement le dossier et tout son contenu. Notons que les flags ont tr√®s souvent un √©quivalent en toute lettre, qui s‚Äô√©crit quant √† lui avec deux tirers. Par exemple, la commande pr√©c√©dente peut s‚Äô√©crire de mani√®re √©quivalente cp --recursive dossierdepart dossierarrivee. Il est fr√©quent de voir les deux syntaxes en pratique, parfois m√™me m√©lang√©es au sein d‚Äôune m√™me commande."
  },
  {
    "objectID": "chapters/linux101.html#variables-denvironnement",
    "href": "chapters/linux101.html#variables-denvironnement",
    "title": "Linux 101",
    "section": "",
    "text": "Comme tout langage de programmation, le langage shell permet d‚Äôassigner et d‚Äôutiliser des variables dans des commandes. Pour afficher le contenu d‚Äôune variable, on utilise la commande echo, qui est l‚Äô√©quivalent de la fonction print en Python ou en R.\n\n\nterminal\n\nMY_VAR=\"toto\"\necho $MY_VAR\n\ntoto\nQuelques remarques importantes :\n\nla syntaxe pour la cr√©ation de variable est pr√©cise : aucun espace d‚Äôun c√¥t√© comme de l‚Äôautre du = ;\nen Shell, on ne manipule que du texte. Dans notre exemple, on aurait donc pu √©crire MY_VAR=toto pour le m√™me r√©sultat. Par contre, si l‚Äôon veut assigner √† une variable une valeur contenant des espaces, les guillemets deviennent indispensables pour ne pas obtenir un message d‚Äôerreur ;\npour acc√©der √† la valeur d‚Äôune variable, on la pr√©fixe d‚Äôun $.\n\nNotre objectif avec ce tutoriel n‚Äôest pas de savoir coder en shell, on ne va donc pas s‚Äôattarder sur les propri√©t√©s des variables. En revanche, introduire ce concept √©tait n√©cessaire pour en pr√©senter un autre, essentiel quant √† lui dans la pratique quotidienne du data scientist : les variables d‚Äôenvironnement. Pour faire une analogie ‚Äî un peu simpliste ‚Äî avec les langages de programmation, ce sont des sortes de variables ‚Äúglobales‚Äù, dans la mesure o√π elles vont √™tre accessibles √† tous les programmes lanc√©s √† partir d‚Äôun terminal, et vont modifier leur comportement.\nLa liste des variables d‚Äôenvironnement peut √™tre affich√©e √† l‚Äôaide de la commande env. Il y a g√©n√©ralement un grand nombre de variables d‚Äôenvironnement pr√©√©xistantes ; en voici un √©chantillon obtenu √† partir du terminal du service VSCode.\n\n\nterminal\n\nenv\n\nSHELL=/bin/bash\nHOME=/home/coder\nLANG=en_US.UTF-8\nCONDA_PYTHON_EXE=/home/coder/local/bin/conda/bin/python\nCette liste illustre la vari√©t√© des utilisations des variables d‚Äôenvironnements :\n\nla variable $SHELL pr√©cise l‚Äôex√©cutable utilis√© pour lancer le terminal ;\nla variable $HOME donne l‚Äôemplacement du r√©pertoire utilisateur. En fait, le symbole ~ que l‚Äôon a rencontr√© plus haut r√©f√©rence cette m√™me variable ;\nla variable LANG sp√©cifie la locale, un concept qui permet de d√©finir la langue et l‚Äôencodage utilis√©s par d√©faut par Linux ;\nla variable CONDA_PYTHON_EXE existe uniquement parce que l‚Äôon a install√© conda comme syst√®me de gestion de packages Python. C‚Äôest l‚Äôexistence de cette variable qui fait que la commande python mon_script.py va utiliser comme binaire la version de Python qui nous int√©resse.\n\nUne variable d‚Äôenvironnement essentielle, et que l‚Äôon est fr√©quemment amen√© √† modifier dans les applications de data science, est la variable $PATH. Elle consiste en une concat√©nation de chemins absolus, s√©par√©s par :, qui sp√©cifie les dossiers dans lesquels Linux va chercher les ex√©cutables lorsque l‚Äôon lance une commande, ainsi que l‚Äôordre de la recherche. Regardons la valeur du $PATH sur le terminal du service VSCode.\n\n\nterminal\n\necho $PATH\n\n/home/coder/local/bin/conda/bin:/home/coder/local/bin/conda/condabin:/home/coder/local/bin/conda/envs/basesspcloud/bin:/home/coder/local/bin/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nL‚Äôordre de recherche est de gauche √† droite. C‚Äôest donc parce que le dossier /home/coder/local/bin/conda/bin est situ√© en premier que l‚Äôinterpr√©teur Python qui sera choisi lorsque l‚Äôon lance un script Python est celui issu de Conda, et non celui contenu par d√©faut dans /usr/bin par exemple.\nL‚Äôexistence et la configuration ad√©quate des variables d‚Äôenvironnement est essentielle pour le bon fonctionnement de nombreux outils tr√®s utilis√©s en data science, comme Git ou encore Spark par exemple. Il est donc n√©cessaire de comprendre leur fonctionnement pour pouvoir lire des documentations techniques et adapter la configuration d‚Äôun serveur en cas de bug li√© √† une variable d‚Äôenvironnement manquante ou mal configur√©e."
  },
  {
    "objectID": "chapters/linux101.html#permissions",
    "href": "chapters/linux101.html#permissions",
    "title": "Linux 101",
    "section": "",
    "text": "La s√©curit√© est un enjeu central en Linux, qui permet une gestion tr√®s fine des permissions sur les diff√©rents fichiers et programmes.\nUne diff√©rence majeure par rapport √† d‚Äôautres syst√®mes d‚Äôexploitation, notamment Windows, est qu‚Äôaucun utilisateur n‚Äôa par d√©faut les droits complets d‚Äôadministrateur (root). Il n‚Äôest donc pas possible nativement d‚Äôacc√©der au parties sensibles du syst√®me, ou bien de lancer certains types de programme. Par exemple, si l‚Äôon essaie de lister les fichiers du dossier /root, on obtient une erreur.\n\n\nterminal\n\nls /root\n\nls: cannot open directory '/root': Permission denied\nDans la pratique du quotidien, certaines op√©rations telles que l‚Äôinstallation de binaires ou de packages n√©cessitent cependant des droits administrateurs. Dans ce cas, il est d‚Äôusage d‚Äôutiliser la commande sudo (Substitute User DO), qui permet de prendre les droits root le temps de l‚Äôex√©cution de la commande.\n\n\nterminal\n\nsudo ls /root\n\nLe dossier /root √©tant vide, la commande ls renvoie une cha√Æne de caract√®res vide, mais nous n‚Äôavons plus de probl√®me de permission. Notons qu‚Äôune bonne pratique de s√©curit√©, en particulier dans les scripts shell que l‚Äôon peut √™tre amen√©s √† √©crire ou ex√©cuter, est de limiter l‚Äôutilisation de cette commande aux cas o√π elle s‚Äôav√®re n√©cessaire.\nUne autre subtilit√© concerne justement l‚Äôex√©cution de scripts shell. Par d√©faut, qu‚Äôil soit cr√©√© par l‚Äôutilisateur ou t√©l√©charg√© d‚Äôinternet, un script n‚Äôest pas ex√©cutable.\n\n\nterminal\n\n1touch test.sh\n2./test.sh\n\n\n1\n\nCr√©er le script test.sh (vide)\n\n2\n\nEx√©cuter le script test.sh\n\n\nbash: ./test.sh: Permission denied\nC‚Äôest bien entendu une mesure de s√©curit√© pour √©viter l‚Äôex√©cution automatique de scripts potentiellement malveillants. Pour pouvoir ex√©cuter un tel script, il faut attribuer des droits d‚Äôex√©cution au fichier avec la commande chmod. Il devient alors possible d‚Äôex√©cuter le script classiquement.\n\n\nterminal\n\n1chmod +x test.sh\n2./test.sh\n\n# Le script √©tant vide, il ne se passe rien\n\n\n1\n\nDonner des droits d‚Äôex√©cution au script test.sh\n\n2\n\nEx√©cuter le script test.sh"
  },
  {
    "objectID": "chapters/linux101.html#les-scripts-shell",
    "href": "chapters/linux101.html#les-scripts-shell",
    "title": "Linux 101",
    "section": "",
    "text": "Maintenant que nous avons vu les variables et les permissions, revenons sur les scripts shell pr√©c√©demment √©voqu√©s. A l‚Äôinstar d‚Äôun script Python, un script shell permet d‚Äôautomatiser une s√©rie de commandes lanc√©es dans un terminal. Le but de ce tutoriel n‚Äôest pas de savoir √©crire des scripts shell complexes, travail g√©n√©ralement d√©volu aux les data engineers ou les sysadmin (administrateurs syst√®me), mais de comprendre leur structure, leur fonctionnement, et de savoir lancer des scripts simples. Ces comp√©tences sont essentielles lorsque l‚Äôon se pr√©occupe de mise en production. A titre d‚Äôexemple, comme nous le verrons dans le chapitre sur la portabilit√©, il est fr√©quent d‚Äôutiliser un script shell comme entrypoint d‚Äôune image docker, afin de sp√©cifier les commandes que doit lancer le conteneur lors de son initialisation.\nIllustrons leur structure ainsi que leur fonctionnement √† l‚Äôaide d‚Äôun script simple. Consid√©rons les commandes suivantes, que l‚Äôon met dans un fichier monscript.sh dans le r√©pertoire courant.\n\n\nterminal\n\n#!/bin/bash\nSECTION=$1\nCHAPTER=$2\nFORMATION_DIR=/home/coder/work/formation\nmkdir -p $FORMATION_DIR/$SECTION/$CHAPTER\ntouch $FORMATION_DIR/$SECTION/$CHAPTER/test.txt\n\nAnalysons la structure de ce script :\n\nla premi√®re ligne est classique, elle se nomme le shebang : elle indique au syst√®me quel interpr√©teur utiliser pour ex√©cuter ce script. Dans notre cas, et de mani√®re g√©n√©rale, on utilise bash (Bourne-Again SHell, l‚Äôimpl√©mentation moderne du shell) ;\nles lignes 2 et 3 assignent √† des variables les arguments pass√©s au script dans la commande. Par d√©faut, ceux-ci sont assign√©s √† des variables n o√π n est la position de l‚Äôargument, en commen√ßant √† 1 ;\nla ligne 4 assigne un chemin √† une variable\nla ligne 5 cr√©e le chemin complet, d√©fini √† partir des variables cr√©√©es pr√©c√©demment. Le param√®tre -p est important : il pr√©cise √† mkdir d‚Äôagir de mani√®re r√©cursive, c‚Äôest √† dire de cr√©er les dossiers interm√©diaires qui n‚Äôexistent pas encore ;\nla ligne 6 cr√©e un fichier texte vide dans le dossier cr√©√© avec la commande pr√©c√©dente.\n\nEx√©cutons maintenant ce script, en prenant soin de lui donner les permission ad√©quates au pr√©alable.\n\n\nterminal\n\nchmod +x monscript.sh\nbash monscript.sh section2 chapitre3\nls formation/section1/chapitre2/\n\ntext.txt\nOp√©ration r√©ussie : le dossier a bien √©t√© cr√©√© et contient un fichier test.txt.\nPour en savoir plus, une Cheat Sheet sur bash tr√®s bien r√©alis√©e."
  },
  {
    "objectID": "chapters/linux101.html#gestionnaire-de-paquets",
    "href": "chapters/linux101.html#gestionnaire-de-paquets",
    "title": "Linux 101",
    "section": "",
    "text": "Une diff√©rence fondamentale entre Linux et Windows tient √† la mani√®re dont on installe un logiciel. Sur Windows, on va chercher un installateur (un fichier ex√©cutable en .exe) sur le site du logiciel, et on l‚Äôex√©cute. En Linux, on passe g√©n√©ralement par un gestionnaire de packages qui va chercher les logiciels sur un r√©pertoire centralis√©, √† la mani√®re de pip en Python par exemple.\nPourquoi cette diff√©rence ? Une raison importante est que, contrairement √† Windows, il existe une multitude de distributions diff√©rentes de Linux (Debian, Ubuntu, Mint, etc.), qui fonctionnent diff√©remment et peuvent avoir diff√©rentes versions. En utilisant le package manager (gestionnaire de paquets) propre √† la distribution en question, on s‚Äôassure de t√©l√©charger le logiciel adapt√© √† sa distribution. Dans ce cours, on fait le choix d‚Äôutiliser une distribution Debian et son gestionnaire de paquets associ√© apt. Debian est en effet un choix populaire pour les servers de part sa stabilit√© et sa simplicit√©, et sera √©galement famili√®re aux utilisateurs d‚ÄôUbuntu, distribution tr√®s populaire pour les ordinateurs personnels et qui est bas√©e sur Debian.\nL‚Äôutilisation d‚Äôapt est tr√®s simple. La seule difficult√© est de savoir le nom du paquet que l‚Äôon souhaite installer, ce qui n√©cessite en g√©n√©ral d‚Äôutiliser un moteur de recherche. L‚Äôinstallation de paquets est √©galement un cas o√π il faut utiliser sudo, puisque cela implique souvent l‚Äôacc√®s √† des r√©pertoires prot√©g√©s.\n\n\nterminal\n\nsudo apt install tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  tree\n0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.\n...\nD√©sinstaller un package est √©galement simple : c‚Äôest l‚Äôop√©ration inverse. Par s√©curit√©, le terminal vous demande si vous √™tes s√ªr de votre choix en vous demandant de tapper la lettre y (yes) ou la lettre n.¬†On peut passer automatiquement cette √©tape en ajoutant le param√®tre -y\n\n\nterminal\n\nsudo apt remove -y tree\n\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following packages will be REMOVED:\n  tree\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n...\nAvant d‚Äôinstaller un package, il est toujours pr√©f√©rable de mettre √† jour la base des packages, pour s‚Äôassurer qu‚Äôon obtiendra bien la derni√®re version.\n\n\nterminal\n\nsudo apt update\n\nHit:1 http://deb.debian.org/debian bullseye InRelease\nHit:2 http://deb.debian.org/debian bullseye-updates InRelease\nHit:3 http://security.debian.org/debian-security bullseye-security InRelease\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nAll packages are up to date."
  },
  {
    "objectID": "chapters/linux101.html#tricks",
    "href": "chapters/linux101.html#tricks",
    "title": "Linux 101",
    "section": "",
    "text": "On l‚Äôa dit et redit : devenir √† l‚Äôaise avec le terminal Linux est essentiel et demande de la pratique. Il existe n√©anmoins quelques astuces qui peuvent grandement simplifier la vie et donc faciliter la prise de bonnes habitudes.\nLa premi√®re est l‚Äôautocompl√©tion. D√®s lors que vous √©crivez une commande contenant un nom d‚Äôex√©cutable, un chemin sur le filesystem, ou autre, n‚Äôh√©sitez pas √† utiliser la touche TAB (touche au-dessus de celle qui verrouille la majuscule) de votre clavier. Dans la majorit√© des cas, cela va vous faire gagner un temps pr√©cieux.\nLa deuxi√®me est de parcourir l‚Äôhistorique de commandes : la fl√®che vers le haut (‚Üë) permet de parcourir l‚Äôhistorique des commandes que vous avez pr√©c√©demment ex√©cut√©es. Chaque fois que vous appuyez sur cette touche, le terminal affiche la derni√®re commande ex√©cut√©e, en remontant dans l‚Äôhistorique √† chaque appui suppl√©mentaire.\nLa troisi√®me, dans le m√™me esprit que la deuxi√®me mais plus √©labor√©e, est la recherche inverse dans l‚Äôhistorique de commandes avec les touches Ctrl+R. Lorsque vous appuyez sur Ctrl+R, une invite de recherche appara√Æt. Vous pouvez alors commencer √† taper des caract√®res de la commande que vous recherchez. Le terminal cherchera dans l‚Äôhistorique des commandes la derni√®re commande correspondant √† ce que vous avez tap√©, et la montrera √† l‚Äô√©cran. Si ce n‚Äôest pas la commande exacte que vous cherchez, vous pouvez continuer √† taper pour affiner la recherche ou appuyer √† nouveau sur Ctrl+R pour rechercher la commande pr√©c√©dente correspondant √† vos crit√®res.\nUne quatri√®me astuce, qui n‚Äôen est pas vraiment une, est de lire la documentation d‚Äôune commande lorsque l‚Äôon n‚Äôest pas s√ªr de sa syntaxe ou des param√®tres admissibles. Via le terminal, la documentation d‚Äôune commande peut √™tre affich√©e en ex√©cutant man suivie de la commande en question, par exemple : man cp. Comme il n‚Äôest pas toujours tr√®s pratique de lire de longs textes dans un petit terminal, on peut √©galement chercher la documentation d‚Äôune commande sur le site man7."
  },
  {
    "objectID": "chapters/git.html",
    "href": "chapters/git.html",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/git.html#pourquoi-faire",
    "href": "chapters/git.html#pourquoi-faire",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi faire ?",
    "text": "Pourquoi faire ?\nLe d√©veloppement rapide de la data science au cours de ces derni√®res ann√©es s‚Äôest accompagn√©e d‚Äôune complexification substantielle des projets. Par ailleurs, les projets sont de plus en plus collaboratifs, que ce soit dans le cadre d‚Äô√©quipes dans un contexte professionnel ou bien pour des contributions √† des projets open-source. Naturellement, ces √©volutions doivent nous amener √† modifier nos mani√®res de travailler pour g√©rer cette complexit√© croissante et continuer √† produire de la valeur √† partir des projets de data science.\nPourtant, tout data scientist s‚Äôest parfois demand√© :\n\nquelle √©tait la bonne version d‚Äôun programme\nqui √©tait l‚Äôauteur d‚Äôun bout de code en particulier\nsi un changement √©tait important ou juste un essai\ncomment fusionner des programmes\netc.\n\nEt il n‚Äôest pas rare de perdre le fil des versions de son projet lorsque l‚Äôon garde trace de celles-ci de fa√ßon manuelle.\nExemple de contr√¥le de version fait ‚Äú√† la main‚Äù\n\nPourtant, il existe un outil informatique puissant afin de r√©pondre √† tous ces besoins : la gestion de version (version control system (VCS) en anglais). Ses avantages sont incontestables et permettent de facilement :\n\nenregistrer l‚Äôhistorique des modifications d‚Äôun ensemble de fichiers\nrevenir √† des versions pr√©c√©dentes d‚Äôun ou plusieurs fichiers\nrechercher les modifications qui ont pu cr√©er des erreurs\ntravailler simultan√©ment sur un m√™me fichier sans risque de perte\npartager ses modifications et r√©cup√©rer celles des autres\nproposer des modifications, les discuter, sans pour autant modifier la derni√®re version existante\nidentifier les auteurs et la date des modifications\n\nEn outre, ces outils fonctionnent avec tous les langages informatiques car ils reposent sur la comparaison des lignes et des caract√®res des programmes, ind√©pendamment du langage. En bref, c‚Äôest la bonne mani√®re pour partager des codes et travailler √† plusieurs sur un projet de data science. En r√©alit√©, il ne serait pas exag√©r√© de dire que l‚Äôutilisation du contr√¥le de version est la bonne pratique la plus fondamentale de tout projet faisant intervenir du code, et qu‚Äôelle conditionne largement toutes les autres."
  },
  {
    "objectID": "chapters/git.html#pourquoi-git",
    "href": "chapters/git.html#pourquoi-git",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi Git  ?",
    "text": "Pourquoi Git  ?\nPlusieurs logiciels de contr√¥le de version existent sur le march√©. En principe, le logiciel Git, d√©velopp√© initialement pour fournir une solution d√©centralis√©e et open-source dans le cadre du d√©veloppement du noyau Linux, est devenu largement h√©g√©monique. Aussi, toutes les application de ce cours s‚Äôeffectueront √† l‚Äôaide du logiciel Git."
  },
  {
    "objectID": "chapters/git.html#pourquoi-github",
    "href": "chapters/git.html#pourquoi-github",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Pourquoi GitHub  ?",
    "text": "Pourquoi GitHub  ?\nTravailler de mani√®re collaborative avec Git implique de synchroniser son r√©pertoire local avec une copie distante, situ√©e sur un serveur h√©bergeant des projets Git. Ce serveur peut √™tre un serveur interne √† une organisation, ou bien √™tre fourni par un h√©bergeur externe. Les deux alternatives les plus populaires en la mati√®re sont GitHub et GitLab. Dans ce cours, nous utiliserons GitHub, qui est devenu au fil des ann√©es la r√©f√©rence pour l‚Äôh√©bergement des projets open-source. En pratique, les deux services sont relativement semblables, et tous les concepts pr√©sent√©s se retrouvent sous une forme similaire sur les deux plateformes."
  },
  {
    "objectID": "chapters/git.html#principes-et-commandes-usuelles",
    "href": "chapters/git.html#principes-et-commandes-usuelles",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Principes et commandes usuelles",
    "text": "Principes et commandes usuelles\nLe graphique suivant illustre les principes fondamentaux de Git.\nGit tout-en-un (Source) \nLorsqu‚Äôon utilise Git, il est important de bien distinguer ce qui se passe en local (sur son poste, sur le serveur sur lequel on travaille‚Ä¶) de ce qui se passe en remote, i.e.¬†en int√©ragissant avec un serveur distant. Comme le montre le graphique, l‚Äôessentiel du contr√¥le de version se passe en r√©alit√© en local.\nEn th√©orie, sur un projet individuel, il est m√™me possible de r√©aliser l‚Äôensemble du contr√¥le de version en mode hors-ligne. Pour cela, il suffit d‚Äôindiquer √† Git le projet (dossier) que l‚Äôon souhaite versionner en utilisant la commande git init. Cette commande a pour effet de cr√©er un dossier .git √† la racine du projet, dans lequel Git va stocker tout l‚Äôhistorique du projet (commits, branches, etc.) et permettre de naviguer entre les versions. A cause du . qui pr√©fixe son nom, ce dossier est g√©n√©ralement cach√© par d√©faut, ce qui n‚Äôest pas probl√©matique dans la mesure o√π il n‚Äôy a jamais besoin de le parcourir ou de le modifier √† la main en pratique. Retenez simplement que c‚Äôest la pr√©sence de ce dossier .git qui fait qu‚Äôun dossier est consid√©r√© comme un projet Git, et donc que vous pouvez utilisez les commandes usuelles de Git dans ce dossier √† l‚Äôaide d‚Äôun terminal : - git status : affiche les modifications du projet par rapport √† la version pr√©c√©dente ; - git add chemin_du_fichier : ajoute un fichier nouveau ou modifi√© √† la zone de staging de Git en vue d‚Äôun commit ; - git add -A : ajoute tous les fichiers nouveaux ou modifi√©s √† la zone de staging ; - git commit -m \"message de commit\" : cr√©e un commit, i.e.¬†une photographie des modifications (ajouts, modifications, suppressions) apport√©es au projet depuis la derni√®re version, et lui assigne un message d√©crivant ces changements. Les commits sont l‚Äôunit√© de base de l‚Äôhistorique du projet construit par Git.\nEn pratique, travailler uniquement en local n‚Äôest pas tr√®s int√©ressant. Pour pouvoir travailler de mani√®re collaborative, on va vouloir synchroniser les diff√©rentes copies locales du projet √† un r√©pertoire centralis√©, qui maintient de fait la ‚Äúsource de v√©rit√©‚Äù (single source of truth). M√™me sur un projet individuel, il fait sens de synchroniser son r√©pertoire local √† une copie distante pour assurer l‚Äôint√©grit√© du code de son projet en cas de probl√®me mat√©riel.\nEn g√©n√©ral, on va donc initialiser le projet dans l‚Äôautre sens : - cr√©er un nouveau projet sur GitHub - g√©n√©rer un jeton d‚Äôacc√®s (personal access token) - cloner le projet en local via la m√©thode HTTPS : git clone https://github.com/&lt;username&gt;/&lt;project_name&gt;.git\nLe projet clon√© est un projet Git ‚Äî il contient le dossier .git ‚Äî synchronis√© par d√©faut avec le r√©pertoire distant. On peut le v√©rifier avec la commande remote de Git :\n\n\nterminal\n\ngit remote -v\n\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (fetch)\norigin  https://github.com/linogaliana/ensae-reproductibilite-website.git (push)\nLe projet local est bien li√© au r√©pertoire distant sur GitHub, auquel Git donne par d√©faut le nom origin. Ce lien permet d‚Äôutiliser les commandes de synchronisation usuelles : - git pull : r√©cup√©rer les changements (fetch) sur le remote et les appliquer au projet local - git push : envoyer les changements locaux sur le remote"
  },
  {
    "objectID": "chapters/git.html#impl√©mentations",
    "href": "chapters/git.html#impl√©mentations",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Impl√©mentations",
    "text": "Impl√©mentations\nGit est un logiciel, qui peut √™tre t√©l√©charg√© sur le site officiel pour diff√©rents syst√®mes d‚Äôexploitation. Il existe cependant diff√©rentes mani√®res d‚Äôutiliser Git : - le client en ligne de commande : c‚Äôest l‚Äôimpl√©mentation standard, et donc la plus compl√®te. C‚Äôest celle qu‚Äôon utilisera dans ce cours. Le client Git est install√© par d√©faut sur les diff√©rents services du SSP Cloud (VSCode, RStudio, Jupyter, etc.) et peut donc √™tre utilis√© via n‚Äôimporte quel terminal. La documentation du SSP Cloud d√©taille la proc√©dure ; - des interfaces graphiques : elles facilitent la prise en main de Git via des guides visuels, mais ne permettent pas de r√©aliser toutes les op√©rations permises par Git - l‚Äôinterface native de RStudio pour les utilisateurs de R : tr√®s compl√®te et stable. La formation au travail collaboratif avec Git et RStudio pr√©sente son utilisation de mani√®re d√©taill√©e ; - le plugin Jupyter-git pour les utilisateurs de Python : elle impl√©mente les principales features de Git, mais s‚Äôav√®re assez instable √† l‚Äôusage."
  },
  {
    "objectID": "chapters/git.html#bonnes-pratiques",
    "href": "chapters/git.html#bonnes-pratiques",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Bonnes pratiques",
    "text": "Bonnes pratiques\nLe contr√¥le de version est une bonne pratique de d√©veloppement en soi‚Ä¶ mais son utilisation admet elle m√™me des bonnes pratiques qui, lorsqu‚Äôelles sont appliqu√©es, permettent d‚Äôen tirer le plus grand profit.\n\nQue versionne-t-on ?\n\nUniquement des fichiers texte\nA chaque commit, Git calcule les diff√©rences entre les versions successives du projet, afin de ne pas avoir √† stocker une image compl√®te de ce dernier √† chaque fois. C‚Äôest ce qui permet aux projets Git de rester tr√®s l√©gers par d√©faut, et donc aux diff√©rentes op√©rations impliquant le remote (clone, push, pull..) d‚Äô√™tre tr√®s rapides.\nLa contrepartie de cette l√©g√®ret√© est une contrainte sur les types d‚Äôobjets que l‚Äôon doit versionner. Les diff√©rences sont calculables uniquement sur des fichiers de type texte : codes source, fichiers texte, fichiers de configuration non-sensibles‚Ä¶ Voici donc une liste non-exhaustive des extensions de fichier que l‚Äôon retrouve fr√©quemment dans un d√©p√¥t Git d‚Äôun projet R ou Python : .py, .R, .Rmd, .txt, .json, .xml, .yaml, .toml, et bien d‚Äôautres.\nEn revanche tous les fichiers binaires ‚Äî pour faire simple, tous les fichiers qui ne peuvent pas √™tre ouverts dans un √©diteur de texte basique sans produire une suite inintelligible de caract√®res ‚Äî n‚Äôont g√©n√©ralement pas destination √† se retrouver sur un d√©p√¥t Git. Du fait de leur formatage (binaire), Git ne peut pas calculer les diff√©rences entre versions pour ces fichiers et c‚Äôest donc le fichier entier qui est sauvegard√© dans l‚Äôhistorique √† chaque changement, ce qui peut tr√®s rapidement faire cro√Ætre la taille du d√©p√¥t. Pour √©viter de versionner ces fichiers par erreur, on va les ajouter au fichier .gitignore (cf.¬†supra).\n\n\nPas de donn√©es\nComme expliqu√© en introduction, le fil rouge de ce cours sur les bonnes pratiques est l‚Äôimportance de bien s√©parer code, donn√©es et environnement d‚Äôex√©cution afin de favoriser la reproductibilit√© des projets de data science. Ce principe doit s‚Äôappliquer √©galement √† l‚Äôusage du contr√¥le de version, et ce pour diff√©rentes raisons.\nA priori, inclure ces donn√©es dans un d√©p√¥t Git peut sembler une bonne id√©e en termes de reproductibilit√©. En machine learning par exemple, on est souvent amen√© √† r√©aliser de nombreuses exp√©rimentations √† partir d‚Äôun m√™me mod√®le appliqu√© √† diff√©rentes transformations des donn√©es initiales, transformations que l‚Äôon pourrait versionner. En pratique, il est g√©n√©ralement pr√©f√©rable de versionner le code qui permet de g√©n√©rer ces transformations et donc les exp√©rimentations associ√©es, dans la mesure o√π le suivi des versions des datasets peut s‚Äôav√©rer rapidement complexe. Pour de plus gros projets, des alternatives sp√©cifiques existent : c‚Äôest le champ du MLOps, domaine en constante expansion qui vise √† rendre les pipelines de machine learning plus reproductibles.\nEnfin, la structure m√™me de Git n‚Äôest techniquement pas faite pour le stockage de donn√©es. Si des petits datasets dans un format texte ne poseront pas de probl√®me, des donn√©es volumineuses (√† partir de plusieurs Mo) vont faire cro√Ætre la taille du d√©p√¥t et donc ralentir significativement les op√©rations de synchronisation avec le remote.\n\n\nPas d‚Äôinformations locales\nL√† encore en vertu du principe de s√©paration donn√©es / code/ environnement, les donn√©es locales, i.e.¬†sp√©cifiques √† l‚Äôenvironnement de travail sur lequel le code a √©t√© ex√©cut√©, n‚Äôont pas vocation √† √™tre versionn√©es. Par exemple, des fichiers de configuration sp√©cifiques √† un poste de travail, des chemins d‚Äôacc√®s sp√©cifiques √† un ordinateur donn√©, etc. Cela demande une plus grande rigueur lors de la construction du projet, mais garantit par l√† m√™me une meilleure reproductiblit√© pour les futurs utilisateurs du projet.\n\n\nPas d‚Äôoutputs\nLes outputs d‚Äôun projet (graphiques, publications, mod√®le entra√Æn√©‚Ä¶) n‚Äôont pas vocation √† √™tre versionn√©, en vertu des diff√©rents arguments pr√©sent√©s ci-dessus : - il ne s‚Äôagit g√©n√©ralement pas de fichiers de type texte ; - le code source du projet doit dans tous les cas permettre des les reg√©n√©rer √† l‚Äôidentique.\n\n\nUtiliser un .gitignore\nOn a list√© pr√©c√©demment un large √©ventail de fichiers qui n‚Äôont, par nature, pas vocation √† √™tre versionn√©. Bien entendu, faire attention √† ne pas ajouter ces diff√©rents fichiers au moment de chaque git add serait assez p√©nible. Git simplifie largement cette proc√©dure en nous donnant la possibilit√© de remplir un fichier .gitignore, situ√© √† la racine du projet, qui sp√©cifie l‚Äôensemble des fichiers et types de fichiers que l‚Äôon ne souhaite pas versionner dans le cadre du projet courant.\nDe mani√®re g√©n√©rale, il y a pour chaque langage des fichiers que l‚Äôon ne souhaitera jamais versionner. Pour en tenir compte, une premi√®re bonne pratique est de choisir le .gitignore associ√© au langage du projet lors de la cr√©ation du d√©p√¥t sur GitHub. Ce faisant, le projet est initialit√© avec un gitignore d√©j√† existant et pr√©-rempli de chemins et de types de fichiers qui ne sont pas √† versionner. Regardons un extrait du gitignore Python pour comprendre sa structure et son fonctionnement.\npip-log.txt\n__pycache__/\n*.log\nChaque ligne du gitignore sp√©cifie un √©l√©ment √† ignorer du contr√¥le de version, √©l√©ment qui peut √™tre un ficher/dossier ou bien une r√®gle concernant un ensemble de fichiers/dossiers. Sauf si sp√©cifi√© explicitement, les chemins sont relatifs √† la racine du projet. L‚Äôextrait du gitignore Python illustre les diff√©rentes possibilit√©s :\n\nligne 1 : ignore le fichier pip-log.txt ;\nligne 2 : ignore le dossier __pycache__/ ;\nligne 3 : ignore tous les fichiers dont l‚Äôextension est .log.\n\nDe nombreuses autres possiblit√©s existent, et sont d√©taill√©es par exemple dans la documentation de Git.\n\n\n\nMessages des commits\nLe commit est l‚Äôunit√© de temps de Git, et donc fondamentalement ce qui permet de remonter dans l‚Äôhistorique d‚Äôun projet. Afin de pouvoir b√©n√©ficier √† plein de cet avantage de Git, il est capital d‚Äôaccompagner ses commits de messages pertinents, en se pla√ßant dans la perspective que l‚Äôon peut √™tre amen√© plusieurs semaines ou mois plus tard √† vouloir retrouver du code dans l‚Äôhistorique de son projet. Les quelques secondes prises √† chaque commit pour r√©fl√©chir √† une description pertinente du bloc de modifications que l‚Äôon apporte au projet peuvent donc faire gagner un temps pr√©cieux √† la longue.\nDe nombreuses conventions existent pour r√©diger des messages de commit pertinents. Nous rappelons ici les plus importantes :\n\ncontenu : le message doit d√©tailler le pourquoi plut√¥t que le comment des modifications. Par exemple, plut√¥t que ‚ÄúAjoute le fichier test.py‚Äù, on pr√©f√©rera √©crire ‚ÄúAjout d‚Äôune s√©rie de tests unitaires‚Äù ;\nstyle : le message doit √™tre √† l‚Äôimp√©ratif et former une phrase (sans point √† la fin) ;\nlongueur : le message du commit doit √™tre court (&lt; 72 caract√®res). S‚Äôil n‚Äôest pas possible de trouver un message de cette taille qui r√©sume le commit, c‚Äôest g√©n√©ralement un signe que le commit regroupe trop de changements (cf.¬†point suivant). Le fait de devoir mettre des + ou des & / et dans un message de commit pour s√©parer les changements est √©galement un bon indicateur d‚Äôun commit trop gros.\n\n\n\nFr√©quence des commits\nDe mani√®re g√©n√©rale, il est conseill√© de r√©aliser des commits r√©guliers lorsque l‚Äôon travaille sur un projet. Une r√®gle simple que l‚Äôon peut par exemple appliquer est la suivante : d√®s lors qu‚Äôun ensemble de modifications forment un tout coh√©rent et peuvent √™tre r√©sum√©es par un message simple, il est temps d‚Äôen faire un commit. Cette approche a de nombreux avantages :\n\nsi l‚Äôon fait suivre chaque commit d‚Äôun push ‚Äî ce qui est conseill√© en pratique ‚Äî on s‚Äôassure de disposer r√©guli√®reemnt d‚Äôune copie de ses travaux, ce qui limite le risque de perte ;\nil est plus facile de revenir en arri√®re en cas d‚Äôerreur si les commits portent sur des changements cibl√©s et coh√©rents ;\nle processus de review d‚Äôune pull request est facilit√©, car les diff√©rents blocs de modification sont plus clairement s√©par√©s ;\ndans une approche d‚Äôint√©gration continue ‚Äî concept que l‚Äôon verra en d√©tail dans le chapitre sur la mise en production ‚Äî faire des commits et des PR r√©guli√®rement permet de d√©ployer de mani√®re continue les changements en production, et donc d‚Äôobtenir les feedbacks des utilisateurs et d‚Äôadapter plus rapidement si besoin."
  },
  {
    "objectID": "chapters/git.html#branches",
    "href": "chapters/git.html#branches",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Branches",
    "text": "Branches\n\nConcept\nLa possibilit√© de cr√©er des branches est l‚Äôune des fonctionnalit√©s majeures de Git. La cr√©ation d‚Äôune branche au sein d‚Äôun projet permet de diverger de la ligne principale de d√©veloppement (g√©n√©ralement appel√©e master, terme tendant √† dispara√Ætre au profit de celui de main) sans impacter cette ligne. Cela permet de s√©parer le nouveau d√©veloppement et de faire cohabiter plusieurs versions, pouvant √©voluer s√©par√©ment et pouvant √™tre facilement rassembl√©es si n√©cessaire.\nPour comprendre comment fonctionnent les branches, il nous faut revenir un peu plus en d√©tail sur la mani√®re dont Git stocke l‚Äôhistorique du projet. Comme nous l‚Äôavons vu, l‚Äôunit√© temporelle de Git est le commit, qui correspond √† une photographie √† un instant donn√© de l‚Äô√©tat du projet (snapshot). Chaque commit est uniquement identifi√© par un hash, une longue suite de caract√®res. La commande git log, qui liste les diff√©rents commits d‚Äôun projet, permet d‚Äôafficher ce hash ainsi que diverses m√©tadonn√©es (auteur, date, message) associ√©es au commit.\n\n\nterminal\n\ngit log\n\ncommit e58b004d3b68bdf28093fe6ad6036b5d13216e55 (HEAD -&gt; master, origin/master, origin/HEAD)\nAuthor: Lino Galiana &lt;xxx@xxx.fr&gt;\nDate:   Tue Mar 22 14:34:04 2022 +0100\n\n    ajoute code √©quivalent python\n\n...\nUne branche est simplement un pointeur vers un commit. Dans l‚Äôexemple pr√©c√©dent, on a imprim√© les informations du dernier commit en date. La branche principale (master) pointe vers ce commit. Si l‚Äôon faisait un nouveau commit, le pointeur se d√©calerait et la branche master pointerait √† pr√©sent sur le nouveau commit.\n\n\nBranches locales\nDans ce contexte, cr√©er une nouvelle branche (en local) revient simplement √† cr√©er un nouveau pointeur vers un commit donn√©. Supposons que l‚Äôon cr√©e une branche testing √† partir du dernier commit.\n\n\nterminal\n\n1git branch testing\n2git branch\n\n\n1\n\nCr√©e une nouvelle branche\n\n2\n\nListe les branches existantes\n\n\n1* master\n2  testing\n\n1\n\nLa branche sur laquelle on se situe\n\n2\n\nLa nouvelle branche cr√©√©e\n\n\nLa figure suivante illustre l‚Äôeffet de cette cr√©ation sur l‚Äôhistorique Git.\n\nD√©sormais, deux branches (master et testing) pointent vers le m√™me commit. Si l‚Äôon effectue √† pr√©sent des commits sur la branche testing, on va diverger de la branche principale, ce qui permet de d√©velopper une nouvelle fonctionnalit√© sans risquer d‚Äôimpacter master.\nPour savoir sur quelle branche on se situe √† instant donn√© ‚Äî et donc sur quelle branche on va commiter ‚Äî Git utilise un pointeur sp√©cial, appel√© HEAD, qui pointe vers la branche courante. On comprend √† pr√©sent mieux la signification de HEAD -&gt; master dans l‚Äôoutput de la commande git log vu pr√©c√©demment. Cet √©l√©ment sp√©cifie la situation locale actuelle et signifie : on se situe actuellement sur la branche master, qui pointe sur le commit e58b004. Pour changer de branche, i.e.¬†d√©placer le HEAD, on utilise la commande git checkout. Par exemple, pour passer de la branche master sur laquelle on est par d√©faut √† la branche testing :\n\n\nterminal\n\ngit checkout testing  # Changement de branche\n\nSwitched to branch 'testing'\nOn se situe d√©sormais sur la branche testing, sur laquelle on peut laisser libre cours √† sa cr√©ativit√© sans risquer d‚Äôimpacer la branche principale du projet. Mais que se passe-t-il si, pendant que l‚Äôon d√©veloppe sur testing, un autre membre du projet commit sur master ? On dit que les historiques ont diverg√©. La figure suivante illustre √† quoi ressemble √† pr√©sent l‚Äôhistorique du projet (et suppose que l‚Äôon est repass√© sur master).\n\nCette divergence n‚Äôest pas probl√©matique en soi : il est normal que les diff√©rentes parties et exp√©rimentations d‚Äôun projet avancent √† diff√©rents rythmes. La difficult√© est de savoir comment r√©concillier les diff√©rents changements si l‚Äôon d√©cide que la branche testing doit √™tre int√©gr√©e dans master. Deux situations peuvent survenir : - les modifications op√©r√©es en parall√®le sur les deux branches ne concernent pas les m√™mes fichiers ou les m√™mes parties des fichiers. Dans ce cas, Git est capable de fusionner (merge) les changements automatiquement et tout se passe sans encombre ; - dans le cas contraire, survient un merge conflict : les branches ont diverg√© de telle sorte qu‚Äôil n‚Äôest pas possible pour Git de fusionner les changements automatiquement. Il faut alors r√©soudre les conflits manuellement.\nLa r√©solution des conflits est une √©tape souvent douloureuse lors de l‚Äôapprentissage de Git. Aussi, nous conseillons dans la mesure du possible de ne pas fusionner des branches manuellement en local avec Git ‚Äî c‚Äôest d‚Äôailleurs pour cette raison que nous n‚Äôavons pas d√©taill√© les commandes pour le faire. Dans les sections suivantes, nous verrons comment une bonne organisation pr√©alable du travail en √©quipe, combin√©e aux outils collaboratifs fournis par GitHub, permet de rendre le processus de fusion des branches largement indolore.\n\n\nBranches remote\nRappellons que toutes les op√©rations que nous avons effectu√©es sur les branches dans cette section se sont pass√©s en local, le r√©pertoire distant est rest√© totalement inchang√©. Pour pouvoir collaborer sur une branche ou bien en faire une pull request (cf.¬†supra), il faut pousser la branche sur le r√©pertoire distant. La commande est simple : git push origin &lt;branche&gt;.\n\n\nterminal\n\ngit push origin testing\n\nCounting objects: 24, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (15/15), done.\nWriting objects: 100% (24/24), 1.91 KiB | 0 bytes/s, done.\nTotal 24 (delta 2), reused 0 (delta 0)\nTo https://github.com/linogaliana/ensae-reproductibilite-website\n * [new branch]      testing -&gt; testing"
  },
  {
    "objectID": "chapters/git.html#workflow-collaboratif",
    "href": "chapters/git.html#workflow-collaboratif",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Workflow collaboratif",
    "text": "Workflow collaboratif\nComme on l‚Äôa vu pr√©c√©demment, si le mod√®le des branches de Git semble id√©al pour g√©rer le travail collaboratif et asynchrone, il peut √©galement s‚Äôav√©rer rapidement complexe √† manipuler en l‚Äôabsence d‚Äôune bonne organisation du travail en √©quipe. De nombreux mod√®les (‚Äúworkflows‚Äù) existent en la mati√®re, avec des complexit√©s plus ou moins grandes selon la nature du projet. Nous conseillons d‚Äôadopter dans la plupart des cas un mod√®le tr√®s simple : le GitHub Flow.\nLe GitHub Flow est une m√©thode d‚Äôorganisation minimaliste du travail collaboratif, qui est progressivement devenue la norme dans les projets open-source. Elle est r√©sum√©e par la figure suivante, dont nous d√©taillons par la suite les diff√©rentes √©tapes.\n\n\nD√©finition des r√¥les des contributeurs\nDans tout projet collaboratif, une premi√®re √©tape essentielle est de bien d√©limiter les r√¥les des diff√©rents contributeurs. Les diff√©rents participants au projet ont en effet g√©n√©ralement des r√¥les diff√©rents dans l‚Äôorganisation, des niveaux diff√©rents de pratique de Git, etc. Il est important de refl√©ter ces diff√©rents r√¥les dans l‚Äôorganisation du travail collaboratif.\nSur les diff√©rents h√©bergeurs de projets Git, cela prend la forme de r√¥les que l‚Äôon attribue aux diff√©rents membres du porjet. Les mainteneurs sont les seuls √† pouvoir √©crire directement sur master. Les contributeurs sont quant √† eux tenus de d√©velopper sur des branches. Cela permet de prot√©ger la branche principale, qui doit rester une version propre et jamais compromise du projet.\nNotons que la possibilit√© de donner des r√¥les sur les projets GitHub n‚Äôest possible que dans le cadre d‚Äôorganisations (payantes), donc dans un contexte professionnel ou de projets open-source d‚Äôune certaine ampleur. Pour des petits projets, il est n√©cessaire de s‚Äôastreindre √† une certaine rigueur individuelle pour respecter cette organisation.\n\n\nD√©veloppement sur des branches de court-terme\nLes contributeurs d√©veloppent uniquement sur des branches. Il est d‚Äôusage de cr√©er une branche par fonctionnalit√©, en lui donnant un nom refl√©tant la fonctionnalit√© en cours de d√©veloppement (ex : ajout-tests-unitaires). Les diff√©rents contributeurs √† la fonctionnalit√© en cours de d√©veloppement font des commits sur la branche, en prenant bien soin de pull r√©guli√®rement les √©ventuels changements pour ne pas risquer de conflits de version. Pour la m√™me raison, il est pr√©f√©rable de faire des branches dites de court-terme, c‚Äôest √† dire propres √† une petite fonctionnalit√©, quite √† diviser une fonctionnalit√© en s√©ries d‚Äôimpl√©mentations. Cela permet de limiter les √©ventuels conflits √† g√©rer lors de la fusion finale de la branche avec master.\n\n\nPull Request\nUne fois la s√©rie de modifications termin√©e, vient le temps de rassembler les diff√©rents travaux, par l‚Äôinterm√©diaire de la fusion entre la branche et master. Il faut alors ‚Äúdemander‚Äù de fusionner (pull request) sur GitHub. Cela ouvre une page li√©e √† la pull request, qui rappelle les diff√©rents changements apport√©s et leurs auteurs, et permet d‚Äôentamer une discussion √† propos de ces changements.\n\n\nProcessus de review\nLes diff√©rents membres du projet peuvent donc analyser et commenter les changements, poser des questions, sugg√©rer des modifications, apporter d‚Äôautres contributions, etc. Il est par exemple possible de mentionner un membre de l‚Äô√©quipe par l‚Äôinterm√©diaire de @personne. Il est √©galement possible de proc√©der √† une code review, par exemple par un d√©veloppeur plus exp√©riment√©.\n\n\nR√©solution des √©ventuels conflits\nEn adoptant cette mani√®re de travailler, master ne sera modifi√©e que via des pull requests. Il ne devrait donc jamais y avoir le moindre conflit √† r√©gler sur master, les seuls conflits possibles se passent sur les branches. Par exemple, dans le cas o√π une autre pull request aurait √©t√© fusionn√©e sur master depuis l‚Äôouverture de la pull request en question.\nDans le cas d‚Äôun conflit √† g√©rer, le conflit doit √™tre r√©solu dans la branche et pas dans master. Voici la marche √† suivre :\n\nappliquez le conseil de survie : faites une copie de sauvegarde de votre clone\ndans votre clone, placez vous sur la branche en question : git checkout nom-de-la-branche\nfusionnez master dans la branche : git merge master\nr√©solvez les √©ventuels conflits dans les fichiers concern√©s\nfinalisez le commit de merge et poussez-le sur la branche remote, ce qui le fera appara√Ætre dans la pull request\n\n\n\nFusion de la branche\nSi tout est en ordre, la branche peut √™tre fusionn√©e. Seuls les mainteneurs, qui ont les droits sur master, peuvent fusionner la pull request. En termes d‚Äôhistorique du projet, deux choix sont possibles : - ‚ÄúCreate a merge commit‚Äù : tous les commits r√©alis√©s sur la branche appara√Ætront dans l‚Äôhistorique du projet ; - ‚ÄúSquash and merge‚Äù : les diff√©rents commits r√©alis√©s sur la branche seront rassembl√©s en un commit unique. Cette option est g√©n√©ralement pr√©f√©rable lorsqu‚Äôon utilise des branches de court-terme : elles permettent de garder l‚Äôhistorique plus lisible."
  },
  {
    "objectID": "chapters/git.html#utiliser-les-issues",
    "href": "chapters/git.html#utiliser-les-issues",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Utiliser les issues",
    "text": "Utiliser les issues\nLa mani√®re la plus simple de contribuer √† un projet open-source est d‚Äôouvrir une issue. Sur GitHub, cela se fait sur la page du projet, sous l‚Äôonglet Issue (cf.¬†documentation officielle). Les issues peuvent avoir diff√©rentes nature : - suggestion d‚Äôam√©lioration (sans code) - notification de bug - rapports d‚Äôexp√©rience - etc.\nLes issues sont une mani√®re tr√®s peu couteuse de contributer √† un projet, mais leur importance est capitale, dans la mesure o√π il est impossible pour les d√©veloppeurs d‚Äôun projet de penser en amont √† toutes les utilisations possibles et donc tous les bugs possibles d‚Äôune application."
  },
  {
    "objectID": "chapters/git.html#proposer-une-pull-request",
    "href": "chapters/git.html#proposer-une-pull-request",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Proposer une pull request",
    "text": "Proposer une pull request\nUne autre mani√®re, plus ambitieuse, de contribuer √† l‚Äôopen source est de proposer des pull requests. Concr√®tement, l‚Äôid√©e est de proposer une am√©lioration ou bien de r√©soudre un bug sous forme de code, que les mainteneurs du projet peuvent ensuite d√©cider d‚Äôint√©grer au code existant.\nLa proc√©dure pour proposer une pull request √† un projet sur lequel on n‚Äôa aucun droit est tr√®s similaire √† celle d√©crite ci-dessus dans le cas normal. La principale diff√©rence est que, du fait de l‚Äôabsence de droits, il est impossible de pousser une branche locale sur le r√©pertoire du projet. On va donc devoir cr√©er au pr√©alable un fork, i.e.¬†une copie du projet que l‚Äôon cr√©e dans son espace personnel sur GitHub. C‚Äôest sur cette copie que l‚Äôon va appliquer la proc√©dure d√©crite pr√©c√©demment, en prenant bien soin de travailler sur une branche et non sur master. Une fois les modifications pertinentes effectu√©es sur la branche du fork, GitHub propose de cr√©er une pull request sur le d√©p√¥t original. Cette pull request sera alors visible des mainteneurs du projet, qui pourront l‚Äô√©valuer et d√©cider d‚Äôadopter (ou non) les changements propos√©s."
  },
  {
    "objectID": "chapters/git.html#respecter-les-r√®gles-de-contribution",
    "href": "chapters/git.html#respecter-les-r√®gles-de-contribution",
    "title": "Versionner son code et travailler collaborativement avec Git",
    "section": "Respecter les r√®gles de contribution",
    "text": "Respecter les r√®gles de contribution\nVouloir contribuer √† un projet open-source est tr√®s louable, mais ne peut pas pour autant se faire n‚Äôimporte comment. Un projet est constitu√© de personnes, qui ont d√©velopp√© ensemble une mani√®re de travailler, des standards de bonnes pratiques, etc. Pour s‚Äôassurer que sa contribution ne reste pas lettre morte, il est indispensable de s‚Äôimpr√©gner un minimum de la culture du projet.\nPour faciliter les contributions, les projets open-source sp√©cifient bien souvent la mani√®re dont les utilisateurs peuvent contribuer ainsi que le format attendu. En g√©n√©ral, ces r√®gles de contribution sont sp√©cifi√©es dans un fichier CONTRIBUTING.md situ√© √† la racine du projet GitHub, ou a d√©faut dans le README du projet. Il est essentiel de bien lire ce document s‚Äôil existe afin de s‚Äôassurer de proposer des contributions pertinentes."
  },
  {
    "objectID": "chapters/evaluation.html",
    "href": "chapters/evaluation.html",
    "title": "Evaluation",
    "section": "",
    "text": "L‚Äôobjectif g√©n√©ral de l‚Äô√©valuation de ce cours est de mettre en pratique les notions √©tudi√©es (bonnes pratiques de d√©veloppement et mise en production) de mani√®re appliqu√©e et r√©aliste, i.e.¬†√† travers un projet bas√© sur une probl√©matique ‚Äúm√©tier‚Äù et des donn√©es r√©elles. Pour cela, l‚Äô√©valuation sera en deux parties :\n\nPar groupe de 3 : un projet √† choisir parmi les 3 parcours (MLOps, app interactive / dashboard, publication reproductible + site web). Id√©alement, on choisira un projet r√©el, effectu√© par exemple dans le cadre d‚Äôun cours pr√©c√©dent et qui g√©n√®re un output propice √† une mise en production.\nSeul : effectuer une revue de code d‚Äôun autre projet. Comp√©tence essentielle et souvent attendue d‚Äôun data scientist, la revue de code sera l‚Äôoccasion de bien int√©grer les bonnes pratiques de d√©veloppement (cf.¬†checklist ci-dessous) et de faire un retour bienveillant sur un autre projet que celui de son groupe.\n\n\n\n\n\n\n\nImportantAvertissement\n\n\n\nCe projet doit mobiliser des donn√©es publiquement accessibles. La r√©cup√©ration et structuration de ces donn√©es peut faire partie des enjeux du projet mais celles-ci ne doivent pas provenir d‚Äôun projet ant√©rieur de votre scolarit√© pour lequel le partage de donn√©es n‚Äôest pas possible."
  },
  {
    "objectID": "chapters/evaluation.html#modalit√©s",
    "href": "chapters/evaluation.html#modalit√©s",
    "title": "Evaluation",
    "section": "",
    "text": "L‚Äôobjectif g√©n√©ral de l‚Äô√©valuation de ce cours est de mettre en pratique les notions √©tudi√©es (bonnes pratiques de d√©veloppement et mise en production) de mani√®re appliqu√©e et r√©aliste, i.e.¬†√† travers un projet bas√© sur une probl√©matique ‚Äúm√©tier‚Äù et des donn√©es r√©elles. Pour cela, l‚Äô√©valuation sera en deux parties :\n\nPar groupe de 3 : un projet √† choisir parmi les 3 parcours (MLOps, app interactive / dashboard, publication reproductible + site web). Id√©alement, on choisira un projet r√©el, effectu√© par exemple dans le cadre d‚Äôun cours pr√©c√©dent et qui g√©n√®re un output propice √† une mise en production.\nSeul : effectuer une revue de code d‚Äôun autre projet. Comp√©tence essentielle et souvent attendue d‚Äôun data scientist, la revue de code sera l‚Äôoccasion de bien int√©grer les bonnes pratiques de d√©veloppement (cf.¬†checklist ci-dessous) et de faire un retour bienveillant sur un autre projet que celui de son groupe.\n\n\n\n\n\n\n\nImportantAvertissement\n\n\n\nCe projet doit mobiliser des donn√©es publiquement accessibles. La r√©cup√©ration et structuration de ces donn√©es peut faire partie des enjeux du projet mais celles-ci ne doivent pas provenir d‚Äôun projet ant√©rieur de votre scolarit√© pour lequel le partage de donn√©es n‚Äôest pas possible."
  },
  {
    "objectID": "chapters/evaluation.html#checklist-des-bonnes-pratiques-de-d√©veloppement",
    "href": "chapters/evaluation.html#checklist-des-bonnes-pratiques-de-d√©veloppement",
    "title": "Evaluation",
    "section": "Checklist des bonnes pratiques de d√©veloppement",
    "text": "Checklist des bonnes pratiques de d√©veloppement\nLes bonnes pratiques de d√©veloppement ci-dessous sont les indispensables de ce cours. Elles doivent √™tre √† la fois appliqu√©es dans les projets de groupe, et √† la base de la revue de code individuelle.\n\nUtilisation de Git\n\nPr√©sence d‚Äôun fichier .gitignore adapt√© au langage et avec des r√®gles additionnelles pour respecter les bonnes pratiques de versioning\nTravail collaboratif : utilisation des branches et des pull requests\n\nPr√©sence d‚Äôun fichier README pr√©sentant le projet : contexte, objectif, comment l‚Äôutiliser ?\nPr√©sence d‚Äôun fichier LICENSE d√©clarant la licence (open-source) d‚Äôexploitation du projet.\nVersioning des packages : pr√©sence d‚Äôun fichier requirements.txt ou d‚Äôun fichier d‚Äôenvironnement environment.yml pour conda\nQualit√© du code\n\nRespect des standards communautaires : utiliser un linter et/ou un formatter\nModularit√© : un script principal qui appelle des modules\n\nStructure des projets\n\nRespect des standards communautaires (cookiecutter)\nModularit√© du projet selon le mod√®le √©voqu√© dans le cours:\n\nCode sur GitHub\nDonn√©es sur S3\nFichiers de configuration (secrets, etc.) √† part\n\n\n\n\n\n\nProposition de modularit√© du projet illustr√©e pour un projet mixte MLOps et dashboard"
  },
  {
    "objectID": "chapters/evaluation.html#projets",
    "href": "chapters/evaluation.html#projets",
    "title": "Evaluation",
    "section": "Projets",
    "text": "Projets\nVoici trois ‚Äúparcours‚Äù possibles afin de mettre en application les concepts et techniques du cours dans le cadre de projets appliqu√©s. Des projets qui sortiraient de ces parcours-types sont tout √† fait possibles et appr√©ci√©s, il suffit d‚Äôen discuter avec les auteurs du cours.\n\nParcours MLOps\n\n\n\n\n\n\nTipObjectif\n\n\n\nA partir d‚Äôun projet existant ou d‚Äôun projet type contest Kaggle, d√©velopper un mod√®le de ML r√©pondant √† une probl√©matique m√©tier, puis la d√©ployer sur une infrastructure de production conform√©ment aux principes du MLOps.\n\n\n√âtapes :\n\nRespecter la checklist des bonnes pratiques de d√©veloppement ;\nD√©velopper un mod√®le de ML qui r√©pond √† un besoin m√©tier ;\nEntra√Æner le mod√®le via validation crois√©e, avec une proc√©dure de fine-tuning des hyperparam√®tres ;\nFormaliser le processus de fine-tuning de mani√®re reproductible via MLFlow ;\nConstruire une API avec Fastapi pour exposer le meilleur mod√®le ;\nCr√©er une image Docker pour mettre √† disposition l‚ÄôAPI ;\nD√©ployer l‚ÄôAPI sur le SSP Cloud ;\nIndustrialiser le d√©ploiement en mode GitOps avec ArgoCD\nG√©rer le monitoring de l‚Äôapplication : logs, dashboard de suivi des performances, etc.\n\n\n\nParcours dashboard / application interactive\n\n\n\n\n\n\nTipObjectif\n\n\n\nA partir d‚Äôun projet existant ou d‚Äôun projet que vous construirez, d√©velopper une application interactive ou un dashboard statique r√©pondant √† une probl√©matique m√©tier, puis d√©ployer sur une infrastructure de production.\n\n\n√âtapes :\n\nRespecter la checklist des bonnes pratiques de d√©veloppement\nD√©velopper une application interactive Streamlit ou un dashboard statique avec Quarto r√©pondant √† une probl√©matique m√©tier\nCr√©er une image Docker permettant d‚Äôexposer l‚Äôapplication en local\nD√©ployer l‚Äôapplication sur le SSP Cloud (application interactive) ou sur Github Pages (site statique)\nCustomiser le th√®me, le CSS etc. pour mettre en valeur au maximum les r√©sultats de la publication et les messages principaux\nAutomatiser l‚Äôingestion des donn√©es en entr√©e pour que le site web se mette √† jour r√©guli√®rement\nIndustrialiser le d√©ploiement en mode GitOps avec ArgoCD\nG√©rer le monitoring de l‚Äôapplication : logs, m√©triques de suivi des performances, etc.\n\n\n\nParcours big data\n\n\n\n\n\n\nTipObjectif\n\n\n\nL‚Äôobjectif de ce parcours est de construire un pipeline type ETL (Extract/Transform/Load) prenant en entr√©e une source de donn√©es massives afin de les mettre √† disposition dans un syst√®me de base de donn√©es optimis√© pour l‚Äôanalyse. Ce parcours est int√©ressant pour les √©tudiant.e.s souhaitant un projet avec une coloration data engineering plus marqu√©e.\n\n\n√âtapes :\n\nRespecter la checklist des bonnes pratiques de d√©veloppement\nExtract : identifier une ou plusieurs sources de donn√©es massives ouvertes (id√©es : 1, 2), et r√©aliser l‚Äôingestion de ces donn√©es sur le service de stockage S3 du SSP Cloud (documentation)\nTransform : en utilisant une technologie big data adopt√© √† la volum√©trie des donn√©es en entr√©e (donn√©es massives : Spark, donn√©es volumineuses : Arrow / DuckDB, toutes disponibles sur le SSP Cloud), effectuer des op√©rations sur les donn√©es brutes (filtrages, agr√©gations, etc.) afin d‚Äôen extraire des sous-ensembles de donn√©es pertinents pour r√©pondre √† une probl√©matique m√©tier\nLoad : charger les tables construites √† l‚Äô√©tape pr√©c√©dente dans un syst√®me de base de donn√©es relationnelle (ex : PostgreSQL, disponible dans le catalogue du SSP Cloud)\nInt√©grer l‚Äôensemble des √©tapes dans un pipeline de donn√©es avec un orchestrateur de traitements (ex : Argo Workflows, disponible dans le catalogue du SSP Cloud) afin d‚Äôautomatiser leur ex√©cution\nConstruire un dashboard minimaliste (par exemple, avec Superset, disponible dans le catalogue du SSP Cloud) afin de valoriser les donn√©es produites\n\n\n\nParcours publication reproductible\n\n\n\n\n\n\nTipObjectif\n\n\n\nA partir d‚Äôun projet existant ou d‚Äôun projet que vous construirez, r√©diger un rapport reproductible √† partir de donn√©es afin de r√©pondre √† une probl√©matique m√©tier, puis le mettre √† disposition √† travers un site web automatiquement g√©n√©r√© et publi√©.\n\n\n√âtapes :\n\nRespecter la checklist des bonnes pratiques de d√©veloppement\nR√©diger un rapport reproductible avec Quarto qui fasse intervenir des donn√©es, du code, de la visualisation de donn√©es, du texte, etc.\nExposer le rapport sous la forme d‚Äôun site web via GitHub Actions\nCustomiser le th√®me, le CSS etc. pour mettre en valeur au maximum les r√©sultats de la publication et les messages principaux\nAutomatiser l‚Äôingestion des donn√©es en entr√©e pour que le site web se mette √† jour r√©guli√®rement\nMettre en place des tests automatis√©s de v√©rification des standards de qualit√© du code (linter), de d√©tection de fautes d‚Äôorthographes/de grammaire, etc.\nG√©n√©rer des slides au format quarto-revealjs afin de pr√©senter les principaux r√©sultats de la publication, et les exposer comme une page du site"
  },
  {
    "objectID": "chapters/evaluation.html#revue-de-code",
    "href": "chapters/evaluation.html#revue-de-code",
    "title": "Evaluation",
    "section": "Revue de code",
    "text": "Revue de code\nSur le projet d‚Äôun groupe diff√©rent du sien (attribu√© al√©atoirement au cours du semestre) :\n\nouvrir une pull request de revue de code via un fork (cf.¬†chapitre sur Git pour la proc√©dure)\ndonner une appr√©ciation g√©n√©rale de la conformit√© du projet √† la checklist des bonnes pratiques de d√©veloppement\nsugg√©rer des pistes d‚Äôam√©lioration du projet\n\nChaque groupe, ayant re√ßu des revues de code de son projet, pourra prendre en compte ces pistes d‚Äôam√©liorations dans la mesure du temps disponible, par le biais d‚Äôune autre pull request qui devra r√©f√©rencer celle de la revue de code. Cette derni√®re partie ne sera cependant pas strictement attendue, elle sera valoris√©e en bonus dans la notation finale."
  },
  {
    "objectID": "chapters/code-quality.html",
    "href": "chapters/code-quality.html",
    "title": "Code quality",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran.\nThis chapter introduces the topic of code quality,\nthe first level in the hierarchy of best practices. It outlines\nwhy code quality matters, general principles for improving it,\nand a few simple tools or practices to enhance code quality.\nThese are explored further in the running example."
  },
  {
    "objectID": "chapters/code-quality.html#why-readable-and-maintainable-code-matters",
    "href": "chapters/code-quality.html#why-readable-and-maintainable-code-matters",
    "title": "Code quality",
    "section": "Why Readable and Maintainable Code Matters",
    "text": "Why Readable and Maintainable Code Matters\n\n‚ÄúThe code is read much more often than it is written.‚Äù\nGuido Van Rossum1\n\nWhen getting started with data science, it‚Äôs natural to think of code in a purely functional way: ‚ÄúI want to complete a given task‚Äîsay, run a classification algorithm‚Äîso I‚Äôll piece together some code, often found online, in a notebook until the task is done.‚Äù The project‚Äôs structure doesn‚Äôt matter much, as long as it loads the necessary data.\nWhile this minimalist and flexible mindset works well during the learning phase, it‚Äôs essential to move past it as you progress‚Äîespecially if you‚Äôre building professional or collaborative projects. Otherwise, you‚Äôll likely end up with code that‚Äôs hard to maintain or improve‚Äîand that will eventually be abandoned.\nIt‚Äôs important to choose, among many ways to solve a problem, a solution that can be understood by others who speak the same programming language. Code is read far more than it‚Äôs written‚Äîit‚Äôs primarily a communication tool. Moreover, maintaining code typically takes more effort than writing it in the first place. That‚Äôs why thinking ahead about code quality and project structure is critical to long-term maintainability.\nTo improve communication and reduce the pain of working with unclear code, developers have attempted‚Äîsometimes informally, sometimes through institutions‚Äîto define conventions. These depend on the language but are based on principles that apply universally to code-based projects."
  },
  {
    "objectID": "chapters/code-quality.html#why-follow-conventions",
    "href": "chapters/code-quality.html#why-follow-conventions",
    "title": "Code quality",
    "section": "Why Follow Conventions?",
    "text": "Why Follow Conventions?\nPython is a very readable language. With a bit of care‚Äînaming things well, managing dependencies, and structuring code properly‚Äîyou can often understand a script without running it. This is one of Python‚Äôs biggest strengths, enabling fast learning and easy understanding of other people‚Äôs code.\nThe Python community has developed a set of widely accepted standards, called PEPs (Python Enhancement Proposals), that serve as the foundation of the ecosystem. The two most well-known are:\n\nPEP8, which defines code style conventions;\nPEP257, which outlines conventions for documentation (docstrings).\n\nThese conventions go beyond syntax. Several standards for project organization have also emerged, which we‚Äôll explore in the next chapter.\n\n\n\n\n\n\nNoteComparison with \n\n\n\n\n\nIn the  ecosystem, formalization has been less structured. The language is more permissive than Python in some ways2. Still, some style standards have emerged, including:\n\nthe tidyverse style guide,\nthe Google R style guide,\nand the MLR style guide‚Ä¶\n\nFor further learning in :\n\nThe Insee training on best practices with Git and , which aligns closely with this course;\nAdditional guidance in the collaborative utilitR documentation;\nThis blog post that links to various resources on the subject.\n\n\n\n\nThese conventions are somewhat arbitrary‚Äîit‚Äôs natural to prefer some styles over others.\nThey‚Äôre also not set in stone. Languages and practices evolve, which means conventions must adapt. Still, adopting the recommended habits‚Äîwhen possible‚Äîwill make your code easier for the community to understand, increase your chances of getting help, and make future maintenance easier.\nThere are many coding style philosophies, but the most important principle is consistency: If you choose a convention‚Äîsay, snake_case (my_function_name) over camelCase (myFunctionName)‚Äîthen stick with it."
  },
  {
    "objectID": "chapters/code-quality.html#a-good-ide-the-first-step-toward-quality",
    "href": "chapters/code-quality.html#a-good-ide-the-first-step-toward-quality",
    "title": "Code quality",
    "section": "A Good IDE: The First Step Toward Quality",
    "text": "A Good IDE: The First Step Toward Quality\nWithout automated code formatting tools, adopting best practices would be time-consuming and difficult to implement daily. Tools that provide diagnostics or automatically format code are incredibly useful. They allow developers to meet minimum quality standards almost instantly, saving time throughout a data science project. These tools are a prerequisite for production deployment, which we‚Äôll discuss later.\nThe first step toward best practices is choosing a suitable development environment. VSCode is an excellent IDE, as we‚Äôll explore in the practical section. It offers autocompletion, built-in diagnostics (unlike Jupyter), and a wide array of extensions to expand functionality:\n\n\n\nExample of diagnostics and actions in VSCode\n\n\nHowever, IDE-level tools are not enough. They require manual interaction, which can be time-consuming and difficult to apply consistently. Fortunately, we also have automated tools for diagnostics and formatting.\n\nAutomated Tools for Code Diagnostics and Formatting\nSince Python is the primary tool of thousands of data scientists, many tools have been developed to reduce the time needed to create a functional project. These tools boost productivity, reduce repetitive tasks, and improve project quality through diagnostics or even automatic fixes.\nThere are two main types of tools:\n\nLinters: programs that check whether code formally adheres to a given guidestyle\n\nThey report issues but do not fix them\n\nFormatters: programs that automatically rewrite code to follow a specific guidestyle\n\nThey modify the code directly\n\n\n\n\n\n\n\n\nTipExamples\n\n\n\n\n\n\nIssues that a linter can catch:\n\nlong or poorly indented lines, unbalanced parentheses, poorly named functions‚Ä¶\n\nIssues that a linter typically won‚Äôt catch:\n\nincorrect function usage, mis-specified arguments, incoherent structure, insufficient documentation‚Ä¶\n\n\n\n\n\n\n\nLinters to Identify Bad Coding Habits\nLinters assess code quality and its potential to trigger explicit or silent errors.\nExamples of issues linters can catch include:\n\nusage of undefined variables (errors)\nunused variables (unnecessary code)\npoor code organization (risk of bugs)\nviolations of code style guidelines\nsyntax errors (e.g., typos)\n\nMost development tools offer built-in diagnostics (and sometimes suggestions). You may need to enable them in the settings, as they‚Äôre often disabled by default to avoid overwhelming beginners.\nHowever, if you don‚Äôt fix issues as you go, the backlog of changes can become overwhelming.\nIn Python, the two most common linters are PyLint and Flake8. In this course, we‚Äôll use PyLint for its practicality and pedagogy. It can be run from the command line as follows:\npip install pylint\npylint myscript.py   # for a single file\npylint src           # for all files in the 'src' folder\n\n\n\n\n\n\nTip\n\n\n\n\n\nOne of the nice features of PyLint is that it gives a score, which is quite informative. We‚Äôll use this in the running project to track how each step improves code quality.\nYou can also set up pre-commit hooks to block commits that don‚Äôt meet a minimum score.\n\n\n\n\n\nFormatters for Bulk Code Cleanup\nA formatter rewrites code directly‚Äîlike a spellchecker, but for style. It can make substantial changes to improve readability.\nThe most widely used formatter is Black. More recently, Ruff‚Äîa hybrid linter/formatter‚Äîhas gained popularity. It builds on Black while integrating diagnostics from other packages.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf your project uses Black, you can add a badge to the README on GitHub:\n\n\n\n\nIt‚Äôs quite instructive to review code after formatting‚Äîit helps identify and correct stylistic habits. You‚Äôll likely notice some rules that contradict your current habits. Try applying these new rules incrementally. Once they become second nature, revisit the guide and tackle the next set of improvements. This step-by-step approach helps raise code quality without getting bogged down in micro-details that distract from the bigger project goals."
  },
  {
    "objectID": "chapters/code-quality.html#sharing-a-path-to-better-code-quality",
    "href": "chapters/code-quality.html#sharing-a-path-to-better-code-quality",
    "title": "Code quality",
    "section": "Sharing: A Path to Better Code Quality",
    "text": "Sharing: A Path to Better Code Quality\n\nOpen Source as a Quality Driver\nBy sharing your code on open-source platforms (see Git chapter), you may receive suggestions or even contributions from other users. But the benefits of openness go further. Public code tends to be better written, better documented, and more thoughtfully structured‚Äîoften because authors want to avoid public embarrassment! Even without external feedback, sharing code encourages higher quality.\n\n\nCode Review\nCode review borrows from academic peer review to improve the quality of Python code. In a review, one or more developers read and evaluate code written by someone else to identify errors and suggest improvements.\nBenefits include:\n\ncatching bugs before they escalate\nensuring consistent style and structure\nenforcing best practices\nidentifying code that could be refactored for clarity or maintainability\n\nIt‚Äôs also a great way to share knowledge: senior developers can help junior ones grow by reviewing their work.\nPlatforms like GitHub  and GitLab  offer convenient code review features: inline discussions, suggestions, etc."
  },
  {
    "objectID": "chapters/code-quality.html#objectives",
    "href": "chapters/code-quality.html#objectives",
    "title": "Code quality",
    "section": "Objectives",
    "text": "Objectives\n\nEncourage conciseness to reduce the risk of error and make the process clearer;\nImprove readability, which is essential to make the process understandable by others but also for yourself when revisiting an old script;\nReduce redundancy, which simplifies code (the don‚Äôt repeat yourself paradigm);\nMinimize the risk of errors due to copy/paste."
  },
  {
    "objectID": "chapters/code-quality.html#advantages-of-functions",
    "href": "chapters/code-quality.html#advantages-of-functions",
    "title": "Code quality",
    "section": "Advantages of Functions",
    "text": "Advantages of Functions\nFunctions have many advantages over long scripts:\n\nLimit the risk of errors caused by copy/paste;\nMake the code more readable and compact;\nOnly one place to modify the code if the processing changes;\nFacilitate code reuse and documentation!\n\n\n\n\n\n\n\nImportantGolden Rule\n\n\n\nYou should use a function whenever a piece of code is used more than twice (don‚Äôt repeat yourself (DRY)).\n\n\n\n\n\n\n\n\nTipRules for Writing Effective Functions\n\n\n\n\nOne task = one function;\nA complex task = a sequence of functions, each performing a simple task;\nLimit the use of global variables.\n\n\n\nRegarding package installation, as we will see in the Project Structure and Portability sections, this should not be managed inside the script, but in a separate element related to the project‚Äôs execution environment3. Those sections also provide practical advice on handling API or database tokens, which should never be written in the code.\nOverly long scripts are not a best practice. It is better to divide all scripts executing a production chain into ‚Äúmonads‚Äù, i.e., small coherent units. Functions are a key tool for this purpose (they help reduce redundancy and are a preferred tool for documenting code).\n\n\n\n\n\n\nCautionExample: Prefer List Comprehensions\n\n\n\n\n\nIn Python, it is recommended to prefer list comprehensions over indented for loops. The latter are generally less efficient and involve a larger number of code lines, whereas list comprehensions are much more concise:\nliste_nombres = range(10)\n\n# very bad\ny = []\nfor x in liste_nombres:\n    if x % 2 == 0:\n        y.append(x*x)\n\n# better\ny = [x*x for x in liste_nombres if x % 2 == 0]\n\n\n\n\nProgramming Advice\nIn the Python programming world, there are two main paradigms:\n\nFunctional programming: builds code by chaining functions, i.e., more or less standardized operations;\nObject-Oriented Programming (OOP): builds code by defining objects of a given class with attributes (intrinsic features) and custom methods to perform class-specific operations.\n\n\n\nExample comparing the two paradigms\n\nThanks ChatGPT for the example:\n\nclass AverageCalculator:\n    def __init__(self, numbers):\n        self.numbers = numbers\n\n    def calculate_average(self):\n        return sum(self.numbers) / len(self.numbers)\n\n# Usage\ncalculator = AverageCalculator([1, 2, 3, 4, 5])\nprint(\"Average (OOP):\", calculator.calculate_average())\n\ndef calculate_average(numbers):\n    return sum(numbers) / len(numbers)\n\n# Usage\nnumbers = [1, 2, 3, 4, 5]\nprint(\"Average (FP):\", calculate_average(numbers))\n\nAverage (OOP): 3.0\nAverage (FP): 3.0\n\n\n\nFunctional programming is more intuitive than OOP and often allows for quicker code development. OOP is a more formalist approach, useful when functions need to adapt to the input object type (e.g., loading different model weights depending on the model type in Pytorch). It avoids üçù spaghetti code that‚Äôs hard to debug.\nHowever, one should remain pragmatic. OOP can be more complex to implement than functional programming. In many cases, well-written functional code is sufficient. For large projects, adopting a defensive programming approach is helpful ‚Äî a precautionary strategy in the functional paradigm that anticipates and manages unexpected situations (e.g., wrong argument type or structure).\n\n\n\n\n\n\nNoteSpaghetti Code\n\n\n\n‚ÄúSpaghetti code‚Äù refers to programming style that leads to tangled code due to excessive use of conditions, exceptions, and complex event handling. It becomes almost impossible to trace the cause of errors without stepping through every line of code ‚Äî and there are many, due to poor practices.\nSpaghetti code prevents determining who, what, and how something happens, making updates time-consuming since one must follow the chain of references line by line.\n\n\n\n\n\n\n\n\nTipA Progressive Example\n\n\n\n\n\nüí° Suppose we have a dataset that uses ‚àí99 to represent missing values. We want to replace all ‚àí99 with NA.\nnp.random.seed(1234)\na = np.random.randint(1, 10, size = (5,6))\ndf = np.insert(\n    a,\n    np.random.choice(len(a), size=6),\n    -99,\n)\ndf = pd.DataFrame(df.reshape((6,6)), columns=[chr(x) for x in range(97, 103)])\nFirst attempt:\ndf2 = df.copy()\ndf2.loc[df2['a'] == -99,'a'] = np.nan\ndf2.loc[df2['b'] == -99,'b'] = np.nan\ndf2.loc[df2['c'] == -99,'c'] = np.nan\ndf2.loc[df2['d'] == -99,'d'] = np.nan\ndf2.loc[df2['e'] == -98,'e'] = np.nan\ndf2.loc[df2['f'] == -99,'e'] = np.nan\nWhat‚Äôs wrong here?\n\n\nHint üí° Look at columns e and g.\n\nTwo copy-paste errors: - -98 instead of -99; - 'e' instead of 'f' in the last line.\n\nNext improvement ‚Äî using a function:\ndef fix_missing(x: pd.Series):\n    x[x == -99] = np.nan\n    return x\n\ndf2 = df.copy()\ndf2['a'] = fix_missing(df['a'])\ndf2['b'] = fix_missing(df['b'])\n...\nStill repetitive and error-prone with column names.\nBest version ‚Äî apply function across all columns:\ndf2 = df.copy()\ndf2 = df2.apply(fix_missing)\nNow the code is: 1. Concise; 2. Robust to data structure changes; 3. Free from hard-coded mistakes; 4. Easily generalizable ‚Äî e.g., apply only on selected columns:\ndf2[['a','b','e']] = df2[['a','b','e']].apply(fix_missing)\n\n\n\nResources like the Hitchhiker‚Äôs Guide to Python and this blog post illustrate these design principles well.\n\n\n\n\n\n\nNoteThe Zen of Python\n\n\n\n\n\nWritten by Tim Peters in 2004, this set of aphorisms embodies Python‚Äôs design philosophy:\nimport this\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\n...\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "chapters/code-quality.html#footnotes",
    "href": "chapters/code-quality.html#footnotes",
    "title": "Code quality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGuido Van Rossum is the creator of , which makes him a voice worth listening to.‚Ü©Ô∏é\nFor example, in , you can use &lt;- or = for assignment, and the language won‚Äôt complain about poor indentation‚Ä¶‚Ü©Ô∏é\nWe will present the two main approaches in Python, their similarities and differences: virtual environments (managed by a requirements.txt file) and conda environments (managed by an environment.yml file).‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/application.html",
    "href": "chapters/application.html",
    "title": "Application",
    "section": "",
    "text": "L‚Äôobjectif de cette mise en application est d‚Äôillustrer les diff√©rentes √©tapes qui s√©parent la phase de d√©veloppement d‚Äôun projet de celle de la mise en production. Elle permettra de mettre en pratique les diff√©rents concepts pr√©sent√©s tout au long du cours.\nL‚Äôobjectif p√©dagogique principal de cette application est d‚Äôadopter un point de vue pragmatique en choisissant des outils et des m√©thodes de travail qui permettent de r√©aliser des objectifs ambitieux de valorisation de donn√©es. Python  sera le trait d‚Äôunion entre les diff√©rentes technologies ou infrastructures que nous utiliserons.\nCette application est un tutoriel pas √† pas pour avoir un projet reproductible et disponible sous plusieurs livrables. Toutes les √©tapes ne sont pas indispensables √† tous les projets de data science et il existe des outils alternatifs √† ceux pr√©sent√©s. N√©anmoins, les outils pr√©sent√©s ont l‚Äôavantage d‚Äô√™tre tr√®s bien int√©gr√©s √† Python, bien configur√©s si vous utilisez le SSPCloud comme nous le recommandons, tout en √©tant agnostiques sur le reste des outils que vous utilisez ; de sorte √† ne pas √™tre bloquants si on remplace l‚Äôune des briques logicielles par une autre.\nNous nous pla√ßons dans une situation initiale correspondant √† la fin de la phase de d√©veloppement d‚Äôun projet de data science. On a un notebook un peu monolithique, qui r√©alise les √©tapes classiques d‚Äôun pipeline de machine learning :\n\nImport de donn√©es ;\nStatistiques descriptives et visualisations ;\nFeature engineering ;\nEntra√Ænement d‚Äôun mod√®le ;\nEvaluation du mod√®le.\n\n\n\nL‚Äôobjectif est d‚Äôam√©liorer le projet de mani√®re incr√©mentale jusqu‚Äô√† pouvoir le mettre en production, en le valorisant sous une forme adapt√©e et en adoptant une m√©thode de travail fluidifiant les √©volutions futures.\nEn am√©liorant et enrichissant progressivement notre projet, nous aboutirons sur le cercle infini du MLOps (Figure¬†1) permettant la coexistence de versions en production et en d√©veloppement.\nLa Figure¬†2 montre que notre point de d√©part initial, √† savoir un notebook, m√©lange tout. Ceci rend tr√®s complexe la mise √† jour de notre mod√®le ou l‚Äôexploitation de notre mod√®le sur de nouvelles donn√©es, ce qui est pourtant la raison d‚Äô√™tre du machine learning qui est pens√© pour l‚Äôextrapolation. Si on vous demande de valoriser votre mod√®le sur de nouvelles donn√©es, vous risquez de devoir refaire tourner tout votre notebook, avec le risque de ne pas retrouver les m√™mes r√©sultats que dans la version pr√©c√©dente.\nLa Figure¬†3 illustre l‚Äôhorizon auquel nous aboutirons √† la fin de cette application. Nous d√©synchronisons les √©tapes d‚Äôentra√Ænement et de pr√©diction, en identifiant mieux les pr√©-requis de chacune et en adoptant des briques technologiques adapt√©es √† celles-ci. Les noms pr√©sents sur cette figure sont encore obscurs, c‚Äôest normal, mais ils vous deviendrons familiers si vous adoptez une infrastructure et une m√©thode de travail √† l‚Äô√©tat de l‚Äôart.\n\n\n\n\n\n\nFigure¬†1: Le cercle infini du MLOps\n\n\n\n\n\n\n\n\n\nFigure¬†2: Illustration de notre point de d√©part\n\n\n\n\n\n\n\n\n\nFigure¬†3: Illustration de l‚Äôhorizon vers lequel on se dirige\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIl est important de bien lire les consignes et d‚Äôy aller progressivement. Certaines √©tapes peuvent √™tre rapides, d‚Äôautres plus fastidieuses ; certaines √™tre assez guid√©es, d‚Äôautres vous laisser plus de libert√©. Si vous n‚Äôeffectuez pas une √©tape, vous risquez de ne pas pouvoir passer √† l‚Äô√©tape suivante qui en d√©pend.\nBien que l‚Äôexercice soit applicable sur toute configuration bien faite, nous recommandons de privil√©gier l‚Äôutilisation du SSP Cloud, o√π tous les outils n√©cessaires sont pr√©-install√©s et pr√©-configur√©s. Le service VSCode ne sera en effet que le point d‚Äôentr√©e pour l‚Äôutilisation d‚Äôoutils plus exigeants sur le plan de l‚Äôinfrastructure: Argo, MLFLow, etc.\n\n\n\n\n\nA l‚Äôheure actuelle, cette application se concentre sur la mise en oeuvre fiable de l‚Äôentra√Ænement de mod√®les de machine learning. Comme vous pouvez le voir, quand on part d‚Äôaussi loin qu‚Äôun projet monolithique dans un notebook, c‚Äôest un travail cons√©quent d‚Äôen arriver √† un pipeline pens√© pour la production. Cette application vise √† vous sensibiliser au fait qu‚Äôavoir la Figure¬†3 en t√™te et adopter une organisation de travail et faire des choix techniques ad√©quats, vous fera √©conomiser des dizaines voire centaines d‚Äôheures lorsque votre mod√®le aura vocation √† passer en production.\nA l‚Äôheure actuelle, cette application ne se concentre que sur une partie du cycle de vie d‚Äôun projet data ; il y a d√©j√† fort √† faire. Nous nous concentrons sur l‚Äôentra√Ænement et la mise √† disposition d‚Äôun mod√®le √† des fins op√©rationnelles. C‚Äôest la premi√®re partie du cycle de vie d‚Äôun mod√®le. Dans une approche MLOps, il faut √©galement penser la maintenance de ce mod√®le et les enjeux que repr√©sentent l‚Äôarriv√©e continue de nouvelles donn√©es, ou le besoin d‚Äôen collecter de nouvelles √† travers des annotations, sur la qualit√© pr√©dictive d‚Äôun mod√®le. Toute entreprise qui ne pense pas cet apr√®s est vou√©e √† se faire doubler par un nouveau venu. Une prochaine version de cette application permettra certainement d‚Äôillustrer certains des enjeux aff√©rants √† la vie en production d‚Äôun mod√®le (supervision, annotations‚Ä¶) sur notre cas d‚Äôusage.\nIl convient aussi de noter que nous ne faisons que parcourir la surface des sujets que nous √©voquons. Ce cours, d√©j√† dense, deviendrait indigeste si nous devions pr√©senter chaque outil dans le d√©tail. Nous laissons donc les curieux approfondir chacun des outils que nous pr√©sentons pour d√©couvrir comment en tirer le maximum (et si vous avez l‚Äôimpression que nous oublions des √©l√©ments cruciaux, les issues et pull requests  sont bienvenues).\n\n\n\nPour simplifier la reprise en cours de ce fil rouge, nous proposons un syst√®me de checkpoints qui s‚Äôappuient sur des tags Git. Ces tags figent le projet tel qu‚Äôil est √† l‚Äôissue d‚Äôun exercice donn√©.\nSi vous faites √©voluer votre projet de mani√®re exp√©rimentale mais d√©sirez tout de m√™me utiliser √† un moment ces checkpoints, il va falloir faire quelques acrobaties Git. Pour cela, nous mettons √† disposition un script qui permet de sauvegarder votre avanc√©e dans un tag donn√© (au cas o√π, √† un moment, vous vouliez revenir dessus) et √©craser la branche main avec le tag en question. Par exemple, si vous d√©sirez reprendre apr√®s l‚Äôexercice 9, vous devrez faire tourner le code dans cette boite :\n  \n    \n      \n        \n      \n      \n        Checkpoint d'exemple      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli92\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli9\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#objectif",
    "href": "chapters/application.html#objectif",
    "title": "Application",
    "section": "",
    "text": "L‚Äôobjectif est d‚Äôam√©liorer le projet de mani√®re incr√©mentale jusqu‚Äô√† pouvoir le mettre en production, en le valorisant sous une forme adapt√©e et en adoptant une m√©thode de travail fluidifiant les √©volutions futures.\nEn am√©liorant et enrichissant progressivement notre projet, nous aboutirons sur le cercle infini du MLOps (Figure¬†1) permettant la coexistence de versions en production et en d√©veloppement.\nLa Figure¬†2 montre que notre point de d√©part initial, √† savoir un notebook, m√©lange tout. Ceci rend tr√®s complexe la mise √† jour de notre mod√®le ou l‚Äôexploitation de notre mod√®le sur de nouvelles donn√©es, ce qui est pourtant la raison d‚Äô√™tre du machine learning qui est pens√© pour l‚Äôextrapolation. Si on vous demande de valoriser votre mod√®le sur de nouvelles donn√©es, vous risquez de devoir refaire tourner tout votre notebook, avec le risque de ne pas retrouver les m√™mes r√©sultats que dans la version pr√©c√©dente.\nLa Figure¬†3 illustre l‚Äôhorizon auquel nous aboutirons √† la fin de cette application. Nous d√©synchronisons les √©tapes d‚Äôentra√Ænement et de pr√©diction, en identifiant mieux les pr√©-requis de chacune et en adoptant des briques technologiques adapt√©es √† celles-ci. Les noms pr√©sents sur cette figure sont encore obscurs, c‚Äôest normal, mais ils vous deviendrons familiers si vous adoptez une infrastructure et une m√©thode de travail √† l‚Äô√©tat de l‚Äôart.\n\n\n\n\n\n\nFigure¬†1: Le cercle infini du MLOps\n\n\n\n\n\n\n\n\n\nFigure¬†2: Illustration de notre point de d√©part\n\n\n\n\n\n\n\n\n\nFigure¬†3: Illustration de l‚Äôhorizon vers lequel on se dirige\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIl est important de bien lire les consignes et d‚Äôy aller progressivement. Certaines √©tapes peuvent √™tre rapides, d‚Äôautres plus fastidieuses ; certaines √™tre assez guid√©es, d‚Äôautres vous laisser plus de libert√©. Si vous n‚Äôeffectuez pas une √©tape, vous risquez de ne pas pouvoir passer √† l‚Äô√©tape suivante qui en d√©pend.\nBien que l‚Äôexercice soit applicable sur toute configuration bien faite, nous recommandons de privil√©gier l‚Äôutilisation du SSP Cloud, o√π tous les outils n√©cessaires sont pr√©-install√©s et pr√©-configur√©s. Le service VSCode ne sera en effet que le point d‚Äôentr√©e pour l‚Äôutilisation d‚Äôoutils plus exigeants sur le plan de l‚Äôinfrastructure: Argo, MLFLow, etc."
  },
  {
    "objectID": "chapters/application.html#ce-que-cette-application-ne-couvre-pas-pour-le-moment",
    "href": "chapters/application.html#ce-que-cette-application-ne-couvre-pas-pour-le-moment",
    "title": "Application",
    "section": "",
    "text": "A l‚Äôheure actuelle, cette application se concentre sur la mise en oeuvre fiable de l‚Äôentra√Ænement de mod√®les de machine learning. Comme vous pouvez le voir, quand on part d‚Äôaussi loin qu‚Äôun projet monolithique dans un notebook, c‚Äôest un travail cons√©quent d‚Äôen arriver √† un pipeline pens√© pour la production. Cette application vise √† vous sensibiliser au fait qu‚Äôavoir la Figure¬†3 en t√™te et adopter une organisation de travail et faire des choix techniques ad√©quats, vous fera √©conomiser des dizaines voire centaines d‚Äôheures lorsque votre mod√®le aura vocation √† passer en production.\nA l‚Äôheure actuelle, cette application ne se concentre que sur une partie du cycle de vie d‚Äôun projet data ; il y a d√©j√† fort √† faire. Nous nous concentrons sur l‚Äôentra√Ænement et la mise √† disposition d‚Äôun mod√®le √† des fins op√©rationnelles. C‚Äôest la premi√®re partie du cycle de vie d‚Äôun mod√®le. Dans une approche MLOps, il faut √©galement penser la maintenance de ce mod√®le et les enjeux que repr√©sentent l‚Äôarriv√©e continue de nouvelles donn√©es, ou le besoin d‚Äôen collecter de nouvelles √† travers des annotations, sur la qualit√© pr√©dictive d‚Äôun mod√®le. Toute entreprise qui ne pense pas cet apr√®s est vou√©e √† se faire doubler par un nouveau venu. Une prochaine version de cette application permettra certainement d‚Äôillustrer certains des enjeux aff√©rants √† la vie en production d‚Äôun mod√®le (supervision, annotations‚Ä¶) sur notre cas d‚Äôusage.\nIl convient aussi de noter que nous ne faisons que parcourir la surface des sujets que nous √©voquons. Ce cours, d√©j√† dense, deviendrait indigeste si nous devions pr√©senter chaque outil dans le d√©tail. Nous laissons donc les curieux approfondir chacun des outils que nous pr√©sentons pour d√©couvrir comment en tirer le maximum (et si vous avez l‚Äôimpression que nous oublions des √©l√©ments cruciaux, les issues et pull requests  sont bienvenues)."
  },
  {
    "objectID": "chapters/application.html#comment-g√©rer-les-checkpoints",
    "href": "chapters/application.html#comment-g√©rer-les-checkpoints",
    "title": "Application",
    "section": "",
    "text": "Pour simplifier la reprise en cours de ce fil rouge, nous proposons un syst√®me de checkpoints qui s‚Äôappuient sur des tags Git. Ces tags figent le projet tel qu‚Äôil est √† l‚Äôissue d‚Äôun exercice donn√©.\nSi vous faites √©voluer votre projet de mani√®re exp√©rimentale mais d√©sirez tout de m√™me utiliser √† un moment ces checkpoints, il va falloir faire quelques acrobaties Git. Pour cela, nous mettons √† disposition un script qui permet de sauvegarder votre avanc√©e dans un tag donn√© (au cas o√π, √† un moment, vous vouliez revenir dessus) et √©craser la branche main avec le tag en question. Par exemple, si vous d√©sirez reprendre apr√®s l‚Äôexercice 9, vous devrez faire tourner le code dans cette boite :\n  \n    \n      \n        \n      \n      \n        Checkpoint d'exemple      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli92\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli9\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-1-sassurer-que-le-script-sex√©cute-correctement",
    "href": "chapters/application.html#√©tape-1-sassurer-que-le-script-sex√©cute-correctement",
    "title": "Application",
    "section": "√âtape 1 : s‚Äôassurer que le script s‚Äôex√©cute correctement",
    "text": "√âtape 1 : s‚Äôassurer que le script s‚Äôex√©cute correctement\nOn va partir du fichier notebook.py qui reprend le contenu du notebook2 mais dans un script classique. Le travail de nettoyage en sera facilit√©.\nLa premi√®re √©tape est simple, mais souvent oubli√©e : v√©rifier que le code fonctionne correctement. Pour cela, nous recommandons de faire un aller-retour entre le script ouvert dans VSCode et un terminal pour le lancer.\n\n\n\n\n\n\nApplicationApplication 1: corriger les erreurs\n\n\n\n\nOuvrir dans VSCode le script titanic.py ;\nEx√©cuter le script en ligne de commande (python titanic.py)3 pour d√©tecter les erreurs ;\nCorriger les deux erreurs qui emp√™chent la bonne ex√©cution ;\nV√©rifier le fonctionnement du script en utilisant la ligne de commande:\n\n\n\nterminal\n\npython titanic.py\n\nLe code devrait afficher des sorties.\n\n\nAide sur les erreurs rencontr√©es\n\nLa premi√®re erreur rencontr√©e est une alerte FileNotFoundError, la seconde est li√©e √† un package.\n\nIl est maintenant temps de commit les changements effectu√©s avec Git4 :\n\n\nterminal\n\ngit add titanic.py\ngit commit -m \"Corrige l'erreur qui emp√™chait l'ex√©cution\"\ngit push\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli1      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli1\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-2-utiliser-un-linter-puis-un-formatter",
    "href": "chapters/application.html#√©tape-2-utiliser-un-linter-puis-un-formatter",
    "title": "Application",
    "section": "√âtape 2: utiliser un linter puis un formatter",
    "text": "√âtape 2: utiliser un linter puis un formatter\nOn va maintenant am√©liorer la qualit√© de notre code en appliquant les standards communautaires. Pour cela, on va principalement utiliser Ruff qui est √† la fois un linter et un formatter5. On va aussi utiliser le linter classique PyLint qui pr√©sente une fonctionnalit√© assez p√©dagogique de notation d‚Äôun script √† l‚Äôaune des standards communautaires √©voqu√©s dans la partie Qualit√© du code.\nCe nettoyage automatique du code permettra, au passage, de restructurer notre script de mani√®re plus naturelle.\n\n\n\n\n\n\nImportant\n\n\n\nPyLint et Ruff sont des packages Python qui s‚Äôutilisent principalement en ligne de commande.\nSi vous avez une erreur qui sugg√®re que votre terminal ne connait pas PyLint ou Ruff, n‚Äôoubliez pas d‚Äôex√©cuter la commande pip install pylint, ou pip install ruff.\n\n\n\n\n\n\n\n\nApplicationApplication 2: rendre lisible le script\n\n\n\n\nDiagnostiquer et √©valuer la qualit√© de titanic.py avec PyLint. Regarder la note obtenue.\nUtiliser le formatter ruff via la commande ruff check titanic.py pour voir les probl√®mes recens√©s par celui-ci. Appliquer l‚Äôoption --fix sugg√©r√©e pour commencer √† formatter le fichier. Evaluer √† nouveau ce fichier avec PyLint.\nUtiliser ruff format titanic.py pour modifier la mise en forme du fichier.\nR√©utiliser PyLint pour diagnostiquer l‚Äôam√©lioration de la qualit√© du script et le travail qui reste √† faire.\nComme la majorit√© du travail restant est √† consacrer aux imports:\n\nMettre tous les imports ensemble en d√©but de script\nRetirer les imports redondants en s‚Äôaidant des diagnostics de votre √©diteur\nR√©ordonner les imports si PyLint ou vous indique de le faire\nCorriger les derni√®res fautes formelles sugg√©r√©es par PyLint\n\nD√©limiter des parties dans votre code pour rendre sa structure plus lisible. Si des parties vous semblent √™tre dans le d√©sordre, vous pouvez r√©ordonner le script (mais n‚Äôoubliez pas de le tester)\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli2      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli22\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli2\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nLe code est maintenant lisible, il obtient √† ce stade une note formelle proche de 10. Mais il n‚Äôest pas encore totalement intelligible ou fiable. Il y a notamment quelques redondances de code auxquelles nous allons nous attaquer par la suite. N√©anmoins, avant cela, occupons-nous de mieux g√©rer certains param√®tres du script: jetons d‚ÄôAPI et chemin des fichiers."
  },
  {
    "objectID": "chapters/application.html#√©tape-3-gestion-des-param√®tres",
    "href": "chapters/application.html#√©tape-3-gestion-des-param√®tres",
    "title": "Application",
    "section": "√âtape 3: gestion des param√®tres",
    "text": "√âtape 3: gestion des param√®tres\n  \n    \n      \n        \n      \n      \n        Reprendre √† partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration propos√©e dans l'application pr√©liminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, apr√®s avoir cl√¥n√© le d√©p√¥t\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli22\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli2\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nL‚Äôex√©cution du code et les r√©sultats obtenus d√©pendent de certains param√®tres d√©finis dans le code. L‚Äô√©tude de r√©sultats alternatifs, en jouant sur des variantes des (hyper)param√®tres, est √† ce stade compliqu√©e car il est n√©cessaire de parcourir le code pour trouver ces param√®tres. De plus, certains param√®tres personnels comme des jetons d‚ÄôAPI ou des mots de passe n‚Äôont pas vocation √† √™tre pr√©sents dans le code.\nIl est plus judicieux de consid√©rer ces param√®tres comme des variables d‚Äôentr√©e du script. Cela peut √™tre fait de deux mani√®res:\n\nAvec des arguments optionnels appel√©s depuis la ligne de commande (Application 3a). Cela peut √™tre pratique pour mettre en oeuvre des tests automatis√©s mais n‚Äôest pas forc√©ment pertinent pour toutes les variables. Nous allons montrer cet usage avec le nombre d‚Äôarbres de notre random forest ;\nEn utilisant un fichier de configuration dont les valeurs sont import√©es dans le script principal (Application 3b).\n\n\n\nUn exemple de d√©finition d‚Äôun argument pour l‚Äôutilisation en ligne de commande\n\n\n\nprenom.py\n\nimport argparse\nparser = argparse.ArgumentParser(description=\"Qui √™tes-vous?\")\nparser.add_argument(\n    \"--prenom\", type=str, default=\"Toto\", help=\"Un pr√©nom √† afficher\"\n)\nargs = parser.parse_args()\nprint(args.prenom)\n\nExemples d‚Äôutilisations en ligne de commande\n\n\nterminal\n\npython prenom.py\npython prenom.py --prenom \"Zinedine\"\n\n\n\n\n\n\n\n\nApplicationApplication 3a: Param√©trisation du script\n\n\n\n\nEn s‚Äôinspirant de l‚Äôexemple ci-dessus üëÜÔ∏è, cr√©er une variable n_trees qui peut √©ventuellement √™tre param√©tr√©e en ligne de commande et dont la valeur par d√©faut est 20 ;\nTester cette param√©trisation en ligne de commande avec la valeur par d√©faut puis 2, 10 et 50 arbres.\n\n\n\nL‚Äôexercice suivant permet de mettre en application le fait de param√©triser un script en utilisant des variables d√©finies dans un fichier YAML.\n\n\n\n\n\n\nApplicationApplication 3b: La configuration dans un fichier d√©di√©\n\n\n\n\nInstaller le package python-dotenv que nous allons utiliser pour charger notre jeton d‚ÄôAPI √† partir d‚Äôune variable d‚Äôenvironnement.\nA partir de l‚Äôexemple de la documentation, utiliser la fonction load_dotenv pour charger dans Python nos variables d‚Äôenvironnement √† partir d‚Äôun fichier (vous pouvez le cr√©er mais ne pas le remplir encore avec les valeurs voulues, ce sera fait ensuite)\nCr√©er la variable comme propos√© ci-dessous et v√©rifier la sortie de Python en faisant tourner titanic.py en ligne de commande ce code\n\n\n\ntitanic.py\n\njeton_api = os.environ[\"JETON_API\"]\n\n\nCela devrait vous renvoyer une erreur, comprenez-vous pourquoi ? Maintenant introduire la valeur voulue pour le jeton d‚ÄôAPI dans le fichier d‚Äôenvironnement lu par dotenv.\nS‚Äôil n‚Äôexiste pas d√©j√†, cr√©er un fichier .gitignore (cf.¬†Chapitre Git). Ajouter dans ce fichier .env car il ne faut pas committer ce fichier. Au passage ajouter __pycache__/ au .gitignore6, cela √©vitera d‚Äôavoir √† le faire ult√©rieurement ;\nCr√©er un fichier README.md o√π vous indiquez qu‚Äôil faut cr√©er un fichier .env pour pouvoir utiliser l‚ÄôAPI.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli3      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli32\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli3\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-4-privil√©gier-la-programmation-fonctionnelle",
    "href": "chapters/application.html#√©tape-4-privil√©gier-la-programmation-fonctionnelle",
    "title": "Application",
    "section": "√âtape 4 : Privil√©gier la programmation fonctionnelle",
    "text": "√âtape 4 : Privil√©gier la programmation fonctionnelle\nNous allons mettre en fonctions les parties importantes de l‚Äôanalyse. Ceci facilitera l‚Äô√©tape ult√©rieure de modularisation de notre projet. Comme cela est √©voqu√© dans les √©l√©ments magistraux de ce cours, l‚Äôutilisation de fonctions va rendre notre code plus concis, plus tra√ßable, mieux document√©.\nCet exercice √©tant chronophage, il n‚Äôest pas obligatoire de le r√©aliser en entier. L‚Äôimportant est de comprendre la d√©marche et d‚Äôadopter fr√©quemment une approche fonctionnelle7. Pour obtenir une chaine enti√®rement fonctionnalis√©e, vous pouvez reprendre le checkpoint.\nPour commencer, cet exercice fait un petit pas de c√¥t√© pour faire comprendre la mani√®re dont les pipelines scikit sont un outil au service des bonnes pratiques.\n\n\n\n\n\n\nApplicationApplication 4 (optionnelle): pourquoi utiliser un pipeline Scikit ?\n\n\n\n\nLe pipeline Scikit d‚Äôestimation et d‚Äô√©valuation vous a √©t√© donn√© tel quel. Regardez, ci-dessous, le code √©quivalent sans utiliser de pipeline Scikit:\n\n\n\nLe code √©quivalent sans pipeline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nimport pandas as pd\nimport numpy as np\n\n# D√©finition des variables\nnumeric_features = [\"Age\", \"Fare\"]\ncategorical_features = [\"Embarked\", \"Sex\"]\n\n# PREPROCESSING ----------------------------\n\n# Handling missing values for numerical features\nnum_imputer = SimpleImputer(strategy=\"median\")\nX_train[numeric_features] = num_imputer.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = num_imputer.transform(X_test[numeric_features])\n\n# Scaling numerical features\nscaler = MinMaxScaler()\nX_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\nX_test[numeric_features] = scaler.transform(X_test[numeric_features])\n\n# Handling missing values for categorical features\ncat_imputer = SimpleImputer(strategy=\"most_frequent\")\nX_train[categorical_features] = cat_imputer.fit_transform(X_train[categorical_features])\nX_test[categorical_features] = cat_imputer.transform(X_test[categorical_features])\n\n# One-hot encoding categorical features\nencoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\nX_train_encoded = encoder.fit_transform(X_train[categorical_features])\nX_test_encoded = encoder.transform(X_test[categorical_features])\n\n# Convert encoded features into a DataFrame\nX_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_features), index=X_train.index)\nX_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_features), index=X_test.index)\n\n# Drop original categorical columns and concatenate encoded ones\nX_train = X_train.drop(columns=categorical_features).join(X_train_encoded)\nX_test = X_test.drop(columns=categorical_features).join(X_test_encoded)\n\n# MODEL TRAINING ----------------------------\n\n# Defining the model\nmodel = RandomForestClassifier(n_estimators=n_trees)\n\n# Fitting the model\nmodel.fit(X_train, y_train)\n\n# EVALUATION ----------------------------\n\n# Scoring\nrdmf_score = model.score(X_test, y_test)\nprint(f\"{rdmf_score:.1%} de bonnes r√©ponses sur les donn√©es de test pour validation\")\n\n# Confusion matrix\nprint(20 * \"-\")\nprint(\"matrice de confusion\")\nprint(confusion_matrix(y_test, model.predict(X_test)))\n\n\nVoyez-vous l‚Äôint√©r√™t de l‚Äôapproche par pipeline en termes de lisibilit√©, √©volutivit√© et fiabilit√© ?\nCr√©er un notebook qui servira de brouillon. Y introduire le code suivant:\n\n\n\nLe code √† copier-coller dans un notebook\n\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\nX_train, y_train = train.drop(\"Survived\", axis=\"columns\"), train[\"Survived\"]\nX_test, y_test = test.drop(\"Survived\", axis=\"columns\"), train[\"Survived\"]\n\nMAX_DEPTH = None\nMAX_FEATURES = \"sqrt\"\nn_trees=20\n\nnumeric_features = [\"Age\", \"Fare\"]\ncategorical_features = [\"Embarked\", \"Sex\"]\n\n# Variables num√©riques\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", MinMaxScaler()),\n    ]\n)\n\n# Variables cat√©gorielles\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n        (\"onehot\", OneHotEncoder()),\n    ]\n)\n\n# Preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"Preprocessing numerical\", numeric_transformer, numeric_features),\n        (\n            \"Preprocessing categorical\",\n            categorical_transformer,\n            categorical_features,\n        ),\n    ]\n)\n\n# Pipeline\npipe = Pipeline(\n    [\n        (\"preprocessor\", preprocessor),\n        (\"classifier\", RandomForestClassifier(\n            n_estimators=n_trees,\n            max_depth=MAX_DEPTH,\n            max_features=MAX_FEATURES\n        )),\n    ]\n)\n\npipe.fit(X_train, y_train)\n\n\nAfficher ce pipeline dans une cellule de votre notebook. Cela vous aide-t-il mieux √† comprendre les diff√©rentes √©tapes du pipeline de mod√©lisation ?\nComment pouvez-vous acc√©der aux √©tapes de preprocessing ?\n\n\n\nComment pouvez-vous faire pour appliquer le pipeline de preprocessing des variables num√©riques (et uniquement celui-ci) √† ce DataFrame ?\n\n\n\nLe DataFrame √† cr√©er pour appliquer un bout de notre pipeline\n\nimport numpy as np\n\nnew_data = {\n    \"Age\": [22, np.nan, 35, 28, np.nan],\n    \"Fare\": [7.25, 8.05, np.nan, 13.00, 15.50]\n}\n\nnew_data = pd.DataFrame(new_data)\n\n\n\nNormalement ce code ne devrait pas prendre plus d‚Äôune demie-douzaine de lignes. Sans pipeline le code √©quivalent, beaucoup plus verbeux et moins fiable, ressemble √† celui-ci\n\n\n\nLe code √©quivalent, sans pipeline\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\n# D√©finition des nouvelles donn√©es\nnew_data = pd.DataFrame({\n    \"Age\": [25, np.nan, 40, 33, np.nan],\n    \"Fare\": [10.50, 7.85, np.nan, 22.00, 12.75]\n})\n\n# D√©finition des transformations (m√™me que dans le pipeline)\nnum_imputer = SimpleImputer(strategy=\"median\")\nscaler = MinMaxScaler()\n\n# Apprentissage des transformations sur X_train (assumant que vous l'avez d√©j√†)\nX_train_numeric = X_train[[\"Age\", \"Fare\"]]  # Supposons que X_train existe\nnum_imputer.fit(X_train_numeric)\nscaler.fit(num_imputer.transform(X_train_numeric))\n\n# Transformation des nouvelles donn√©es\nnew_data_imputed = num_imputer.transform(new_data)\nnew_data_scaled = scaler.transform(new_data_imputed)\n\n# Cr√©ation du DataFrame final\nnew_data_preprocessed = pd.DataFrame(\n    new_data_scaled,\n    columns=[\"Age_scaled\", \"Fare_scaled\"]  # G√©n√©rer des noms de colonnes adapt√©s\n)\n\n# Affichage du DataFrame\nprint(new_data_preprocessed)\n\n\nImaginons que vous ayez d√©j√† des donn√©es pr√©process√©es:\n\n\n\nCr√©er des donn√©es pr√©process√©es\n\nimport numpy as np\nimport pandas as pd\n\nnew_data = pd.DataFrame({\n    \"Age\": [25, np.nan, 40, 33, np.nan],\n    \"Fare\": [10.50, 7.85, np.nan, 22.00, 12.75],\n    \"Embarked\": [\"S\", \"C\", np.nan, \"Q\", \"S\"],\n    \"Sex\": [\"male\", \"female\", \"male\", np.nan, \"female\"]\n})\nnew_y = np.random.randint(0, 2, size=len(new_data))\n\npreprocessed_data = pd.DataFrame(\n    pipe[:-1].transform(new_data),\n    columns = preprocessor_numeric.get_feature_names_out()\n)\npreprocessed_data\n\n\nD√©terminer le score en pr√©diction sur ces donn√©es\n\n\n\n\nMaintenant, revenons √† notre chaine de production et appliquons des fonctions pour la rendre plus lisible, plus fiable et plus modulaire.\n\n\n\n\n\n\nApplicationApplication 4: adoption des standards de programmation fonctionnelle\n\n\n\nCette application peut √™tre chronophage, vous pouvez aller plus ou moins loin dans la fonctionalisation de votre script en fonction du temps dont vous disposez.\n\nCr√©er une fonction qui int√®gre les diff√©rentes √©tapes du pipeline (preprocessing et d√©finition du mod√®le). Cette fonction prend en param√®tre le nombre d‚Äôarbres (argument obligatoire) et des arguments optionnels suppl√©mentaires (les colonnes sur lesquelles s‚Äôappliquent les diff√©rentes √©tapes du pipeline, max_depth et max_features).\nCr√©er une fonction d‚Äô√©valuation renvoyant le score obtenu et la matrice de confusion, √† l‚Äôissue d‚Äôune estimation (mais cette estimation est faite en amont de la fonction, pas au sein de celle-ci)\nD√©placer toutes les fonctions ensemble, en d√©but de script. Si besoin, ajouter des param√®tres √† votre fichier d‚Äôenvironnement pour cr√©er de nouvelles variables comme les chemins des donn√©es.\nEn profiter pour supprimer le code zombie qu‚Äôon a gard√© jusqu‚Äô√† pr√©sent mais qui ne correspond pas vraiment √† des op√©rations utiles √† notre chaine de production\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli4      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli42\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli4\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCela ne se remarque pas encore vraiment car nous avons de nombreuses d√©finitions de fonctions mais notre chaine de production est beaucoup plus concise (le script fait environ 150 lignes dont une centaine issues de d√©finitions de fonctions g√©n√©riques). Cette auto-discipline facilitera grandement les √©tapes ult√©rieures. Cela aurait √©t√© n√©anmoins beaucoup moins co√ªteux en temps d‚Äôadopter ces bons gestes de mani√®re plus pr√©coce."
  },
  {
    "objectID": "chapters/application.html#√©tape-1-modularisation",
    "href": "chapters/application.html#√©tape-1-modularisation",
    "title": "Application",
    "section": "√âtape 1 : modularisation",
    "text": "√âtape 1 : modularisation\nNous allons profiter de la modularisation pour adopter une structure applicative pour notre code. Celui-ci n‚Äô√©tant en effet plus lanc√© que depuis la ligne de commande, on peut consid√©rer qu‚Äôon construit une application g√©n√©rique o√π un script principal (main.py) encapsule des √©l√©ments issus d‚Äôautres scripts Python.\n\n\n\n\n\n\nApplicationApplication 5: modularisation\n\n\n\n\nD√©placer les fonctions dans une s√©rie de fichiers d√©di√©s:\n\nbuild_pipeline.py: script avec la d√©finition du pipeline\ntrain_evaluate.py: script avec les fonctions d‚Äô√©valuation du projet\n\nSp√©cifier les d√©pendances (i.e.¬†les packages √† importer) dans les modules pour que ceux-ci puissent s‚Äôex√©cuter ind√©pendamment ;\nRenommer titanic.py en main.py pour suivre la convention de nommage des projets Python ;\nImporter les fonctions n√©cessaires √† partir des modules.\nV√©rifier que tout fonctionne bien en ex√©cutant le script main √† partir de la ligne de commande :\n\n\n\nterminal\n\npython main.py\n\n\nOptionnel: profitez en pour mettre un petit coup de formatter √† votre projet, si vous ne l‚Äôavez pas fait r√©guli√®rement.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli5      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli52\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli5\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-2-adopter-une-architecture-standardis√©e-de-projet",
    "href": "chapters/application.html#√©tape-2-adopter-une-architecture-standardis√©e-de-projet",
    "title": "Application",
    "section": "√âtape 2 : adopter une architecture standardis√©e de projet",
    "text": "√âtape 2 : adopter une architecture standardis√©e de projet\nOn dispose maintenant d‚Äôune application Python fonctionnelle. N√©anmoins, le projet est certes plus fiable mais sa structuration laisse √† d√©sirer et il serait difficile de rentrer √† nouveau dans le projet dans quelques temps.\n\n\nEtat actuel du projet üôà\n\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ .env\n‚îú‚îÄ‚îÄ data.csv\n‚îú‚îÄ‚îÄ train.csv\n‚îú‚îÄ‚îÄ test.csv\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ build_pipeline.py\n‚îú‚îÄ‚îÄ train_evaluate.py\n‚îú‚îÄ‚îÄ titanic.ipynb\n‚îî‚îÄ‚îÄ main.py\n\nComme cela est expliqu√© dans la partie Structure des projets, on va adopter une structure certes arbitraire mais qui va faciliter l‚Äôautodocumentation de notre projet. De plus, une telle structure va faciliter des √©volutions optionnelles comme la packagisation du projet. Passer d‚Äôune structure modulaire bien faite √† un package est quasi-imm√©diat en Python.\nOn va donc modifier l‚Äôarchitecture de notre projet pour la rendre plus standardis√©e. Pour cela, on va s‚Äôinspirer des structures cookiecutter qui g√©n√®rent des templates de projet. En l‚Äôoccurrence notre source d‚Äôinspiration sera le template datascience issu d‚Äôun effort communautaire.\n\n\n\n\n\n\nNote\n\n\n\nL‚Äôid√©e de cookiecutter est de proposer des templates que l‚Äôon utilise pour initialiser un projet, afin de b√¢tir √† l‚Äôavance une structure √©volutive. La syntaxe √† utiliser dans ce cas est la suivante :\n\n\nterminal\n\npip install cookiecutter\ncookiecutter https://github.com/drivendata/cookiecutter-data-science\n\nIci, on a d√©j√† un projet, on va donc faire les choses dans l‚Äôautre sens : on va s‚Äôinspirer de la structure propos√©e afin de r√©organiser celle de notre projet selon les standards communautaires.\n\n\nEn s‚Äôinspirant du cookiecutter data science on va adopter la structure suivante:\n\n\nStructure recommand√©e\n\napplication\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ .env\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ raw\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data.csv\n‚îÇ   ‚îî‚îÄ‚îÄ derived\n‚îÇ       ‚îú‚îÄ‚îÄ test.csv\n‚îÇ       ‚îî‚îÄ‚îÄ train.csv\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb\n‚îî‚îÄ‚îÄ src\n    ‚îú‚îÄ‚îÄ pipeline\n    ‚îÇ   ‚îî‚îÄ‚îÄ build_pipeline.py\n    ‚îî‚îÄ‚îÄ models\n        ‚îî‚îÄ‚îÄ train_evaluate.py\n\n\n\n\n\n\n\nApplicationApplication 6: adopter une structure lisible\n\n\n\n\n(optionnel) Analyser et comprendre la structure de projet propos√©e par le template ;\nModifier l‚Äôarborescence du projet selon le mod√®le ;\nMettre √† jour l‚Äôimport des d√©pendances, le fichier de configuration et main.py avec les nouveaux chemins ;\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli6      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli62\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli6\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-3-mieux-tracer-notre-chaine-de-production",
    "href": "chapters/application.html#√©tape-3-mieux-tracer-notre-chaine-de-production",
    "title": "Application",
    "section": "√âtape 3: mieux tracer notre chaine de production",
    "text": "√âtape 3: mieux tracer notre chaine de production\n\nIndiquer l‚Äôenvironnement minimal de reproductibilit√©\nLe script main.py n√©cessite un certain nombre de packages pour √™tre fonctionnel. Chez vous les packages n√©cessaires sont bien s√ªr install√©s mais √™tes-vous assur√© que c‚Äôest le cas chez la personne qui testera votre code ?\nAfin de favoriser la portabilit√© du projet, il est d‚Äôusage de ‚Äúfixer l‚Äôenvironnement‚Äù, c‚Äôest-√†-dire d‚Äôindiquer dans un fichier toutes les d√©pendances utilis√©es ainsi que leurs version. Nous proposons de cr√©er un fichier requirements.txt minimal, sur lequel nous reviendrons dans la partie consacr√©e aux environnements reproductibles.\nLe fichier requirements.txt est conventionnellement localis√© √† la racine du projet. Ici on ne va pas fixer les versions, on raffinera ce fichier ult√©rieurement.\n\n\n\n\n\n\nApplicationApplication 7a: cr√©ation du requirements.txt\n\n\n\n\nCr√©er un fichier requirements.txt avec la liste des packages n√©cessaires\nAjouter une indication dans README.md sur l‚Äôinstallation des packages gr√¢ce au fichier requirements.txt\n\n\n\n\n\nTracer notre cha√Æne\nQuand votre projet passera en production, vous aurez un acc√®s limit√© √† celui-ci. Il est donc important de faire remonter, par le biais du logging des informations critiques sur votre projet qui vous permettront de savoir o√π il en est (si vous avez acc√®s √† la console o√π il tourne) ou l√† o√π il s‚Äôest arr√™t√©.\nL‚Äôutilisation de print montre rapidement ses limites pour cela. Les informations enregistr√©es ne persistent pas apr√®s la session et sont quelques peu rudimentaires.\nPour faire du logging, la librairie consacr√©e depuis longtemps en Python est‚Ä¶ logging. Il existe aussi une librairie nomm√©e loguru qui est un peu plus simple √† configurer (l‚Äôinstanciation du logger est plus ais√©e) et plus agr√©able gr√¢ce √† ses messages en couleurs qui permettent de visuellement trier les informations.\n\nL‚Äôexercice suivant peut √™tre fait avec les deux librairies, cela ne change pas grand chose. Les prochaines applications repartiront de la version utilisant la librairie standard logging.\n\n\n\n\n\n\nApplicationApplication 7b: remont√©e de messages par logging\n\n\n\n\nVersion utilisant loggingVersion utilisant loguru\n\n\n\nAller sur la documentation de la librairie ici et sur ce tutoriel pour trouver des sources d‚Äôinspiration sur la configuration et l‚Äôutilisation de logging.\nPour afficher les messages dans la console et dans un fichier de log, s‚Äôinspirer de cette r√©ponse sur stack overflow.\nTester en ligne de commande votre code et observer le fichier de log\n\n\n\n\nInstaller loguru et l‚Äôajouter au requirements.txt\nEn s‚Äôaidant du README du projet sur Github, remplacer nos print par diff√©rents types de messages (info, success, etc.).\nTester l‚Äôex√©cution du script en ligne de commande et observer vos sorties\nMettre √† jour le logger pour enregistrer dans un fichier de log. Ajouter celui-ci au .gitignore puis tester en ligne de commande votre script. Ouvrir le fichier en question, refaites tourner le script et regardez son √©volutoin.\nIl est possible avec loguru de capturer les erreurs des fonctions gr√¢ce au syst√®me de cache d√©crit ici. Introduire une erreur dans une des fonctions (par exemple dans create_pipeline) avec un code du type raise ValueError(\"Probl√®me ici\")\n\n\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli7      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli72\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli7\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#stockageS3",
    "href": "chapters/application.html#stockageS3",
    "title": "Application",
    "section": "√âtape 4 : stocker les donn√©es de mani√®re externe",
    "text": "√âtape 4 : stocker les donn√©es de mani√®re externe\nJusqu‚Äô√† pr√©sent, nous avons une cha√Æne reproductible puisque si on r√©cup√®re le code, on r√©cup√®re aussi les donn√©es. Ce n‚Äôest pas une bonne pratique. Nous allons maintenant s√©parer conceptuellement code et donn√©es. Cela va nous amener √† nous poser trois questions : o√π stocker les donn√©es ? dans quel format ? comment les lire ?\nLa r√©ponse √† ces trois formats permettra d‚Äôavoir le nec plus ultra des bonnes pratiques: des donn√©es au format Parquet h√©berg√© sur le cloud et lues dans Python par l‚Äôinterm√©diaire de DuckDB.\nPour cette partie, il faut avoir un service VSCode dont les jetons d‚Äôauthentification √† S3 sont valides. Si vous avez un doute, vous reporter √† Tip¬†1.\n\n\n\n\n\n\nTip¬†1: Pr√©-requis pour la prochaine application\n\n\n\n\n\nPour les prochaines applications, on va supposer l‚Äôutilisation d‚Äôun syst√®me de stockage cloud de type S3 (pour en savoir plus sur l‚Äôint√©r√™t de cette approche, se reporter au chapitre d√©di√©).\nSans perte de g√©n√©ralit√©, on va supposer que le S3 en question est celui du SSPCloud (mais cela fonctionnerait de mani√®re assez transparente si nous proposions d‚Äôutiliser un cloud provider priv√© comme AWS). Pour en savoir plus sur l‚Äôadaptation de ce tutoriel hors du SSPCloud, se reporter vers l‚Äôencadr√© d√©di√© (Important¬†1).\nPour que votre Python interagisse avec S3, il vous faut des jetons d‚Äôauthentification. Ils permettent de dire √† S3 que c‚Äôest bien vous qui essayez de consommer la donn√©e √† laquelle vous avez le droit par le biais d‚Äôun Python qu‚Äôil ne conna√Æt pas a priori.\nDes jetons d‚Äôauthentification sont cr√©√©s automatiquement pour vous lorsque vous lancez un service interactif VSCode sur le SSPCloud. Ils ont cependant une dur√©e de vie limit√©e (5 jours). Si votre service a des jetons ayant expir√©, vous allez avoir une erreur 403 lorsque vous essaierai d‚Äôinteragir avec S3.\nDeux approches sont possibles:\n\nR√©cup√©rer des jetons valides sur https://datalab.sspcloud.fr/account/storage et les renseigner √† l‚Äôendroit ad√©quat dans votre service VSCode.\nCr√©er un nouveau service qui b√©n√©ficiera automatiquement de l‚Äôinjection de jetons √† jour.\n\nLa 2e voie est recommand√©e. Le plus simple est de 1/ cliquer sur le bouton suivant\n\n2/ remplir l‚Äôonglet Git avec l‚Äôurl de votre d√©p√¥t pour b√©n√©ficier du clonage automatique de celui-ci et 3/ faire le checkpoint post appli7.\nDe cette mani√®re votre VSCode sera presque pr√™t √† l‚Äôemploi, √† l‚Äôimage de ce que vous aviez fait pour l‚Äôapplication 0. Il vous suffira d‚Äôouvrir un terminal et faire pip install -r requirements.txt && python main.py pour pouvoir d√©marrer l‚Äôapplication.\n\n\n\n  \n    \n      \n        \n      \n      \n        Reprendre √† partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration propos√©e dans l'application pr√©liminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, apr√®s avoir cl√¥n√© le d√©p√¥t\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli72\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli7\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nL‚Äô√©tape pr√©c√©dente nous a permis d‚Äôisoler la configuration. Nous avons conceptuellement isol√© les donn√©es du code lors des applications pr√©c√©dentes. Cependant, nous n‚Äôavons pas √©t√© au bout du chemin car le stockage des donn√©es reste conjoint √† celui du code. Nous allons maintenant dissocier ces deux √©l√©ments.\n\n\n\n\n\n\nImportant¬†1: Et si vous utilisez une infrastructure cloud qui n‚Äôest pas le SSPCloud ? (une id√©e saugrenue mais sait-on jamais)\n\n\n\n\n\nLes exemples √† venir peuvent tr√®s bien √™tre r√©pliqu√©s sur n‚Äôimporte quel cloud provider qui propose une solution de type S3, qu‚Äôil s‚Äôagisse d‚Äôun cloud provider priv√© (AWS, GCP, Azure, etc.) ou d‚Äôune r√©instanciation ad hoc du projet Onyxia, le logiciel derri√®re le SSPCloud.\nPour un syst√®me de stockage S3, il suffit de changer les param√®tres de connexion de s3fs (endpoint, region, etc.). Pour les stockages sur GCP, les codes sont presque √©quivalents, il suffit de remplacer la librairie s3fs par gcfs; ces deux librairies sont en fait des briques d‚Äôun standard plus g√©n√©ral de gestion de syst√®mes de fichiers en Python ffspec.\n\n\n\nLe sc√©nario type de la premi√®re partie de l‚Äôapplication (appli8a) est que nous avons une source brute, re√ßue sous forme de CSV, dont on ne peut changer le format. Un doublon de celle-ci a √©t√© √©crit sur S3 et nous allons utiliser le fichier depuis cet emplacement Il aurait √©t√© id√©al d‚Äôavoir un format plus adapt√© au traitement de donn√©es pour ce fichier mais ce n‚Äô√©tait pas de notre ressort.\nNotre chaine va aller chercher ce fichier, travailler dessus jusqu‚Äô√† valoriser celui-ci sous la forme de notre matrice de confusion. N√©anmoins, ici, nous avons une donn√©e qui ne change jamais. On n‚Äôa pas de mise √† jour fr√©quente de donn√©es qui justifie de ne pas changer, une fois pour toute, notre format de donn√©es afin d‚Äôen adopter un plus pertinent. En l‚Äôoccurrence, nous allons utiliser le format Parquet. C‚Äôest l‚Äôobjet de l‚Äôappli 8c.\nCette application va se d√©rouler en quatre temps:\n\nUpload de notre source brute (CSV) sur S3.\nIllustration de l‚Äôusage des librairies cloud native pour lire celle-ci.\nConversion au format Parquet de cette donn√©e et utilisation de DuckDB pour la lire.\nPartage public de cette donn√©e pour la rendre accessible de mani√®re plus simple √† nos futures applications.\n\n\n\n\n\n\n\nApplicationApplication 8a: ajout de donn√©es sur le syst√®me de stockage S3\n\n\n\nLe SSPCloud repose sur une impl√©mentation particuli√®re du protocole S3 qui s‚Äôappelle MinIO. Cela √©tend les fonctionnalit√©s de manipulation de fichiers en ligne de commande √† des donn√©es n‚Äô√©tant pas pr√©sentes dans le filesystem local mais dans le cloud. En pratique\nEn pratique, l‚Äôimpl√©mentation MinIO permet notamment de manipuler des fichiers √† distance comme s‚Äôils √©taient en local. Les commandes de manipulation de fichier du chapitre Linux 101 (lister, copier, supprimer‚Ä¶) sont √©tendues gr√¢ce √† l‚Äôajout d‚Äôun petit mc devant celle-ci.\nIllustrons cela en copiant les donn√©es de d√©part vers votre bucket personnel.\n\n\nIndice\n\nStructure √† adopter:\n\n\nterminal\n\nBUCKET_PERSONNEL=\"nom_utilisateur_sspcloud\"\nmc cp data.csv s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.csv\n\nen modifiant la variable BUCKET_PERSONNEL, l‚Äôemplacement de votre bucket personnel\n\nAvant de modifier notre code Python, on va lister les fichiers se trouvant dans notre bucket. En ligne de commande, sur notre poste local, on ferait ls (cf.¬†Linux 101). Cela ne va pas beaucoup diff√©rer avec les librairies cloud native:\n\n\nterminal\n\n1MY_BUCKET=\"mon_nom_utilisateur_sspcloud\"\n2CHEMIN = \"ensae-reproductibilite/data/raw\"\nmc ls s3/${MY_BUCKET}/${CHEMIN}\n\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\nEn utilisant l‚Äôexplorateur de fichiers S3 du SSPCloud (datalab.sspcloud.fr/file-explorer), faites une deuxi√®me v√©rification que vos donn√©es sont bien sauvegard√©es sur S3.\nVous pouvez ensuite supprimer le CSV de donn√©es pr√©sent dans votre dossier local, il ne nous sera plus utile.\n\n\nOn va maintenant lire directement une donn√©e stock√©e sur S3 avec Python. Pour illustrer le fait qu‚Äô√™tre sur un syst√®me cloud avec les librairies adapt√©es change peu notre code, on va lire directement un fichier CSV stock√© sur le SSPCloud, sans passer par un fichier en local8.\nPour illustrer la coh√©rence avec un syst√®me de fichier local, voici trois solutions pour lire le fichier que vous venez de mettre sur S3 (s‚Äôappuyant sur s3fs, Arrow ou DuckDB). Attention, il faut avoir des jetons de connexion √† S3 √† jour. Si vous avez cette erreur\n\nA client error (InvalidAccessKeyId) occurred when calling the ListBuckets operation: The AWS Access Key Id you provided does not exist in our records.\n\nc‚Äôest que vos identifiants de connexion ne sont plus √† jour (pour des raisons de s√©curit√©, ils sont r√©guli√®rement renouvel√©s). Dans ce cas, consulter Important¬†1.\n\n\n\n\n\n\nApplicationApplication 8b: importer une donn√©e depuis un syst√®me de stockage S3\n\n\n\nCet exercice illustre trois mani√®res de lire des donn√©es sur S3 au format CSV. Ce choix est arbitraire car il existe de nombreux frameworks concurrents ou compl√©mentaires √† Pandas, Arrow ou DuckDB qui permettent de lire des donn√©es depuis S3.\nPour le moment on va garder comme framework de pr√©dilection Pandas mais la bascule vers Parquet nous fera privil√©gier DuckDB dans les prochaines applications.\nDans un notebook, copier-coller et mettre √† jour ces deux variables qui seront utilis√©es dans diff√©rents exemples:\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n2CHEMIN_FICHIER = \"ensae-reproductibilite/data/raw/data.csv\"\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\nMaintenant lire la donn√©e depuis S3 avec ce code:\n\nAvec Pandas et s3fsAvec Pyarrow et s3fsAvec DuckDB\n\n\nimport s3fs\nimport pandas as pd\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\nwith fs.open(f\"s3://{MY_BUCKET}/{CHEMIN_FICHIER}\") as f:\n    df = pd.read_csv(f)\n\ndf\n\n\nimport s3fs\nfrom pyarrow import csv\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\nwith fs.open(f\"s3://{MY_BUCKET}/{CHEMIN_FICHIER}\") as f:\n    df = csv.read_csv(f)\n\ndf\n\n\nimport os\nimport duckdb\n\ncon = duckdb.connect(database=\":memory:\")\n\nquery_definition = f\"SELECT * FROM read_csv('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')\"\ndf = con.sql(query_definition)\n\ndf\n\n\n\n\n\nPour illustrer le fonctionnement simple de S3 avec les fichiers Parquet, on propose de faire la m√™me chose. Ici il n‚Äôest pas pertinent de mesurer le gain de temps car le fichier est tr√®s l√©ger en CSV et s‚Äôimporte ainsi instantan√©ment. Pour faire des exercices plus pertinents sur ce sujet, afin de d√©couvrir certaines bonnes pratiques, regarder les exercices suppl√©mentaires dans le chapitre d√©di√© √† Parquet).\n\n\n\n\n\n\nApplicationApplication 8c: importer une donn√©e depuis un syst√®me de stockage S3\n\n\n\nD‚Äôabord, on va convertir notre fichier au format Parquet avec ce code (√† faire tourner qu‚Äôune fois donc √† ne pas mettre dans main.py mais dans un notebook ou un script jetable)\nimport os\nimport duckdb\n\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n2CHEMIN_FICHIER = \"ensae-reproductibilite/data/raw/data.csv\"\n\ncon = duckdb.connect(database=\":memory:\")\n\nquery_definition = f\"SELECT * FROM read_csv('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')\"\ncon.sql(\n    f\"\"\"\n        COPY (\n            SELECT * \n            FROM read_csv_auto('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')\n        )\n        TO 's3://{MY_BUCKET}/{CHEMIN_FICHIER.replace(\"csv\", \"parquet\")}'\n        (FORMAT PARQUET);\n    \"\"\"\n)\n\n1\n\nChanger avec le bucket\n\n2\n\nChanger en fonction du chemin voulu\n\n\n1MY_BUCKET = \"mon_nom_utilisateur_sspcloud\"\n2CHEMIN_PARQUET = \"ensae-reproductibilite/data/raw/data.parquet\"\n\n1\n\nRemplacer ici par la valeur appropri√©e\n\n2\n\nChanger en fonction du chemin voulu\n\n\n\nAvec Pandas et s3fsAvec Pyarrow et s3fsAvec DuckDB\n\n\nimport s3fs\nimport pandas as pd\n\nchemin\n\nfs = s3fs.S3FileSystem(client_kwargs={\"endpoint_url\": \"https://minio.lab.sspcloud.fr\"})\n\ndf = pd.read_parquet(f\"s3://{MY_BUCKET}/{CHEMIN_PARQUET}\", filesystem=fs)\n\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\ns3 = pa.fs.S3FileSystem(endpoint_override =\"https://minio.lab.sspcloud.fr\")\n\ndf = pq.read_table(f\"{MY_BUCKET}/{CHEMIN_PARQUET}\", filesystem=s3)\n\ndf\n\n\nimport os\nimport duckdb\n\ncon = duckdb.connect(database=\":memory:\")\n\nquery_definition = f\"SELECT * FROM read_parquet('s3://{MY_BUCKET}/{CHEMIN_PARQUET}')\"\ndf = con.sql(query_definition)\n\ndf\n\n\n\n\n\n\n\n\n\n\n\nTipExercices suppl√©mentaires sur le format Parquet\n\n\n\n\n\nNous n‚Äôavons pas explicit√© pourquoi Parquet est un choix pertinent pour la plupart des cha√Ænes data. Ici la donn√©e est de taille r√©duite, cela ne nous p√©nalisera pas beaucoup de faire du CSV et du Pandas m√™me si nous recommandons plut√¥t de faire du Parquet et de pousser le plus loin possible DuckDB avant de revenir √† Pandas.\nPour d√©couvrir, en pratique, les raisons de ces choix techniques, vous pouvez aller voir les exercices suppl√©mentaires √† la fin du chapitre sur les donn√©es).\n\n\n\n\n\n\n\n\n\nApplicationApplication 8d: adapter notre cha√Æne pour lire des donn√©es Parquet avec DuckDB\n\n\n\nPar d√©faut, le contenu de votre bucket est priv√©, seul vous y avez acc√®s. Pour pouvoir lire votre donn√©e, vos applications externes devront utiliser des jetons vous identifiant. Ici, comme nous utilisons une donn√©e publique, vous pouvez rendre accessible celle-ci √† tous en lecture. Dans le jargon S3, cela signifie donner un acc√®s anonyme √† votre donn√©e.\nLe mod√®le de commande √† utiliser dans le terminal est le suivant:\n\n\nterminal\n\n1BUCKET_PERSONNEL=\"nom_utilisateur_sspcloud\"\n\nmc anonymous set download s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/\n\n\n1\n\nRemplacer par le nom de votre bucket.\n\n\nLes URL de t√©l√©chargement seront de la forme https://minio.lab.sspcloud.fr/&lt;BUCKET_PERSONNEL&gt;/ensae-reproductibilite/data/raw/&lt;filename&gt;.&lt;extension&gt;\nOn va adapter progressivement notre cha√Æne √† l‚Äôutilisation du format Parquet.\nD‚Äôabord, il faut faire √©voluer nos chemins:\n1URL_RAW = \"\"\ndata_path = os.environ.get(\"data_path\", URL_RAW)\n\n1\n\nModifier avec URL_RAW un lien de la forme \"https://minio.lab.sspcloud.fr/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.parquet\" (ne laissez pas ${BUCKET_PERSONNEL}, remplacez par la vraie valeur!).\n\n\nMaintenant, il faut lire les donn√©es Parquet avec DuckDB. Modifier le code d‚Äôimport des donn√©es pour utiliser DuckDB plut√¥t que Pandas. Vous pouvez, apr√®s avoir import√© les donn√©es, convertir √† nouveau en DataFrame Pandas avec la m√©thode to_df.\nNe pas oublier de mettre √† jour 1/ le .gitignore 2/ requirements.txt pour √™tre en mesure de r√©installer les packages si n√©cessaire.\n\n\n\n\n\n\n\n\nApplicationApplication 8d: partage de donn√©es sur le syst√®me de stockage S3\n\n\n\n\nRemplacer la d√©finition de data_path pour utiliser, par d√©faut, directement l‚ÄôURL dans l‚Äôimport. Modifier, si cela est pertinent, aussi votre fichier .env.\nAjouter le dossier data/ au .gitignore ainsi que les fichiers *.parquet\nSupprimer le dossier data de votre projet et faites git rm --cached -r data\nV√©rifier le bon fonctionnement de votre application.\n\n\n\nMaintenant qu‚Äôon a arrang√© la structure de notre projet, c‚Äôest l‚Äôoccasion de supprimer le code qui n‚Äôest plus n√©cessaire au bon fonctionnement de notre projet (cela r√©duit la charge de maintenance9).\nPour vous aider, vous pouvez utiliser vulture de mani√®re it√©rative pour vous assister dans le nettoyage de votre code.\n\n\nterminal\n\npip install vulture\nvulture .\n\n\n\nExemple de sortie\n\n\n\nterminal\n\nvulture .\n\nsrc/data/import_data.py:3: unused function 'split_and_count' (60% confidence)\nsrc/pipeline/build_pipeline.py:12: unused function 'split_train_test' (60% confidence)\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli8      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli82\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli8\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-1-proposer-des-tests-unitaires-optionnel",
    "href": "chapters/application.html#√©tape-1-proposer-des-tests-unitaires-optionnel",
    "title": "Application",
    "section": "√âtape 1 : proposer des tests unitaires (optionnel)",
    "text": "√âtape 1 : proposer des tests unitaires (optionnel)\nNotre code comporte un certain nombre de fonctions g√©n√©riques. On peut vouloir tester leur usage sur des donn√©es standardis√©es, diff√©rentes de celles du Titanic.\nM√™me si la notion de tests unitaires prend plus de sens dans un package, nous pouvons proposer dans le projet des exemples d‚Äôutilisation de la fonction, ceci peut √™tre p√©dagogique.\nNous allons utiliser unittest pour effectuer des tests unitaires. Cette approche n√©cessite quelques notions de programmation orient√©e objet ou une bonne discussion avec ChatGPT.\n\n\n\n\n\n\nApplicationApplication 9: test unitaire (optionnel)\n\n\n\nDans le dossier tests/, cr√©er avec l‚Äôaide d‚Äôune IA assistante (ChatGPT, Claude, Copilot.) un test pour la fonction create_pipeline.\n\nEffectuer le test unitaire en ligne de commande avec unittest (python -m unittest tests/test_pipeline.py). Corriger le test unitaire en cas d‚Äôerreur.\nSi le temps le permet, proposer des variantes ou d‚Äôautres tests.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli9      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli92\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli9\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\n\n\n\n\nNoteLe taux de couverture\n\n\n\n\n\nLorsqu‚Äôon effectue des tests unitaires, on cherche g√©n√©ralement √† tester le plus de lignes possibles de son code. On parle de taux de couverture (coverage rate) pour d√©signer la statistique mesurant cela.\nCela peut s‚Äôeffectuer de la mani√®re suivante avec le package coverage:\n\n\nterminal\n\ncoverage run -m unittest tests/test_pipeline.py\ncoverage report -m\n\nName                             Stmts   Miss  Cover   Missing\n--------------------------------------------------------------\nsrc/pipeline/build_pipeline.py      11      0   100%\ntests/test_pipeline.py              46      1    98%   75\n--------------------------------------------------------------\nTOTAL                               57      1    98%\nLe taux de couverture est souvent mis en avant par les gros projets comme indicateur de leur qualit√©. Il existe d‚Äôailleurs des badges Github d√©di√©s."
  },
  {
    "objectID": "chapters/application.html#√©tape-2-transformer-son-projet-en-package-optionnel",
    "href": "chapters/application.html#√©tape-2-transformer-son-projet-en-package-optionnel",
    "title": "Application",
    "section": "√âtape 2 : transformer son projet en package (optionnel)",
    "text": "√âtape 2 : transformer son projet en package (optionnel)\nNotre projet est modulaire, ce qui le rend assez simple √† transformer en package, en s‚Äôinspirant de la structure du cookiecutter adapt√©, issu En fait, il ne nous manque qu‚Äôun fichier essentiel, le principal distinguant un projet classique d‚Äôun package : pyproject.toml. de cet ouvrage.\nCe fichier permet de contr√¥ler de mani√®re formelle l‚Äôinteraction entre nos scripts et le monde ext√©rieur (l‚Äôisolation plus ou moins forte entre notre package et le reste de l‚Äô√©cosyst√®me Python) tout en donnant des indications √† Python sur la mani√®re de construire et installer notre package fait maison.\nOn va cr√©er un package nomm√© titanicml qui encapsule tout notre code et qui sera appel√© par notre script principal. La structure attendue est la suivante:\n\n\nStructure vis√©e\n\napplication\n‚îú‚îÄ‚îÄ README.md                               ‚îê\n‚îú‚îÄ‚îÄ docs                                    ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ notebooks                           ‚îÇ Package documentation and examples\n‚îÇ       ‚îî‚îÄ‚îÄ titanic.ipynb                   ‚îÇ\n‚îÇ       ‚îî‚îÄ‚îÄ main.py                         ‚îò\n‚îú‚îÄ‚îÄ pyproject.toml                          ‚îê\n‚îú‚îÄ‚îÄ src/titanicml                           ‚îÇ\n‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                         ‚îÇ Package source code, metadata\n‚îÇ   ‚îú‚îÄ‚îÄ pipeline                            ‚îÇ and build instructions\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ build_pipeline.py               ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ models                              ‚îÇ\n‚îÇ       ‚îî‚îÄ‚îÄ train_evaluate.py               ‚îò\n‚îî‚îÄ‚îÄ tests                                   ‚îê\n    ‚îî‚îÄ‚îÄ test_pipeline.py                    ‚îò Package tests\n\n\n\nRappel: structure actuelle\n\napplication\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb\n‚îú‚îÄ‚îÄ main.py\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ requirements.txt\n‚îî‚îÄ‚îÄ src\n    ‚îú‚îÄ‚îÄ pipeline\n    ‚îÇ   ‚îî‚îÄ‚îÄ build_pipeline.py\n    ‚îî‚îÄ‚îÄ models\n        ‚îî‚îÄ‚îÄ train_evaluate.py\n\nIl existe plusieurs frameworks pour construire un package. Nous allons privil√©gier uv, le nouveau venu dans l‚Äô√©cosyst√®me √† Poetry ou Setuptools.\n\n\n\n\n\n\nApplicationApplication 10: packagisation (optionnel)\n\n\n\n\nDepuis la ligne de commande, en se placant dans le dossier racine ~/work, cr√©er le squelette de notre package avec la commande:\nuv init --lib titanicml\nCopier les sous-dossiers de notre src/ actuel dans le dossier src/ cr√©e dans le package titanicml\nOuvrir le fichier pyproject.toml pr√©sent dans titanicml/ et observer la liste des d√©pendances vide. La gestion des d√©pendances est un peu plus formelle dans un package: c‚Äôest ici que la liste des d√©pendances indispensables √† notre projet plut√¥t que dans un requirements.txt.\nBien que ceci sera expliqu√© plus tard, vous pouvez ajouter des d√©pendances en tapant, en ligne de commande:\nuv add pandas PyYAML scikit-learn python-dotenv duckdb\n\nCela devrait mettre √† jour le fichier pyproject.toml pour qu‚Äôil ressemble √† ceci:\n```{.python filename=\"pyproject.toml\"}\n[project]\nname = \"titanicml\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nauthors = [\n    { name = \"daffyduck\", email = \"daffy.duck@ensae.fr\" }\n]\nrequires-python = \"&gt;=3.13\"\ndependencies = [\n    \"duckdb&gt;=1.4.4\",\n    \"pandas&gt;=3.0.0\",\n    \"python-dotenv&gt;=1.2.1\",\n    \"pyyaml&gt;=6.0.3\",\n    \"scikit-learn&gt;=1.8.0\",\n]\n\n[build-system]\nrequires = [\"uv_build&gt;=0.9.14,&lt;0.10.0\"]\nbuild-backend = \"uv_build\"\n```\n\nD√©placer le dossier tests/ dans notre package.\nCompiler le package avec la commande uv build. Cela a cr√©√© un wheel (une version compil√©e) de votre package que nous pouvons installer avec\npip install dist/titanicml-0.1.0-py3-none-any.whl\nModifier le contenu de docs/main.py pour importer les fonctions de notre package titanicml et tester en ligne de commande ce fichier\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli10      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli102\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli10\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#anaconda",
    "href": "chapters/application.html#anaconda",
    "title": "Application",
    "section": "√âtape 1 : un environnement pour rendre le projet portable",
    "text": "√âtape 1 : un environnement pour rendre le projet portable\nPour qu‚Äôun projet soit portable, il doit remplir deux conditions:\n\nNe pas n√©cessiter de d√©pendance qui ne soient pas renseign√©es quelque part ;\nNe pas proposer des d√©pendances inutiles, qui ne sont pas utilis√©es dans le cadre du projet.\n\nLe prochain exercice vise √† mettre ceci en oeuvre. Comme expliqu√© dans le chapitre portabilit√©, le choix du gestionnaire d‚Äôenvironnement est laiss√© libre. Il est recommand√© de privil√©gier venv si vous d√©couvrez la probl√©matique de la portabilit√©.\n\nEnvironnement virtuel venvEnvironnement condaEnvironnement virtuel via uv\n\n\nL‚Äôapproche la plus l√©g√®re est l‚Äôenvironnement virtuel. Nous avons en fait implicitement d√©j√† commenc√© √† aller vers cette direction en cr√©ant un fichier requirements.txt.\n\n\n\n\n\n\nApplicationApplication 11a: environnement virtuel venv\n\n\n\n\nEx√©cuter pip freeze en ligne de commande et observer la (tr√®s) longue liste de package\nCr√©er l‚Äôenvironnement virtuel titanic en s‚Äôinspirant de la documentation officielle10 ou du chapitre d√©di√©\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin install√©\nLe SSPCloud, par d√©faut, fonctionne sur un environnement conda. Le d√©sactiver en faisant conda deactivate.\nActiver l‚Äôenvironnement et v√©rifier l‚Äôinstallation de Python maintenant utilis√©e par votre machine \nV√©rifier directement depuis la ligne de commande que Python ex√©cute bien une commande11 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la m√™me chose mais avec import pandas as pd\nInstaller les packages √† partir du requirements.txt. Tester √† nouveau import pandas as pd pour comprendre la diff√©rence.\nEx√©cuter pip freeze et comprendre la diff√©rence avec la situation pr√©c√©dente.\nV√©rifier que le script main.py fonctionne bien. Sinon ajouter les packages manquants dans le requirements.txt et reprendre de mani√®re it√©rative √† partir de la question 7.\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier √† Git.\n\n\n\nAide pour la question 4\n\nApr√®s l‚Äôactivation, vous pouvez v√©rifier quel python est utilis√© de cette mani√®re\n\n\nterminal\n\nwhich python\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11a2\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli11a\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\nLes environnements conda sont plus lourds √† mettre en oeuvre que les environnements virtuels mais peuvent permettre un contr√¥le plus formel des d√©pendances.\n\n\n\n\n\n\nApplicationApplication 11b: environnement conda\n\n\n\n\nEx√©cuter conda env export en ligne de commande et observer la (tr√®s) longue liste de package\nCr√©er un environnement titanic avec conda create\nActiver l‚Äôenvironnement et v√©rifier l‚Äôinstallation de Python maintenant utilis√©e par votre machine \nV√©rifier directement depuis la ligne de commande que Python ex√©cute bien une commande12 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la m√™me chose mais avec import pandas as pd\nInstaller les packages qu‚Äôon avait list√© dans le requirements.txt pr√©c√©demment. Ne pas faire un pip install -r requirements.txt afin de privil√©gier conda install\nEx√©cuter √† nouveau conda env export et comprendre la diff√©rence avec la situation pr√©c√©dente13.\nV√©rifier que le script main.py fonctionne bien. Sinon installer les packages manquants et reprndre de mani√®re it√©rative √† partir de la question 7.\nQuand main.py fonctionne, faire conda env export &gt; environment.yml pour figer l‚Äôenvironnement de travail.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11b2\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli11b\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\nuv est le new kid in the game pour g√©rer les environnements virtuels avec Python.\n\n\n\n\n\n\nApplicationApplication 11c: environnement virtuel venv (via uv)\n\n\n\n\nApr√®s avoir install√© uv, ex√©cuter uv init . et supprimer le fichier hello.py g√©n√©r√©. Ouvrir le pyproject.toml et observer sa structure.\nEx√©cuter uv pip freeze en ligne de commande et observer la (tr√®s) longue liste de package\nCr√©er un environnement virtuel titanic par le biais d‚Äôuv (documentation) sous le nom titanic\nUtiliser ls pour observer et comprendre le contenu du dossier titanic/bin install√©\nActiver l‚Äôenvironnement et v√©rifier l‚Äôinstallation de Python maintenant utilis√©e par votre machine \nV√©rifier directement depuis la ligne de commande que Python ex√©cute bien une commande14 avec:\n\n\n\nterminal\n\npython -c \"print('Hello')\"\n\n\nFaire la m√™me chose mais avec import pandas as pd. Maintenant, essayer uv run main.py en ligne de commande: comprenez-vous ce qu‚Äôil se passe ?\nInstaller de mani√®re it√©rative les packages √† partir d‚Äôuv add (documentation) et en testant avec uv run main.py: avez-vous remarqu√© la vitesse √† laquelle cela a √©t√© quand vous avez fait uv add pandas ?\nObserver votre pyproject.toml. Regarder le lockfile uv.lock. G√©n√©rer automatiquement le requirements.txt en faisant pip compile et regarder celui-ci.\nAjouter le dossier titanic/ au .gitignore pour ne pas ajouter ce dossier √† Git.\n\n\n\nAide pour la question 5\n\nApr√®s l‚Äôactivation, vous pouvez v√©rifier quel python est utilis√© de cette mani√®re\n\n\nterminal\n\nwhich python\n\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli11c      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli11c2\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli11c\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#shell",
    "href": "chapters/application.html#shell",
    "title": "Application",
    "section": "√âtape 2: construire l‚Äôenvironnement de notre application via un script shell",
    "text": "√âtape 2: construire l‚Äôenvironnement de notre application via un script shell\nLes environnements virtuels permettent de mieux sp√©cifier les d√©pendances de notre projet, mais ne permettent pas de garantir une portabilit√© optimale. Pour cela, il faut recourir √† la technologie des conteneurs. L‚Äôid√©e est de construire une machine, en partant d‚Äôune base quasi-vierge, qui permette de construire √©tape par √©tape l‚Äôenvironnement n√©cessaire au bon fonctionnement de notre projet. C‚Äôest le principe des conteneurs Docker .\nLeur m√©thode de construction √©tant un peu difficile √† prendre en main au d√©but, nous allons passer par une √©tape interm√©diaire afin de bien comprendre le processus de production.\n\nNous allons d‚Äôabord cr√©er un script shell, c‚Äôest √† dire une suite de commandes Linux permettant de construire l‚Äôenvironnement √† partir d‚Äôune machine vierge ;\nNous transformerons celui-ci en Dockerfile dans un deuxi√®me temps. C‚Äôest l‚Äôobjet de l‚Äô√©tape suivante.\n\n\nEnvironnement virtuel venvEnvironnement conda\n\n\n\n\n\n\n\n\nApplicationApplication 12a : cr√©er un fichier d‚Äôinstallation de A √† Z\n\n\n\n\nCr√©er un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le d√©p√¥t\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11a avec git checkout appli11a\nVia l‚Äôexplorateur de fichiers, cr√©er le fichier install.sh √† la racine du projet avec le contenu suivant:\n\n\n\nScript √† cr√©er sous le nom install.sh\n\n\n\ninstall.sh\n\n#!/bin/bash\n\n# Install Python\napt-get -y update\napt-get install -y python3-pip python3-venv\n\n# Create empty virtual environment\npython3 -m venv titanic\nsource titanic/bin/activate\n\n# Install project dependencies\npip install -r requirements.txt\n\n\n\nChanger les permissions sur le script pour le rendre ex√©cutable\n\n\n\nterminal\n\nchmod +x install.sh\n\n\nEx√©cuter le script depuis la ligne de commande avec des droits de super-utilisateur (n√©cessaires pour installer des packages via apt)\n\n\n\nterminal\n\nsudo ./install.sh\n\n\nV√©rifier que le script main.py fonctionne correctement dans l‚Äôenvironnement virtuel cr√©√©\n\n\n\nterminal\n\nsource titanic/bin/activate\npython3 main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli12a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12a2\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli12a\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \n\n\n\n\n\n\n\n\nApplicationApplication 12b : cr√©er un fichier d‚Äôinstallation de A √† Z\n\n\n\n\nCr√©er un service ubuntu sur le SSP Cloud\nOuvrir un terminal\nCloner le d√©p√¥t\nSe placer dans le dossier du projet avec cd\nSe placer au niveau du checkpoint 11b avec git checkout appli11b\nVia l‚Äôexplorateur de fichiers, cr√©er le fichier install.sh √† la racine du projet avec le contenu suivant:\n\n\n\nScript √† cr√©er sous le nom install.sh\n\n\n\ninstall.sh\n\napt-get -y update && apt-get -y install wget\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \\\n    rm -f Miniconda3-latest-Linux-x86_64.sh\n\nPATH=\"/miniconda/bin:${PATH}\"\n\n# Create environment\nconda create -n titanic pandas PyYAML scikit-learn -c conda-forge\nconda activate titanic\n\nPATH=\"/miniconda/envs/titanic/bin:${PATH}\"\n\npython main.py\n\n\n\nChanger les permissions sur le script pour le rendre ex√©cutable\n\n\n\nterminal\n\nchmod +x install.sh\n\n\nEx√©cuter le script depuis la ligne de commande avec des droits de super-utilisateur (n√©cessaires pour installer des packages via apt)\n\n\n\nterminal\n\nsudo ./install.sh\n\n\nV√©rifier que le script main.py fonctionne correctement dans l‚Äôenvironnement virtuel cr√©√©\n\n\n\nterminal\n\nconda activate titanic\npython3 main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli12b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli12b2\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli12b\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#docker",
    "href": "chapters/application.html#docker",
    "title": "Application",
    "section": "√âtape 3: conteneuriser l‚Äôapplication avec Docker",
    "text": "√âtape 3: conteneuriser l‚Äôapplication avec Docker\n\n\n\n\n\n\nNote\n\n\n\nCette application n√©cessite l‚Äôacc√®s √† une version interactive de Docker. Il n‚Äôy a pas beaucoup d‚Äôinstances en ligne disponibles.\nNous proposons deux solutions:\n\nInstaller Docker sur sa machine ;\nSe rendre sur l‚Äôenvironnement bac √† sable Play with Docker\n\nSinon, elle peut √™tre r√©alis√©e en essai-erreur par le biais des services d‚Äôint√©gration continue de Github  ou Gitlab . N√©anmoins, nous pr√©senterons l‚Äôutilisation de ces services plus tard, dans la prochaine partie.\n\n\nMaintenant qu‚Äôon sait que ce script pr√©paratoire fonctionne, on va le transformer en Dockerfile pour anticiper la mise en production. Comme la syntaxe Docker est l√©g√®rement diff√©rente de la syntaxe Linux classique (voir le chapitre portabilit√©), il va √™tre n√©cessaire de changer quelques instructions mais ceci sera tr√®s l√©ger.\nOn va tester le Dockerfile dans un environnement bac √† sable pour ensuite pouvoir plus facilement automatiser la construction de l‚Äôimage Docker.\n\n\n\n\n\n\nApplicationApplication 13: cr√©ation de l‚Äôimage Docker\n\n\n\nSe placer dans un environnement avec Docker, par exemple Play with Docker\n\nCr√©ation du Dockerfile\n\nDans le terminal Linux, cloner votre d√©p√¥t Github\nRepartir de la derni√®re version √† disposition. Par exemple, si vous avez privil√©gi√© l‚Äôenvironnement virtuel venv, ce sera:\n\n\n\nterminal\n\n1git stash\ngit checkout appli12a\n\n\n1\n\nPour annuler les modifications depuis le dernier commit\n\n\n\nCr√©er via la ligne de commande un fichier texte vierge nomm√© Dockerfile (la majuscule au d√©but du mot est importante)\n\n\n\nCommande pour cr√©er un Dockerfile vierge depuis la ligne de commande\n\n\n\nterminal\n\ntouch Dockerfile\n\n\n\nOuvrir ce fichier via un √©diteur de texte et copier le contenu suivant dedans:\n\n\n\nPremier Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCMD [\"python3\", \"main.py\"]\n\n\n\n\nConstruire (build) l‚Äôimage\n\nUtiliser docker build pour cr√©er une image avec le tag my-python-app\n\n\n\nterminal\n\ndocker build . -t my-python-app\n\n\nV√©rifier les images dont vous disposez. Vous devriez avoir un r√©sultat proche de celui-ci :\n\n\n\nterminal\n\ndocker images\n\nREPOSITORY      TAG       IMAGE ID       CREATED              SIZE\nmy-python-app   latest    188957e16594   About a minute ago   879MB\n\n\nTester l‚Äôimage: d√©couverte du cache\nL‚Äô√©tape de build a fonctionn√©: une image a √©t√© construite.\nMais fait-elle effectivement ce que l‚Äôon attend d‚Äôelle ?\nPour le savoir, il faut passer √† l‚Äô√©tape suivante, l‚Äô√©tape de run.\n\n\nterminal\n\ndocker run -it my-python-app\n\npython3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory\nLe message d‚Äôerreur est clair : Docker ne sait pas o√π trouver le fichier main.py. D‚Äôailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont n√©cessaires pour faire tourner le code, par exemple le dossier src.\n\nAvant l‚Äô√©tape CMD, copier les fichiers n√©cessaires sur l‚Äôimage afin que l‚Äôapplication dispose de tous les √©l√©ments n√©cessaires pour √™tre en mesure de fonctionner.\n\n\n\nNouveau Dockerfile\n\n\n\nterminal\n\nFROM ubuntu:22.04\n\n# Install Python\nRUN apt-get -y update && \\\n    apt-get install -y python3-pip\n\n# Install project dependencies\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY main.py .\nCOPY src ./src\nCMD [\"python3\", \"main.py\"]\n\n\n\nRefaire tourner l‚Äô√©tape de build\nRefaire tourner l‚Äô√©tape de run. A ce stade, la matrice de confusion doit fonctionner üéâ. Vous avez cr√©√© votre premi√®re application reproductible !\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIci, le cache permet d‚Äô√©conomiser beaucoup de temps. Par besoin de refaire tourner toutes les √©tapes, Docker agit de mani√®re intelligente en faisant tourner uniquement les √©tapes qui ont chang√©.\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli13      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli132\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli13\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-1-mise-en-place-de-tests-automatis√©s",
    "href": "chapters/application.html#√©tape-1-mise-en-place-de-tests-automatis√©s",
    "title": "Application",
    "section": "√âtape 1: mise en place de tests automatis√©s",
    "text": "√âtape 1: mise en place de tests automatis√©s\nAvant d‚Äôessayer de mettre en oeuvre la cr√©ation de notre image Docker de mani√®re automatis√©e, nous allons pr√©senter la logique de l‚Äôint√©gration continue en testant de mani√®re automatis√©e notre script main.py.\nPour cela, nous allons partir de la structure propos√©e dans l‚Äôaction officielle. La documentation associ√©e est ici. Des √©l√©ments succincts de pr√©sentation de la logique d√©clarative des actions Github sont disponibles dans le chapitre sur la mise en production. N√©anmoins, la meilleure √©cole pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et d‚Äôobserver les actions Github mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !\n\n\n\n\n\n\nApplicationApplication 14: premier script d‚Äôint√©gration continue\n\n\n\nA partir de l‚Äôexemple pr√©sent dans la documentation officielle de Github , on a d√©j√† une base de d√©part qui peut √™tre modifi√©e. Les questions suivantes permettront d‚Äôautomatiser les tests et le diagnostic qualit√© de notre code15\n\nCr√©er un fichier .github/workflows/test.yaml avec le contenu de l‚Äôexemple de la documentation\nAvec l‚Äôaide de la documentation, introduire une √©tape d‚Äôinstallation des d√©pendances. Utiliser le fichier requirements.txt pour installer les d√©pendances.\nUtiliser pylint pour v√©rifier la qualit√© du code. Ajouter l‚Äôargument --fail-under=6 pour renvoyer une erreur en cas de note trop basse16\nUtiliser une √©tape appelant notre application en ligne de commande (python main.py) pour tester que la matrice de confusion s‚Äôaffiche bien.\nCr√©er un secret stockant une valeur du JETON_API. Ne le faites pas commencer par un ‚Äú$‚Äù comme √ßa vous pourrez regarder la log ult√©rieurement\nAller voir votre test automatis√© dans l‚Äôonglet Actions de votre d√©p√¥t sur Github\n(optionnel): Cr√©er un artefact √† partir du fichier de log que vous cr√©ez dans main.py\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli14      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli142\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli14\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nMaintenant, nous pouvons observer que l‚Äôonglet Actions s‚Äôest enrichi. Chaque commit va entra√Æner une s√©rie d‚Äôactions automatis√©es.\nSi l‚Äôune des √©tapes √©choue, ou si la note de notre projet est mauvaise, nous aurons une croix rouge (et nous recevrons un mail). On pourra ainsi d√©tecter, en d√©veloppant son projet, les moments o√π on d√©grade la qualit√© du script afin de la r√©tablir imm√©diatemment."
  },
  {
    "objectID": "chapters/application.html#√©tape-2-automatisation-de-la-livraison-de-limage-docker",
    "href": "chapters/application.html#√©tape-2-automatisation-de-la-livraison-de-limage-docker",
    "title": "Application",
    "section": "√âtape 2: Automatisation de la livraison de l‚Äôimage Docker",
    "text": "√âtape 2: Automatisation de la livraison de l‚Äôimage Docker\nMaintenant, nous allons automatiser la mise √† disposition de notre image sur DockerHub (le lieu de partage des images Docker). Cela facilitera sa r√©utilisation mais aussi des valorisations ult√©rieures.\nL√† encore, nous allons utiliser une s√©rie d‚Äôactions pr√©-configur√©es.\nPour que Github puisse s‚Äôauthentifier aupr√®s de DockerHub, il va falloir d‚Äôabord interfacer les deux plateformes. Pour cela, nous allons utiliser un jeton (token) DockerHub que nous allons mettre dans un espace s√©curis√© associ√© √† votre d√©p√¥t Github.\n\n\n\n\n\n\nApplicationApplication 15a: configuration\n\n\n\n\nSe rendre sur https://hub.docker.com/ et cr√©er un compte. Il est recommand√© d‚Äôassocier ce compte √† votre compte Github.\nCr√©er un d√©p√¥t public application\nAller dans les param√®tres de votre compte et cliquer, √† gauche, sur Security\nCr√©er un jeton personnel d‚Äôacc√®s, ne fermez pas l‚Äôonglet en question, vous ne pouvez voir sa valeur qu‚Äôune fois.\nDans le d√©p√¥t Github de votre projet, cliquer sur l‚Äôonglet Settings et cliquer, √† gauche, sur Secrets and variables puis dans le menu d√©roulant en dessous sur Actions. Sur la page qui s‚Äôaffiche, aller dans la section Repository secrets\nCr√©er un jeton DOCKERHUB_TOKEN √† partir du jeton que vous aviez cr√©√© sur Dockerhub. Valider\nCr√©er un deuxi√®me secret nomm√© DOCKERHUB_USERNAME ayant comme valeur le nom d‚Äôutilisateur que vous avez cr√©√© sur Dockerhub\n\n\n\nEtape optionnelle suppl√©mentaire si on met en production un site web\n\n\nDans le d√©p√¥t Github de votre projet, cliquer sur l‚Äôonglet Settings et cliquer, √† gauche, sur Actions. Donner les droits d‚Äô√©criture √† vos actions sur le d√©p√¥t du projet (ce sera n√©cessaire pour Github Pages)\n\n\n\n\n\nA ce stade, nous avons donn√© les moyens √† Github de s‚Äôauthentifier avec notre identit√© sur Dockerhub. Il nous reste √† mettre en oeuvre l‚Äôaction en s‚Äôinspirant de la documentation officielle. On ne va modifier que trois √©l√©ments dans ce fichier. Effectuer les actions suivantes:\n\n\n\n\n\n\nApplicationApplication 15b: automatisation de l‚Äôimage Docker\n\n\n\n\nEn s‚Äôinspirant de ce template, cr√©er le fichier .github/workflows/prod.yml qui va build et push l‚Äôimage sur le DockerHub. Il va √™tre n√©cessaire de changer l√©g√®rement ce mod√®le :\n\nRetirer la condition restrictive sur les commits pour lesquels sont lanc√©s cette automatisation. Pour cela, remplacer le contenu de on de sorte √† avoir\n\non:\n  push:\n    branches:\n      - main\n      - dev\n\nChanger le tag √† la fin pour mettre username/application:latest o√π username est le nom d‚Äôutilisateur sur DockerHub;\nOptionnel: changer le nom de l‚Äôaction\n\nFaire un commit et un push de ces fichiers\n\nComme on est fier de notre travail, on va afficher √ßa avec un badge sur le README (partie optionnelle).\n\nSe rendre dans l‚Äôonglet Actions et cliquer sur une des actions list√©es.\nEn haut √† droite, cliquer sur ...\nS√©lectionner Create status badge\nR√©cup√©rer le code Markdown propos√©\nCopier dans votre README.md le code markdown propos√©\n\n\n\nCr√©er le badge\n\n\n\n\n\nMaintenant, il nous reste √† tester notre application dans l‚Äôespace bac √† sable ou en local, si Docker est install√©.\n\n\n\n\n\n\nApplicationApplication 15b (partie optionnelle): Tester l‚Äôapplication\n\n\n\n\nSe rendre sur l‚Äôenvironnement bac √† sable Play with Docker ou dans votre environnement Docker de pr√©dilection.\nR√©cup√©rer et lancer l‚Äôimage :\n\n\n\nterminal\n\ndocker run -it username/application:latest\n\nüéâ La matrice de confusion doit s‚Äôafficher ! Vous avez grandement facilit√© la r√©utilisation de votre image.\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli15      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli152\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli15\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-1-d√©velopper-une-api-en-local",
    "href": "chapters/application.html#√©tape-1-d√©velopper-une-api-en-local",
    "title": "Application",
    "section": "√âtape 1: d√©velopper une API en local",
    "text": "√âtape 1: d√©velopper une API en local\nLe premier livrable devenu classique dans un projet impliquant du machine learning est la mise √† disposition d‚Äôun mod√®le par le biais d‚Äôune API (voir chapitre sur la mise en production). Le framework FastAPI va permettre de rapidement transformer notre application Python en une API fonctionnelle.\n\n\n\n\n\n\nApplicationApplication 16: Mise √† disposition sous forme d‚ÄôAPI locale\n\n\n\n\nInstaller fastAPI et uvicorn puis les ajouter au requirements.txt\nRenommer le fichier main.py en train.py.\nDans ce script, ajouter une sauvegarde du mod√®le apr√®s l‚Äôavoir entra√Æn√©, sous le format joblib.\nFaire tourner\n\n\n\nterminal\n\npython train.py\n\npour enregistrer en local votre mod√®le de production.\n\nModifier les appels √† main.py dans votre Dockerfile et vos actions Github sous peine d‚Äôessuyer des √©checs lors de vos actions Github apr√®s le prochain push.\nAjouter model.joblib au .gitignore car Git n‚Äôest pas fait pour ce type de fichiers.\n\nNous allons maintenant passer au d√©veloppement de l‚ÄôAPI. Comme d√©couvrir FastAPI n‚Äôest pas l‚Äôobjet de cet enseignement, nous donnons directement le mod√®le pour cr√©er l‚ÄôAPI. Si vous d√©sirez tester de vous-m√™mes, vous pouvez cr√©er votre fichier sans vous r√©f√©rer √† l‚Äôexemple.\n\nCr√©er le fichier app/api.py permettant d‚Äôinitialiser l‚ÄôAPI:\n\n\n\nFichier app/api.py\n\n\n\napp/api.py\n\n\"\"\"A simple API to expose our trained RandomForest model for Tutanic survival.\"\"\"\nfrom fastapi import FastAPI\nfrom joblib import load\n\nimport pandas as pd\n\nmodel = load('model.joblib')\n\napp = FastAPI(\n    title=\"Pr√©diction de survie sur le Titanic\",\n    description=\n    \"Application de pr√©diction de survie sur le Titanic üö¢ &lt;br&gt;Une version par API pour faciliter la r√©utilisation du mod√®le üöÄ\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\n\n@app.get(\"/\", tags=[\"Welcome\"])\ndef show_welcome_page():\n    \"\"\"\n    Show welcome page with model name and version.\n    \"\"\"\n\n    return {\n        \"Message\": \"API de pr√©diction de survie sur le Titanic\",\n        \"Model_name\": 'Titanic ML',\n        \"Model_version\": \"0.1\",\n    }\n\n\n@app.get(\"/predict\", tags=[\"Predict\"])\nasync def predict(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\"\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    prediction = \"Survived üéâ\" if int(model.predict(df)) == 1 else \"Dead ‚ö∞Ô∏è\"\n\n    return prediction\n\n\n\nD√©ployer l‚ÄôAPI en local avec la commande suivante.\n\n\n\nterminal\n\nuvicorn app.api:app\n\n\nObserver l‚Äôoutput dans la console. Notre API est d√©sormais d√©ploy√©e en local, plus pr√©cis√©ment sur le localhost, un serveur web local d√©ploy√© √† l‚Äôadresse http://127.0.0.1. L‚ÄôAPI est d√©ploy√©e sur le port par d√©faut utilis√© par uvicorn, soit le port 8000.\nSans fermer le terminal pr√©c√©dent, ouvrir un nouveau terminal. Tester le bon d√©ploiement de l‚ÄôAPI en requ√™tant son endpoint. Pour cela, on envoie une simple requ√™te GET sur le endpoint via l‚Äôutilitaire curl.\n\n\n\nterminal\n\ncurl \"http://127.0.0.1:8000\"\n\n\nSi tout s‚Äôest bien pass√©, on devrait avoir r√©cup√©r√© une r√©ponse (au format JSON) affichant le message d‚Äôaccueil de notre API. Dans ce cas, on va pouvoir requ√™ter notre mod√®le via l‚ÄôAPI.\nEn vous inspirant du code qui d√©finit le endpoint /predict dans le code de l‚ÄôAPI (app/api.py), effectuer sur le m√™me mod√®le que la requ√™te pr√©c√©dente une requ√™te qui calcule la survie d‚Äôune femme de 32 ans qui aurait pay√© son billet 16 dollars et aurait embarqu√© au port S.\n\n\n\nSolution\n\n\n\nterminal\n\ncurl \"http://127.0.0.1:8000/predict?sex=female&age=32&fare=16&embarked=S\"\n\n\n\nToujours sans fermer le terminal qui d√©ploie l‚ÄôAPI, ouvrir une session Python et tester une requ√™te avec des param√®tres diff√©rents, avec la librairie requests :\n\n\n\nSolution\n\nimport requests\n\nURL = \"http://127.0.0.1:8000/predict?sex=male&age=25&fare=80&embarked=S\"\nrequests.get(URL).json()\n\n\nUne fois que l‚ÄôAPI a √©t√© test√©e, vous pouvez fermer l‚Äôapplication en effectuant CTRL+C depuis le terminal o√π elle est lanc√©e.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli16      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli162\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli16\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#√©tape-2-d√©ployer-lapi-de-mani√®re-manuelle",
    "href": "chapters/application.html#√©tape-2-d√©ployer-lapi-de-mani√®re-manuelle",
    "title": "Application",
    "section": "√âtape 2: d√©ployer l‚ÄôAPI de mani√®re manuelle",
    "text": "√âtape 2: d√©ployer l‚ÄôAPI de mani√®re manuelle\nA ce stade, nous avons d√©ploy√© l‚ÄôAPI seulement localement, dans le cadre d‚Äôun terminal qui tourne en arri√®re-plan. C‚Äôest une mise en production manuelle, pas franchement p√©renne. Ce mode de d√©ploiement est tr√®s pratique pour la phase de d√©veloppement, afin de s‚Äôassurer que l‚ÄôAPI fonctionne comme attendue. Pour p√©renniser la mise en production, on va √©liminer l‚Äôaspect artisanal de celle-ci.\nIl est temps de passer √† l‚Äô√©tape de d√©ploiement, qui permettra √† notre API d‚Äô√™tre accessible, √† tout moment, via une URL sur le web et d‚Äôavoir un serveur, en arri√®re plan, qui effectuera les op√©rations pour r√©pondre √† une requ√™te. Pour se faire, on va utiliser les possibilit√©s offertes par Kubernetes, technologie sur laquelle est bas√©e l‚Äôinfrastructure SSP Cloud.\n\n\n\n\n\n\nImportantEt si vous n‚Äôutilisez pas le SSPCloud ? (une id√©e saugrenue mais sait-on jamais)\n\n\n\n\n\nLes exemples √† venir peuvent tr√®s bien √™tre r√©pliqu√©s sur n‚Äôimporte quel cloud provider qui propose une solution d‚Äôordonnancement type Kubernetes. Il existe √©galement des fournisseurs de services d√©di√©s, g√©n√©ralement associ√©s √† une impl√©mentation, par exemple pour Streamlit. Ces services sont pratiques si on n‚Äôa pas le choix mais il faut garder √† l‚Äôesprit qu‚Äôils peuvent constituer un mur de la production car vous ne contr√¥lez pas l‚Äôenvironnement en question, qui peut se distinguer de votre environnement de d√©veloppement.\nEt si jamais vous voulez avoir un SSPCloud dans votre entreprise c‚Äôest possible: le logiciel Onyxia sur lequel repose cette infrastructure est open source et est, d√©j√†, r√©impl√©ment√© par de nombreux acteurs. Pour b√©n√©ficier d‚Äôun accompagnement dans la cr√©ation d‚Äôune telle infrastructure, rdv sur le Slack du projet Onyxia:\n\n\n\n\n\n\n\n\n\n\nApplicationApplication 17: Dockeriser l‚ÄôAPI (int√©gration continue)\n\n\n\n\nCr√©er un script app/run.sh √† la racine du projet qui lance le script train.py puis d√©ploie localement l‚ÄôAPI. Attention, quand on se place dans le monde des conteneurs et plus g√©n√©ralement des infrastructures cloud, on ne va plus d√©ployer sur le localhost mais sur ‚Äúl‚Äôensemble des interfaces r√©seaux‚Äù. Lorsqu‚Äôon d√©ploie une application web dans un conteneur, on va donc toujours devoir sp√©cifier un host valant 0.0.0.0 (et non plus localhost ou, de mani√®re √©quivalente, http://127.0.0.1).\n\n\n\nFichier run.sh\n\n\n\napi/run.sh\n\n#/bin/bash\n\npython3 train.py\nuvicorn app.api:app --host \"0.0.0.0\"\n\n\n\nDonner au script api/run.sh des permissions d‚Äôex√©cution : chmod +x api/run.sh\nAjouter COPY app ./app pour avoir les fichiers n√©cessaires au lancement dans l‚ÄôAPI dans l‚Äôimage\nModifier COPY train.py . pour tenir compte du nouveau nom du fichier\nChanger l‚Äôinstruction CMD du Dockerfile pour ex√©cuter le script api/run.sh au lancement du conteneur (CMD [\"bash\", \"-c\", \"./app/run.sh\"])\nCommit et push les changements\nUne fois le CI termin√©, v√©rifier que le nouveau tag latest a √©t√© push√© sur le DockerHub. R√©cup√©rer la nouvelle image dans votre environnement de test de Docker et v√©rifier que l‚ÄôAPI se d√©ploie correctement.\n\n\n\nTester l‚Äôimage sur le SSP Cloud\n\nLancer dans un terminal la commande suivante pour pull l‚Äôapplication depuis le DockerHub et la d√©ployer en local :\n\n\nterminal\n\nkubectl run -it api-ml --image=votre_compte_docker_hub/application:latest\n\n\n\nSi tout se passe correctement, vous devriez observer dans la console un output similaire au d√©ploiement en local de la partie pr√©c√©dente. Cette fois, l‚Äôapplication est d√©ploy√©e √† l‚Äôadresse http://0.0.0.0:8000. On ne peut n√©anmoins pas directement l‚Äôexploiter √† ce stade : si le conteneur de l‚ÄôAPI est d√©ploy√©, il manque un ensemble de ressources Kubernetes qui permettent de d√©ployer proprement l‚ÄôAPI √† tout utilisateur. C‚Äôest l‚Äôobjet de l‚Äôapplication suivante !\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli17      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli172\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli17\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nNous avons pr√©par√© la mise √† disposition de notre API mais √† l‚Äôheure actuelle elle n‚Äôest pas accessible de mani√®re ais√©e car il est n√©cessaire de lancer manuellement une image Docker pour pouvoir y acc√©der. Ce type de travail est la sp√©cialit√© de Kubernetes que nous allons utiliser pour g√©rer la mise √† disposition de notre API.\n\n\n\n\n\n\nApplicationApplication 18: Mettre √† disposition l‚ÄôAPI (d√©ploiement manuel)\n\n\n\nCette partie n√©cessite d‚Äôavoir √† disposition une infrastructure cloud.\n\nCr√©er un dossier deployment √† la racine du projet qui va contenir les fichiers de configuration n√©cessaires pour d√©ployer sur un cluster Kubernetes\nEn vous inspirant de la documentation, y ajouter un premier fichier deployment.yaml qui va sp√©cifier la configuration du Pod √† lancer sur le cluster\n\n\n\nFichier deployment/deployment.yaml\n\n#| filename: \"deployment/deployment.yaml\"\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n        image: votre_compte_docker_hub/application:latest\n        ports:\n        - containerPort: 8000\n\n\nEn vous inspirant de la documentation, y ajouter un second fichier service.yaml qui va cr√©er une ressource Service permettant de donner une identit√© fixe au Pod pr√©c√©demment cr√©√© au sein du cluster\n\n\n\nFichier deployment/service.yaml\n\n\n\ndeployment/service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: titanic-service\nspec:\n  selector:\n    app: titanic\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8000\n\n\n\nEn vous inspirant de la documentation, y ajouter un troisi√®me fichier ingress.yaml qui va cr√©er une ressource Ingress permettant d‚Äôexposer le service via une URL en dehors du cluster\n\n\n\nFichier deployment/ingress.yaml\n\n#| filename: \"deployment/ingress.yaml\"\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: titanic-ingress\nspec:\n  ingressClassName: nginx\n  tls:\n  - hosts:\n    - votre_nom_d_application.lab.sspcloud.fr\n  rules:\n  - host: votre_nom_d_application.lab.sspcloud.fr\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: titanic-service\n            port:\n              number: 80\n\nMettez l‚ÄôURL auquel vous voulez exposer votre service. Sur le mod√®le de titanic.lab.sspcloud.fr (mais ne tentez pas celui-l√†, il est d√©j√† pris üòÉ)\nMettre cette m√™me URL ici aussi\n\n\n\nAppliquer ces fichiers de configuration sur le cluster : kubectl apply -f deployment/\nV√©rifier le bon d√©ploiement de l‚Äôapplication (c‚Äôest √† dire du Pod qui encapsule le conteneur) √† l‚Äôaide de la commande kubectl get pods\nSi tout a correctement fonctionn√©, vous devriez pouvoir acc√©der depuis votre navigateur √† l‚ÄôAPI √† l‚ÄôURL sp√©cifi√©e dans le fichier deployment/ingress.yaml. Par exemple https://api-titanic-test.lab.sspcloud.fr/ si vous avez mis celui-ci plus t√¥t\nExplorer le swagger de votre API √† l‚Äôadresse https://api-titanic-test.lab.sspcloud.fr/docs. Il s‚Äôagit d‚Äôune page de documentation standard √† la plupart des APIs, bien utiles pour tester des requ√™tes de mani√®re interactive.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli18      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli182\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli18\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nOn peut remarquer quelques voies d‚Äôam√©lioration de notre approche qui seront ult√©rieurement trait√©es:\n\nL‚Äôentra√Ænement du mod√®le est r√©-effectu√© √† chaque lancement d‚Äôun nouveau conteneur. On relance donc autant de fois un entra√Ænement qu‚Äôon d√©ploie de conteneurs pour r√©pondre √† nos utilisateurs. Ce sera l‚Äôobjet de la partie MLOps de fiabiliser et optimiser cette partie du pipeline.\nil est n√©cessaire de (re)lancer manuellement kubectl apply -f deployment/ √† chaque changement de notre code. Autrement dit, lors de cette application, on a am√©lior√© la fiabilit√© du lancement de notre API mais un lancement manuel est encore indispensable. Comme dans le reste de ce cours, on va essayer d‚Äô√©viter un geste manuel pouvant √™tre source d‚Äôerreur en privil√©giant l‚Äôautomatisation et l‚Äôarchivage dans des scripts. C‚Äôest l‚Äôobjet de la prochaine √©tape."
  },
  {
    "objectID": "chapters/application.html#etape-3-automatiser-le-d√©ploiement-d√©ploiement-en-continu",
    "href": "chapters/application.html#etape-3-automatiser-le-d√©ploiement-d√©ploiement-en-continu",
    "title": "Application",
    "section": "Etape 3: automatiser le d√©ploiement (d√©ploiement en continu)",
    "text": "Etape 3: automatiser le d√©ploiement (d√©ploiement en continu)\n\n\n\n\n\n\nImportantClarification sur la branche de travail, les tags et l‚Äôimage Docker utilis√©e\n\n\n\n\n\nA partir de maintenant, il est n√©cessaire de clarifier la branche principale sur laquelle nous travaillons. Toutes les prochaines applications supposeront que vous travaillez depuis la branche main. Si vous avez chang√© de branche, vous pouvez fusionner celle-ci √† main.\nSi vous avez utilis√© un tag pour sauter une ou plusieurs √©tapes, il va √™tre n√©cessaire de se placer sur une branche car vous √™tes en head detached. Si vous avez utilis√© les scripts automatis√©s de checkpoint, cette gymnastique a √©t√© faite pour vous.\nLes prochaines applications vont √©galement n√©cessiter d‚Äôutiliser une image Docker. Si vous avez suivi de mani√®re lin√©aire cette application, votre image Docker devrait exister depuis l‚Äôapplication 15 si vous avez push√© votre d√©p√¥t √† ce moment l√†.\nN√©anmoins, si vous n‚Äôavez pas fait cette application, vous pouvez utiliser le checkpoint de l‚Äôapplication 18 et faire un git push origin main --force (√† ne pas reproduire sur vos projets!) qui devrait d√©clencher les op√©rations c√¥t√© Github pour construire et livrer votre image Docker. Cela n√©cessite quelques op√©rations de votre c√¥t√©, notamment la cr√©ation d‚Äôun token Dockerhub √† renseigner en secret Github. Pour vous refra√Æchir la m√©moire sur le sujet, vous pouvez retourner consulter l‚Äôapplication 15.\n\n\n\nQu‚Äôest-ce qui peut d√©clencher une √©volution n√©cessitant de mettre √† jour l‚Äôensemble de notre processus de production ?\nRegardons √† nouveau notre pipeline:\n\nLes inputs de notre pipeline sont donc:\n\nLa configuration. Ici, on peut consid√©rer que notre .env de configuration, les secrets renseign√©s √† Github ou encore le requirements.txt rel√®vent de cette cat√©gorie ;\nLes donn√©es. Nos donn√©es sont statiques et n‚Äôont pas vocation √† √©voluer. Si c‚Äô√©tait le cas, il faudrait en tenir compte dans notre automatisation (Note¬†1). ;\nLe code. C‚Äôest l‚Äô√©l√©ment principal qui √©volue chez nous. Id√©alement, on veut automatiser le processus au maximum en faisant en sorte qu‚Äô√† chaque mise √† jour de notre code (un push sur Github), les √©tapes ult√©rieures (production de l‚Äôimage Docker, etc.) se lancent. N√©anmoins, on veut aussi √©viter qu‚Äôune erreur puisse donner lieu √† une mise en production non-fonctionnelle, on va donc maintenir une action manuelle minimale comme garde-fou.\n\n\n\n\n\n\n\nNote¬†1: Et le versionning des donn√©es ?\n\n\n\nIci, nous nous pla√ßons dans le cas simple o√π les donn√©es brutes re√ßues sont fig√©es. Ce qui peut changer est la mani√®re dont on constitue nos √©chantillons train/test. Il sera donc utile de logguer les donn√©es en question par le biais de MLFlow. Mais il n‚Äôest pas n√©cessaire de versionner les donn√©es brutes.\nSi celles-ci √©voluaient, il pourrait √™tre utile de versionner les donn√©es, √† la mani√®re dont on le fait pour le code. Git n‚Äôest pas l‚Äôoutil appropri√© pour cela. Parmi les outils populaires de versionning de donn√©es, bien int√©gr√©s avec S3, il y a, sur le SSPCloud, lakefs.\n\n\nPour automatiser au maximum la mise en production, on va utiliser un nouvel outil : ArgoCD. Ainsi, au lieu de devoir appliquer manuellement la commande kubectl apply √† chaque modification des fichiers de d√©ploiement (pr√©sents dans le dossier kubernetes/), c‚Äôest l‚Äôop√©rateur ArgoCD, d√©ploy√© sur le cluster, qui va d√©tecter les changements de configuration du d√©ploiement et les appliquer automatiquement.\nC‚Äôest l‚Äôapproche dite GitOps : le d√©p√¥t Git du d√©ploiement fait office de source de v√©rit√© unique de l‚Äô√©tat voulu de l‚Äôapplication, tout changement sur ce dernier doit donc se r√©percuter imm√©diatement sur le d√©ploiement effectif.\n\n\n\n\n\n\nApplicationApplication 19a: Automatiser la mise √† disposition de l‚ÄôAPI (d√©ploiement continu)\n\n\n\n\nLancer un service ArgoCD sur le SSPCloud depuis la page Mes services (catalogue Automation). Laisser les configurations par d√©faut.\nSur GitHub, cr√©er un d√©p√¥t application-deployment qui va servir de d√©p√¥t GitOps, c‚Äôest √† dire un d√©p√¥t qui sp√©cifie le param√©trage du d√©ploiement de votre application.\nAjouter un dossier deployment √† votre d√©p√¥t GitOps, dans lequel on mettra les trois fichiers de d√©ploiement qui permettent de d√©ployer notre application sur Kubernetes (deployment.yaml, service.yaml, ingress.yaml).\nA la racine de votre d√©p√¥t GitOps, cr√©ez un fichier application.yml avec le contenu suivant, en prenant bien soin de modifier les lignes annot√©es avec des informations pertinentes :\n\n\napplication.yaml\n\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: ensae-mlops\nspec:\n  project: default\n  source:\n1    repoURL: https://github.com/&lt;your_github_username&gt;/application-deployment.git\n2    targetRevision: main\n3    path: deployment\n  destination:\n    server: https://kubernetes.default.svc\n4    namespace: user-&lt;your_sspcloud_username&gt;\n  syncPolicy:\n    automated:\n      selfHeal: true\n\n\n1\n\nL‚ÄôURL de votre d√©p√¥t Github  faisant office de d√©p√¥t GitOps.\n\n2\n\nLa branche √† partir de laquelle vous d√©ployez.\n\n3\n\nLe nom du dossier contenant vos fichiers de d√©ploiement Kubernetes.\n\n4\n\nVotre namespace Kubernetes. Sur le SSPCloud, cela prend la forme user-${username}.\n\n\nPousser sur Github le d√©p√¥t GitOps.\nDans ArgoCD, cliquez sur New App puis Edit as a YAML. Copiez-collez le contenu de application.yml et cliquez sur Create.\nObservez dans l‚Äôinterface d‚ÄôArgoCD le d√©ploiement progressif des ressources n√©cessaires √† votre application sur le cluster. Joli non ?\nV√©rifiez que votre API est bien d√©ploy√©e en utilisant l‚ÄôURL d√©finie dans le fichier ingress.yml.\nSupprimer du code applicatif le dossier deployment puisque c‚Äôest maintenant votre d√©p√¥t de d√©ploiement qui le contr√¥le.\nIndiquer dans le README.md que le d√©ploiement de votre application (dont vous pouvez mettre l‚ÄôURL dans le README) est contr√¥l√© par un autre d√©p√¥t.\n\n\n\nSi cela a fonctionn√©, vous devriez maintenant voir votre application dans votre tableau de bord ArgoCD:\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli19a      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli19a2\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli19a\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nA pr√©sent, nous avons tous les outils √† notre disposition pour construire un vrai pipeline de CI/CD, automatis√© de bout en bout. Il va nous suffire pour cela de mettre √† bout les composants :\n\ndans la partie 4 de l‚Äôapplication, nous avons construit un pipeline de CI : on a donc seulement √† faire un commit sur le d√©p√¥t de l‚Äôapplication pour lancer l‚Äô√©tape de build et de mise √† disposition de la nouvelle image sur le DockerHub ;\ndans l‚Äôapplication pr√©c√©dente, nous avons construit un pipeline de CD : ArgoCD suit en permanence l‚Äô√©tat du d√©p√¥t GitOps, tout commit sur ce dernier lancera donc automatiquement un red√©ploiement de l‚Äôapplication.\n\nIl y a donc un √©l√©ment qui fait la liaison entre ces deux pipelines et qui nous sert de garde-fou en cas d‚Äôerreur : la version de l‚Äôapplication.\n\n\n\n\n\n\nApplicationApplication 19b : Mettre √† jour la version en production\n\n\n\nJusqu‚Äô√† maintenant, on a utilis√© le tag latest pour d√©finir la version de notre application. En pratique, lorsqu‚Äôon passe de la phase de d√©veloppement √† celle de production, on a plut√¥t envie de versionner proprement les versions de l‚Äôapplication afin de savoir ce qui est d√©ploy√©. On va pour cela utiliser les tags avec Git, qui vont se propager au nommage de l‚Äôimage Docker.\n\nModifier le fichier de CI prod.yml pour assurer la propagation des tags.\n\n\n\nFichier .github/workflows/prod.yml\n\n\n\n.github/workflows/prod.yml\n\nname: Construction image Docker\n\non:\n  push:\n    branches:\n      - main\n      - dev\n    tags:\n      - 'v*.*.*'\n\njobs:\n  docker:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n\n      -\n        name: Docker meta\n        id: meta\n        uses: docker/metadata-action@v5\n        with:\n1          images: linogaliana/application\n\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Build and push\n        uses: docker/build-push-action@v5\n        with:\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n\n1\n\nModifier ici !\n\n\n\n\nDans le d√©p√¥t de l‚Äôapplication, mettre √† jour le code dans app/main.py pour changer un √©l√©ment de l‚Äôinterface de votre documentation. Par exemple, mettre en gras un titre.\n\n\napp/main.py\n\napp = FastAPI(\n    title=\"D√©monstration du mod√®le de pr√©diction de survie sur le Titanic\",\n    description=\n    \"&lt;b&gt;Application de pr√©diction de survie sur le Titanic&lt;/b&gt; üö¢ &lt;br&gt;Une version par API pour faciliter la r√©utilisation du mod√®le üöÄ\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\nCommit et push les changements.\nTagger le commit effectu√© pr√©c√©demment et push le nouveau tag :\n\n\nterminal\n\ngit tag v0.0.1\ngit push --tags\n\nV√©rifier sur le d√©p√¥t GitHub de l‚Äôapplication que ce commit lance bien un pipeline de CI associ√© au tag v1.0.0. Une fois termin√©, v√©rifier sur le DockerHub que le tag v0.0.1 existe bien parmi les tags disponibles de l‚Äôimage.\n\nLa partie CI a correctement fonctionn√©. Int√©ressons-nous √† pr√©sent √† la partie CD.\n\nSur le d√©p√¥t GitOps, mettre √† jour la version de l‚Äôimage √† d√©ployer en production dans le fichier deployment/deployment.yaml\n\n\n\nFichier deployment/deployment.yaml\n\n\n\ndeployment/deployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: titanic-deployment\n  labels:\n    app: titanic\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: titanic\n  template:\n    metadata:\n      labels:\n        app: titanic\n    spec:\n      containers:\n      - name: titanic\n1        image: linogaliana/application:v0.0.1\n        ports:\n        - containerPort: 8000\n\n\n1\n\nRemplacer ici par le d√©p√¥t applicatif ad√©quat\n\n\n\n\nApr√®s avoir committ√© et push√©, observer dans ArgoCD le statut de votre application. Normalement, l‚Äôop√©rateur devrait avoir automatiquement identifi√© le changement, et mettre √† jour le d√©ploiement pour en tenir compte.\n\n\n\nV√©rifier que l‚ÄôAPI a bien √©t√© mise √† jour.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli19b      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli19b2\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli19b\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#etape-4-construire-un-site-web",
    "href": "chapters/application.html#etape-4-construire-un-site-web",
    "title": "Application",
    "section": "Etape 4: construire un site web",
    "text": "Etape 4: construire un site web\n\n\n\n\n\n\nCautionSi vous prenez ce projet fil rouge en cours de route\n\n\n\n\n\n\n\nterminal\n\ngit checkout appli19\ngit checkout -b dev\ngit push origin dev\n\n\n\n\n\n\n\n\n\nOn va proposer un nouveau livrable pour parler √† un public plus large. Pour faire ce site web, on va utiliser Quarto et d√©ployer sur Github Pages.\n\n\n\n\n\n\nApplicationApplication 20: Cr√©ation d‚Äôun site web pour valoriser le projet\n\n\n\nquarto create project website mysite\n\nFaire remonter d‚Äôun niveau _quarto.yml\nSupprimer about.qmd, d√©placer index.qmd vers la racine de notre projet.\nRemplacer le contenu de index.qmd par celui-ci et retirer about.qmd des fichiers √† compiler.\nD√©placer styles.css √† la racine du projet\nMettre √† jour le .gitignore avec les instructions suivantes\n\n/.quarto/\n*.html\n*_files\n_site/\n\nEn ligne de commande, faire quarto preview\nObserver le site web g√©n√©r√© en local\n\nEnfin, on va construire et d√©ployer automatiquement ce site web gr√¢ce au combo Github Actions et Github Pages:\n\nCr√©er une branche gh-pages √† partir des lignes suivantes\n\n\n\nterminal\n\ngit checkout --orphan gh-pages\ngit reset --hard # make sure all changes are committed before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\n\n\nRevenir √† votre branche principale (main normalement)\nCr√©er un fichier .github/workflows/website.yaml avec le contenu de ce fichier\nModifier le README pour indiquer l‚ÄôURL de votre site web et de votre API\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli20      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli202\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli20\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#revenir-sur-le-code-dentra√Ænement-du-mod√®le-pour-faire-de-la-validation-crois√©e",
    "href": "chapters/application.html#revenir-sur-le-code-dentra√Ænement-du-mod√®le-pour-faire-de-la-validation-crois√©e",
    "title": "Application",
    "section": "Revenir sur le code d‚Äôentra√Ænement du mod√®le pour faire de la validation crois√©e",
    "text": "Revenir sur le code d‚Äôentra√Ænement du mod√®le pour faire de la validation crois√©e\nPour pouvoir faire ceci, il va falloir changer un tout petit peu notre code applicatif dans sa phase d‚Äôentra√Ænement.\n\n\n\n\n\n\nApplicationApplication 21 (optionnelle): restructuration de la cha√Æne\n\n\n\n\nFaire les modifications suivantes pour restructurer notre pipeline afin de mieux distinguer les √©tapes d‚Äôestimation et d‚Äô√©valuation\n\n\n\nModification de train.py pour faire une grid search\n\n\n\ntrain.py\n\n\"\"\"\nPrediction de la survie d'un individu sur le Titanic\n\"\"\"\n\nimport os\nfrom dotenv import load_dotenv\nimport argparse\nfrom loguru import logger\nfrom joblib import dump\n\nimport pathlib\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom src.pipeline.build_pipeline import create_pipeline\nfrom src.models.train_evaluate import evaluate_model\n\n\n# ENVIRONMENT CONFIGURATION ---------------------------\n\nlogger.add(\"recording.log\", rotation=\"500 MB\")\nload_dotenv()\n\nparser = argparse.ArgumentParser(description=\"Param√®tres du random forest\")\nparser.add_argument(\n    \"--n_trees\", type=int, default=20, help=\"Nombre d'arbres\"\n)\nargs = parser.parse_args()\n\nURL_RAW = \"https://minio.lab.sspcloud.fr/lgaliana/ensae-reproductibilite/data/raw/data.csv\"\n\nn_trees = args.n_trees\njeton_api = os.environ.get(\"JETON_API\", \"\")\ndata_path = os.environ.get(\"data_path\", URL_RAW)\ndata_train_path = os.environ.get(\"train_path\", \"data/derived/train.parquet\")\ndata_test_path = os.environ.get(\"test_path\", \"data/derived/test.parquet\")\nMAX_DEPTH = None\nMAX_FEATURES = \"sqrt\"\n\nif jeton_api.startswith(\"$\"):\n    logger.info(\"API token has been configured properly\")\nelse:\n    logger.warning(\"API token has not been configured\")\n\n\n# IMPORT ET STRUCTURATION DONNEES --------------------------------\n\np = pathlib.Path(\"data/derived/\")\np.mkdir(parents=True, exist_ok=True)\n\nTrainingData = pd.read_csv(data_path)\n\ny = TrainingData[\"Survived\"]\nX = TrainingData.drop(\"Survived\", axis=\"columns\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.1\n)\npd.concat([X_train, y_train], axis = 1).to_parquet(data_train_path)\npd.concat([X_test, y_test], axis = 1).to_parquet(data_test_path)\n\n\n# PIPELINE ----------------------------\n\n\n# Create the pipeline\npipe = create_pipeline(\n    n_trees, max_depth=MAX_DEPTH, max_features=MAX_FEATURES\n)\n\nparam_grid = {\n    \"classifier__n_estimators\": [10, 20, 50],\n    \"classifier__max_leaf_nodes\": [5, 10, 50],\n}\n\npipe_cross_validation = GridSearchCV(\n    pipe,\n    param_grid=param_grid,\n    scoring=[\"accuracy\", \"precision\", \"recall\", \"f1\"],\n    refit=\"f1\",\n    cv=5,\n    n_jobs=5,\n    verbose=1,\n)\n\npipe_cross_validation.fit(X_train, y_train)\n\npipe = pipe_cross_validation.best_estimator_\n\n\n# ESTIMATION ET EVALUATION ----------------------\n\npipe.fit(X_train, y_train)\n\nwith open(\"model.joblib\", \"wb\") as f:\n    dump(pipe, f)\n\n# Evaluate the model\nscore, matrix = evaluate_model(pipe, X_test, y_test)\n\nlogger.success(f\"{score:.1%} de bonnes r√©ponses sur les donn√©es de test pour validation\")\nlogger.debug(20 * \"-\")\nlogger.info(\"Matrice de confusion\")\nlogger.debug(matrix)\n\n\n\nDans le code de l‚ÄôAPI (app/api.py), changer la version du mod√®le mis en oeuvre en ‚Äú0.2‚Äù (dans la fonction show_welcome_page)\nApr√®s avoir committ√© cette nouvelle version du code applicatif, tagguer ce d√©p√¥t avec le tag v0.0.2\nModifier deployment/deployment.yaml dans le code GitOps pour utiliser ce tag.\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli21      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli212\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli21\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#garder-une-trace-des-entra√Ænements-de-notre-mod√®le-gr√¢ce-au-register-de-mlflow",
    "href": "chapters/application.html#garder-une-trace-des-entra√Ænements-de-notre-mod√®le-gr√¢ce-au-register-de-mlflow",
    "title": "Application",
    "section": "Garder une trace des entra√Ænements de notre mod√®le gr√¢ce au register de MLFlow",
    "text": "Garder une trace des entra√Ænements de notre mod√®le gr√¢ce au register de MLFlow\n  \n    \n      \n        \n      \n      \n        Reprendre √† partir d'ici      \n      \n    \n    \n      \n\n        Si vous n'avez plus de VSCode actif avec la configuration propos√©e dans l'application pr√©liminaire, vous pouvez repartir de ce service:    \n      \n    \n    Et ensuite, apr√®s avoir cl√¥n√© le d√©p√¥t\n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli212\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli21\n          3\n          Nettoyer derri√®re nous"
  },
  {
    "objectID": "chapters/application.html#enregistrer-nos-premiers-entra√Ænements",
    "href": "chapters/application.html#enregistrer-nos-premiers-entra√Ænements",
    "title": "Application",
    "section": "Enregistrer nos premiers entra√Ænements",
    "text": "Enregistrer nos premiers entra√Ænements\n\n\n\n\n\n\nApplicationApplication 22 : archiver nos entra√Ænements avec MLFlow\n\n\n\n\nLancer MLFlow depuis l‚Äôonflet Mes services du SSPCloud. Attendre que le service soit bien lanc√©. Cela cr√©era un service dont l‚ÄôURL est de la forme https://user-{username}.user.lab.sspcloud.fr. Ce service MLFlow communiquera avec les VSCode que vous ouvrirez ult√©rieurement √† partir de cet URL ainsi qu‚Äôavec le syst√®me de stockage S317.\nRegarder la page Experiments. Elle ne contient que Default √† ce stade, c‚Äôest normal.\n\n\nUne fois le service MLFlow fonctionnel, lancer un nouveau VSCode pour b√©n√©ficier de la connexion automatique entre les services interactifs du SSPCloud et les services d‚Äôautomatisation comme MLFlow.\nCl√¥ner votre projet, vous situer sur la branche de travail.\nDans la section de passage des param√®tres de notre ligne de commande, introduire ce morceau de code:\n\nparser = argparse.ArgumentParser(description=\"Param√®tres du random forest\")\nparser.add_argument(\n    \"--n_trees\", type=int, default=20, help=\"Nombre d'arbres\"\n)\nparser.add_argument(\n    \"--experiment_name\", type=str, default=\"titanicml\", help=\"MLFlow experiment name\"\n)\nargs = parser.parse_args()\n\nA la fin du script train.py, ajouter le code suivant\n\n\n\nCode √† ajouter\n\n\n\nfin de train.py\n\n# LOGGING IN MLFLOW -----------------\n\nmlflow_server = os.getenv(\"MLFLOW_TRACKING_URI\")\n\nlogger.info(f\"Saving experiment in {mlflow_server}\")\n\nmlflow.set_tracking_uri(mlflow_server)\nmlflow.set_experiment(args.experiment_name)\n\n\ninput_data_mlflow = mlflow.data.from_pandas(\n    TrainingData, source=data_path, name=\"Raw dataset\"\n)\ntraining_data_mlflow = mlflow.data.from_pandas(\n    pd.concat([X_train, y_train], axis=1), source=data_path, name=\"Training data\"\n)\n\n\nwith mlflow.start_run():\n\n    # Log datasets\n    mlflow.log_input(input_data_mlflow, context=\"raw\")\n    mlflow.log_input(training_data_mlflow, context=\"raw\")\n\n    # Log parameters\n    mlflow.log_param(\"n_trees\", n_trees)\n    mlflow.log_param(\"max_depth\", MAX_DEPTH)\n    mlflow.log_param(\"max_features\", MAX_FEATURES)\n\n    # Log best hyperparameters from GridSearchCV\n    best_params = pipe_cross_validation.best_params_\n    for param, value in best_params.items():\n        mlflow.log_param(param, value)\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", score)\n\n    # Log confusion matrix as an artifact\n    matrix_path = \"confusion_matrix.txt\"\n    with open(matrix_path, \"w\") as f:\n        f.write(str(matrix))\n    mlflow.log_artifact(matrix_path)\n\n    # Log model\n    mlflow.sklearn.log_model(pipe, \"model\")\n\n\n\nAjouter mlruns/* dans .gitignore\nTester train.py en ligne de commande\nObserver l‚Äô√©volution de la page Experiments. Cliquer sur un des run. Observer toutes les m√©tadonn√©es archiv√©es (hyperparam√®tres, m√©triques d‚Äô√©valuation, requirements.txt dont MLFlow a fait l‚Äôinf√©rence, etc.)\nObserver le code propos√© par MLFlow pour r√©cup√©rer le run en question. Tester celui-ci dans un notebook sur le fichier interm√©diaire de test au format Parquet\nEn ligne de commande, faites tourner pour une autre valeur de n_trees. Retourner √† la liste des runs en cliquant √† nouveau sur ‚Äútitanicml‚Äù dans les exp√©rimentations\nDans l‚Äôonglet Table, s√©lectionner plusieurs exp√©rimentations, cliquer sur Columns et ajouter la statistique d‚Äôaccuracy. Ajuster la taille des colonnes pour la voir et classer les mod√®les par score d√©croissants\nCliquer sur Compare apr√®s en avoir s√©lectionn√© plusieurs. Afficher un scatterplot des performances en fonction du nombre d‚Äôestimateurs. Conclure.\nAjouter mlflow au requirements.txt\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli22      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli222\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli22\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nCette appplication illustre l‚Äôun des premiers apports de MLFlow: on garde une trace de nos exp√©rimentations: le mod√®le est archiv√© avec les param√®tres et des m√©triques de performance. On peut donc retrouver de plusieurs mani√®res un mod√®le qui nous avait tap√© dans l‚Äôoeil.\nN√©anmoins, persistent un certain nombre de voies d‚Äôam√©lioration dans notre pipeline.\n\nOn entra√Æne le mod√®le en local, de mani√®re s√©quentielle, et en lan√ßant nous-m√™mes le script train.py.\nPis encore, √† l‚Äôheure actuelle, cette √©tape d‚Äôestimation n‚Äôest pas s√©par√©e de la mise √† disposition du mod√®le par le biais de notre API. On archive des mod√®les mais on les utilise pas ult√©rieurement.\n\nLes prochaines applications permettront d‚Äôam√©liorer ceci."
  },
  {
    "objectID": "chapters/application.html#consommation-dun-mod√®le-archiv√©-sur-mlflow",
    "href": "chapters/application.html#consommation-dun-mod√®le-archiv√©-sur-mlflow",
    "title": "Application",
    "section": "Consommation d‚Äôun mod√®le archiv√© sur MLFlow",
    "text": "Consommation d‚Äôun mod√®le archiv√© sur MLFlow\nA l‚Äôheure actuelle, notre pipeline est lin√©aire:\n\nCeci nous g√™ne pour faire √©voluer notre mod√®le: on ne dissocie pas ce qui rel√®ve de l‚Äôentra√Ænement du mod√®le de son utilisation. Un pipeline plus cyclique permettra de mieux dissocier l‚Äôexp√©rimentation de la production:\n\n\n\n\n\n\n\nApplicationApplication 23 : passer en production un mod√®le avec MLFlow\n\n\n\n\nSi vous avez entra√Æn√© plusieurs mod√®les avec des n_trees diff√©rents, utiliser l‚Äôinterface de MLFlow pour s√©lectionner le ‚Äúmeilleur‚Äù. Cliquer sur le mod√®le en question et faire l‚Äôaction ‚ÄúRegister Model‚Äù. L‚Äôenregistrer comme le mod√®le de ‚Äúproduction‚Äù\nRendez-vous sur l‚Äôonglet Models et observez cet entrep√¥t de mod√®les. Cliquez sur le mod√®le de production. Vous pourrez par ce biais suivre ses diff√©rentes versions.\nOuvrir un notebook temporaire et observer le r√©sultat.\n\n\n\nExemple de code √† tester\n\nimport mlflow\nimport pandas as pd\n\nmodel_name = \"production\"\nmodel_version = \"latest\"\n\n# Load the model from the Model Registry\nmodel_uri = f\"models:/{model_name}/{model_version}\"\nlogged_model = mlflow.sklearn.load_model(model_uri)\n\n# GENERATE PREDICTION DATA ---------------------\n\ndef create_data(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\",\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    return df\n\n\ndata = pd.concat(\n    [create_data(age=40), create_data(sex=\"male\")]\n)\n\n# PREDICTION ---------------------\n\nlogged_model.predict(pd.DataFrame(data))\n\n\nOn va adapter le code applicatif de notre API pour tenir compte de ce mod√®le de production.\n\n\n\nVoir le script app/api.py propos√©\n\n\"\"\"A simple API to expose our trained RandomForest model for Tutanic survival.\"\"\"\nfrom fastapi import FastAPI\nimport mlflow\n\nimport pandas as pd\n\n# Preload model -------------------\n\nmodel_name = \"production\"\nmodel_version = \"latest\"\n\n# Load the model from the Model Registry\nmodel_uri = f\"models:/{model_name}/{model_version}\"\nmodel = mlflow.sklearn.load_model(model_uri)\n\n# Define app -------------------------\n\n\napp = FastAPI(\n    title=\"Pr√©diction de survie sur le Titanic\",\n    description=\n    \"Application de pr√©diction de survie sur le Titanic üö¢ &lt;br&gt;Une version par API pour faciliter la r√©utilisation du mod√®le üöÄ\" +\\\n        \"&lt;br&gt;&lt;br&gt;&lt;img src=\\\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\\\" width=\\\"200\\\"&gt;\"\n    )\n\n\n@app.get(\"/\", tags=[\"Welcome\"])\ndef show_welcome_page():\n    \"\"\"\n    Show welcome page with model name and version.\n    \"\"\"\n\n    return {\n        \"Message\": \"API de pr√©diction de survie sur le Titanic\",\n        \"Model_name\": 'Titanic ML',\n        \"Model_version\": \"0.3\",\n    }\n\n\n@app.get(\"/predict\", tags=[\"Predict\"])\nasync def predict(\n    sex: str = \"female\",\n    age: float = 29.0,\n    fare: float = 16.5,\n    embarked: str = \"S\"\n) -&gt; str:\n    \"\"\"\n    \"\"\"\n\n    df = pd.DataFrame(\n        {\n            \"Sex\": [sex],\n            \"Age\": [age],\n            \"Fare\": [fare],\n            \"Embarked\": [embarked],\n        }\n    )\n\n    prediction = \"Survived üéâ\" if int(model.predict(df)) == 1 else \"Dead ‚ö∞Ô∏è\"\n\n    return prediction\n\nLes changements principaux de ce code sont:\n\non va chercher le mod√®le de production\non met √† jour la version de notre API pour signaler √† nos clients que celle-ci a √©volu√©\n\n\nOn va retirer l‚Äôentra√Ænement de la s√©quence d‚Äôop√©ration du api/run.sh. En supprimant la ligne relative √† l‚Äôentra√Ænement du mod√®le, vous devriez avoir\n\n#/bin/bash\nuvicorn app.api:app --host \"0.0.0.0\"\nMettons en production cette nouvelle version. Cela implique de faire les gestes suivants:\n\nCommit de ce changement dans main\nPublier un tag v0.0.3 pour le code applicatif\nMettre √† jour notre manifeste dans le d√©p√¥t GitOps.\n\nEn premier lieu, il faut changer la version de r√©f√©rence pour utiliser le tag v0.0.3.\nDe plus, il faut d√©clarer la variable d‚Äôenvironnement MLFLOW_TRACKING_URI qui indique √† Python l‚Äôentrep√¥t de mod√®les o√π aller chercher celui en production. La bonne pratique est de d√©finir ceci hors du code, dans un fichier de configuration donc, ce qui est l‚Äôobjet de notre manifeste deployment.yaml. On peut donc changer de cette mani√®re ce fichier:\n\n\n\n\nLe mod√®le deployment.yaml propos√©\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: titanic-deployment\nlabels:\n    app: titanic\nspec:\nreplicas: 1\nselector:\n    matchLabels:\n    app: titanic\ntemplate:\n    metadata:\n    labels:\n        app: titanic\n    spec:\n    containers:\n    - name: titanic\n1        image: linogaliana/application:v0.0.3\n        ports:\n        - containerPort: 8000\n        env:\n        - name: MLFLOW_TRACKING_URI\n2            value: https://user-${USERNAME}-mlflow.user.lab.sspcloud.fr\n        resources:\n        limits:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n\n1\n\nLe tag de notre code applicatif\n\n2\n\nLa variable d‚Äôenvironnement √† adapter en fonction de l‚Äôadresse du d√©p√¥t demod√®les utilis√©. Remplacer par votre URL MLFlow.\n\n\n\n\nPour s‚Äôassurer que l‚Äôapplication fonctionne bien, on peut aller voir les logs de la machine qui fait tourner notre code. Pour √ßa, faire kubectl get pods et, en supposant que votre service soit nomm√© titanic dans vos fichiers YAML de configuration, r√©cup√©rer le nom commen√ßant par titanic-deployment-* et faire kubectl logs titanic-deployment-*\n\n\n\n  \n    \n      \n        \n      \n      \n        Checkpoint post appli23      \n      \n    \n    \n      \n\n        \n        \n          \n            terminal\n          \n          \n            curl -sSL https://raw.githubusercontent.com/ensae-reproductibilite/website/refs/heads/main/chapters/applications/overwrite.sh -o update.sh && chmod +x update.sh\n./update.sh appli232\nrm -f update.sh\n          \n        \n        \n          1\n          R√©cup√©rer le script de checkpoint\n          2\n          Avancer √† l‚Äô√©tat √† l‚Äôissue de l‚Äôapplication appli23\n          3\n          Nettoyer derri√®re nous\n        \n\n        \n          \n            \n          \n        \n      \n    \n  \n  \nA ce stade, nous avons am√©lior√© la fiabilit√© de notre application car nous utilisons le meilleur mod√®le. N√©anmoins, nos entra√Ænements sont encore manuels. L√† encore il y a des gains possibles car cela para√Æt p√©nible √† la longue de devoir syst√©matiquement relancer des entra√Ænements manuellement pour tester des variations de tel ou tel param√®tre. Heureusement, nous allons pouvoir automatiser ceci √©galement."
  },
  {
    "objectID": "chapters/application.html#industrialiser-les-entra√Ænements-de-nos-mod√®les",
    "href": "chapters/application.html#industrialiser-les-entra√Ænements-de-nos-mod√®les",
    "title": "Application",
    "section": "Industrialiser les entra√Ænements de nos mod√®les",
    "text": "Industrialiser les entra√Ænements de nos mod√®les\nPour industrialiser nos entra√Ænements, nous allons cr√©er des processus parall√®les ind√©pendants pour chaque combinaison de nos hyperparam√®tres.\nCe travail nous am√®ne de l‚Äôapproche pipeline √† mi chemin entre data science et data engineering. Il existe plusieurs outils pour faire ceci, g√©n√©ralement issus de la sph√®re du data engineering. L‚Äôoutil le plus complet sur le SSPCloud, bien int√©gr√© √† l‚Äô√©cosyst√®me Kubernetes, est Argo Workflows18.\nChaque combinaison d‚Äôhyperparam√®tres sera un processus isol√© √† l‚Äôissue duquel sera loggu√© le r√©sultat dans MLFlow. Ces entra√Ænements auront lieu en parall√®le.\nNous allons construire, dans les deux prochaines applications, un pipeline simple prenant cette forme19:\n\n\n\n\n\n\n\n\n\n\n\n(a) Via Argo Workflows\n\n\n\n\n\n\n\n\n\n\n\n(b) Via Github Actions\n\n\n\n\n\n\n\nFigure¬†4: Pipeline d‚Äôentra√Ænement de nos mod√®les avec deux outils d‚Äôautomatisation diff√©rents\n\n\n\nL‚Äôoutil permettant une int√©gration native de notre pipeline dans l‚Äôinfrastructure cloud (SSPCloud) que nous avons utilis√©e jusqu‚Äô√† pr√©sent est Argo Workflows. N√©anmoins, pour illustrer la modularit√© de notre cha√Æne, permise par l‚Äôadoption de Docker, nous allons montrer que les serveurs d‚Äôint√©gration continue de Github peuvent tr√®s bien servir d‚Äôenvironnement d‚Äôex√©cution, sans rien perdre de ce que nous avons mis en oeuvre pr√©c√©demment (logging des mod√®les dans MLFlow, r√©cup√©ration de donn√©es depuis S3, etc.)\n\n\n\n\n\n\nApplicationApplication 24 : industrialisation des entra√Ænements avec Argo Workflow\n\n\n\nA l‚Äôheure actuelle, notre entra√Ænement ne d√©pend que d‚Äôun hyperparam√®tre fix√© √† partir de la ligne de commande: n_trees. Nous allons commencer par ajouter un argument √† notre chaine de production (code applicatif):\n\nDans train.py, dans la section relative au parsing de nos arguments, ajouter ce bout de code\n\nparser.add_argument(\n    \"--max_features\",\n    type=str, default=\"sqrt\",\n    choices=['sqrt', 'log2'],\n    help=\"Number of features to consider when looking for the best split\"\n)\net remplacer la d√©finition de MAX_FEATURES par l‚Äôargument fourni en ligne de commande:\nMAX_FEATURES = args.max_features\n\nFaire un commit, taguer cette version (v0.0.4) et pusher le tag\nMaintenant, dans le d√©p√¥t GitOps, cr√©er un fichier argo-workflow/manifest.yaml\n\n\n\nLe mod√®le propos√©\n\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\ngenerateName: titanic-training-workflow-\nnamespace: user-lgaliana\nspec:\nentrypoint: main\nserviceAccountName: workflow\narguments:\n    parameters:\n    # The MLflow tracking server is responsible to log the hyper-parameter and model metrics.\n    - name: mlflow-tracking-uri\n1        value: https://user-${USERNAME}-mlflow.user.lab.sspcloud.fr/\n    - name: mlflow-experiment-name\n2        value: titanicml\n    - name: model-training-conf-list\n        value: |\n        [\n            { \"n_trees\": 10, \"max_features\": \"log2\" },\n            { \"n_trees\": 20, \"max_features\": \"sqrt\" },\n            { \"n_trees\": 20, \"max_features\": \"log2\" },\n            { \"n_trees\": 50, \"max_features\": \"sqrt\" }\n        ]\ntemplates:\n    # Entrypoint DAG template\n    - name: main\n    dag:\n        tasks:\n        # Task 0: Start pipeline\n        - name: start-pipeline\n            template: start-pipeline-wt\n        # Task 1: Train model with given params\n        - name: train-model-with-params\n            dependencies: [ start-pipeline ]\n            template: run-model-training-wt\n            arguments:\n            parameters:\n                - name: max_features\n                value: \"{{item.max_features}}\"\n                - name: n_trees\n                value: \"{{item.n_trees}}\"\n            # Pass the inputs to the task using \"withParam\"\n            withParam: \"{{workflow.parameters.model-training-conf-list}}\"\n    # Now task container templates are defined\n    # Worker template for task 0 : start-pipeline\n    - name: start-pipeline-wt\n    inputs:\n    container:\n        image: busybox\n        command: [ sh, -c ]\n        args: [ \"echo Starting pipeline\" ]\n    # Worker template for task-1 : train model with params\n    - name: run-model-training-wt\n    inputs:\n        parameters:\n        - name: n_trees\n        - name: max_features\n    container:\n3        image: ****/application:v0.0.4\n        imagePullPolicy: Always\n        command: [sh, -c]\n        args: [\n        \"python3 train.py --n_trees={{inputs.parameters.n_trees}} --max_features={{inputs.parameters.max_features}}\"\n        ]\n        env:\n        - name: MLFLOW_TRACKING_URI\n            value: \"{{workflow.parameters.mlflow-tracking-uri}}\"\n        - name: MLFLOW_EXPERIMENT_NAME\n            value: \"{{workflow.parameters.mlflow-experiment-name}}\"\n        - name: AWS_DEFAULT_REGION\n            value: us-east-1\n        - name: AWS_S3_ENDPOINT\n            value: minio.lab.sspcloud.fr\n\n1\n\nChanger pour votre entrepot de mod√®le\n\n2\n\nLe nom de l‚Äôexp√©rimentation MLFLow dont nous allons avoir besoin (on propose de continuer sur titanicml)\n\n3\n\nChanger l‚Äôimage Docker  ici\n\n\n\n\nObserver l‚ÄôUI d‚ÄôArgo Workflow dans vos services ouverts du SSPCloud. Vous devriez retrouver Figure¬†4 (a) dans celle-ci.\n\n\n\nNous pouvons maintenant passer √† la version Github. Celle-ci est optionnelle car elle vient surtout d√©montrer l‚Äôint√©r√™t d‚Äôavoir une chaine modulaire et la dissociation que cela permet entre l‚Äôenvironnement d‚Äôex√©cution et les autres environnements n√©cessaires √† notre chaine (notamment le stockage code et le logging).\n\n\n\n\n\n\nApplicationApplication 25 (optionnelle) : Github Actions comme ordonnanceur\n\n\n\nPour que Github sache o√π aller chercher MLFlow et S3 et comment s‚Äôy identifier, il va falloir lui donner un certain de variables d‚Äôenvironnement. Il est hors de question de mettre celles-ci dans le code. Heureusement, Github propose la possibilit√© de renseigner des secrets: nous allons utiliser ceux-ci.\n\nAller dans les param√®tres de votre projet GitOps et dans la section Secrets and variables\nVous allez avoir besoin de cr√©er les secrets suivants:\n\nMLFLOW_TRACKING_PASSWORD\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_SESSION_TOKEN\n\n\nLes valeurs √† renseigner sont √† r√©cup√©rer √† diff√©rents endroits:\n\nPour les secrets li√©s √† S3 (AWS_*), ceux-ci sont dans l‚Äôespace Mon compte du SSPCloud. Ils ont une dur√©e de validit√© limit√©e: si vous devez refaire tourner le code dans quelques jours, il faudra les mettre √† jour (ou passer par un compte de service comme indiqu√© pr√©c√©demment)\nLe mot de passe de MLFlow est dans le README de votre service, qui s‚Äôaffiche quand vous cliquez sur le bouton Ouvrir depuis la page Mes services\n\n\nReprendre ce mod√®le d‚Äôaction √† mettre dans votre d√©p√¥t GitOps (.github/workflows/train.yaml par exemple).\n\n\n\nMod√®le d‚Äôaction Github\n\nname: Titanic Model Training\n\non:\n  push:\n    branches:\n      - main\n  workflow_dispatch:\n\njobs:\n  start-pipeline:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Start Pipeline\n        run: echo \"Starting pipeline\"\n\n  train-model:\n    needs: start-pipeline\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        model-config:\n          - { n_trees: 10, max_features: \"log2\" }\n          - { n_trees: 20, max_features: \"sqrt\" }\n          - { n_trees: 20, max_features: \"log2\" }\n          - { n_trees: 50, max_features: \"sqrt\" }\n    container:\n1      image: ***/application:v0.0.4\n    env:\n      MLFLOW_TRACKING_URI: \"https://user-lgaliana-mlflow.user.lab.sspcloud.fr/\"\n      MLFLOW_EXPERIMENT_NAME: \"titanicml\"\n      MLFLOW_TRACKING_PASSWORD: \"${{ secrets.MLFLOW_TRACKING_PASSWORD }}\"\n      AWS_DEFAULT_REGION: \"us-east-1\"\n      AWS_S3_ENDPOINT: \"minio.lab.sspcloud.fr\"\n      AWS_ACCESS_KEY_ID: \"${{ secrets.AWS_ACCESS_KEY_ID }}\"\n      AWS_SECRET_ACCESS_KEY: \"${{ secrets.AWS_SECRET_ACCESS_KEY }}\"\n      AWS_SESSION_TOKEN: \"${{ secrets.AWS_SESSION_TOKEN }}\"\n\n    steps:\n      - name: Checkout Repository\n        uses: actions/checkout@v4\n        with:\n2            repository: 'ensae-reproductibilite/application'\n            ref: appli24\n\n      - name: Train Model\n        run: |\n          python3 train.py --n_trees=${{ matrix.model-config.n_trees }} --max_features=${{ matrix.model-config.max_features }}\n\n1\n\nMettre votre image ici. Si vous n‚Äôen avez pas, vous pouvez mettre linogaliana/application:v0.0.4\n\n2\n\nOn reprend le code applicatif de l‚Äôapplication pr√©c√©dente. Vous pouvez remplacer par votre d√©p√¥t et une r√©f√©rence adapt√©e si vous pr√©f√©rez\n\n\n\n\nPusher et observer l‚ÄôUI de Github depuis l‚Äôonglet Actions. Vous devriez retrouver Figure¬†4 (b) dans celle-ci."
  },
  {
    "objectID": "chapters/application.html#footnotes",
    "href": "chapters/application.html#footnotes",
    "title": "Application",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIl y a quelques diff√©rences entre le VSCode server mis √† disposition sur le SSPCloud et la version desktop sur laquelle s‚Äôappuient beaucoup de ressources. A quelques extensions pr√™ts (Data Wrangler, Copilot), les diff√©rences sont n√©anmoins minimes.‚Ü©Ô∏é\nL‚Äôexport dans un script .py a √©t√© fait directement depuis VSCode. Comme cela n‚Äôest pas vraiment l‚Äôobjet du cours, nous passons cette √©tape et fournissons directement le script expurg√© du texte interm√©diaire. Mais n‚Äôoubliez pas que cette d√©marche, fr√©quente quand on a d√©marr√© sur un notebook et qu‚Äôon d√©sire consolider en faisant la transition vers des scripts, n√©cessite d‚Äô√™tre attentif pour ne pas risquer de faire une erreur.‚Ü©Ô∏é\nIl est √©galement possible avec VSCode d‚Äôex√©cuter le script ligne √† ligne de mani√®re interactive ligne √† ligne (MAJ+ENTER). N√©anmoins, cela n√©cessite de s‚Äôassurer que le working directory de votre console interactive est le bon. Celle-ci se lance selon les param√®tres pr√©configur√©s de VSCode et les votres ne sont peut-√™tre pas les m√™mes que les notres. Vous pouvez changer le working directory dans le script en utilisant le package os mais peut-√™tre allez vous d√©couvrir ult√©rieurement qu‚Äôil y a de meilleures pratiques‚Ä¶‚Ü©Ô∏é\nEssayez de commit vos changements √† chaque √©tape de l‚Äôexercice, c‚Äôest une bonne habitude √† prendre.‚Ü©Ô∏é\nPendant longtemps, Black √©tait le formatter de r√©f√©rence en Python et on retrouve de nombreuses ressources sur internet qui l‚Äôutilisent. Ruff est une version √©volu√©e, plus rapide et plus complet, qui est tr√®s rapidement devenu un outil standard des utilisateurs de Python.‚Ü©Ô∏é\nIl est normal d‚Äôavoir des dossiers __pycache__ qui tra√Ænent en local : ils se cr√©ent automatiquement √† l‚Äôex√©cution d‚Äôun script en Python. N√©anmoins, il ne faut pas associer ces fichiers √† Git, voil√† pourquoi on les ajoute au .gitignore.‚Ü©Ô∏é\nNous proposons ici d‚Äôadopter le principe de la programmation fonctionnelle. Pour encore fiabiliser un processus, il serait possible d‚Äôadopter le paradigme de la programmation orient√©e objet (POO). Celle-ci est plus rebutante et demande plus de temps au d√©veloppeur. L‚Äôarbitrage co√ªt-avantage est n√©gatif pour notre exemple, nous proposons donc de nous en passer. N√©anmoins, pour une mise en production r√©elle d‚Äôun mod√®le, il peut √™tre utle de l‚Äôadopter car certains frameworks, √† commencer par les pipelines scikit, exigeront certaines classes et m√©thodes si vous d√©sirez brancher des objets ad hoc √† ceux-ci.‚Ü©Ô∏é\nAlors oui, c‚Äôest vrai, S3 se distingue d‚Äôun syst√®me de fichiers classiques comme on peut le lire dans certains posts √©nerv√©s sur la question (par exemple sur Reddit). Mais du point de vue de l‚Äôutilisateur Python plut√¥t que de l‚Äôarchitecte cloud, on va avoir assez peu de diff√©rence avec un syst√®me de fichier local. C‚Äôest pour le mieux, cela r√©duit la difficult√© √† rentrer dans cette technologie.‚Ü©Ô∏é\nLorsqu‚Äôon d√©veloppe du code qui finalement ne s‚Äôav√®re plus n√©cessaire, on a souvent un cas de conscience √† le supprimer et on pr√©f√®re le mettre de c√¥t√©. Au final, ce syndr√¥me de Diog√®ne est mauvais pour la p√©rennit√© du projet : on se retrouve √† devoir maintenir une base de code qui n‚Äôest, en pratique, pas utilis√©e. Ce n‚Äôest pas un probl√®me de supprimer un code ; si finalement celui-ci s‚Äôav√®re utile, on peut le retrouver gr√¢ce √† l‚Äôhistorique Git et les outils de recherche sur Github. Le package vulture est tr√®s pratique pour diagnostiquer les morceaux de code inutiles dans un projet.‚Ü©Ô∏é\nSi vous d√©sirez aussi contr√¥ler la version de Python, ce qui peut √™tre important dans une perspective de portabilit√©, vous pouvez ajouter une option, par exemple -p python3.10. N√©anmoins nous n‚Äôallons pas nous embarasser de cette nuance pour la suite car nous pourrons contr√¥ler la version de Python plus finement par le biais de Docker.‚Ü©Ô∏é\nL‚Äôoption -c pass√©e apr√®s la commande python permet d‚Äôindiquer √† Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu‚Äôon va directement lui fournir.‚Ü©Ô∏é\nL‚Äôoption -c pass√©e apr√®s la commande python permet d‚Äôindiquer √† Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu‚Äôon va directement lui fournir.‚Ü©Ô∏é\nPour comparer les deux listes, vous pouvez utiliser la fonctionnalit√© de split du terminal sur VSCode pour comparer les outputs de conda env export en les mettant en face √† face.‚Ü©Ô∏é\nL‚Äôoption -c pass√©e apr√®s la commande python permet d‚Äôindiquer √† Python que la commande ne se trouve pas dans un fichier mais sera dans le texte qu‚Äôon va directement lui fournir.‚Ü©Ô∏é\nIl est tout √† fait normal de ne pas parvenir √† cr√©er une action fonctionnelle du premier coup. N‚Äôh√©sitez pas √† pusher votre code apr√®s chaque question pour v√©rifier que vous parvenez bien √† r√©aliser chaque √©tape. Sinon vous risquez de devoir corriger bout par bout un fichier plus cons√©quent.‚Ü©Ô∏é\nIl existe une approche alternative pour faire des tests r√©guliers: les hooks Git. Il s‚Äôagit de r√®gles qui doivent √™tre satisfaites pour que le fichier puisse √™tre committ√©. Cela assure que chaque commit remplisse des crit√®res de qualit√© afin d‚Äô√©viter le probl√®me de la procrastination.\nLa documentation de pylint offre des explications suppl√©mentaires. Ici, nous allons adopter une approche moins ambitieuse en demandant √† notre action de faire ce travail d‚Äô√©valuation de la qualit√© de notre code‚Ü©Ô∏é\nPar cons√©quent, MLFLow b√©n√©ficie de l‚Äôinjection automatique des tokens pour pouvoir lire/√©crire sur S3. Ces jetons ont la m√™me dur√©e avant expiration que ceux de vos services interactifs VSCode. Il faut donc, par d√©faut, supprimer et rouvrir un service MLFLow r√©guli√®rement. La mani√®re d‚Äô√©viter cela est de cr√©er des service account sur https://minio-console.lab.sspcloud.fr/ et de les renseigner sur la page.‚Ü©Ô∏é\nIl existe d‚Äôautres outils d‚Äôordonnancement de pipelines tr√®s utilis√©s dans l‚Äôindustrie, notamment Airflow.\nCe dernier est plus utilis√©, en pratique, qu‚ÄôArgo Workflow mais, m√™me s‚Äôil est disponible sur le SSPCloud aussi, est moins pens√© autour de Kubernetes que l‚Äôest Argo.\nPour mieux comprendre la diff√©rence entre Argo et Airflow, la philosphie diff√©rente de ces deux outils et leurs avantages comparatifs, cette courte vid√©o est int√©ressante:\n\n‚Ü©Ô∏é\nIl serait bien s√ªr possible d‚Äôaller beaucoup plus loin dans la d√©finition du pipeline.\nPar exemple, il est possible, si le framework utilis√© pour la mod√©lisation n‚Äôint√®gre pas la notion de pipeline au niveau de Python de faire ceci au niveau d‚ÄôArgo. Cela donnerait un pipeline prenant cette forme:\n\nN√©anmoins, ici, nous utilisons Scikit qui permet d‚Äôint√©grer le preprocessing comme une √©tape de mod√©lisation. Nous n‚Äôavons donc pas d‚Äôint√©r√™t √† d√©finir ceci comme une t√¢che autonome, raison pour laquelle notre pipeline appara√Æt plus simple.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/big-data.html",
    "href": "chapters/big-data.html",
    "title": "Traitement des donn√©es volumineuses",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran.\nThe big data phenomenon is now well-documented: the generation and collection of data from a multitude of sources (IoT sensors, daily interactions on social media, online transactions, mobile devices, etc.) drastically increases the volume of data available for analysis. There are many reasons to focus on such data in data science projects: high availability, greater granularity of observed phenomena, and large datasets needed for training increasingly data-hungry models (like LLMs), among others.\nBig data is often defined as a situation where the data volume is so large that it can no longer be processed on a single machine. This relative definition may seem reductive but has the advantage of highlighting that a data source, depending on the time and environment, may require different skill sets. Moving to big data is not just a matter of scale‚Äîit often involves a fundamental shift in the computing infrastructure, with strong implications for the expertise required and the scalability of the data pipelines.\nProcessing such data introduces new challenges, often summarized as the ‚Äúthree Vs‚Äù, a widely accepted way to characterize these data sources (Sagiroglu and Sinanc 2013):\nWhen considering putting a data science project based on large datasets into production, adopting good development practices is not just recommended‚Äîit is essential. Massive datasets introduce significant complexity at every stage of the data science project lifecycle, from collection and storage to processing and analysis. Systems must be designed not only to handle the current data load but also to be scalable for future growth. Good practices enable this scalability by promoting modular architectures, reusable code, and technologies suited for large-scale data processing.\nTo address these challenges, technology choices are crucial. In this course, we will focus on three main aspects to guide those choices: computing infrastructure, data formats suited for high volumes, and frameworks (software solutions and their ecosystems) used for data processing."
  },
  {
    "objectID": "chapters/big-data.html#evolution-of-data-infrastructures",
    "href": "chapters/big-data.html#evolution-of-data-infrastructures",
    "title": "Traitement des donn√©es volumineuses",
    "section": "Evolution of Data Infrastructures",
    "text": "Evolution of Data Infrastructures\nHistorically, data has been stored in databases, systems designed to store and organize information. These systems emerged in the 1950s and saw significant growth with relational databases in the 1980s. This technology proved especially effective for organizing corporate ‚Äúbusiness‚Äù data and served as the foundation of data warehouses, long considered the standard for data storage infrastructure (Chaudhuri and Dayal 1997). While technical implementations can vary, their core idea is simple: data from various heterogeneous sources is integrated into a relational database system according to business rules via ETL (extract-transform-load) processes, making it accessible for a range of uses (statistical analysis, reporting, etc.) using the standardized SQL language (Figure¬†1).\n\n\n\n\n\n\nFigure¬†1: Architecture of a data warehouse. Source: airbyte.com\n\n\n\nIn the early 2000s, the growing adoption of big data practices exposed the limitations of traditional data warehouses. On one hand, data increasingly came in diverse formats (structured, semi-structured, and unstructured), often evolving as new features were added to data collection platforms. These dynamic, heterogeneous formats fit poorly with the ordered nature of data warehouses, which require schemas to be defined a priori. To address this, data lakes were developed‚Äîsystems that allow for the collection and storage of large volumes of diverse data types (Figure¬†2).\n\n\n\n\n\n\nFigure¬†2: Architecture of a data lake. Source: cartelis.com\n\n\n\nAdditionally, the enormous size of these data sets made it increasingly difficult to process them on a single machine. This is when Google introduced the MapReduce paradigm (Ghemawat, Gobioff, and Leung 2003; Dean and Ghemawat 2008), which laid the foundation for a new generation of distributed data processing systems. Traditional infrastructures used vertical scalability‚Äîadding more powerful or additional resources to a single machine. However, this quickly became expensive and hit hardware limits. Distributed architectures use horizontal scalability: by using many parallel, lower-powered servers and adapting algorithms to this distributed logic, massive datasets can be processed using commodity hardware. This led to the emergence of the Hadoop ecosystem, combining complementary technologies: a data lake (HDFS - Hadoop Distributed File System), a distributed processing engine (MapReduce), and tools for data integration and transformation (Figure¬†3). This ecosystem expanded with tools like Hive (which converts SQL queries into distributed MapReduce tasks) and Spark (which overcomes certain technical limitations of MapReduce and provides APIs in multiple languages, including Java, Scala, and Python). Hadoop‚Äôs success was profound‚Äîit enabled organizations to process petabyte-scale datasets in real-time using widely accessible programming languages.\nThis technological shift fueled the big data revolution, enabling new types of questions to be answered using vast datasets. Philosophically, it marked a shift from collecting only the data needed for known purposes, to storing as much data as possible and evaluating its usefulness later during analysis. This approach is typical of NoSQL environments (‚ÄúNot only SQL‚Äù), where data is stored at each transactional event but in more flexible formats than traditional databases. JSON, derived from web transactions, is especially prominent. Depending on the structure of the data, different tools are used to query it: ElasticSearch or MongoDB for text data, Spark for tabular data, and so on. All these tools share a common trait: they are highly horizontally scalable, making them ideal for server farms.\n\n\n\n\n\n\nFigure¬†3: Schematic of a Hadoop architecture. Large datasets are split into blocks, and both storage and processing are distributed across multiple compute nodes. Algorithms are adapted to this distributed setup via MapReduce: first, a ‚Äúmap‚Äù function is applied to each block (e.g., count word frequencies), then a ‚Äúreduce‚Äù step aggregates these results (e.g., compute total frequencies across blocks). Output data is often much smaller than input data and can be brought back locally for further tasks like visualization. Source: glennklockwood.com\n\n\n\nBy the late 2010s, Hadoop architectures began to decline in popularity. In traditional Hadoop setups, storage and compute are co-located by design: data segments are processed on the servers where they are stored, avoiding network traffic. This architecture scales linearly, increasing both storage and compute capacity‚Äîeven if only one is needed. In a provocative article titled ‚ÄúBig Data is Dead‚Äù (Tigani 2023), Jordan Tigani (one of the founding engineers of Google BigQuery) argues that this model no longer suits modern data workloads. First, he explains, ‚Äúin practice, data size grows much faster than compute needs.‚Äù Most use cases don‚Äôt require querying all stored data‚Äîjust recent subsets or specific columns. Second, ‚Äúthe big data frontier keeps receding‚Äù: thanks to more powerful and cheaper servers, fewer workloads require distributed systems (Figure¬†4). Additionally, new storage formats (see Section¬†2) make data handling more efficient. As a result, properly decoupling storage from compute often leads to simpler, more efficient infrastructures.\n\n\n\n\n\n\nFigure¬†4: ‚ÄúThe big data frontier keeps receding‚Äù: the share of data workloads that cannot be handled by a single machine has steadily declined. Source: motherduck.com"
  },
  {
    "objectID": "chapters/big-data.html#the-role-of-cloud-technologies",
    "href": "chapters/big-data.html#the-role-of-cloud-technologies",
    "title": "Traitement des donn√©es volumineuses",
    "section": "The Role of Cloud Technologies",
    "text": "The Role of Cloud Technologies\nBuilding on Tigani‚Äôs analysis, we observe a growing shift toward more flexible, loosely coupled architectures. The rise of cloud technologies has been pivotal in this transition, for several reasons. Technically, network latency is no longer the bottleneck it was during Hadoop‚Äôs heyday, making the co-location of compute and storage less necessary. In terms of usage, it‚Äôs not just that data volumes are growing‚Äîit‚Äôs also the diversity of data and processing needs that is expanding. Modern infrastructures must support various data formats (from structured tables to unstructured media) and a wide range of compute requirements‚Äîfrom parallel data processing to deep learning on GPUs (Li et al. 2020).\nTwo cloud-native technologies have become central to this modern flexibility: containerization and object storage. Containerization ensures reproducibility and portability‚Äîcrucial for production environments‚Äîand will be discussed in the Portability and Deployment chapters. In this section, we focus on object storage, the default standard in modern data infrastructures.\nSince containers are stateless by nature, a persistent storage layer is needed to store input and output data across computations (Figure¬†5). In container-based infrastructures, object storage has become dominant‚Äîpopularized by Amazon‚Äôs S3 (Simple Storage Service) (Mesnier, Ganger, and Riedel 2003; Samundiswary and Dongre 2017). To understand its popularity, it‚Äôs helpful to contrast object storage with other storage types.\nThere are three main storage models: file systems, block storage, and object storage (Figure¬†5). File systems organize data in a hierarchical structure‚Äîlike a traditional desktop environment‚Äîbut they don‚Äôt scale well and require manual access management. Block storage, like that on hard drives, offers fast low-latency access‚Äîideal for databases‚Äîbut also struggles with scalability and cost. Object storage, on the other hand, breaks data into ‚Äúobjects‚Äù stored in a flat namespace and assigned unique IDs and metadata. It removes the need for hierarchical structures, lowering storage costs.\n\n\n\n\n\n\nFigure¬†5: Comparison of storage types. Source: bytebytego.com\n\n\n\nObject storage‚Äôs characteristics make it ideal for containerized data science infrastructures. It‚Äôs highly scalable, supports large files, and works well with distributed systems. It also enhances user autonomy by exposing data via APIs like Amazon‚Äôs S3, allowing direct interaction from code (R, Python, etc.) and fine-grained access control via tokens. Most importantly, object storage supports decoupled architectures, where compute and storage are independent and remotely accessible. This improves flexibility and efficiency.\n\n\n\n\n\n\nFigure¬†6: In container-based infrastructure (which is stateless by nature), object storage provides the persistence layer. MinIO is an open-source object storage solution that integrates natively with Kubernetes and supports the S3 API‚Äînow the industry standard‚Äîensuring compatibility across environments. Source: lemondeinformatique.fr"
  },
  {
    "objectID": "chapters/big-data.html#application-3-1",
    "href": "chapters/big-data.html#application-3-1",
    "title": "Traitement des donn√©es volumineuses",
    "section": "Application 3",
    "text": "Application 3\n\n\n\n\n\n\nApplicationPart 3a: What if we filter rows?\n\n\n\n\n\nAdd a row filtering step in our queries:\n\nWith DuckDB, modify the query with WHERE DEPT IN ('18', '28', '36')\nWith Arrow, modify the to_table step as follows: dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n\n\n\nCorrection de cet exercice\nimport pyarrow.dataset as ds\nimport pyarrow.compute as pc\nimport duckdb\n\n@measure_performance\ndef summarize_filter_parquet_arrow(*args, **kwargs):\n\n    dataset = ds.dataset(\"data/RPindividus.parquet\", format=\"parquet\")\n    table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n    grouped_table = (\n        table\n        .group_by([\"AGED\", \"DEPT\"])\n        .aggregate([(\"IPONDI\", \"sum\")])\n        .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n        .to_pandas()\n    )\n\n    return (\n        grouped_table\n    )\n\n@measure_performance\ndef summarize_filter_parquet_duckdb(*args, **kwargs):\n    con = duckdb.connect(\":memory:\")\n    query = \"\"\"\n    FROM read_parquet('data/RPindividus_24.parquet')\n    SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n    WHERE DEPT IN ('11','31','34')\n    GROUP BY AGED, DEPT\n    \"\"\"\n\n    return (con.sql(query).to_df())\n\n\nsummarize_filter_parquet_arrow()\nsummarize_filter_parquet_duckdb()\n\n\n\n\n\n‚ùìÔ∏è Why don‚Äôt we save time with row filters (or even lose time), unlike with column filters?\nData is not organized in row blocks the way it is in column blocks. Fortunately, there‚Äôs a way around this: partitioning!\n\n\n\n\n\n\nApplicationPart 3: Partitioned Parquet\n\n\n\n\n\nLazy evaluation and Arrow optimizations already bring considerable performance gains. But we can do even better! When we know that data will regularly be filtered based on a specific variable, it is a good idea to partition the Parquet file by that variable.\n\nBrowse the documentation for pyarrow.parquet.write_to_dataset to understand how to specify a partitioning key when writing a Parquet file. Several methods are possible.\nImport the full census individual table from \"data/RPindividus.parquet\" using pyarrow.dataset.dataset and export it as a partitioned table to \"data/RPindividus_partitionne.parquet\", partitioned by region (REGION) and department (DEPT).\nExamine the file tree structure of the exported table to see how the partitioning was applied.\nModify the import, filtering, and aggregation functions using Arrow or DuckDB to now use the partitioned Parquet file. Compare this to using the non-partitioned file.\n\n\n\nAnswer to question 2 (writing the partitioned Parquet)\nimport pyarrow.parquet as pq\ndataset = ds.dataset(\n    \"data/RPindividus.parquet\", format=\"parquet\"\n).to_table()\n\npq.write_to_dataset(\n    dataset,\n    root_path=\"data/RPindividus_partitionne\",\n    partition_cols=[\"REGION\", \"DEPT\"]\n)\n\n\n\n\nCorrection de la question 4 (lecture du Parquet partitionn√©)\nimport pyarrow.dataset as ds\nimport pyarrow.compute as pc\nimport duckdb\n\n@measure_performance\ndef summarize_filter_parquet_partitioned_arrow(*args, **kwargs):\n\n    dataset = ds.dataset(\"data/RPindividus_partitionne/\", partitioning=\"hive\")\n    table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n\n    grouped_table = (\n        table\n        .group_by([\"AGED\", \"DEPT\"])\n        .aggregate([(\"IPONDI\", \"sum\")])\n        .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n        .to_pandas()\n    )\n\n    return (\n        grouped_table\n    )\n\n@measure_performance\ndef summarize_filter_parquet_complete_arrow(*args, **kwargs):\n\n    dataset = ds.dataset(\"data/RPindividus.parquet\")\n    table = dataset.to_table(filter=pc.field(\"DEPT\").isin(['18', '28', '36']))\n\n    grouped_table = (\n        table\n        .group_by([\"AGED\", \"DEPT\"])\n        .aggregate([(\"IPONDI\", \"sum\")])\n        .rename_columns([\"AGED\", \"DEPT\", \"n_indiv\"])\n        .to_pandas()\n    )\n\n    return (\n        grouped_table\n    )\n\n\n@measure_performance\ndef summarize_filter_parquet_complete_duckdb(*args, **kwargs):\n    con = duckdb.connect(\":memory:\")\n    query = \"\"\"\n    FROM read_parquet('data/RPindividus.parquet')\n    SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n    WHERE DEPT IN ('11','31','34')\n    GROUP BY AGED, DEPT\n    \"\"\"\n\n    return (con.sql(query).to_df())\n\n\n@measure_performance\ndef summarize_filter_parquet_partitioned_duckdb(*args, **kwargs):\n    con = duckdb.connect(\":memory:\")\n    query = \"\"\"\n    FROM read_parquet('data/RPindividus_partitionne/**/*.parquet', hive_partitioning = True)\n    SELECT AGED, DEPT, SUM(IPONDI) AS n_indiv\n    WHERE DEPT IN ('11','31','34')\n    GROUP BY AGED, DEPT\n    \"\"\"\n\n    return (con.sql(query).to_df())\n\n\nsummarize_filter_parquet_complete_arrow()\nsummarize_filter_parquet_partitioned_arrow()\nsummarize_filter_parquet_complete_duckdb()\nsummarize_filter_parquet_partitioned_duckdb()\n\n\n\n\n\n‚ùìÔ∏è When making data available in Parquet format, how should you choose the partitioning key(s)? What limitation should be kept in mind?"
  },
  {
    "objectID": "chapters/big-data.html#to-go-further",
    "href": "chapters/big-data.html#to-go-further",
    "title": "Traitement des donn√©es volumineuses",
    "section": "To go further",
    "text": "To go further\n\nThe training on good practices with R and Git developed by Insee, with content very similar to what‚Äôs presented in this chapter.\nA workshop on the Parquet format and DuckDB ecosystem for EHESS, with R and Python examples using the same data source as this application.\nThe getting started guide for census data in Parquet format with examples using DuckDB in WASM (directly in the browser, without R or Python installation)."
  },
  {
    "objectID": "chapters/deployment.html",
    "href": "chapters/deployment.html",
    "title": "D√©ploiement",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran.\n\n\n\n\n\n\n\nPage en construction.\n\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "chapters/galerie.html",
    "href": "chapters/galerie.html",
    "title": "Galerie d‚Äôexemples",
    "section": "",
    "text": "Une galerie d‚Äôexemple de projets √† venir\n\n\n\n\n\n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            Mod√®le de carte\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            PrimePredict\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n  \n     \n      \n        \n          \n            \n          \n         \n        \n          \n            ResultAthle\n            \n\n            \n              \n            \n\n            \n            \n              \n                \n                \n              \n            \n            \n            \n              \n                \n                  Github \n                \n              \n              \n                \n                  Website \n                \n                          \n            \n          \n        \n      \n     \n  \n\n\n\nNo matching items\n\n\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "chapters/introduction.html",
    "href": "chapters/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "See slides (in French) or click here"
  },
  {
    "objectID": "chapters/introduction.html#origin",
    "href": "chapters/introduction.html#origin",
    "title": "Introduction",
    "section": "Origin",
    "text": "Origin\nThe notion of ‚Äúbest practices‚Äù as used in this course originates from the software development community. It emerged in response to several observations:\n\n‚ÄúCode is read much more often than it is written‚Äù (Guido Van Rossum);\nMaintaining code often requires (much) more effort than writing it initially;\nThe person maintaining the codebase is likely not the one who wrote it.\n\nIn light of these realities, the developer community has conventionally agreed on an informal set of rules recognized as producing more reliable, scalable, and maintainable software over time. Like language conventions, some may seem arbitrary‚Äîbut they support a critical goal: enabling code to be shared and communicated effectively. This may seem secondary at first, but it‚Äôs a key factor in the success of open source languages, which thrive on shared experience and collaboration.\n\n\n\n\n\n\nNoteThe 12 Factor App\n\n\n\n\n\nRecently, as software has evolved toward cloud-based web applications, many of these best practices were formalized in a manifesto known as the 12 Factor App. The rise of the cloud‚Äîi.e., standardized infrastructures external to traditional in-house data systems‚Äîmakes adopting good practices more crucial than ever."
  },
  {
    "objectID": "chapters/introduction.html#why-care-about-best-practices",
    "href": "chapters/introduction.html#why-care-about-best-practices",
    "title": "Introduction",
    "section": "Why Care About Best Practices?",
    "text": "Why Care About Best Practices?\n\nWhy should this matter to a data scientist, whose job is to derive insights from data‚Äînot build applications?\n\nDue to the rapid growth of data science and the increasing size of typical projects, the data scientist‚Äôs work is becoming more similar in some ways to that of a developer:\n\nData science projects involve intensive coding;\nCollaboration is required on large-scale projects;\nMassive datasets require working on technically complex big data infrastructures;\nThe data scientist must collaborate with technical roles to deploy models and make them accessible to users.\n\nThus, it makes sense for modern data scientists to take interest in the best practices adopted by developers. Naturally, these need to be tailored to data-centered projects. The upside is significant: projects that adopt best practices are much cheaper to evolve‚Äîmaking them more competitive in the ever-changing data science ecosystem, where tools, data, and user expectations constantly shift."
  },
  {
    "objectID": "chapters/introduction.html#a-continuum-of-best-practices",
    "href": "chapters/introduction.html#a-continuum-of-best-practices",
    "title": "Introduction",
    "section": "A Continuum of Best Practices",
    "text": "A Continuum of Best Practices\nBest practices should not be viewed in a binary way: it‚Äôs not that some projects follow them and others don‚Äôt. Best practices come with a cost, which should not be overlooked‚Äîeven though they prevent future costs, especially in maintenance. It‚Äôs better to view best practices as a spectrum, and position your project on it based on cost-benefit analysis, particularly in terms of improving reproducibility.\nThe appropriate threshold depends on trade-offs specific to your project:\n\nAmbitions: Will the project grow or evolve? Is it meant to become collaborative‚Äîwithin a team or as open source? Are the outputs intended for public release?\nResources: What human resources are available? For open-source work, is there a potential contributor community?\nConstraints: Are there tight deadlines? Specific quality requirements? Is deployment expected? Are there major security concerns?\nTarget audience: Who will consume the project‚Äôs data products? What‚Äôs their technical level, and how much time will they spend engaging with your work?\n\nWe are not suggesting that every data science project must follow all the best practices covered in this course. That said, we strongly believe every data scientist should consider these questions and continuously improve their practices.\nIn particular, we believe it‚Äôs possible to define a core set‚Äîi.e., a minimal set of best practices that provide more value than they cost to implement. Here‚Äôs our suggestion for such a baseline:\n\nUse dedicated tools to check code quality (see Code Quality);\nUse a standardized project structure with ready-made templates (see Project Architecture);\nUse Git to version your code, whether or not you‚Äôre working with others (see Version Control and Collaboration with Git);\nManage project dependencies with virtual environments (see Portability).\n\nBeyond this minimal baseline, decisions should weigh costs and benefits. But adopting this foundational level of reproducibility will make further progress much easier as your project grows.\nLet‚Äôs now look at the core principles promoted by this course and how the content is logically structured."
  },
  {
    "objectID": "chapters/introduction.html#code-as-a-communication-tool",
    "href": "chapters/introduction.html#code-as-a-communication-tool",
    "title": "Introduction",
    "section": "Code as a Communication Tool",
    "text": "Code as a Communication Tool\nThe first best practice to adopt is to view code as a communication tool, not just a functional one. Code doesn‚Äôt exist solely to perform a task‚Äîit‚Äôs meant to be shared, reused, and maintained, whether in a team or an open-source context.\nTo support this communication, conventions have been developed regarding code quality and project structure. These are covered in the chapters Code Quality and Project Architecture.\nFor the same reasons, applying version control principles is essential. These provide continuous documentation of the project, which greatly improves its reusability and maintainability. We revisit the use of Git in the chapter Version Control and Collaborative Work with Git."
  },
  {
    "objectID": "chapters/introduction.html#working-collaboratively",
    "href": "chapters/introduction.html#working-collaboratively",
    "title": "Introduction",
    "section": "Working Collaboratively",
    "text": "Working Collaboratively\nRegardless of context, data scientists typically work in team-based projects. This requires defining a work organization and using tools that enable secure, efficient collaboration.\nWe present a modern way to collaborate using Git and GitHub in the reminder chapter Version Control and Collaborative Work with Git. Later chapters will build on this collaborative approach and refine it using the DevOps methodology4."
  },
  {
    "objectID": "chapters/introduction.html#maximizing-reproducibility",
    "href": "chapters/introduction.html#maximizing-reproducibility",
    "title": "Introduction",
    "section": "Maximizing Reproducibility",
    "text": "Maximizing Reproducibility\nThe third pillar of best practices in this course is reproducibility.\nA project is reproducible when the same code and data can be used to reproduce the same results. It‚Äôs important to distinguish this from replicability. Replicability is a scientific concept‚Äîmeaning the same experimental process yields similar results on different datasets. Reproducibility is a technical concept: it doesn‚Äôt guarantee scientific validity but ensures that the protocol is specified and shared in a way that allows others to reproduce the results.\nReproducibility is the guiding theme of this course: all concepts covered in the chapters contribute to it. Producing code and projects that follow community conventions and using version control contribute to making code more readable and documented‚Äîand therefore reproducible.\nHowever, achieving full reproducibility requires going further‚Äîby considering the concept of an execution environment. Code doesn‚Äôt run in a vacuum; it runs in an environment (e.g., personal computer, server), and those environments can differ greatly (OS, installed libraries, security policies, etc.). That‚Äôs why we must consider code portability‚Äîi.e., its ability to run as expected across different environments, which we explore in the dedicated chapter."
  },
  {
    "objectID": "chapters/introduction.html#facilitating-production-deployment",
    "href": "chapters/introduction.html#facilitating-production-deployment",
    "title": "Introduction",
    "section": "Facilitating Production Deployment",
    "text": "Facilitating Production Deployment\nFor a data science project to ultimately create value, it must be deployed in a usable form that reaches its audience. This implies two things:\n\nChoosing the right distribution format, i.e., one that best highlights the results to the intended users;\nTransitioning the project from its development environment to a production infrastructure, i.e., one that allows the project output to be robustly deployed and accessible on demand.\n\nIn the chapter Deploy and Showcase Your Data Science Project, we propose ways to address both needs. We present common output formats (API, app, automated report, website) that help make data science projects accessible, and the modern tools used to produce them.\nWe then explain the essential concepts of production infrastructure and demonstrate them with examples of deployments in a modern cloud environment.\nThis is, in a way, the reward for following best practices: once you‚Äôve put in the effort to write quality code, properly version it, and make it portable, deploying your project becomes significantly easier."
  },
  {
    "objectID": "chapters/introduction.html#opening-the-door-to-industrialization",
    "href": "chapters/introduction.html#opening-the-door-to-industrialization",
    "title": "Introduction",
    "section": "Opening the Door to Industrialization",
    "text": "Opening the Door to Industrialization\nBy simplifying a project‚Äôs structure, you make it easier to scale. In data science, this may take the form of industrializing model training to select the ‚Äúbest‚Äù model from a much broader set‚Äîfar beyond what an ad hoc approach would allow.\nHowever, every model learns from past data, and a model that works today may no longer be valid tomorrow. To account for this ever-changing reality, we will explore key principles of MLOps. Though the term is a buzzword, it represents a meaningful set of practices for data scientists, covered in the dedicated chapter."
  },
  {
    "objectID": "chapters/introduction.html#supplementary-chapters",
    "href": "chapters/introduction.html#supplementary-chapters",
    "title": "Introduction",
    "section": "Supplementary Chapters",
    "text": "Supplementary Chapters\nSeveral tools presented in this course, such as Git and Docker, require terminal usage and a basic understanding of how Linux systems work. In the chapter Demystifying the Linux Terminal for Autonomy, we cover the essential Linux knowledge a data scientist needs to deploy projects independently and apply development best practices."
  },
  {
    "objectID": "chapters/introduction.html#teaching-approach",
    "href": "chapters/introduction.html#teaching-approach",
    "title": "Introduction",
    "section": "Teaching Approach",
    "text": "Teaching Approach\nThe guiding principle of this course is that only practice‚Äîespecially hands-on experience with real-world problems‚Äîcan effectively develop understanding of computing concepts. As such, a large part of the course will consist of applying key ideas to concrete use cases. Each chapter will conclude with applications rooted in realistic data science problems.\nA running example illustrates how a reproducible project evolves by progressively applying the practices discussed throughout the course.\nFor the course evaluation, students will be asked to take a personal project‚Äîideally already completed‚Äîand apply as many of the best practices introduced here as possible."
  },
  {
    "objectID": "chapters/introduction.html#programming-languages",
    "href": "chapters/introduction.html#programming-languages",
    "title": "Introduction",
    "section": "Programming Languages",
    "text": "Programming Languages\nThe principles presented in this course are mostly language-agnostic.\nThis is not just an editorial decision‚Äîwe believe it‚Äôs central to the topic of best practices. Too often, language differences between development phases (e.g., R or Python) and production phases (e.g., Java) create artificial barriers that limit a data science project‚Äôs potential impact.\nBy contrast, when the different teams involved in a project‚Äôs lifecycle adopt a shared set of best practices, they also develop a shared vocabulary‚Äîgreatly easing the deployment process.\nA compelling example is containerization: if the data scientist provides a Docker image as the output of their development work, and a data engineer handles its deployment, then the underlying programming language becomes largely irrelevant. While simplistic, this example captures the essence of how best practices enhance communication within a project.\nExamples in this course will primarily use Python. The main reason is that despite its shortcomings, Python is widely taught in both data science and computer science programs. It serves as a bridge between data users and developers‚Äîtwo essential roles in production workflows.\nThat said, the same principles can be applied with other languages, and we strongly encourage students to practice this transfer of skills."
  },
  {
    "objectID": "chapters/introduction.html#execution-environment",
    "href": "chapters/introduction.html#execution-environment",
    "title": "Introduction",
    "section": "Execution Environment",
    "text": "Execution Environment\nLike programming language, the principles in this course are agnostic to the infrastructure used to run the examples. It is not only possible but desirable to apply best practices to both solo projects on a personal computer and collaborative projects intended for production deployment.\nThat said, we have chosen the SSP Cloud platform as our reference environment throughout the course. Developed at Insee and available to students at statistical schools, it offers several advantages:\n\nStandardized development environment: SSP Cloud servers use a uniform configuration‚Äîspecifically, the Debian Linux distribution‚Äîwhich ensures reproducibility across course examples;\nBuilt on a Kubernetes cluster, SSP Cloud offers robust infrastructure for automated deployment of potentially data-intensive applications‚Äîmaking it possible to simulate a true production environment;\nSSP Cloud follows modern data science infrastructure standards, enabling learners to internalize best practices organically:\n\nServices are run in containers configured via Docker images, which ensures strong reproducibility of deployments‚Äîat the cost of some initial complexity during development;\nThe platform is based on a cloud-native architecture, composed of modular software building blocks. This encourages strict separation of code, data, configuration, and execution environment‚Äîa major principle of good practice that will be revisited throughout the course.\n\n\nTo learn more about this platform, see this page."
  },
  {
    "objectID": "chapters/introduction.html#additional-resources",
    "href": "chapters/introduction.html#additional-resources",
    "title": "Introduction",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMIT‚Äôs Missing Semester"
  },
  {
    "objectID": "chapters/introduction.html#footnotes",
    "href": "chapters/introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou‚Äôre probably most familiar with the Jupyter Notebook. While very convenient for writing exploratory code or sharing annotated code, we‚Äôll see its limitations in collaborative or large-scale projects.‚Ü©Ô∏é\nWe will define this central concept more formally later. For now, you can think of it as an always-on environment designed to deliver data products‚Äîoften in the form of a production server or a computing cluster that must remain continuously available.‚Ü©Ô∏é\nThe strong entanglement of best practices, reproducibility, and deployment actually made it hard for us to settle on a course title. Some names on our shortlist were ‚ÄúBest Practices in Data Science‚Äù or ‚ÄúBest Practices for Reproducibility in Data Science‚Äù. However, since best practices are a means and deployment is the end, we decided to emphasize the latter.‚Ü©Ô∏é\nA methodology focused on automating and integrating design and delivery workflows prior to deployment. Like best practices, this approach originated in software development but has become essential for data scientists.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/mlops.html",
    "href": "chapters/mlops.html",
    "title": "Introduction aux enjeux du MLOps",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran.\nDans les chapitres pr√©c√©dents, nous avons vu qu‚Äôune majorit√© des projets data-driven restaient au stade de l‚Äôexp√©rimentation, et qu‚Äôune des raisons pour expliquer ce ph√©nom√®ne √©tait l‚Äôexistence de frictions emp√™chant l‚Äôam√©lioration continue des projets. Dans le cadre des projets bas√©s sur des mod√®les de machine learning, cette probl√©matique devient encore plus cruciale : en suppl√©ment des enjeux sur le cycle de vie de la donn√©e intervient la dimension suppl√©mentaire du cycle de vie des mod√®les. Parmi les principaux enjeux, une question souvent √©lud√©e dans les enseignements ou les nombreuses ressources en ligne sur le machine learning est la probl√©matique des r√©-entra√Ænements p√©riodiques, guid√©s par l‚Äôutilisation faite des mod√®les et les retours des utilisateurs, afin de maintenir √† jour la base de connaissance des mod√®les et ainsi garantir leur pouvoir pr√©dictif. Ce sujet du r√©-entra√Ænement des mod√®les rend les aller-retours entre les phases d‚Äôexp√©rimentation et de production n√©cessairement fr√©quents. Pour faciliter la mise en place de pipelines favorisant ces boucles de r√©troaction, une nouvelle approche a √©merg√© : le MLOps, qui vise l√† encore √† mobiliser les concepts et outils issus de l‚Äôapproche DevOps tout en les adaptant au contexte et aux sp√©cificit√©s des projets de machine learning."
  },
  {
    "objectID": "chapters/mlops.html#du-devops-au-mlops",
    "href": "chapters/mlops.html#du-devops-au-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Du DevOps au MLOps",
    "text": "Du DevOps au MLOps\nL‚Äôapproche MLOps s‚Äôest construite sur les bases de l‚Äôapproche DevOps. En cela, on peut consid√©rer qu‚Äôil s‚Äôagit simplement d‚Äôune extension de l‚Äôapproche DevOps, d√©velopp√©e pour r√©pondre aux d√©fis sp√©cifiques li√©s √† la gestion du cycle de vie des mod√®les de machine learning. Le MLOps int√®gre les principes de collaboration et d‚Äôautomatisation propres au DevOps, mais prend √©galement en compte tous les aspects li√©s aux donn√©es et aux mod√®les de machine learning.\n\n\n\n\n\n\n\nA mettre en regard √† la boucle du DevOps\n\n\n\nLe MLOps implique l‚Äôautomatisation des t√¢ches telles que la gestion des donn√©es, le suivi des versions des mod√®les, leurs d√©ploiements, ainsi que l‚Äô√©valuation continue de la performance des mod√®les en production. De la m√™me mani√®re que le DevOps, le MLOps met l‚Äôaccent sur la collaboration √©troite entre les √©quipes de d√©veloppement et d‚Äôadministration syst√®me d‚Äôune part, ainsi que les √©quipes de data science d‚Äôautre part. Cette collaboration est cl√© pour garantir une communication efficace tout au long du cycle de vie du mod√®le de machine learning et fludifier le passage entre les √©tapes d‚Äôexp√©rimentation et de passage en production."
  },
  {
    "objectID": "chapters/mlops.html#principes-du-mlops",
    "href": "chapters/mlops.html#principes-du-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Principes du MLOps",
    "text": "Principes du MLOps\nPuisque le MLOps est ainsi une extension des principes du DevOps aux enjeux du machine learning, les principes g√©n√©raux sont les m√™mes que ceux √©voqu√©s pr√©c√©demment mais ceux-ci s‚Äôadaptent √† la probl√©matique de la gestion du cycle de vie d‚Äôun mod√®le:\n\nla reproductibilit√© : les r√©sultats de chaque exp√©rimentation, fructueuse comme infructueuse, doivent pouvoir √™tre reproduits sans co√ªt. Cela implique d‚Äôabord une certaine rigueur dans la gestion des packages, la gestion des environnements, la gestion des librairies syst√®me, le contr√¥le de version du code, etc.\nle contr√¥le de version: au-del√† du simple suivi des versions du code, pour reproduire de mani√®re identique les r√©sultats d‚Äôun code c‚Äôest l‚Äôensemble des inputs et param√®tres influen√ßant l‚Äôentra√Ænement d‚Äôun mod√®le (donn√©es d‚Äôentra√Ænement, hyper-param√®tres, etc.) qui doivent √™tre versionn√©es avec le mod√®le ;\nl‚Äôautomatisation : afin de favoriser les boucles r√©troactives d‚Äôam√©lioration continue, le cycle de vie du mod√®le (tests, build, validation, d√©ploiement) doit √™tre automatis√© au maximum. Les outils issus de l‚Äôapproche DevOps, en particulier l‚Äôint√©gration et d√©ploiement continus (CI/CD), doivent √™tre mobilis√©s ;\nla collaboration : valoriser une culture de travail collaborative autour des projets de ML, dans laquelle la communication au sein des √©quipes doit permettre de r√©duire le travail en silos et b√©n√©ficier des expertises des diff√©rents m√©tiers parti prenantes d‚Äôun mod√®le (analystes, data engineers, devs..). Sur le plan technique, les outils MLOps utilis√©s doivent favoriser le travail collaboratif sur les donn√©es, le mod√®le et le code utilis√©s par le projet ;\nl‚Äôam√©lioration continue : une fois d√©ploy√©, il est essentiel de s‚Äôassurer que le mod√®le fonctionne bien comme attendu en √©valuant ses performances sur des donn√©es r√©elles √† l‚Äôaide d‚Äôoutils de monitoring en continu. Dans le cas d‚Äôune d√©gradation des performances dans le temps, un r√©-entra√Ænement p√©riodique ou un entra√Ænement en continu du mod√®le doivent √™tre envisag√©s.\n\nPour plus de d√©tails, voir Kreuzberger, K√ºhl, and Hirschl (2023)."
  },
  {
    "objectID": "chapters/mlops.html#entra√Ænements-des-mod√®les",
    "href": "chapters/mlops.html#entra√Ænements-des-mod√®les",
    "title": "Introduction aux enjeux du MLOps",
    "section": "1Ô∏è‚É£ Entra√Ænements des mod√®les",
    "text": "1Ô∏è‚É£ Entra√Ænements des mod√®les\nLa premi√®re √©tape d‚Äôun projet de machine learning correspond √† tout ce que l‚Äôon effectue jusqu‚Äô√† l‚Äôentra√Ænement des premiers mod√®les. Cette √©tape est un processus it√©ratif et fastidieux qui ne suit pas un d√©veloppement lin√©aire : les m√©thodes de r√©cup√©ration des donn√©es peuvent √™tre changeantes, le preprocessing peut varier, de m√™me que la s√©lection des features pour le mod√®le (feature engineering), et les algorithmes test√©s peuvent √™tre nombreux‚Ä¶ On est donc aux antipodes des hypoth√®ses habituelles de stabilit√© n√©cessaires √† l‚Äôentra√Ænement et la validit√© externe dans les enseignements de machine learning.\nGarder une trace de tous les essais effectu√©s appara√Æt indispensable afin de savoir ce qui a fonctionn√© ou non. Le besoin d‚Äôarchiver ne concerne pas que les m√©triques de performances associ√©es √† un jeu de param√®tres. Ceux-ci ne sont qu‚Äôune partie des ingr√©dients n√©cessaires pour aboutir √† une estimation. L‚Äôensemble des inputs d‚Äôun processus de production (code, donn√©es, configuration logicielle, etc.) est √©galement √† conserver pour √™tre en mesure de r√©pliquer une exp√©rimentation.\n\n\n\n\n\n\nNoteLe tracking server de MLFlow, un environnement id√©al pour archiver des exp√©rimentations\n\n\n\nLa phase exploratoire est rendue tr√®s simple gr√¢ce au Tracking Server de MLFlow. Comme cela sera expliqu√© ult√©rieurement, lors de l‚Äôex√©cution d‚Äôun run, MLflow enregistre tout un tas de m√©tadonn√©es qui permettent de retrouver toutes les informations relatives √† ce run : la date, le hash du commit, les param√®tres du mod√®le, le dataset utilis√©, les m√©triques sp√©cifi√©es, etc. Cela permet non seulement de comparer les diff√©rents essais r√©alis√©s, mais aussi d‚Äô√™tre capable de reproduire un run pass√©.\n\n\nDe mani√®re g√©n√©rale, cette phase exploratoire est r√©alis√©e par le data scientist ou le ML engineer dans des notebooks. Ces notebooks sont en effet parfaitement adapt√©s pour cette √©tape puisqu‚Äôils permettent une grande flexibilit√© et sont particuli√®rement commodes pour effectuer des tests. En revanche, lorsque l‚Äôon souhaite aller plus loin et que l‚Äôon vise une mise en production de son projet, les notebooks ne sont plus adapt√©s, et cela pour diverses raisons :\n\nla collaboration est grandement limit√©e √† cause d‚Äôune compatibilit√© tr√®s faible avec les outils de contr√¥le de version standard (notamment Git).\nl‚Äôautomatisation de pipeline est beaucoup plus compliqu√©e et peu lisible. Il existe certes des packages qui permettent d‚Äôautomatiser des pipelines de notebooks comme Elyra par exemple, mais ce n‚Äôest clairement pas l‚Äôapproche que nous vous recommandons car les scripts sont beaucoup moins usine √† gaz.\nLes workflows sont souvent moins clairs, mal organis√©s (toutes les fonctions d√©finies dans le m√™me fichier affectant la lisibilit√© du code par exemple) voire peu reproductibles car les cellules sont rarement ordonn√©es de sorte √† ex√©cuter le code de mani√®re lin√©aire.\nLes notebooks offrent g√©n√©ralement une modularit√© insuffisante lorsque l‚Äôon veut travailler avec des composants de machine learning complexes.\n\nToutes ces raisons nous am√®nent √† vous conseiller de r√©duire au maximum votre utilisation de notebooks et de restreindre leur utilisation √† la phase exploratoire ou √† la diffusion de r√©sultats/rapports. Passer le plus t√¥t possible √† des scripts .py vous permettra de r√©duire le co√ªt de la mise en production. Pour reprendre ce qui a d√©j√† √©t√© √©voqu√© dans le chapitre Architecture des projets, nous vous invitons √† favoriser une structure modulaire de sorte √† pouvoir industrialiser votre projet.\nUne autre sp√©cificit√© pouvant impacter la mise en production concerne la mani√®re dont l‚Äôentra√Ænement est r√©alis√©. Il existe pour cela 2 √©coles qui ont chacune leurs avantages et d√©savantages : le batch training et l‚Äôonline training.\n\nBatch training\nLe batch training est la mani√®re usuelle d‚Äôentra√Æner un mod√®le de machine learning. Cette m√©thode consiste √† entra√Æner son mod√®le sur un jeu de donn√©es fixe d‚Äôune seule traite. Le mod√®le est entra√Æn√© sur l‚Äôint√©gralit√© des donn√©es disponibles et les pr√©dictions sont r√©alis√©es sur de nouvelles donn√©es. Cela signifie que le mod√®le n‚Äôest pas mis √† jour une fois qu‚Äôil est entra√Æn√©, et qu‚Äôil est n√©cessaire de le r√©-entra√Æner si l‚Äôon souhaite ajuster ses poids. Cette m√©thode est relativement simple √† mettre en ≈ìuvre : il suffit d‚Äôentra√Æner le mod√®le une seule fois, de le d√©ployer, puis de le r√©-entra√Æner ult√©rieurement en cas de besoin. Cependant, cette simplicit√© comporte des inconv√©nients : le mod√®le reste statique, n√©cessitant un r√©-entra√Ænement fr√©quent pour int√©grer de nouvelles donn√©es. Par exemple, dans le cas de la d√©tection de spams, si un nouveau type de spam appara√Æt, le mod√®le entra√Æn√© en batch ne sera pas capable de le d√©tecter sans un r√©-entra√Ænement complet. De plus, cette m√©thode peut rapidement exiger une grande quantit√© de m√©moire en fonction de la taille du jeu de donn√©es, ce qui peut poser des contraintes sur l‚Äôinfrastructure et prolonger consid√©rablement le temps d‚Äôentra√Ænement.\n\n\nOnline training\nL‚Äôonline training se pr√©sente comme l‚Äôantith√®se du batch training, car il se d√©roule de mani√®re incr√©mentale. Dans ce mode d‚Äôentra√Ænement, de petits lots de donn√©es sont envoy√©s s√©quentiellement √† l‚Äôalgorithme, ce qui permet √† celui-ci de mettre √† jour ses poids √† chaque nouvelle donn√©e re√ßue. Cette approche permet au mod√®le de d√©tecter efficacement les variations dans les donn√©es en temps r√©el. Il est toutefois crucial de bien ajuster le learning rate afin d‚Äô√©viter que le mod√®le oublie les informations apprises sur les donn√©es pr√©c√©dentes. L‚Äôun des principaux avantages de cette m√©thode est sa capacit√© √† permettre un entra√Ænement continu m√™me lorsque le mod√®le est en production, ce qui se traduit par une r√©duction des co√ªts computationnels. De plus, l‚Äôonline training est particuli√®rement adapt√© aux situations o√π les donn√©es d‚Äôentr√©e √©voluent fr√©quemment, comme dans le cas des pr√©dictions de cours de bourse. Cependant, sa mise en ≈ìuvre dans un contexte de production est bien plus complexe que celle du batch training, et les frameworks traditionnels de machine learning tels que Scikit-learn, PyTorch, TensorFlow et Keras ne sont pas compatibles avec cette approche.\n\n\nDistribuer l‚Äôoptimisation des hyperparam√®tres\nUne autre sp√©cificit√© des mod√®les de machine learning r√©side dans le nombre important d‚Äôhyperparam√®tres √† optimiser, lesquels peuvent sensiblement impacter les performances du mod√®le. L‚Äôapproche standard pour r√©aliser cette optimisation est ce qu‚Äôon appelle un Grid Search. Il s‚Äôagit simplement de lister toutes les combinaisons d‚Äôhyperparam√®tres √† tester et d‚Äôentra√Æner successivement des mod√®les avec ces combinaisons pr√©d√©finies. Il n‚Äôest pas difficile de comprendre que cette technique est tr√®s co√ªteuse en temps de calcul lorsque le nombre d‚Äôhyperparam√®tres √† optimiser et leurs modalit√©s √† tester sont √©lev√©s. Cependant, cette optimisation est indispensable pour entra√Æner le meilleur mod√®le pour notre t√¢che, et si s‚Äôinspirer de la litt√©rature est crucial pour limiter le domaine d‚Äôoptimisation de nos hyperparam√®tres, r√©aliser un Grid Search est une √©tape incontournable.\nAinsi, pour s‚Äôinscrire dans l‚Äôapproche du MLOps, une bonne m√©thode est d‚Äôautomatiser cette optimisation des hyperparam√®tres en la distribuant sur un cluster lorsqu‚Äôon dispose de l‚Äôinfrastructure ad√©quate. L‚Äôid√©e est de cr√©er des processus ind√©pendants, chacun li√©s √† une combinaison de nos hyperparam√®tres, et d‚Äôentra√Æner notre mod√®le sur ceux-ci puis d‚Äôenregister les informations √† archiver dans un environnement ad√©quat, par exemple dans MLFlow.\nIl existe un moteur de workflow populaire pour orchestrer des t√¢ches parall√®les sur un cluster Kubernetes : Argo Workflow. Le principe est de d√©finir un workflow dans lequel chaque √©tape correspond √† un conteneur contenant uniquement ce qui est strictement n√©cessaire √† l‚Äôex√©cution de cette √©tape. Ainsi, on s‚Äôapproche de la perfection en ce qui concerne la reproductibilit√©, car on ma√Ætrise totalement les installations n√©cessaires √† l‚Äôex√©cution de notre entra√Ænement. Un workflow √† plusieurs √©tapes peut ainsi √™tre mod√©lis√© comme un graphe acyclique orient√©, et l‚Äôexemple ci-dessous repr√©sente un cas d‚Äôoptimisation d‚Äôhyperparam√®tres :\n\n\n\nWorkflow d‚Äôoptimisation d‚Äôhyperparam√®tres en parall√®le\n\n\nCette approche permet d‚Äôex√©cuter facilement en parall√®le des t√¢ches intensives en calcul de mani√®re totalement reproductible. √âvidemment, l‚Äôutilisation de tels workflows ne se limite pas √† l‚Äôoptimisation d‚Äôhyperparam√®tres mais peut √©galement √™tre utilis√©e pour le preprocessing de donn√©es, la cr√©ation de pipelines d‚ÄôETL, etc. D‚Äôailleurs, √† l‚Äôorigine, ces outils ont √©t√© pens√© pour ces t√¢ches et permettent ainsi de d√©finir un processus de donn√©es comme un ensemble de transformations sous la forme de diagramme acyclique dirig√© (DAG)."
  },
  {
    "objectID": "chapters/mlops.html#servir-un-mod√®le-ml-√†-des-utilisateurs",
    "href": "chapters/mlops.html#servir-un-mod√®le-ml-√†-des-utilisateurs",
    "title": "Introduction aux enjeux du MLOps",
    "section": "2Ô∏è‚É£ Servir un mod√®le ML √† des utilisateurs",
    "text": "2Ô∏è‚É£ Servir un mod√®le ML √† des utilisateurs\nUne partie tr√®s importante, parfois n√©glig√©e, des projets de machine learning est la mise √† disposition des mod√®les entra√Æn√©s √† d‚Äôautres utilisateurs. Puisque vous avez parfaitement suivi les diff√©rents chapitres de ce cours, votre projet est en th√©orie totalement reproductible. Une mani√®re triviale de transmettre le mod√®le que vous avez s√©lectionn√© serait de partager votre code et toutes les informations n√©cessaires pour qu‚Äôune personne tierce r√©-entra√Æne votre mod√®le de son c√¥t√©. √âvidemment, ce proc√©d√© n‚Äôest pas optimal, car il suppose que tous les utilisateurs disposent des ressources/infrastructures/connaissances n√©cessaires pour r√©aliser l‚Äôentra√Ænement.\nL‚Äôobjectif est donc de mettre √† disposition votre mod√®le de mani√®re simple et efficace. Pour cela, plusieurs possibilit√©s s‚Äôoffrent √† vous en fonction de votre projet, et il est important de se poser quelques questions pr√©alables :\n\nQuel format est le plus pertinent pour mettre √† disposition des utilisateurs ?\nLes pr√©dictions du mod√®le doivent-elles √™tre r√©alis√©es par lots (batch) ou en temps r√©el (online) ?\nQuelle infrastructure utiliser pour d√©ployer notre mod√®le de machine learning ?\n\nDans le cadre de ce cours, nous avons choisi d‚Äôutiliser une API REST pour mettre √† disposition un mod√®le de machine learning. Cela nous semble √™tre la m√©thode la plus adapt√©e dans une grande majorit√© des cas, car elle r√©pond √† plusieurs crit√®res :\n\nSimplicit√© : les API REST permettent de cr√©er une porte d‚Äôentr√©e qui peut cacher la complexit√© sous-jacente du mod√®le, facilitant ainsi sa mise √† disposition.\nStandardisation : l‚Äôun des principaux avantages des API REST est qu‚Äôelles reposent sur le standard HTTP. Cela signifie qu‚Äôelles sont agnostiques au langage de programmation utilis√© et que les requ√™tes peuvent √™tre r√©alis√©es en XML, JSON, HTML, etc.\nModularit√© : le client et le serveur sont ind√©pendants. En d‚Äôautres termes, le stockage des donn√©es, l‚Äôinterface utilisateur ou encore la gestion du mod√®le sont compl√®tement s√©par√©s de la mise √† disposition (le serveur).\nPassage √† l‚Äô√©chelle : la s√©paration entre le serveur et le client permet aux API REST d‚Äô√™tre tr√®s flexibles et facilite le passage √† l‚Äô√©chelle (scalability). Elles peuvent ainsi s‚Äôadapter √† la charge de requ√™tes concurrentes.\n\nL‚Äôexposition d‚Äôun mod√®le de machine learning peut √™tre r√©sum√©e par le sch√©ma suivant :\n\n\n\nExposer un mod√®le de ML via une API\n\n\nComme le montre le sch√©ma, l‚ÄôAPI est ex√©cut√©e dans un conteneur afin de garantir un environnement totalement autonome et isol√©. Seules les d√©pendances n√©cessaires √† l‚Äôex√©cution du mod√®le et au fonctionnement de l‚ÄôAPI ne sont int√©gr√©es √† ce conteneur. Travailler avec des images docker l√©g√®res pr√©sente plusieurs avantages. Tout d‚Äôabord, cr√©er une image ne contenant que le strict n√©cessaire au fonctionnement de votre application permet justement de savoir ce qui est absolument indispensable et ce qui est superflu. De plus, plus votre image est l√©g√®re, plus son temps de t√©l√©chargement depuis votre Hub d‚Äôimages (e.g.¬†Dockerhub) sera rapide √† chaque cr√©ation de conteneur de votre application. Les conteneurs ont l‚Äôavantage d‚Äô√™tre totalement portables et offrent la possibilit√© de mettre √† l‚Äô√©chelle votre application de mani√®re simple et efficace. Par exemple, si l‚Äôon imagine que vous avez d√©ploy√© votre mod√®le et que vous souhaitez le requ√™ter un grand nombre de fois dans un laps de temps court, il est alors pr√©f√©rable de cr√©er plusieurs instances de votre application pour que les calculs puissent √™tre effectu√©s en parall√®le. L‚Äôavantage de proc√©der de cette mani√®re est qu‚Äôune fois qu‚Äôon a cr√©√© l‚Äôimage sous-jacente √† notre application, il est ensuite tr√®s simple de cr√©er une multitude de conteneurs (replicas) toutes bas√©es sur l‚Äôimage en question.\nPour tout ce qui concerne le d√©ploiement de votre application, vous pouvez vous r√©f√©rer au chapitre Mise en production. Techniquement, il n‚Äôy a aucune difficult√© suppl√©mentaire lorsque l‚Äôon veut avoir une approche MLOps lors de cette √©tape. L‚Äôunique subtilit√© √† avoir en t√™te est que l‚Äôon souhaite maintenant faire communiquer notre application avec MLflow. En effet, chaque d√©ploiement est bas√© sur une version particuli√®re du mod√®le et il est n√©cessaire de renseigner quelques informations afin de r√©cup√©rer le bon mod√®le au sein de notre entrep√¥t de mod√®le. Comme pour tout d√©ploiement sous Kubernetes, il faut tout d‚Äôabord cr√©er les 3 fichiers YAML : deployment.yaml, service.yaml, ingress.yaml. Ensuite, comme vous pouvez le voir sur le sch√©ma, notre API doit pouvoir √™tre reli√©e √† MLflow qui lui-m√™me a besoin d‚Äô√™tre connect√© √† un espace de stockage (ici s3/MinIO) qui contient l‚Äôentrep√¥t des mod√®les. Pour cela, dans le fichier deployment.yaml, on rajoute simplement quelques variables d‚Äôenvironnement qui nous permettent de cr√©er de lien √† savoir :\n\nMLFLOW_S3_ENDPOINT_URL : L‚ÄôURL de l‚Äôendpoint S3 utilis√© par MLflow pour stocker les donn√©es (et mod√®les)\nMLFLOW_TRACKING_URI : L‚ÄôURI du serveur de suivi MLflow, qui sp√©cifie o√π les informations concernant les mod√®les sont stock√©es.\nAWS_ACCESS_KEY_ID : L‚Äôidentifiant d‚Äôacc√®s utilis√© pour authentifier l‚Äôacc√®s aux services de stockage s3.\nAWS_SECRET_ACCESS_KEY : La cl√© secr√®te utilis√©e pour authentifier l‚Äôacc√®s aux services de stockage s3.\nAWS_DEFAULT_REGION : Identifie la r√©gion S3 pour laquelle vous souhaitez envoyer les demandes aux serveurs. \n\nPour faciliter le d√©ploiement continu (voir chapitre Mise en production), il est conseill√© de rajouter des variables d‚Äôenvironnement sp√©cifiant la version du mod√®le √† d√©ployer ainsi que le nom du mod√®le √† d√©ployer. En effet, en sp√©cifiant ces valeurs dans le fichier deployment.yaml, cela va permettre de d√©clencher un nouveau d√©ploiement d√®s lors que l‚Äôon modifiera ces valeurs.\nIl est bon de noter que MLflow permet √©galement de d√©ployer directement un mod√®le MLflow. Vous pouvez aller regarder la documentation si cela vous int√©resse. Cette option est relativement r√©cente et pas encore tout √† fait mature mais se base sur les m√™mes technologies que celles pr√©sent√©es dans ce cours (Kubernetes, S3, etc.). C‚Äôest pour cela que nous avons pr√©f√©r√© d√©tailler le d√©veloppement de notre propre API en utilisant le framework FastAPI, qui est devenu le standard pour le d√©veloppement d‚ÄôAPI en Python.\n\nD√©ployer sur Kubernetes (plutot dans chap mise en prod ?) \nBatch vs real-time prediction"
  },
  {
    "objectID": "chapters/mlops.html#observabilit√©-en-temps-r√©el-dun-mod√®le-de-ml",
    "href": "chapters/mlops.html#observabilit√©-en-temps-r√©el-dun-mod√®le-de-ml",
    "title": "Introduction aux enjeux du MLOps",
    "section": "3Ô∏è‚É£ Observabilit√© en temps r√©el d‚Äôun mod√®le de ML",
    "text": "3Ô∏è‚É£ Observabilit√© en temps r√©el d‚Äôun mod√®le de ML\nUne fois la mod√©lisation r√©alis√©e, le mod√®le entra√Æn√©, optimis√© et mis √† disposition des utilisateurs gr√¢ce √† un d√©ploiement sur un serveur, on peut consid√©rer que le travail est fini. Du point de vue du data-scientist stricto sensu, cela peut √™tre le cas, puisque l‚Äôon consid√®re souvent que le domaine du data-scientist s‚Äôarr√™te √† la s√©lection du mod√®le √† d√©ployer, le d√©ploiement √©tant r√©alis√© par ce qu‚Äôon appelle les data-engineers. Pourtant, une fois d√©ploy√© dans un environnement de production, le mod√®le n‚Äôa pas r√©alis√© l‚Äôint√©gralit√© de son cycle de vie. En production, le cycle de vie d‚Äôun mod√®le de machine learning suivant l‚Äôapproche MLOps peut √™tre sch√©matis√© de la mani√®re suivante :\n\n\n\n\n\n\nFigure¬†1: Source : martinfowler.com\n\n\n\nOn retrouve les diff√©rentes composantes du MLOps avec les donn√©es (DataOps), les mod√®les (ModelOps) et le code (DevOps). Ces composantes rendent le cycle de vie d‚Äôun mod√®le de machine learning complexe impliquant plusieurs parties prenantes autour du projet. En r√®gle g√©n√©rale, on observe trois parties prenantes principales :\n\nData-scientists/Data-engineers\nIT/DevOps\n√âquipes m√©tiers\n\nQuelques fois, les data-scientists peuvent √™tre int√©gr√©s aux √©quipes m√©tiers et les data-engineers aux √©quipes IT. Cela peut simplifier les √©changes entre les deux √©quipes, mais cela peut √©galement entra√Æner un travail en silos et cloisonner les deux √©quipes aux expertises, attentes et vocabulaires tr√®s diff√©rents. Or, la communication est primordiale pour permettre une bonne gestion du cycle de vie du mod√®le de machine learning et notamment pour surveiller le mod√®le dans son environnement de production.\nIl est extr√™mement important de surveiller comment le mod√®le se comporte une fois d√©ploy√© pour s‚Äôassurer que les r√©sultats renvoy√©s sont conformes aux attentes. Cela permet d‚Äôanticiper des changements dans les donn√©es, une baisse des performances ou encore d‚Äôam√©liorer le mod√®le de mani√®re continue. Il est √©galement n√©cessaire que notre mod√®le soit toujours accessible, que notre application soit bien dimensionn√©e, etc. C‚Äôest pour cela que la surveillance (monitoring) d‚Äôun mod√®le de machine learning est un enjeu capital dans l‚Äôapproche MLOps.\nLe terme surveillance peut renvoyer √† plusieurs d√©finitions en fonction de l‚Äô√©quipe dans laquelle l‚Äôon se situe. Pour une personne travaillant dans l‚Äô√©quipe informatique, surveiller une application signifie v√©rifier sa validit√© technique. Elle va donc s‚Äôassurer que la latence n‚Äôest pas trop √©lev√©e, que la m√©moire est suffisante ou encore que le stockage sur le disque est bien proportionn√©. Pour un data-scientist ou une personne travaillant dans l‚Äô√©quipe m√©tier, ce qui va l‚Äôint√©resser est la surveillance du mod√®le d‚Äôun point de vue m√©thodologique. Malheureusement, il n‚Äôest pas souvent √©vident que contr√¥ler la performance en temps r√©el d‚Äôun mod√®le de machine learning. Il est rare que l‚Äôon connaisse la vraie valeur au moment de la pr√©diction du mod√®le (sinon on ne s‚Äôemb√™terait pas √† construire un mod√®le !) et on ne peut pas vraiment savoir s‚Äôil s‚Äôest tromp√© ou non. Il est donc commun d‚Äôutiliser des proxies pour anticiper une potentielle d√©gradation de la performance de notre mod√®le. On distingue g√©n√©ralement 2 principaux types de d√©gradation d‚Äôun mod√®le de machine learning : le data drift et le concept drift.\n\n\n\nSource : whylabs.ai\n\n\n\nData drift\nOn parle de data drift lorsque l‚Äôon observe un changement de distribution dans les donn√©es utilis√©es en entr√©e du mod√®le. En d‚Äôautres termes, il y a data drift lorsque les donn√©es utilis√©es lors de l‚Äôentra√Ænement sont sensiblement diff√©rentes des donn√©es utilis√©es lors de l‚Äôinf√©rence en production. Imaginons que vous souhaitez rep√©rer des habitations √† partir d‚Äôimages satellites. Vous entra√Ænez votre mod√®le sur des donn√©es datant par exemple de f√©vrier 2022, et une fois en production vous essayer de rep√©rer les habitations tous les mois suivants. Vous constatez finalement durant l‚Äô√©t√© que votre mod√®le n‚Äôest plus du tout aussi performant puisque les images satellites de juillet diff√®rent fortement de celle de f√©vrier. La distribution des donn√©es d‚Äôentra√Ænement n‚Äôest plus proche de celle d‚Äôinf√©rence, \\(P_{train}(X) \\neq P_{inference}(X)\\). Les data drifts apparaissent d√®s lors que les propri√©t√©s statistiques des donn√©es changent et cela peut venir de plusieurs facteurs en fonction de votre mod√®le : changements de comportement, dynamique de march√©, nouvelles r√©glementations politiques, probl√®me de qualit√© des donn√©es, etc. Il n‚Äôest pas si simple de d√©tecter rapidement des data drifts, cela suppose de surveiller de mani√®re continue la distribution des donn√©es en entr√©e et en sortie de votre mod√®le sur un certain laps de temps et d‚Äôidentifier quand celles-ci diff√®rent significativement de la distribution des donn√©es d‚Äôentra√Ænement. Pour obtenir une id√©e visuelle, on peut cr√©er des repr√©sentations graphiques comme des histogrammes pour comparer les distributions √† plusieurs p√©riodes dans le temps, voire des bo√Ætes √† moustaches. On peut aussi calculer des m√©triques, qui seront plus simples d‚Äôutilisation si l‚Äôon souhaite automatiser un syst√®me d‚Äôalerte, comme des distances entre distributions (distance de Bhattacharyya, divergence de Kullback-Leibler, distance de Hellinger) ou effectuer des tests statistiques (Test de Kolmogorov-Smirnov, Test du œá¬≤). Pour r√©sumer, la d√©tection d‚Äôun data drift peut s‚Äôeffectuer en plusieurs √©tapes :\n\nD√©finition d‚Äôune r√©f√©rence : on d√©finit la distribution de r√©f√©rence (e.g.¬†celle utilis√©e lors de l‚Äôentra√Ænement).\nD√©finition de seuils : on d√©termine en dessous de quelles valeurs de nos m√©triques cela peut √™tre consid√©r√© comme un data drift.\nSurveillance continue : soit en temps r√©el, soit de mani√®re p√©riodique (relativement courte), on compare nos distributions et on calcule les m√©triques d√©finies pr√©alablement.\nAlerte et correction : on met en place un syst√®me d‚Äôalerte automatique d√®s lors que nos m√©triques indiquent la pr√©sence d‚Äôun data drift, puis on agit en cons√©quence (r√©-entra√Ænement sur de nouvelles donn√©es, ajustement des param√®tres du mod√®le, etc.).\n\n\n\nConcept drift\nOn parle de concept drift lorsque l‚Äôon observe un changement dans la relation statistique entre les features (\\(X\\)) et la variable √† pr√©dire (\\(Y\\)) au cours du temps. En termes math√©matiques, on consid√®re qu‚Äôil y a un concept drift d√®s lors que \\(P_{train}(Y|X) \\neq P_{inference}(Y|X)\\) alors m√™me que \\(P_{train}(X) = P_{inference}(X)\\). Cela peut avoir un impact important sur les performances du mod√®le si la relation diff√®re fortement. Par exemple, un mod√®le de pr√©diction de la demande de masques chirurgicaux entra√Æn√© sur des donn√©es avant la pand√©mie de COVID-19 deviendra totalement inad√©quat pour effectuer des pr√©dictions lors de cette pand√©mie, car il y a eu un changement dans la relation entre la demande de masques chirurgicaux et les features utilis√©es pour pr√©dire cette demande. Dans le cas d‚Äôun concept drift, on sera plus tent√© de surveiller des m√©triques de performance pour rep√©rer une potentielle anomalie. Dans le cas o√π l‚Äôon poss√®de un jeu de test gold standard, alors on sera en capacit√© de calculer de nombreuses m√©triques usuelles de machine learning (√† savoir l‚Äôaccuracy, la precision, le recall ou le F1-score pour des probl√®mes de classification, et toutes les m√©triques d‚Äôerreurs - MSE, RMSE, MAE, ‚Ä¶ - pour les probl√®mes de r√©gression) et rep√©rer une baisse tendancielle ou brutale des performances. Dans le cas o√π l‚Äôon n‚Äôa pas de jeu de test gold standard, on s‚Äôattachera √† d√©terminer des proxys qui peuvent √™tre li√©s √† des m√©triques de performance ou alors utiliser des algorithmes de d√©tection de changement dans le flux de donn√©es (Drift Detection Method, Early Drift Detection Method, Adaptive Windowing)."
  },
  {
    "objectID": "chapters/mlops.html#r√©-entra√Ænement-dun-mod√®le-ml",
    "href": "chapters/mlops.html#r√©-entra√Ænement-dun-mod√®le-ml",
    "title": "Introduction aux enjeux du MLOps",
    "section": "4Ô∏è‚É£ R√©-entra√Ænement d‚Äôun mod√®le ML",
    "text": "4Ô∏è‚É£ R√©-entra√Ænement d‚Äôun mod√®le ML\nD√®s lors que l‚Äôon a constat√© une baisse de la performance de notre mod√®le gr√¢ce √† notre surveillance fine, il faut ensuite pallier au probl√®me et red√©ployer un mod√®le avec des performances satisfaisantes. On est donc √† la fin du cycle de vie de notre mod√®le, ce qui va nous reconduire au d√©but du cycle pour un nouveau mod√®le comme l‚Äôillustre la figure Figure¬†1. Le r√©-entra√Ænement est partie int√©grante d‚Äôun projet de machine learning d√®s lors que celui-ci est mis en production. Il existe plusieurs m√©thodes pour r√©-entra√Æner de la plus basique √† la plus MLOps-compatible.\nLa m√©thode classique est de r√©aliser un nouvel entra√Ænement from scratch en ajoutant les nouvelles donn√©es √† notre disposition dans le jeu d‚Äôentra√Ænement. Cela permet au mod√®le de conna√Ætre les derni√®res relations entre les features et la variable √† pr√©dire. Cependant, r√©-entra√Æner un mod√®le peut √™tre particuli√®rement co√ªteux lorsque l‚Äôon travaille sur de gros mod√®les dont les ressources computationnelles n√©cessaires sont importantes. Il est aussi possible de fine-tuner un mod√®le d√©j√† pr√©-entra√Æn√©. Dans ce cas-l√†, on n‚Äôa pas besoin de repartir de z√©ro, on repart des poids optimis√©s lors du premier entra√Ænement et on les r√©-optimise en utilisant les nouvelles donn√©es √† notre disposition. Cette m√©thode est naturellement beaucoup moins longue √† r√©aliser et est moins co√ªteuse, notamment lorsque la quantit√© de nouvelles donn√©es est faible par rapport √† la quantit√© des donn√©es utilis√©es lors du premier entra√Ænement.\nL‚Äôapproche MLOps consiste √† automatiser ce r√©-entra√Ænement, qu‚Äôon appelle √©galement entra√Ænement continu, de sorte √† obtenir un cycle de vie totalement automatis√©. En effet, le r√©-entra√Ænement est fondamental pour s‚Äôassurer que le mod√®le de machine learning est constamment en train de fournir des pr√©dictions coh√©rentes, tout en minimisant les interventions manuelles. L‚Äôobjectif est donc de cr√©er un processus qui lance de nouveaux entra√Ænements de mani√®re automatique en prenant en compte les derni√®res informations disponibles. Les entra√Ænements peuvent √™tre d√©clench√©s soit de mani√®re p√©riodique (tous les lundis √† 2h du matin), d√®s lors qu‚Äôune alerte a √©t√© d√©clench√©e dans notre syst√®me de monitoring, ou bien d√®s qu‚Äôon a une quantit√© de nouvelles donn√©es suffisante pour r√©aliser un online training par exemple.\nL‚Äôutilisation d‚Äôoutils d‚Äôorchestration de workflow comme Argo Workflow ou Airflow est donc indispensable pour r√©aliser cette automatisation de mani√®re pertinente."
  },
  {
    "objectID": "chapters/mlops.html#d√©fis-organisationnels-du-mlops",
    "href": "chapters/mlops.html#d√©fis-organisationnels-du-mlops",
    "title": "Introduction aux enjeux du MLOps",
    "section": "5Ô∏è‚É£ D√©fis organisationnels du MLOps",
    "text": "5Ô∏è‚É£ D√©fis organisationnels du MLOps\nOutre les sp√©cificit√©s techniques pr√©c√©demment explicit√©es, le MLOps pr√©sente √©galement plusieurs d√©fis en termes organisationnels et manag√©riaux. En effet, dans la plupart des organisations, les √©quipes data transverses ou int√©gr√©es dans diff√©rents d√©partements m√©tier sont relativement jeunes et peuvent manquer de ressources qualifi√©es pour g√©rer le d√©ploiement et le maintien en condition op√©rationnelle de syst√®mes ML complexes. Ces √©quipes se composent principalement de data scientists qui se concentrent sur le d√©veloppement des mod√®les de machine learning, mais n‚Äôont pas les comp√©tences n√©cessaires pour g√©rer le d√©ploiement et la maintenance d‚Äôapplications compl√®tes.\nDe plus, les √©quipes data √©voluent encore trop souvent en silo, sans communiquer avec les diff√©rentes √©quipes techniques avec lesquelles elles devraient interagir pour mettre en production leurs mod√®les. Or ces √©quipes techniques, souvent compos√©es d‚Äôinformaticiens/d√©veloppeurs, ne connaissent pas forc√©ment les sp√©cificit√©s des mod√®les de machine learning, accentuant d‚Äôautant plus la n√©cessit√© d‚Äôune communication continue entre ces √©quipes.\nUne autre difficult√© pouvant intervenir lors du d√©ploiement est la diff√©rence d‚Äôenvironnements utilis√©s ainsi que les diff√©rents langages connus entre les deux √©quipes. Il n‚Äôest pas rare que les data-scientists d√©veloppent des mod√®les en Python tandis que les √©quipes informatiques g√®rent leur serveur de production dans un langage diff√©rent, comme Java par exemple.\nAinsi, l‚Äôapproche MLOps engendre aussi des d√©fis manag√©riaux qui impliquent de faire converger les comp√©tences entre les √©quipes afin de fluidifier la mise en production de mod√®les de machine learning.\n\n\n\nGouvernance d‚Äôun projet de machine learning"
  },
  {
    "objectID": "chapters/mlops.html#pourquoi-mlflow",
    "href": "chapters/mlops.html#pourquoi-mlflow",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Pourquoi MLflow ?",
    "text": "Pourquoi MLflow ?\nIl existe aujourd‚Äôhui de nombreux outils pour orchestrer des t√¢ches et des pipelines de donn√©es. Parmi les plus populaires (selon leur ‚≠ê GitHub), on peut citer Airflow, Luigi, MLflow, Argo Workflow, Prefect ou encore Kubeflow, BentoML‚Ä¶ Il est difficile d‚Äôaffirmer s‚Äôil y en a un meilleur qu‚Äôun autre ; en r√©alit√©, votre choix d√©pend surtout de votre infrastructure informatique et de votre projet. En l‚Äôoccurrence ici, nous avons fait le choix d‚Äôutiliser MLflow pour sa simplicit√© d‚Äôutilisation gr√¢ce √† une interface web bien faite, parce qu‚Äôil int√®gre l‚Äôensemble du cycle de vie d‚Äôun mod√®le et √©galement parce qu‚Äôil s‚Äôint√®gre tr√®s bien avec Kubernetes. De plus, il est pr√©sent dans le catalogue du SSP Cloud, ce qui simplifie grandement son installation. Afin d‚Äôint√©grer les dimensions d‚Äôint√©gration et de d√©ploiement continus, nous utiliserons √©galement Argo CD et Argo Workflow dans la boucle. Ceux-ci sont privil√©gi√©s par rapport √† Airflow car ils sont optimis√©s pour les clusters Kubernetes qui repr√©sentent aujourd‚Äôhui la norme des cloud en ligne ou on premise.\n\n\n\nVue d‚Äôensemble de MLFlow. Source: https://dzlab.github.io\n\n\nMLflow est une plateforme qui permet d‚Äôoptimiser le d√©veloppement du cycle de vie d‚Äôun mod√®le de machine learning. Elle permet de suivre en d√©tail les diff√©rentes exp√©rimentations, de packager son code pour garantir la reproductibilit√©, et de servir un mod√®le √† des utilisateurs. MLFlow poss√®de √©galement une API qui permet d‚Äô√™tre compatible avec la majorit√© des librairies de machine learning (PyTorch, Scikit-learn, XGBoost, etc.) mais √©galement diff√©rents langages (Python, R et Java)."
  },
  {
    "objectID": "chapters/mlops.html#les-projets-mlflow",
    "href": "chapters/mlops.html#les-projets-mlflow",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Les projets MLflow",
    "text": "Les projets MLflow\nMLflow propose un format pour packager son projet de data science afin de favoriser la r√©utilisation et la reproductibilit√© du code. Ce format s‚Äôappelle tout simplement MLflow Project. Concr√®tement, un MLflow project n‚Äôest rien d‚Äôautre qu‚Äôun r√©pertoire contenant le code et les ressources n√©cessaires (donn√©es, fichiers de configuration‚Ä¶) pour l‚Äôex√©cution de votre projet. Il est r√©sum√© par un fichier MLproject qui liste les diff√©rentes commandes pour ex√©cuter une pipeline ainsi que les d√©pendances n√©cessaires. En g√©n√©ral, un projet MLflow a la structure suivante :\nProjet_ML/\n‚îú‚îÄ‚îÄ artifacts/\n‚îÇ   ‚îú‚îÄ‚îÄ model.bin\n‚îÇ   ‚îî‚îÄ‚îÄ train_text.txt\n‚îú‚îÄ‚îÄ code/\n‚îÇ   ‚îú‚îÄ‚îÄ main.py\n‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py\n‚îú‚îÄ‚îÄ MLmodel\n‚îú‚îÄ‚îÄ conda.yaml\n‚îú‚îÄ‚îÄ python_env.yaml\n‚îú‚îÄ‚îÄ python_model.pkl\n‚îî‚îÄ‚îÄ requirements.txt\nEn plus de packager son projet, MLflow permet √©galement de packager son mod√®le, quel que soit la librairie de machine learning sous-jacente utilis√©e (parmi celles compatibles avec MLflow, c‚Äôest-√†-dire toutes les librairies que vous utilisez !). Ainsi, deux mod√®les entra√Æn√©s avec des librairies diff√©rentes, disons PyTorch et Keras, peuvent √™tre d√©ploy√©s et requ√™t√©s de la m√™me mani√®re gr√¢ce √† cette surcouche ajout√©e par MLflow.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIl est √©galement possible de packager son propre mod√®le personnalis√© ! Pour cela vous pouvez suivre le tutoriel pr√©sent dans la documentation.\n\n\nAutrement dit, un projet MLFlow archive l‚Äôensemble des √©l√©ments n√©cessaires pour reproduire un entra√Ænement donn√© d‚Äôun mod√®le ou pour r√©utiliser celui-ci √† tout moment."
  },
  {
    "objectID": "chapters/mlops.html#le-serveur-de-suivi-tracking-server",
    "href": "chapters/mlops.html#le-serveur-de-suivi-tracking-server",
    "title": "Introduction aux enjeux du MLOps",
    "section": "Le serveur de suivi (tracking server)",
    "text": "Le serveur de suivi (tracking server)\nLe tracking server est le lieu o√π sont archiv√©s l‚Äôensemble des entra√Ænements d‚Äôun mod√®le. Attention, il ne s‚Äôagit pas du serveur sur lequel les mod√®les sont entra√Æn√©s mais de celui o√π les entra√Ænements sont archiv√©s apr√®s avoir eu lieu. Au-del√† de stocker seulement les poids d‚Äôun mod√®le, c‚Äôest l‚Äôensemble de l‚Äôenvironnement n√©cessaire qui peut √™tre retrouv√© dans ce serveur.\n\n\n\n\n\nTechniquement, cela prend la forme d‚Äôune API et d‚Äôune interface utilisateur pour enregistrer les param√®tres, les versions du code, les m√©triques ou encore les artefacts associ√©s √† un entra√Ænement.\n\n\n\nSource: Databricks\n\n\nEn arri√®re plan, MLFlow va enregistrer tout ceci dans un bucket S3. N√©anmoins, l‚Äôutilisateur n‚Äôaura pas √† se soucier de cela puisque c‚Äôest MLFLow qui fera l‚Äôinterface entre l‚Äôutilisateur et le syst√®me de stockage. Avec son API, MLFLow fournit m√™me une mani√®re simplifi√©e de r√©cup√©rer ces objets archiv√©s, par exemple avec un code prenant la forme\nimport mlflow\nmodel = mlflow.pyfunc.load_model(model_uri=\"runs:/d16076a3ec534311817565e6527539c\")\nLe tracking server est tr√®s utile pour comparer les diff√©rentes exp√©rimentations que vous avez effectu√©es, pour les stocker et √©galement pour √™tre capable de les reproduire. En effet, chaque run sauvegarde la source des donn√©es utilis√©es, mais √©galement le commit sur lequel le run est bas√©.\nA la mani√®re de Git qui permet d‚Äôidentifier chaque moment de l‚Äôhistoire d‚Äôun projet √† partir d‚Äôun identifiant unique, MLFlow permet de r√©cup√©rer chaque entra√Ænement d‚Äôun mod√®le √† partir d‚Äôun SHA. N√©anmoins, en pratique, certains mod√®les ont un statut √† part, notamment ceux en production."
  },
  {
    "objectID": "chapters/mlops.html#lentrep√¥t-de-mod√®les-model-registry",
    "href": "chapters/mlops.html#lentrep√¥t-de-mod√®les-model-registry",
    "title": "Introduction aux enjeux du MLOps",
    "section": "L‚Äôentrep√¥t de mod√®les (model registry)",
    "text": "L‚Äôentrep√¥t de mod√®les (model registry)\nUne fois que l‚Äôon a effectu√© diff√©rentes exp√©rimentations et pu s√©lectionner les mod√®les qui nous satisfont, il est temps de passer √† l‚Äô√©tape suivante du cycle de vie d‚Äôun mod√®le. En effet, le mod√®le choisi doit ensuite pouvoir passer dans un environnement de production ou de pr√©-production. Or, conna√Ætre l‚Äô√©tat d‚Äôun mod√®le dans son cycle de vie n√©cessite une organisation tr√®s rigoureuse et n‚Äôest pas si ais√©. MLflow a d√©velopp√© une fonctionnalit√© qui permet justement de simplifier cette gestion des versions des mod√®les gr√¢ce √† son Model Registry. Cet entrep√¥t permet d‚Äôajouter des tags et des alias √† nos mod√®les pour d√©finir leur position dans leur cycle de vie et ainsi pouvoir les r√©cup√©rer de mani√®re efficace.\nDe mani√®re g√©n√©rale, un mod√®le de machine learning passe par 4 stades qu‚Äôil est n√©cessaire de conna√Ætre en tout temps :\n\nExp√©rimental\nEn √©valuation\nEn production\nArchiv√©"
  },
  {
    "objectID": "chapters/mlops.html#mlflow-en-r√©sum√©",
    "href": "chapters/mlops.html#mlflow-en-r√©sum√©",
    "title": "Introduction aux enjeux du MLOps",
    "section": "MLflow en r√©sum√©",
    "text": "MLflow en r√©sum√©\nMLflow est donc un projet open-source qui fournit une plateforme pour suivre le cycle de vie d‚Äôun mod√®le de machine learning de bout en bout. Ce n‚Äôest pas le seul outil disponible et il n‚Äôest peut-√™tre pas le plus adapt√© √† certains de vos projets pr√©cis. En revanche, il pr√©sente selon nous plusieurs avantages, en premier lieu sa prise en main tr√®s simple et sa capacit√© √† r√©pondre aux besoins de l‚Äôapproche MLOps. Il faut garder √† l‚Äôesprit que cet environnement est encore tr√®s r√©cent et que de nouveaux projets open-source √©mergent chaque jour, donc il est n√©cessaire de rester √† jour sur les derni√®res √©volutions.\nPour r√©sumer, MLFlow permet :\n\nde simplifier le suivi de l‚Äôentra√Ænement des mod√®les de machine learning gr√¢ce √† son API et √† son tracking server\nd‚Äôint√©grer les principaux frameworks de machine learning de mani√®re simple\nd‚Äôint√©grer son propre framework si besoin\nde standardiser son script d‚Äôentra√Ænement et donc de pouvoir l‚Äôindustrialiser, pour r√©aliser un fine-tuning des hyperparam√®tres, par exemple\nde packager ses mod√®les, de sorte √† pouvoir les requ√™ter de mani√®re simple et harmonis√©e entre les diff√©rents frameworks\nde stocker ses mod√®les de mani√®re pertinente en leur affectant des tags et en favorisant le suivi de leur cycle de vie"
  },
  {
    "objectID": "chapters/projects-architecture.html",
    "href": "chapters/projects-architecture.html",
    "title": "Structure des projets",
    "section": "",
    "text": "D√©rouler les slides ci-dessous ou cliquer ici pour afficher les slides en plein √©cran."
  },
  {
    "objectID": "chapters/projects-architecture.html#demonstration-by-example",
    "href": "chapters/projects-architecture.html#demonstration-by-example",
    "title": "Structure des projets",
    "section": "Demonstration by Example",
    "text": "Demonstration by Example\nHere‚Äôs an example of a project structure that might bring back memories:\n‚îú‚îÄ‚îÄ report.qmd\n‚îú‚îÄ‚îÄ correlation.png\n‚îú‚îÄ‚îÄ data.csv\n‚îú‚îÄ‚îÄ data2.csv\n‚îú‚îÄ‚îÄ fig1.png\n‚îú‚îÄ‚îÄ figure 2 (copy).png\n‚îú‚îÄ‚îÄ report.pdf\n‚îú‚îÄ‚îÄ partial data.csv\n‚îú‚îÄ‚îÄ script.R\n‚îî‚îÄ‚îÄ script_final.py\nSource : eliocamp.github.io\nThe following project structure makes it difficult to understand the project. Some key questions arise:\n\nWhat are the input data to the pipeline?\nIn what order are the intermediate data generated?\nWhat is the purpose of the graphical outputs?\nAre all the scripts actually used in this project?\n\nBy structuring the folder using simple rules ‚Äî for example, organizing it into inputs and outputs folders ‚Äî we can significantly improve the project‚Äôs readability.\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ raw\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.csv\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data2.csv\n‚îÇ   ‚îî‚îÄ‚îÄ derived\n‚îÇ       ‚îî‚îÄ‚îÄ partial data.csv\n‚îú‚îÄ‚îÄ src\n‚îÇ   ‚îú‚îÄ‚îÄ script.py\n‚îÇ   ‚îú‚îÄ‚îÄ script_final.py\n‚îÇ   ‚îî‚îÄ‚îÄ report.qmd\n‚îî‚îÄ‚îÄ output\n    ‚îú‚îÄ‚îÄ fig1.png\n    ‚îú‚îÄ‚îÄ figure 2 (copy).png\n    ‚îú‚îÄ‚îÄ figure10.png\n    ‚îú‚îÄ‚îÄ correlation.png\n    ‚îî‚îÄ‚îÄ report.pdf\n\n\n\n\n\n\nNote\n\n\n\nSince Git is a prerequisite, every project includes a .gitignore file (this is especially important when working with data that must not end up on Github or Gitlab).\nA project also includes a README.md file at the root ‚Äî we will come back to this later.\nA project using continuous integration will also include specific files:\n\nif you‚Äôre using Gitlab, the instructions are stored in the gitlab-ci.yml file;\nif you‚Äôre using Github, this happens in the .github/workflows directory.\n\n\n\nBy simply changing the file names, the project structure becomes much more readable:\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ .gitignore\n‚îú‚îÄ‚îÄ data\n‚îÇ   ‚îú‚îÄ‚îÄ raw\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dpe_logement_202103.csv\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dpe_logement_202003.csv\n‚îÇ   ‚îî‚îÄ‚îÄ derived\n‚îÇ       ‚îî‚îÄ‚îÄ dpe_logement_merged_preprocessed.csv\n‚îú‚îÄ‚îÄ src\n‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py\n‚îÇ   ‚îú‚îÄ‚îÄ generate_plots.py\n‚îÇ   ‚îî‚îÄ‚îÄ report.qmd\n‚îî‚îÄ‚îÄ output\n    ‚îú‚îÄ‚îÄ histogram_energy_diagnostic.png\n    ‚îú‚îÄ‚îÄ barplot_consumption_pcs.png\n    ‚îú‚îÄ‚îÄ correlation_matrix.png\n    ‚îî‚îÄ‚îÄ report.pdf\nNow, the type of input data to the pipeline is clear, and the relationship between scripts, intermediate data, and outputs is transparent."
  },
  {
    "objectID": "chapters/projects-architecture.html#separating-code-data-and-execution-environment-storage",
    "href": "chapters/projects-architecture.html#separating-code-data-and-execution-environment-storage",
    "title": "Structure des projets",
    "section": "Separating Code, Data, and Execution Environment Storage",
    "text": "Separating Code, Data, and Execution Environment Storage\nSeparating the storage of code, data, and the execution environment is important for several reasons:\n\nData Security\nBy separating data from code, it‚Äôs harder to accidentally access sensitive information.\nConsistency and Portability\nAn isolated environment ensures that the code runs reproducibly, regardless of the host machine.\nModularity and Flexibility\nYou can adapt or update components (code, data, environment) independently.\n\nThe next chapter will focus on dependency management. It will show how to link the environment and code to improve project portability."
  },
  {
    "objectID": "chapters/projects-architecture.html#sensitive-configurations-secrets-and-tokens",
    "href": "chapters/projects-architecture.html#sensitive-configurations-secrets-and-tokens",
    "title": "Structure des projets",
    "section": "Sensitive Configurations: Secrets and Tokens",
    "text": "Sensitive Configurations: Secrets and Tokens\nRunning code may depend on personal parameters (authentication tokens, passwords‚Ä¶). They should never appear in shared source code.\n‚úÖ Best practice: store these configurations in a separate file, not versioned (.gitignore), in YAML format ‚Äî more readable than JSON.\n\nExample secrets.yaml\ntoken:\n    api_insee: \"toto\"\n    api_github: \"tokengh\"\npwd:\n    base_pg: \"monmotdepasse\"\n\n\nReading in Python\nimport yaml\n\nwith open('secrets.yaml') as f:\n    secrets = yaml.safe_load(f)\n\n# using the secret\njeton_insee = secrets['token']['api_insee']\nThis mechanism turns the file into a Python dictionary that is easy to navigate.\n\n\n\n\n\n\nNoteTests unitaires\n\n\n\n\n\nLes tests unitaires sont des tests automatis√©s qui v√©rifient le bon fonctionnement d‚Äôune unit√© de code, comme une fonction ou une m√©thode. L‚Äôobjectif est de s‚Äôassurer que chaque unit√© de code fonctionne correctement avant d‚Äô√™tre int√©gr√©e dans le reste du programme.\nLes tests unitaires sont utiles lorsqu‚Äôon travaille sur un code de taille cons√©quente ou lorsqu‚Äôon partage son code √† d‚Äôautres personnes, car ils permettent de s‚Äôassurer que les modifications apport√©es ne cr√©ent pas de nouvelles erreurs.\nEn Python, on peut utiliser le package unittest pour √©crire des tests unitaires. Voici un exemple tir√© de ce site :\n# fichier test_str.py\nimport unittest\n\nclass ChaineDeCaractereTest(unittest.TestCase):\n\n    def test_reversed(self):\n        resultat = reversed(\"abcd\")\n        self.assertEqual(\"dcba\", \"\".join(resultat))\n\n    def test_sorted(self):\n        resultat = sorted(\"dbca\")\n        self.assertEqual(['a', 'b', 'c', 'd'], resultat)\n\n    def test_upper(self):\n        resultat = \"hello\".upper()\n        self.assertEqual(\"HELLO\", resultat)\n\n    def test_erreur\n\nif __name__ == '__main__':\n    unittest.main()\nPour v√©rifier que les tests fonctionnent, on ex√©cute ce script depuis la ligne de commande :\npython3 test_str.py\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nSi on √©crit des tests unitaires, il est important de les maintenir ! Prendre du temps pour √©crire des tests unitaires qui ne sont pas maintenus et donc ne renvoient plus de diagnostics pertinents est du temps perdu."
  },
  {
    "objectID": "chapters/projects-architecture.html#transformer-son-projet-en-package-python",
    "href": "chapters/projects-architecture.html#transformer-son-projet-en-package-python",
    "title": "Structure des projets",
    "section": "Transformer son projet en package Python",
    "text": "Transformer son projet en package Python\nLe package est la structure aboutie d‚Äôun projet Python autosuffisant. Il s‚Äôagit d‚Äôune mani√®re formelle de contr√¥ler la reproductibilit√© d‚Äôun projet car :\n\nle package assure une gestion coh√©rente des d√©pendances\nle package offre une certaine structure pour la documentation\nle package facilite la r√©utilisation du code\nle package permet des √©conomies d‚Äô√©chelle, car on peut r√©utiliser l‚Äôun des packages pour un autre projet\nle package facilite le debuggage car il est plus facile d‚Äôidentifier une erreur quand elle est dans un package\n‚Ä¶\n\nEn Python, le package est une structure peu contraignante si on a adopt√© les bonnes pratiques de structuration de projet. √Ä partir de la structure modulaire pr√©c√©demment √©voqu√©e, il n‚Äôy a qu‚Äôun pas vers le package : l‚Äôajout d‚Äôun fichier pyproject.toml qui contr√¥le la construction du package (voir ici).\nIl existe plusieurs outils pour installer un package dans le syst√®me √† partir d‚Äôune structure de fichiers locale. Les deux principaux sont :\n\nsetuptools\npoetry\n\nLe package fait la transition entre un code modulaire et un code portable, concept sur lequel nous reviendrons dans le prochain chapitre.\n:::"
  },
  {
    "objectID": "chapters/projects-architecture.html#cookiecutters",
    "href": "chapters/projects-architecture.html#cookiecutters",
    "title": "Structure des projets",
    "section": "Cookiecutters",
    "text": "Cookiecutters\nIn Python, there are standardized project structure templates: called cookiecutters. These are community-maintained templates for project directory trees (.py files as well as documentation, config, etc.) that can be used as a starting point.\nThe idea behind cookiecutter is to offer ready-to-use templates to initialize a project with a scalable structure. We‚Äôll follow the structure proposed by the cookiecutter data science community template.\nThe syntax to use is:\n$ pip install cookiecutter\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science\nThe template is customizable, particularly for integrating with remote storage systems. The generated directory tree is large enough to support diverse project types ‚Äî you typically won‚Äôt need every single component included by default.\n\n\nFull structure generated by the cookiecutter data science template\n\n(Identique au bloc ci-dessus ‚Äì d√©j√† internationalis√©)\n\n\n\n\n\n\n\nNoteUnit Tests\n\n\n\n\n\nUnit tests are automated tests that verify the proper functioning of a unit of code, such as a function or a method. The goal is to ensure that each unit of code works correctly before being integrated into the rest of the program.\nUnit tests are helpful when working with large codebases or when sharing code with others, because they ensure that modifications don‚Äôt introduce new bugs.\nIn Python, the unittest package can be used to write unit tests. Here‚Äôs an example from this site:\n# file test_str.py\nimport unittest\n\nclass StringTest(unittest.TestCase):\n\n    def test_reversed(self):\n        result = reversed(\"abcd\")\n        self.assertEqual(\"dcba\", \"\".join(result))\n\n    def test_sorted(self):\n        result = sorted(\"dbca\")\n        self.assertEqual(['a', 'b', 'c', 'd'], result)\n\n    def test_upper(self):\n        result = \"hello\".upper()\n        self.assertEqual(\"HELLO\", result)\n\n    def test_erreur\n\nif __name__ == '__main__':\n    unittest.main()\nTo verify that the tests work, run this script from the command line:\npython3 test_str.py\n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nIf you write unit tests, it‚Äôs important to maintain them! Spending time writing unit tests that are no longer maintained and no longer provide useful diagnostics is time wasted."
  },
  {
    "objectID": "chapters/projects-architecture.html#turning-your-project-into-a-python-package",
    "href": "chapters/projects-architecture.html#turning-your-project-into-a-python-package",
    "title": "Structure des projets",
    "section": "Turning Your Project Into a Python Package",
    "text": "Turning Your Project Into a Python Package\nA package is the finalized structure of a self-contained Python project. It provides a formal way to ensure the reproducibility of a project because:\n\nthe package handles dependencies consistently\nthe package offers built-in documentation structure\nthe package facilitates code reuse\nthe package enables scalability‚Äîyou can reuse a package across projects\nthe package simplifies debugging since it‚Äôs easier to pinpoint errors in a package\n‚Ä¶\n\nIn Python, packages are relatively easy to set up if you follow good project structuring practices. From the previously discussed modular structure, it‚Äôs a short step to a package: simply add a pyproject.toml file to control how the package is built (see here).\nThere are several tools for installing a package locally from a file structure. The two most common are:\n\nsetuptools\npoetry\n\nThe package bridges the gap between modular and portable code, a topic we‚Äôll revisit in the next chapter."
  },
  {
    "objectID": "chapters/projects-architecture.html#footnotes",
    "href": "chapters/projects-architecture.html#footnotes",
    "title": "Structure des projets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this regard, Python is much more reliable than R. In R, if two scripts use functions with the same name but from different packages, there will be a conflict. In Python, each module is imported as its own package.‚Ü©Ô∏é"
  },
  {
    "objectID": "chapters/galerie/2024/model.html",
    "href": "chapters/galerie/2024/model.html",
    "title": "Mod√®le de carte",
    "section": "",
    "text": "Une description en quelques mots du projet\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "chapters/galerie/2024/resultAthle.html",
    "href": "chapters/galerie/2024/resultAthle.html",
    "title": "ResultAthle",
    "section": "",
    "text": "ResultAthle est un projet visant √† rendre les outils statistiques d‚Äôanalyse de performance plus accessibles au niveau amateur en athl√©tisme. Il aborde les d√©fis de la collecte de r√©sultats et le manque de statistiques descriptives accessibles pour les clubs.\n\n\n\nReuseCC BY-NC 4.0"
  }
]