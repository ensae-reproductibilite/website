:::: {.application}

## Application 8a: ajout de données sur le système de stockage `S3`

Pour commencer, à partir de la ligne de commande,
utiliser l'utilitaire [MinIO](https://min.io/docs/minio/linux/reference/minio-mc.html)
pour copier les données `data/raw/data.csv`
vers votre
bucket personnel. Les données intermédiaires
peuvent être laissées en local mais doivent être ajoutées
au `.gitignore`.


<details>
<summary>Indice</summary>

Structure à adopter:

```{.python filename="terminal"}
BUCKET_PERSONNEL="nom_utilisateur_sspcloud"
mc cp data/raw/data.csv s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.csv
```

en modifiant la variable `BUCKET_PERSONNEL`, l'emplacement de votre bucket personnel
</details>

Pour se simplifier la vie, dans les prochaines applications,
on va utiliser des URL de téléchargement des fichiers
(comme si ceux-ci étaient sur n'importe quel espace de stockage) plutôt que d'utiliser
une librairie `S3` compatible comme `boto3` ou `s3fs`.

Néanmoins, il est utile de les utiliser une fois pour comprendre la logique. Pour aller plus loin sur ces librairies, vous pouvez consulter [cette page](https://pythonds.linogaliana.fr/content/modern-ds/s3.html#cas-pratique-stocker-les-donn%C3%A9es-de-son-projet-sur-le-ssp-cloud) du cours de 2A de _Python pour la data science_.

Pour commencer, on va lister les fichiers se trouvant dans un _bucket_. En ligne de commande, sur notre poste local, on ferait `ls` ([cf. Linux 101](/chapters/linux-101.qmd)). Cela ne va pas beaucoup différer avec les librairies _cloud native_:

::: {.panel-tabset}

## Avec `s3fs`

Dans un _notebook_, copier-coller ce code, le modifier et exécuter:

```{.python}
import s3fs

fs = s3fs.S3FileSystem(client_kwargs={"endpoint_url": "https://minio.lab.sspcloud.fr"})

MY_BUCKET = "mon_nom_utilisateur_sspcloud" #<1>
CHEMIN = "ensae-reproductibilite/data/raw" #<2>
fs.ls(f"s3://{MY_BUCKET}/{CHEMIN}")
```
1. Changer avec le bucket
2. Changer en fonction du chemin voulu


## Avec `mc`

Dans un terminal, copier-coller ligne à ligne ce code, le modifier et exécuter:


```{.python}
import s3fs

MY_BUCKET="mon_nom_utilisateur_sspcloud" #<1>
CHEMIN = "ensae-reproductibilite/data/raw" #<2>
mc ls s3/${MY_BUCKET}/${CHEMIN}
```
1. Changer avec le bucket
2. Changer en fonction du chemin voulu


:::

::::


On va maintenant lire directement une donnée stockée sur `S3`. Pour illustrer le fait que cela change peu notre code d'être sur un système _cloud_ avec les librairies adaptées, on va lire directement un fichier `CSV` stocké sur le `SSPCloud`, sans passer par un fichier en local[^fss3].

[^fss3]: Alors oui, c'est vrai, `s3` se distingue d'un système de fichiers classiques comme on peut le lire dans certains _posts_ énervés sur la question (par exemple sur [Reddit](https://www.reddit.com/r/aws/comments/dplfoa/why_is_s3fs_such_a_bad_idea/?tl=fr)). Mais du point de vue de l'utilisateur `Python` plutôt que de l'architecte _cloud_, on va avoir assez peu de différence avec un système de fichier local. C'est pour le mieux, cela réduit la difficulté à rentrer dans cette technologie.

:::: {.application}

## Application 8b: importer une donnée depuis un système de stockage `S3`

Pour illustrer la cohérence avec un système de fichier local, voici trois solutions pour lire le fichier que vous venez de mettre sur `S3`. Attention, il faut avoir des jetons de connexion à `S3` à jour. Si vous avez cette erreur

> A client error (InvalidAccessKeyId) occurred when calling the ListBuckets operation: The AWS Access Key Id you provided does not exist in our records.

c'est que vos identifiants de connexion ne sont plus à jour (pour des raisons de sécurité, ils sont régulièrement renouvelés). Dans ce cas, recréez un service `VSCode` avec le bouton proposé plus haut.

Dans un _notebook_, copier-coller et mettre à jour ces deux variables qui seront utilisées dans différents exemples:

```{.python}
MY_BUCKET = "mon_nom_utilisateur_sspcloud" #<1>
CHEMIN_FICHIER = "ensae-reproductibilite/data/raw/data.csv" #<2>
```
1. Changer avec le bucket
2. Changer en fonction du chemin voulu

::: {.panel-tabset}

## Avec `Pandas` et `s3fs`

```{.python}
import s3fs
import pandas as pd

fs = s3fs.S3FileSystem(client_kwargs={"endpoint_url": "https://minio.lab.sspcloud.fr"})

with fs.open(f"s3://{MY_BUCKET}/{CHEMIN_FICHIER}") as f:
    df = pd.read_csv(f)

df
```

## Avec `Pyarrow` et `s3fs`

```{.python}
import s3fs
from pyarrow import csv

fs = s3fs.S3FileSystem(client_kwargs={"endpoint_url": "https://minio.lab.sspcloud.fr"})

with fs.open(f"s3://{MY_BUCKET}/{CHEMIN_FICHIER}") as f:
    df = csv.read_csv(f)

df
```

## Avec `DuckDB`

```{.python}
import os
import duckdb

con = duckdb.connect(database=":memory:")

con.execute(
    f"""
CREATE SECRET secret (
    TYPE S3,
    KEY_ID '{os.environ["AWS_ACCESS_KEY_ID"]}',
    SECRET '{os.environ["AWS_SECRET_ACCESS_KEY"]}',
    ENDPOINT 'minio.lab.sspcloud.fr',
    SESSION_TOKEN '{os.environ["AWS_SESSION_TOKEN"]}',
    REGION 'us-east-1',
    URL_STYLE 'path',
    SCOPE 's3://{MY_BUCKET}/'
);
"""
)

query_definition = f"SELECT * FROM read_csv('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')"
df = con.sql(query_definition)

df
```

:::

Pour illustrer le fonctionnement encore plus simple de S3 avec les fichiers `Parquet`, on propose de copier un `Parquet` mis à disposition dans un _bucket_ collectiv vers votre _bucket_ personnel:

```{.python}
BUCKET_PERSONNEL="nom_utilisateur_sspcloud" #<1>

curl -o rp.parquet "https://minio.lab.sspcloud.fr/projet-formation/bonnes-pratiques/data/REGION=11/part-0.parquet" #<2>

mc cp rp.parquet s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/example/rp.parquet

rm rp.parquet
```
1. Remplacer par le nom de votre _bucket_.
2. Télécharger le fichier `Parquet` mis à dispositoin
3. Copier dans le _bucket_ voulu
4. Effacer le fichier copié temporairement en local

Pour lire ceux-ci, tester les exemples de code suivants:

```{.python}
MY_BUCKET = "mon_nom_utilisateur_sspcloud" #<1>
CHEMIN_FICHIER = "ensae-reproductibilite/data/example/rp.parquet"
```
1. Remplacer ici par la valeur appropriée

::: {.panel-tabset}

## Avec `Pandas` et `s3fs`

```{.python}
import s3fs
import pandas as pd

fs = s3fs.S3FileSystem(client_kwargs={"endpoint_url": "https://minio.lab.sspcloud.fr"})

df = pd.read_parquet(f"s3://{MY_BUCKET}/{CHEMIN_FICHIER}", filesystem=fs)

df
```

## Avec `Pyarrow` et `s3fs`

```{.python}
import pyarrow as pa
import pyarrow.parquet as pq

s3 = pa.fs.S3FileSystem(endpoint_override ="https://minio.lab.sspcloud.fr")

df = pq.read_table(f"{MY_BUCKET}/{CHEMIN_FICHIER}", filesystem=s3)

df
```

## Avec `DuckDB`

```{.python}
import os
import duckdb

con = duckdb.connect(database=":memory:")

con.execute(
    f"""
CREATE SECRET secret (
    TYPE S3,
    KEY_ID '{os.environ["AWS_ACCESS_KEY_ID"]}',
    SECRET '{os.environ["AWS_SECRET_ACCESS_KEY"]}',
    ENDPOINT 'minio.lab.sspcloud.fr',
    SESSION_TOKEN '{os.environ["AWS_SESSION_TOKEN"]}',
    REGION 'us-east-1',
    URL_STYLE 'path',
    SCOPE 's3://{MY_BUCKET}/'
);
"""
)

query_definition = f"SELECT * FROM read_parquet('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')"
df = con.sql(query_definition)

df
```

:::

Pour aller plus loin sur le format `Parquet`, notamment découvrir comment importer des données partitionnées, vous pouvez traduire en `Python` les exemples issus de la [formation aux bonnes pratiques avec `R`](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html#/application-3-2) de l'Insee.

::::



::: {.application}
## Application 8c: privilégier le format `Parquet` dans notre chaîne

Dans `main.py`, remplacer le format csv initialement prévu par un format parquet:

```{.python file="main.py"}
data_train_path = os.environ.get("train_path", "data/derived/train.parquet")
data_test_path = os.environ.get("test_path", "data/derived/test.parquet")
```

Et modifier l'écriture des données pour utiliser `to_parquet` plutôt que `to_csv` pour écrire les fichiers intermédiaires:

```{.python filename="main.py"}
pd.concat([X_train, y_train], axis = 1).to_parquet(data_train_path)
pd.concat([X_test, y_test], axis = 1).to_parquet(data_test_path)
```


:::


::: {.application}

## Application 8d: partage de données sur le système de stockage `S3`

Par défaut, le contenu de votre _bucket_ est privé, seul vous y avez accès. Pour pouvoir lire votre donnée, vos
applications externes devront utiliser des jetons vous identifiant. Ici, comme nous utilisons une donnée publique,
vous pouvez rendre accessible celle-ci à tous en lecture. Dans le jargon S3, cela signifie donner un accès anonyme à votre donnée.

Le modèle de commande à utiliser dans le terminal est le suivant:

```{.bash filename="terminal"}
BUCKET_PERSONNEL="nom_utilisateur_sspcloud" #<1>

mc anonymous set download s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/
```
1. Remplacer par le nom de votre _bucket_.



Les URL de téléchargement seront de la forme
`https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/data.csv`


- Remplacer la définition de  `data_path` pour utiliser, par défaut, directement l'URL dans l'import. Modifier, si cela est pertinent, aussi votre fichier `.env`.

```{.python file="main.py"}
URL_RAW = "" #<1>
data_path = os.environ.get("data_path", URL_RAW)
```
1. Modifier avec `URL_RAW` un lien de la forme `"https://minio.lab.sspcloud.fr/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.csv"` (ne laissez pas `${BUCKET_PERSONNEL}`, remplacez par la vraie valeur!).


- Ajouter le dossier `data/` au `.gitignore` ainsi que les fichiers `*.parquet`
- Supprimer le dossier `data` de votre projet et faites `git rm --cached -r data`

- Vérifier le bon fonctionnement de votre application.


:::

Maintenant qu'on a arrangé la structure de notre projet, c'est l'occasion
de supprimer le code qui n'est plus nécessaire au bon fonctionnement de notre
projet (cela réduit la charge de maintenance[^pourapres]).

Pour vous aider, vous pouvez
utiliser [`vulture`](https://pypi.org/project/vulture/) de manière itérative
pour vous assister dans le nettoyage de votre code.

```{.python filename="terminal"}
pip install vulture
vulture .
```

<details>
<summary>
Exemple de sortie
</summary>

```{.bash filename="terminal"}
vulture .
```

```{.python}
src/data/import_data.py:3: unused function 'split_and_count' (60% confidence)
src/pipeline/build_pipeline.py:12: unused function 'split_train_test' (60% confidence)
```

</details>


[^pourapres]: Lorsqu'on développe du code qui finalement ne s'avère plus nécessaire, on a souvent un cas de conscience à le supprimer et on préfère le mettre de côté. Au final, ce syndrôme de Diogène est mauvais pour la pérennité du projet : on se retrouve à devoir maintenir une base de code qui n'est, en pratique, pas utilisée. Ce n'est pas un problème de supprimer un code ; si finalement celui-ci s'avère utile, on peut le retrouver grâce à l'historique `Git` et les outils de recherche sur `Github`. Le _package_ `vulture` est très pratique pour diagnostiquer les morceaux de code inutiles dans un projet.

{{< checkpoint appli8 >}}

