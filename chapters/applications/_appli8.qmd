:::: {.application}

## Application 8a: ajout de données sur le système de stockage `S3`

Le SSPCloud repose sur une implémentation particulière du protocole S3 qui s'appelle [MinIO](https://min.io/docs/minio/linux/reference/minio-mc.html). Cela étend les fonctionnalités de manipulation de fichiers en ligne de commande à des données n'étant pas présentes dans le _filesystem_ local mais dans le _cloud_. En pratique

En pratique, l'implémentation MinIO permet notamment de manipuler des fichiers à distance comme s'ils étaient en local. Les commandes de manipulation de fichier du chapitre [Linux 101](/chapters/linux101.qmd) (lister, copier, supprimer...) sont étendues grâce à l'ajout d'un petit `mc` devant celle-ci. 

Illustrons cela en copiant les données de départ vers votre bucket personnel. 

<details>
<summary>Indice</summary>

Structure à adopter:

```{.python filename="terminal"}
MY_BUCKET="nom_utilisateur_sspcloud"
mc cp data/raw/titanic.csv s3/${MY_BUCKET}/ensae-reproductibilite/data/raw/data.csv
```

en modifiant la variable `MY_BUCKET`, l'emplacement de votre bucket personnel
</details>

Avant de modifier notre code `Python`, on va lister les fichiers se trouvant dans notre _bucket_. En ligne de commande, sur notre poste local, on ferait `ls` ([cf. Linux 101](/chapters/linux101.qmd)). Cela ne va pas beaucoup différer avec les librairies _cloud native_:

```{.python filename="terminal"}
MY_BUCKET="mon_nom_utilisateur_sspcloud" #<1>
CHEMIN="ensae-reproductibilite/data/raw" #<2>
mc ls s3/${MY_BUCKET}/${CHEMIN}
```
1. Changer avec le bucket
2. Changer en fonction du chemin voulu

En utilisant l'explorateur de fichiers S3 du SSPCloud ([datalab.sspcloud.fr/file-explorer](https://datalab.sspcloud.fr/file-explorer)), faites une deuxième vérification que vos données sont bien sauvegardées sur `S3`. 

Vous pouvez ensuite supprimer le CSV de données présent dans votre dossier local, il ne nous sera plus utile. 

::::




On va maintenant lire directement une donnée stockée sur `S3` avec `Python`. Pour illustrer le fait qu'être sur un système _cloud_ avec les librairies adaptées change peu notre code, on va lire directement un fichier `CSV` stocké sur le `SSPCloud`, sans passer par un fichier en local[^fss3].

[^fss3]: Alors oui, c'est vrai, `S3` se distingue d'un système de fichiers classiques comme on peut le lire dans certains _posts_ énervés sur la question (par exemple sur [Reddit](https://www.reddit.com/r/aws/comments/dplfoa/why_is_s3fs_such_a_bad_idea/?tl=fr)). Mais du point de vue de l'utilisateur `Python` plutôt que de l'architecte _cloud_, on va avoir assez peu de différence avec un système de fichier local. C'est pour le mieux, cela réduit la difficulté à rentrer dans cette technologie.

Pour illustrer la cohérence avec un système de fichier local, voici trois solutions pour lire le fichier que vous venez de mettre sur `S3` (s'appuyant sur `s3fs`, `Arrow` ou `DuckDB`). Attention, il faut avoir des jetons de connexion à `S3` à jour. Si vous avez cette erreur

> A client error (InvalidAccessKeyId) occurred when calling the ListBuckets operation: The AWS Access Key Id you provided does not exist in our records.

c'est que vos identifiants de connexion ne sont plus à jour (pour des raisons de sécurité, ils sont régulièrement renouvelés). Dans ce cas, consulter @imp-no-sspcloud.



:::: {.application}

## Application 8b: importer une donnée depuis un système de stockage `S3`

Cet exercice illustre trois manières de lire des données sur `S3` au format `CSV`. Ce choix est arbitraire car il existe de nombreux frameworks concurrents ou complémentaires à `Pandas`, `Arrow` ou `DuckDB` qui permettent de lire des données depuis `S3`.

Pour le moment on va garder comme _framework_ de prédilection `Pandas` mais la bascule vers `Parquet` nous fera privilégier `DuckDB` dans les prochaines applications.

Dans un _notebook_, copier-coller et mettre à jour ces deux variables qui seront utilisées dans différents exemples:

```{.python}
MY_BUCKET = "mon_nom_utilisateur_sspcloud" #<1>
CHEMIN_FICHIER = "ensae-reproductibilite/data/raw/data.csv" #<2>
```
1. Changer avec le bucket
2. Changer en fonction du chemin voulu

Maintenant lire la donnée depuis S3 avec ce code:

::: {.panel-tabset}

## Avec `Pandas` et `s3fs`

```{.python}
import s3fs
import pandas as pd

fs = s3fs.S3FileSystem(client_kwargs={"endpoint_url": "https://minio.lab.sspcloud.fr"})

with fs.open(f"s3://{MY_BUCKET}/{CHEMIN_FICHIER}") as f:
    df = pd.read_csv(f)

df
```

## Avec `Pyarrow` et `s3fs`

```{.python}
import s3fs
from pyarrow import csv

fs = s3fs.S3FileSystem(client_kwargs={"endpoint_url": "https://minio.lab.sspcloud.fr"})

with fs.open(f"s3://{MY_BUCKET}/{CHEMIN_FICHIER}") as f:
    df = csv.read_csv(f)

df
```

## Avec `DuckDB`

```{.python}
import os
import duckdb

con = duckdb.connect(database=":memory:")

query_definition = f"SELECT * FROM read_csv('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')"
df = con.sql(query_definition)

df
```

:::

::::

Pour illustrer le fonctionnement simple de S3 avec les fichiers `Parquet`, on propose de faire la même chose. Ici il n'est pas pertinent de mesurer le gain de temps car le fichier est très léger en CSV et s'importe ainsi instantanément. Pour faire des exercices plus pertinents sur ce sujet, afin de découvrir certaines bonnes pratiques, regarder les exercices supplémentaires dans [le chapitre dédié à `Parquet`](/chapters/big-data.qmd)).


:::: {.application}

## Application 8c: importer une donnée depuis un système de stockage `S3`

D'abord, on va convertir notre fichier au format Parquet avec ce code (à faire tourner qu'une fois donc à ne pas mettre dans `main.py` mais dans un _notebook_ ou un script jetable)

```{.python}
import os
import duckdb

MY_BUCKET = "mon_nom_utilisateur_sspcloud" #<1>
CHEMIN_FICHIER = "ensae-reproductibilite/data/raw/data.csv" #<2>

con = duckdb.connect(database=":memory:")

query_definition = f"SELECT * FROM read_csv('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')"
con.sql(
    f"""
        COPY (
            SELECT * 
            FROM read_csv_auto('s3://{MY_BUCKET}/{CHEMIN_FICHIER}')
        )
        TO 's3://{MY_BUCKET}/{CHEMIN_FICHIER.replace("csv", "parquet")}'
        (FORMAT PARQUET);
    """
)
```
1. Changer avec le bucket
2. Changer en fonction du chemin voulu


```{.python}
MY_BUCKET="mon_nom_utilisateur_sspcloud" #<1>
CHEMIN_PARQUET="ensae-reproductibilite/data/raw/data.parquet" #<2>
```
1. Remplacer ici par la valeur appropriée
2. Changer en fonction du chemin voulu


::: {.panel-tabset}

## Avec `DuckDB`

```{.python}
import os
import duckdb

con = duckdb.connect(database=":memory:")

query_definition = f"SELECT * FROM read_parquet('s3://{MY_BUCKET}/{CHEMIN_PARQUET}')"
df = con.sql(query_definition)

df
```


## Avec `Pandas` et `s3fs`

```{.python}
import s3fs
import pandas as pd

chemin

fs = s3fs.S3FileSystem(client_kwargs={"endpoint_url": "https://minio.lab.sspcloud.fr"})

df = pd.read_parquet(f"s3://{MY_BUCKET}/{CHEMIN_PARQUET}", filesystem=fs)
```


:::

## Avec `Pyarrow` et `s3fs`

```{.python}
import pyarrow as pa
import pyarrow.parquet as pq

s3 = pa.fs.S3FileSystem(endpoint_override ="https://minio.lab.sspcloud.fr")

df = pq.read_table(f"{MY_BUCKET}/{CHEMIN_PARQUET}", filesystem=s3)

df
```

::::

::: {.callout-tip collapse="true"}
## Exercices supplémentaires sur le format `Parquet`

Nous n'avons pas explicité pourquoi `Parquet` est un choix pertinent pour la plupart des chaînes _data_. Ici la donnée est de taille réduite, cela ne nous pénalisera pas beaucoup de faire du `CSV` et du `Pandas` même si nous recommandons plutôt de faire du `Parquet` et de pousser le plus loin possible `DuckDB` avant de revenir à `Pandas`. 

Pour découvrir, en pratique, les raisons de ces choix techniques, vous pouvez aller voir les exercices supplémentaires à la fin du [chapitre sur les données](/chapters/big-data.qmd)).

:::




::: {.application}
## Application 8d: adapter notre chaîne pour lire des données `Parquet` avec `DuckDB`

Par défaut, le contenu de votre _bucket_ est privé, seul vous y avez accès. Pour pouvoir lire votre donnée, vos applications externes devront utiliser des jetons vous identifiant. Ici, comme nous utilisons une donnée publique, vous pouvez rendre accessible celle-ci à tous en lecture. Dans le jargon S3, cela signifie donner un accès anonyme à votre donnée.

Le modèle de commande à utiliser dans le terminal est le suivant:

```{.bash filename="terminal"}
BUCKET_PERSONNEL="nom_utilisateur_sspcloud" #<1>

mc anonymous set download s3/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/
```
1. Remplacer par le nom de votre _bucket_.


Les URL de téléchargement seront de la forme
`https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/<filename>.<extension>`

On va adapter progressivement notre chaîne à l'utilisation du format Parquet.

D'abord, il faut faire évoluer nos chemins:

```{.python file="main.py"}
URL_RAW = "" #<1>
```
1. Modifier `URL_RAW` avec un lien de la forme `"https://minio.lab.sspcloud.fr/${BUCKET_PERSONNEL}/ensae-reproductibilite/data/raw/data.parquet"` (ne laissez pas `${BUCKET_PERSONNEL}`, remplacez par la vraie valeur!).

Maintenant, il faut lire les données `Parquet` avec DuckDB. Modifier le code d'import des données pour utiliser DuckDB plutôt que Pandas pour la lecture. 

Il va falloir modifier quelque peu la suite du code pour s'adapter au nouveau moteur d'exécution de nos traitements de données. Essayez d'utiliser le plus tard possible `Pandas` dans la chaîne (via `.to_df()`): normalement vous ne pourrez plus éviter `Pandas` à partir du moment où vous passerez au _pipeline_ de _machine learning_ (`Scikit` ne supporte pas les _dataframes_ `DuckDB`). Mais, si vous avez utilisé les _checkpoints_ ou gardé la logique initiale d'utilisation de `DuckDB` sur le *dataframe* `Pandas`, les évolutions à faire du script sont minimes.  

Enfin, nettoyer notre dossier de données en local maintenant que le CSV n'est plus nécessaire. 

- Ajouter le dossier `data/` au `.gitignore` ainsi que les fichiers `*.parquet`
- Supprimer le dossier `data` de votre projet et faites `git rm --cached -r data`

:::


Maintenant qu'on a arrangé la structure de notre projet, c'est l'occasion
de supprimer le code qui n'est plus nécessaire au bon fonctionnement de notre
projet (cela réduit la charge de maintenance[^pourapres]).

Pour vous aider, vous pouvez
utiliser [`vulture`](https://pypi.org/project/vulture/) de manière itérative
pour vous assister dans le nettoyage de votre code.

```{.python filename="terminal"}
pip install vulture
vulture .
```

<details>
<summary>
Exemple de sortie
</summary>

```{.bash filename="terminal"}
vulture .
```

```{.python}
main.py:53: unused variable 'jeton_api' (60% confidence)
main.py:147: unused variable 'rdmf_score_tr' (60% confidence)
```

</details>


[^pourapres]: Lorsqu'on développe du code qui finalement ne s'avère plus nécessaire, on a souvent un cas de conscience à le supprimer et on préfère le mettre de côté. Au final, ce syndrôme de Diogène est mauvais pour la pérennité du projet : on se retrouve à devoir maintenir une base de code qui n'est, en pratique, pas utilisée. Ce n'est pas un problème de supprimer un code ; si finalement celui-ci s'avère utile, on peut le retrouver grâce à l'historique `Git` et les outils de recherche sur `Github`. Le _package_ `vulture` est très pratique pour diagnostiquer les morceaux de code inutiles dans un projet.

{{< checkpoint appli8 >}}

