::: {.application}
## Application 24 : industrialisation des entraînements avec `Argo Workflow`

A l'heure actuelle, notre entraînement ne dépend que d'un hyperparamètre fixé à partir de la ligne de commande: `n_trees`. Nous allons commencer par ajouter un argument à notre chaine de production (code applicatif):

1. Dans `train.py`, dans la section relative au _parsing_ de nos arguments, ajouter ce bout de code

```{.python}
parser.add_argument(
    "--max_features",
    type=str, default="sqrt",
    choices=['sqrt', 'log2'],
    help="Number of features to consider when looking for the best split"
)
```

et remplacer la définition de `MAX_FEATURES` par l'argument fourni en ligne de commande:

```{.python}
MAX_FEATURES = args.max_features
```


2. Faire un _commit_, taguer cette version (`v0.0.4`) et _pusher_ le _tag_

3. Maintenant, dans le dépôt `GitOps`, créer un fichier `argo-workflow/manifest.yaml`

<details>

<summary>
Le modèle proposé
</summary>

```{.python}
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
generateName: titanic-training-workflow-
namespace: user-lgaliana
spec:
entrypoint: main
serviceAccountName: workflow
arguments:
    parameters:
    # The MLflow tracking server is responsible to log the hyper-parameter and model metrics.
    - name: mlflow-tracking-uri
        value: https://user-${USERNAME}-mlflow.user.lab.sspcloud.fr/ #<1>
    - name: mlflow-experiment-name
        value: titanicml #<2>
    - name: model-training-conf-list
        value: |
        [
            { "n_trees": 10, "max_features": "log2" },
            { "n_trees": 20, "max_features": "sqrt" },
            { "n_trees": 20, "max_features": "log2" },
            { "n_trees": 50, "max_features": "sqrt" }
        ]
templates:
    # Entrypoint DAG template
    - name: main
    dag:
        tasks:
        # Task 0: Start pipeline
        - name: start-pipeline
            template: start-pipeline-wt
        # Task 1: Train model with given params
        - name: train-model-with-params
            dependencies: [ start-pipeline ]
            template: run-model-training-wt
            arguments:
            parameters:
                - name: max_features
                value: "{{item.max_features}}"
                - name: n_trees
                value: "{{item.n_trees}}"
            # Pass the inputs to the task using "withParam"
            withParam: "{{workflow.parameters.model-training-conf-list}}"
    # Now task container templates are defined
    # Worker template for task 0 : start-pipeline
    - name: start-pipeline-wt
    inputs:
    container:
        image: busybox
        command: [ sh, -c ]
        args: [ "echo Starting pipeline" ]
    # Worker template for task-1 : train model with params
    - name: run-model-training-wt
    inputs:
        parameters:
        - name: n_trees
        - name: max_features
    container:
        image: ****/application:v0.0.4 #<3>
        imagePullPolicy: Always
        command: [sh, -c]
        args: [
        "python3 train.py --n_trees={{inputs.parameters.n_trees}} --max_features={{inputs.parameters.max_features}}"
        ]
        env:
        - name: MLFLOW_TRACKING_URI
            value: "{{workflow.parameters.mlflow-tracking-uri}}"
        - name: MLFLOW_EXPERIMENT_NAME
            value: "{{workflow.parameters.mlflow-experiment-name}}"
        - name: AWS_DEFAULT_REGION
            value: us-east-1
        - name: AWS_S3_ENDPOINT
            value: minio.lab.sspcloud.fr
```
1. Changer pour votre entrepot de modèle
2. Le nom de l'expérimentation `MLFLow` dont nous allons avoir besoin (on propose de continuer sur `titanicml`)
3. Changer l'image `Docker` {{< fa brands docker >}} ici

</details>

4. Observer l'UI d'`Argo Workflow` dans vos services ouverts du _SSPCloud_. Vous devriez retrouver @fig-pipeline-argo dans celle-ci.

:::
