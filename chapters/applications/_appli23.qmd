::: {.application}
## Application 23 : passer en production un mod√®le avec MLFlow

1. Si vous avez entra√Æn√© plusieurs mod√®les avec des `n_trees` diff√©rents, utiliser l'interface de `MLFlow` pour s√©lectionner le "meilleur". Cliquer sur le mod√®le en question et faire l'action _"Register Model"_. L'enregistrer comme le mod√®le de _"production"_

2. Rendez-vous sur l'onglet `Models` et observez cet entrep√¥t de mod√®les. Cliquez sur le mod√®le de production. Vous pourrez par ce biais suivre ses diff√©rentes versions.

3. Ouvrir un notebook temporaire et observer le r√©sultat.

<details>
<summary>
Exemple de code √† tester
</summary>


```{.python file="Dans une cellule de notebook"}
import mlflow
import pandas as pd

model_name = "production"
model_version = "latest"

# Load the model from the Model Registry
model_uri = f"models:/{model_name}/{model_version}"
logged_model = mlflow.sklearn.load_model(model_uri)

# GENERATE PREDICTION DATA ---------------------

def create_data(
    sex: str = "female",
    age: float = 29.0,
    fare: float = 16.5,
    embarked: str = "S",
) -> str:
    """
    """

    df = pd.DataFrame(
        {
            "Sex": [sex],
            "Age": [age],
            "Fare": [fare],
            "Embarked": [embarked],
        }
    )

    return df


data = pd.concat(
    [create_data(age=40), create_data(sex="male")]
)

# PREDICTION ---------------------

logged_model.predict(pd.DataFrame(data))
```

</details>

4. On va adapter le code applicatif de notre API pour tenir compte de ce mod√®le de production.

<details>
<summary>
Voir le script `app/api.py` propos√©
</summary>

```{.python file="app/api.py"}
"""A simple API to expose our trained RandomForest model for Tutanic survival."""
from fastapi import FastAPI
import mlflow

import pandas as pd

# Preload model -------------------

model_name = "production"
model_version = "latest"

# Load the model from the Model Registry
model_uri = f"models:/{model_name}/{model_version}"
model = mlflow.sklearn.load_model(model_uri)

# Define app -------------------------


app = FastAPI(
    title="Pr√©diction de survie sur le Titanic",
    description=
    "Application de pr√©diction de survie sur le Titanic üö¢ <br>Une version par API pour faciliter la r√©utilisation du mod√®le üöÄ" +\
        "<br><br><img src=\"https://media.vogue.fr/photos/5faac06d39c5194ff9752ec9/1:1/w_2404,h_2404,c_limit/076_CHL_126884.jpg\" width=\"200\">"
    )


@app.get("/", tags=["Welcome"])
def show_welcome_page():
    """
    Show welcome page with model name and version.
    """

    return {
        "Message": "API de pr√©diction de survie sur le Titanic",
        "Model_name": 'Titanic ML',
        "Model_version": "0.3",
    }


@app.get("/predict", tags=["Predict"])
async def predict(
    sex: str = "female",
    age: float = 29.0,
    fare: float = 16.5,
    embarked: str = "S"
) -> str:
    """
    """

    df = pd.DataFrame(
        {
            "Sex": [sex],
            "Age": [age],
            "Fare": [fare],
            "Embarked": [embarked],
        }
    )

    prediction = "Survived üéâ" if int(model.predict(df)) == 1 else "Dead ‚ö∞Ô∏è"

    return prediction
```

</details>

Les changements principaux de ce code sont:

* on va chercher le mod√®le de production
* on met √† jour la version de notre API pour signaler √† nos clients que celle-ci a √©volu√©

5. On va retirer l'entra√Ænement de la s√©quence d'op√©ration du `api/run.sh`. En supprimant la ligne relative √† l'entra√Ænement du mod√®le, vous devriez avoir

```{.python}
#/bin/bash
uvicorn app.api:app --reload --host "0.0.0.0" --port 5000
```

Mettons en production cette nouvelle version. Cela implique de faire les gestes suivants:

6. _Commit_ de ce changement dans `main`

7. Publier un tag `v0.0.3` pour le code applicatif

8. Mettre √† jour notre manifeste dans le d√©p√¥t `GitOps`.
   * En premier lieu, il faut changer la version de r√©f√©rence pour utiliser le tag `v0.0.3`.
   * De plus, il faut d√©clarer la variable d'environnement `MLFLOW_TRACKING_URI` qui indique √† `Python` l'entrep√¥t de mod√®les o√π aller chercher celui en production. La bonne pratique est de d√©finir ceci hors du code, dans un fichier de configuration donc, ce qui est l'objet de notre manifeste `deployment.yaml`. On peut donc changer de cette mani√®re ce fichier:

<details>
<summary>
Le mod√®le `deployment.yaml` propos√©
</summary>

```{.python}
apiVersion: apps/v1
kind: Deployment
metadata:
name: titanic-deployment
labels:
    app: titanic
spec:
replicas: 1
selector:
    matchLabels:
    app: titanic
template:
    metadata:
    labels:
        app: titanic
    spec:
    containers:
    - name: titanic
        image: linogaliana/application:v0.0.3 #<1>
        ports:
        - containerPort: 5000
        env:
        - name: MLFLOW_TRACKING_URI
            value: https://user-${USERNAME}-mlflow.user.lab.sspcloud.fr #<2>
        resources:
        limits:
            memory: "2Gi"
            cpu: "1000m"
```
1. Le _tag_ de notre code applicatif
2. La variable d'environnement √† adapter en fonction de l'adresse du d√©p√¥t demod√®les utilis√©. Remplacer par votre URL `MLFlow`.

</details>

11.  Pour s'assurer que l'application fonctionne bien, on peut aller voir les _logs_ de la machine qui fait tourner notre code. Pour √ßa, faire `kubectl get pods` et, en supposant que votre service soit nomm√© `titanic` dans vos fichiers YAML de configuration, r√©cup√©rer le nom commen√ßant par `titanic-deployment-*` et faire `kubectl logs titanic-deployment-*`

:::


{{< checkpoint appli23 >}}
