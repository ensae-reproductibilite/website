---
title: "Application"
description: |
  Une application fil rouge pour illustrer l'intérêt d'appliquer graduellement les bonnes pratiques dans une optique de mise en production d'une application de data science.
order: 10
href: chapters/application.html
image: /rocket.png
---


<details>
<summary>
Dérouler les _slides_ ci-dessous ou [cliquer ici](https://ensae-reproductibilite.github.io/slides/#/title-slide)
pour afficher les slides en plein écran.
</summary>


<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://ensae-reproductibilite.github.io/slides/#/title-slide"></iframe></div>

</details>

# Introduction

L'objectif de cette mise en application est d'**illustrer les différentes étapes qui séparent la phase de développement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.

L'objectif pédagogique principal de cette application est d'adopter un point de vue pragmatique en choisissant des outils et des méthodes de travail qui permettent de réaliser des objectifs ambitieux de valorisation de données. `Python` sera le trait d'union entre les différentes technologies ou infrastructures que nous utiliserons.

Cette application est un tutoriel pas à pas pour avoir un projet reproductible et disponible sous plusieurs livrables.
Toutes les étapes ne sont pas indispensables à tous les projets de _data science_ et il existe des outils alternatifs à ceux présentés. Néanmoins, les outils présentés ont l'avantage d'être très bien intégrés à `Python`, bien configurés si vous utilisez le `SSPCloud` comme nous le recommandons, tout en étant agnostiques sur le reste des outils que vous utilisez ; de sorte à ne pas être bloquants si on remplace l'une des briques logicielles par une autre.

Nous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d'un projet de data science.
On a un _notebook_ un peu monolithique, qui réalise les étapes classiques d'un *pipeline* de *machine learning* :

- Import de données ;
- Statistiques descriptives et visualisations ;
- *Feature engineering* ;
- Entraînement d'un modèle ;
- Evaluation du modèle.

## Objectif {-}

**L'objectif est d'améliorer le projet de manière incrémentale jusqu'à pouvoir le mettre en production, en le valorisant sous une forme adaptée et en adoptant une méthode de travail fluidifiant les évolutions futures.**

La @fig-start montre que notre point de départ initial, à savoir un _notebook_, mélange tout. Ceci rend très complexe la mise à jour de notre modèle ou l'exploitation de notre modèle sur de nouvelles données, ce qui est pourtant la raison d'être du _machine learning_ qui est pensé pour l'extrapolation. Si on vous demande de valoriser votre modèle sur de nouvelles données, vous risquez de devoir refaire tourner tout votre _notebook_, avec le risque de ne pas retrouver les mêmes résultats que dans la version précédente.

La @fig-end illustre l'horizon auquel nous aboutirons à la fin de cette application. Nous désynchronisons les étapes d'entraînement et de prédiction, en identifiant mieux les pré-requis de chacune et en adoptant des briques technologiques adaptées à celles-ci. Les noms présents sur cette figure sont encore obscurs, c'est normal, mais ils vous deviendrons familiers si vous adoptez une infrastructure et une méthode de travail à l'état de l'art.


![Illustration de notre point de départ](/drawio/starting_point.png){#fig-start width="60%"}


![Illustration de l'horizon vers lequel on se dirige](/drawio/end_point.png){#fig-end}



::: {.callout-important}
Il est important de bien lire les consignes et d'y aller progressivement.
Certaines étapes peuvent être rapides, d'autres plus fastidieuses ;
certaines être assez guidées, d'autres vous laisser plus de liberté.
Si vous n'effectuez pas une étape, vous risquez de ne pas pouvoir passer à
l'étape suivante qui en dépend.

Bien que l'exercice soit applicable sur toute configuration bien faite, nous
recommandons de privilégier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), où tous les
outils nécessaires sont pré-installés et pré-configurés. Le service `VSCode`
ne sera en effet que le point d'entrée pour l'utilisation d'outils plus exigeants
sur le plan de l'infrastructure: _Argo_, _MLFLow_, etc.
:::

## Ce que cette application ne couvre pas (pour le moment)

A l'heure actuelle, cette application se concentre sur la mise en oeuvre fiable de l'entraînement de modèles de machine learning. Comme vous pouvez le voir, quand on part d'aussi loin qu'un projet monolithique dans un _notebook_, c'est un travail conséquent d'en arriver à un _pipeline_ pensé pour la production. Cette application vise à vous sensibiliser au fait qu'avoir la @fig-end en tête et adopter une organisation de travail et faire des choix techniques adéquats, vous fera économiser des dizaines voire centaines d'heures lorsque votre modèle aura vocation à passer en production.

A l'heure actuelle, cette application ne se concentre que sur une partie du cycle de vie d'un projet _data_ ; il y a déjà fort à faire. Nous nous concentrons sur l'entraînement et la mise à disposition d'un modèle à des fins opérationnelles. C'est la première partie du cycle de vie d'un modèle. Dans une approche MLOps, il faut également penser la maintenance de ce modèle et les enjeux que représentent l'arrivée continue de nouvelles données, ou le besoin d'en collecter de nouvelles à travers des annotations, sur la qualité prédictive d'un modèle. Toute entreprise qui ne pense pas cet après est vouée à se faire doubler par un nouveau venu. Une prochaine version de cette application permettra certainement d'illustrer certains des enjeux afférants à la vie en production d'un modèle (supervision, annotations...) sur notre cas d'usage.

Il convient aussi de noter que nous ne faisons que parcourir la surface des sujets que nous évoquons. Ce cours, déjà dense, deviendrait indigeste si nous devions présenter chaque outil dans le détail. Nous laissons donc les curieux approfondir chacun des outils que nous présentons pour découvrir comment en tirer le maximum (et si vous avez l'impression que nous oublions des éléments cruciaux, les [_issues_ et _pull requests {{< fa brands github >}}_](https://github.com/ensae-reproductibilite/website) sont bienvenues).

## Comment gérer les _checkpoints_ ?

Pour simplifier la reprise en cours de ce fil rouge, nous proposons un système de _checkpoints_ qui s'appuient sur des _tags_ `Git`. Ces _tags_ figent le projet tel qu'il est à l'issue d'un exercice donné.

Si vous faites évoluer votre projet de manière expérimentale mais désirez tout de même utiliser à un moment ces checkpoints, il va falloir faire quelques acrobaties `Git`. Pour cela, nous mettons à disposition un script qui permet de sauvegarder votre avancée dans un _tag_ donné (au cas où, à un moment, vous vouliez revenir dessus) et écraser la branche `main` avec le _tag_ en question. Par exemple, si vous désirez reprendre après l'exercice 9, vous devrez faire tourner le code dans cette boite :

{{< checkpoint appli9 "Checkpoint d'exemple">}}

Celui-ci sauvegarde votre avancée dans un tag nommé `dev_before_appli9`, le pousse sur votre dépôt `Github` {{< fa brands github >}} puis force votre branche à adopter l'état du tag `appli9`.



# Partie 0 : initialisation du projet

Nous allons prendre comme point de départ un projet livré exclusivement avec un _notebook_, à la manière d'un challenge _Kaggle_. Vous pourrez ainsi voir à quel point ce type de livrable est très loin d'être satisfaisant si on veut que le projet soit réutilisable.

{{< include "./applications/_appli0.qmd" >}}

# Partie 1 : qualité du script

Cette première partie vise à **rendre le projet conforme aux bonnes pratiques** présentées dans le cours.

Elle fait intervenir les notions suivantes :

- Utilisation du **terminal** (voir [Linux 101](/chapters/linux101.qmd)) ;
- **Qualité du code** (voir [Qualité du code](/chapters/code-quality.qmd)) ;
- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;
- **Contrôle de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;
- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).


Le plan de la partie est le suivant :

1. S'assurer que le script fonctionne ;
2. Nettoyer le code des scories formelles avec un _linter_ et un _formatter_ ;
3. Paramétrisation du script ;
4. Utilisation de fonctions.


## Étape 1 : s'assurer que le script s'exécute correctement

On va partir du fichier `notebook.py` qui reprend le contenu
du _notebook_[^jupytext] mais dans un script classique.
Le travail de nettoyage en sera facilité.

[^jupytext]: L'export dans un script `.py` a été fait
        directement depuis `VSCode`. Comme
        cela n'est pas vraiment l'objet du cours, nous passons cette étape et fournissons
        directement le script expurgé du texte intermédiaire. Mais n'oubliez
        pas que cette démarche, fréquente quand on a démarré sur un _notebook_ et
        qu'on désire consolider en faisant la transition vers des
        scripts, nécessite d'être attentif pour ne pas risquer de faire une erreur.

La première étape est simple, mais souvent oubliée : **vérifier que le code fonctionne correctement**.
Pour cela, nous recommandons de faire un aller-retour entre le script ouvert dans `VSCode`
et un terminal pour le lancer.


{{< include "./applications/_appli1.qmd" >}}


## Étape 2: utiliser un _linter_ puis un _formatter_

On va maintenant améliorer la qualité de notre code en appliquant les standards communautaires.
Pour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/)
et le _formatter_ [`Black`](https://github.com/psf/black).
Si vous désirez un outil deux en un, il est possible d'utiliser [`Ruff`](https://github.com/astral-sh/ruff-vscode)
en complément ou substitut.


Ce nettoyage automatique du code permettra, au passage, de restructurer notre
script de manière plus naturelle.


::: {.callout-important}
[`PyLint`](https://pylint.readthedocs.io/en/latest/), [`Black`](https://black.readthedocs.io/en/stable/) et [`Ruff`](https://docs.astral.sh/ruff/)
sont des _packages_ `Python` qui
s'utilisent principalement en ligne de commande.

Si vous avez une erreur qui suggère
que votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/), [`Black`](https://black.readthedocs.io/en/stable/),
ou [`Ruff`](https://docs.astral.sh/ruff/), n'oubliez pas d'exécuter la commande `pip install pylint`, `pip install black` ou `pip install ruff`.
:::


Le _linter_ [`PyLint`](https://pylint.readthedocs.io/en/latest/) renvoie alors une série d'irrégularités,
en précisant à chaque fois la ligne de l'erreur et le message d'erreur associé (ex : mauvaise identation).
Il renvoie finalement une note sur 10,
qui estime la qualité du code à l'aune des standards communautaires évoqués
dans la partie [Qualité du code](/chapters/code-quality.html).

{{< include "./applications/_appli2.qmd" >}}

Le code est maintenant lisible, il obtient à ce stade une note formelle proche de 10.
Mais il n'est pas encore totalement intelligible ou fiable.
Il y a notamment
quelques redondances de code auxquelles nous allons nous attaquer par la suite.
Néanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script:
jetons d'API et chemin des fichiers.


## Étape 3: gestion des paramètres

{{< checkpoint appli2 "Reprendre à partir d'ici" "true" "pre-appli3" >}}

L'exécution du code et les résultats obtenus
dépendent de certains paramètres définis dans le code. L'étude de résultats
alternatifs, en jouant sur
des variantes des (hyper)paramètres, est à ce stade compliquée
car il est nécessaire de parcourir le code pour trouver
ces paramètres. De plus, certains paramètres personnels
comme des jetons
d'API ou des mots de passe n'ont pas vocation à
être présents dans le code.

Il est plus judicieux de considérer ces paramètres comme des
variables d'entrée du script. Cela peut être fait de deux
manières:

1. Avec des __arguments optionnels__ appelés depuis la ligne de commande _(Application 3a)_.
Cela peut être pratique pour mettre en oeuvre des tests automatisés mais
n'est pas forcément pertinent pour toutes les variables. Nous allons montrer
cet usage avec le nombre d'arbres de notre _random forest_ ;
2. En utilisant un __fichier de configuration__ dont les valeurs sont importées dans
le script principal _(Application 3b)_.


<details>
<summary>
Un exemple de définition d'un argument pour l'utilisation en ligne de commande
</summary>

```{.python filename="prenom.py"}
import argparse
parser = argparse.ArgumentParser(description="Qui êtes-vous?")
parser.add_argument(
    "--prenom", type=str, default="Toto", help="Un prénom à afficher"
)
args = parser.parse_args()
print(args.prenom)
```

Exemples d'utilisations en ligne de commande

```{.bash filename="terminal"}
python prenom.py
python prenom.py --prenom "Zinedine"
```

</details>

{{< include "./applications/_appli3.qmd" >}}


## Étape 4 : Privilégier la programmation fonctionnelle

Nous allons **mettre en fonctions les parties importantes de l'analyse**.
Ceci facilitera l'étape ultérieure de modularisation de notre projet. Comme cela est évoqué dans les éléments magistraux de ce cours, l'utilisation de fonctions va rendre notre code plus concis, plus traçable, mieux documenté.

Cet exercice étant chronophage, il n'est __pas obligatoire de le réaliser en entier__. L'important est de
comprendre la démarche et d'adopter fréquemment une approche fonctionnelle[^POO]. Pour obtenir
une chaine entièrement fonctionnalisée, vous pouvez reprendre le _checkpoint_.

[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser
un processus, il serait possible d'adopter le paradigme de la __programmation orientée objet (POO)__. Celle-ci est
plus rebutante et demande plus de temps au développeur. L'arbitrage coût-avantage est négatif pour notre
exemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d'un modèle,
il peut être utle de l'adopter car certains _frameworks_, à commencer par les _pipelines_ `scikit`, exigeront certaines classes et méthodes si vous désirez brancher des objets  _ad hoc_ à ceux-ci.

{{< include "./applications/_appli4_optionnelle.qmd" >}}

{{< include "./applications/_appli4.qmd" >}}

Cela ne se remarque pas encore vraiment car nous avons de nombreuses définitions de fonctions
mais notre chaine de production est beaucoup plus
concise (le script fait environ 150 lignes dont une centaine issues de définitions de fonctions génériques).
Cette auto-discipline facilitera grandement
les étapes ultérieures. Cela aurait été néanmoins beaucoup moins coûteux en temps d'adopter
ces bons gestes de manière plus précoce.


# Partie 2 : adoption d'une structure modulaire {#partie2}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est déjà considérablement rapprochés d'un
possible partage du code : celui-ci est lisible et intelligible.
Le code est proprement versionné sur un
dépôt `GitHub`.

Cependant, le projet est encore perfectible: il est encore difficile de rentrer
dedans si on ne sait pas exactement ce qu'on recherche. L'objectif de cette partie
est d'isoler les différentes étapes de notre _pipeline_.
Outre le gain de clarté pour notre projet, nous économiserons beaucoup de peines
pour la mise en production ultérieure de notre modèle.


![Etat du _pipeline_ avant la modularisation](/chapters/applications/figures/pipeline_post_appli4.png)


Dans cette partie nous allons continuer les améliorations
incrémentales de notre projet avec les étapes suivantes:

1. Modularisation du code `Python` pour séparer les différentes
étapes de notre _pipeline_ ;
1. Adopter une structure standardisée pour notre projet afin
d'autodocumenter l'organisation de celui-ci ;
1. Documenter les _packages_ indispensables à l'exécution du code ;
4. Stocker les données dans un environnement adéquat
afin de continuer la démarche de séparer conceptuellement les données du code en de la configuration.


## Étape 1 : modularisation

Nous allons profiter de la modularisation pour adopter une structure
applicative pour notre code. Celui-ci n'étant en effet plus lancé
que depuis la ligne de commande, on peut considérer qu'on construit
une application générique où un script principal (`main.py`)
encapsule des éléments issus d'autres scripts `Python`.


{{< include "./applications/_appli5.qmd" >}}


## Étape 2 : adopter une architecture standardisée de projet

On dispose maintenant d'une application `Python` fonctionnelle.
Néanmoins, le projet est certes plus fiable mais sa structuration
laisse à désirer et il serait difficile de rentrer à nouveau
dans le projet dans quelques temps.

<details>
<summary>Etat actuel du projet 🙈</summary>

```
├── .gitignore
├── .env
├── data.csv
├── train.csv
├── test.csv
├── README.md
├── build_pipeline.py
├── train_evaluate.py
├── titanic.ipynb
└── main.py
```


</details>

Comme cela est expliqué dans la
partie [Structure des projets](/chapters/projects-architecture.html),
on va adopter une structure certes arbitraire mais qui va
faciliter l'autodocumentation de notre projet. De plus, une telle structure va faciliter des évolutions optionnelles
comme la _packagisation_ du projet. Passer d'une structure modulaire
bien faite à un _package_ est quasi-immédiat en `Python`.

On va donc modifier l'architecture de notre projet pour la rendre plus standardisée.
Pour cela, on va s'inspirer des structures
[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)
qui génèrent des _templates_ de projet. En l'occurrence
notre source d'inspiration sera le [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)
issu d'un effort communautaire.

::: {.callout-note}
L'idée de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de bâtir à l'avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante :

```{.bash filename="terminal"}
pip install cookiecutter
cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

Ici, on a déjà un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.
:::

En s'inspirant du _cookiecutter data science_
on va adopter la structure suivante:

<details>
<summary>
Structure recommandée
</summary>

```
application
├── main.py
├── .env
├── README.md
├── data
│   ├── raw
│   │   └── data.csv
│   └── derived
│       ├── test.csv
│       └── train.csv
├── notebooks
│   └── titanic.ipynb
└── src
    ├── pipeline
    │   └── build_pipeline.py
    └── models
        └── train_evaluate.py
```

</details>

{{< include "./applications/_appli6.qmd" >}}


## Étape 3: mieux tracer notre chaine de production

### Indiquer l'environnement minimal de reproductibilité

Le script `main.py` nécessite un certain nombre de packages pour
être fonctionnel. Chez vous les packages nécessaires sont
bien sûr installés mais êtes-vous assuré que c'est le cas
chez la personne qui testera votre code ?

Afin de favoriser la portabilité du projet,
il est d'usage de _"fixer l'environnement"_,
c'est-à-dire d'indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version.
Nous proposons de créer un fichier `requirements.txt` minimal, sur lequel nous reviendrons
dans la partie consacrée aux environnements reproductibles.

Le fichier `requirements.txt` est conventionnellement localisé à la racine du projet.
Ici on ne va pas fixer les versions, on raffinera ce fichier ultérieurement.

{{< include "./applications/_appli7a.qmd" >}}

### Tracer notre chaîne

Quand votre projet passera en production, vous aurez un accès limité à celui-ci. Il est donc important de faire remonter, par le biais du _logging_ des informations critiques sur votre projet qui vous permettront de savoir où il en est (si vous avez accès à la console où il tourne) ou là où il s'est arrêté.

L'utilisation de `print` montre rapidement ses limites pour cela. Les informations enregistrées ne persistent pas après la session et sont quelques peu rudimentaires.

Pour faire du _logging_, la librairie consacrée depuis longtemps en `Python` est... [`logging`](https://docs.python.org/3/library/logging.html). Il existe aussi une librairie nommée [`loguru`](https://github.com/Delgan/loguru) qui est un peu plus simple à configurer (l'instanciation du _logger_ est plus aisée) et plus agréable grâce à ses messages en couleurs qui permettent de visuellement trier les informations.

![](https://raw.githubusercontent.com/Delgan/loguru/master/docs/_static/img/demo.gif)

L'exercice suivant peut être fait avec les deux librairies, cela ne change pas grand chose. Les prochaines applications repartiront de la version utilisant la librairie standard `logging`.


{{< include "./applications/_appli7b.qmd" >}}


## Étape 4 : stocker les données de manière externe {#stockageS3}

Pour cette partie, il faut avoir un service `VSCode` dont les jetons d'authentification à `S3` sont valides. Pour cela, si vous êtes sur le `SSPCloud`, le plus simple est de recréer un nouveau service avec le bouton suivant

<a href="https://datalab.sspcloud.fr/launcher/ide/vscode-python?name=ENSAE%20Mise%20en%20production&version=2.1.24&s3=region-ec97c721&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Fensae-reproductibilite%2Fwebsite%2Frefs%2Fheads%2Fmain%2Fchapters%2Fapplications%2Finit.sh»&kubernetes.role=«admin»&networking.user.enabled=true&autoLaunch=false" target="_blank" rel="noopener" data-original-href="https://datalab.sspcloud.fr/launcher/ide/vscode-python?name=ENSAE%20Mise%20en%20production&version=2.1.24&s3=region-ec97c721&init.personalInit=«https%3A%2F%2Fraw.githubusercontent.com%2Fensae-reproductibilite%2Fwebsite%2Frefs%2Fheads%2Fmain%2Fchapters%2Fapplications%2Finit.sh»&kubernetes.role=«admin»&networking.user.enabled=true&autoLaunch=false"><img src="https://img.shields.io/badge/SSP%20Cloud-Lancer_avec_VSCode-blue?logo=visualstudiocode&amp;logoColor=blue" alt="Onyxia"></a>

et remplir l'onglet `Git` comme ça votre `VSCode` sera pré à l'emploi (cf. application 0).

Une fois que vous avez un `VSCode` fonctionnel, il est possible de reprendre cette application fil rouge depuis le _checkpoint_ précédent.

{{< checkpoint appli7 "Reprendre à partir d'ici" "true" "pre-appli8" >}}

Enfin, il vous suffira d'ouvrir un terminal et faire `pip install -r requirements.txt && python main.py` pour pouvoir démarrer l'application.

L'étape précédente nous a permis d'isoler la configuration. Nous avons conceptuellement isolé les données du code lors des applications précédentes. Cependant, nous n'avons pas été au bout du chemin car le stockage des données reste conjoint à celui du code. Nous allons maintenant dissocier ces deux éléments.

::: {.callout-warning collapse="true"}
## Pour en savoir plus sur le système de stockage `S3`

Pour mettre en oeuvre cette étape, il peut être utile de
comprendre un peu comme fonctionne le SSP Cloud.
Vous devrez suivre la [documentation du SSP Cloud](https://inseefrlab.github.io/docs.sspcloud.fr/docs/fr/storage.html) pour la réaliser. Une aide-mémoire est également disponible dans le cours
de 2e année de l'ENSAE [Python pour la _data science_](https://linogaliana-teaching.netlify.app/reads3/#).
:::

::: {.callout-warning collapse="true"}
## Pour en savoir plus sur le format `Parquet`

L'objectif de cette application est de montrer comment utiliser le format `Parquet` dans une chaîne production  ; un objectif somme toute modeste.

Si vous voulez aller plus loin dans la découverte du format `Parquet`, vous pouvez consulter [cette ressource `R`](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/complete.html#/application-3-2) très similaire à ce cours (oui elle est faite par les mêmes auteurs...) et essayer de faire les exercices avec votre librairie `Python` de prédilection (`PyArrow` ou `DuckDB`)

:::


::: {.callout-important collapse="true"}
## Et si vous utilisez une infrastructure _cloud_ qui n'est pas le `SSPCloud` ? (une idée saugrenue mais sait-on jamais)

Les exemples à venir peuvent très bien être répliqués sur n'importe quel _cloud provider_ qui propose une solution de type `S3`, qu'il s'agisse d'un _cloud provider_ privé (AWS, GCP, Azure, etc.) ou d'une réinstanciation _ad hoc_ du projet [`Onyxia`](https://www.onyxia.sh/), le logiciel derrière le `SSPCloud`.

Pour un système de stockage `S3`, il suffit de changer les paramètres de connexion de `s3fs` (_endpoint_, _region_, etc.). Pour les stockages sur `GCP`, les codes sont presque équivalents, il suffit de remplacer la librairie [`s3fs`](https://pypi.org/project/s3fs/) par [`gcfs`](https://github.com/fsspec/gcsfs); ces deux librairies sont en fait des briques d'un standard plus général de gestion de systèmes de fichiers en `Python` [`ffspec`](https://filesystem-spec.readthedocs.io/en/latest/).

:::


Le chapitre sur la [structure des projets](/chapters/projects-architecture.qmd)
développe l'idée qu'il est recommandé de converger vers un modèle
où environnements d'exécution, de stockage du code et des données sont conceptuellement
séparés. Ce haut niveau d'exigence est un gain de temps important
lors de la mise en production car au cours de cette dernière, le projet
est amené à être exécuté sur une infrastructure informatique dédiée
qu'il est bon d'anticiper. Schématiquement, nous visons la structure de projet suivante:

![](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/slides/img/environment_clean.png)

A l'heure actuelle, les données sont stockées dans le dépôt. C'est une
mauvaise pratique. En premier lieu, `Git` n'est techniquement
pas bien adapté au stockage de données. Ici ce n'est pas très grave
car il ne s'agit pas de données volumineuses et ces dernières ne sont
pas modifiées au cours de notre chaine de traitement.

La raison principale
est que les données traitées par les _data scientists_
sont généralement soumises à des clauses de
confidentialités ([RGPD](https://www.cnil.fr/fr/rgpd-de-quoi-parle-t-on), [secret statistique](https://www.insee.fr/fr/information/1300624)...). Mettre ces données sous contrôle de version
c'est prendre le risque de les divulguer à un public non habilité.
Il est donc recommandé de privilégier des outils techniques adaptés au
stockage de données.

L'idéal, dans notre cas, est d'utiliser une solution de stockage externe.
On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud.
Cela nous permettra de supprimer les données de `Github` tout en maintenant la reproductibilité
de notre projet [^history].

[^history]: Attention, les données ont été _committées_ au moins une fois. Les supprimer
du dépôt ne les efface pas de l'historique. Si cette erreur arrive, le mieux est de supprimer
le dépôt en ligne, créer un nouvel historique `Git` et partir de celui-ci pour des publications
ultérieures sur `Github`. Néanmoins l'idéal serait de ne pas s'exposer à cela. C'est justement
l'objet des bonnes pratiques de ce cours: un `.gitignore` bien construit et une séparation des
environnements de stockage du code et
des données seront bien plus efficaces pour vous éviter ces problèmes que tout les conseils de
vigilance que vous pourrez trouver ailleurs.

Plus concrètement, nous allons adopter le pipeline suivant pour notre projet:

![](/chapters/applications/figures/pipeline_appli8.png)

Le scénario type est que nous avons une source brute, reçue sous forme de CSV, dont on ne peut changer le format. Il aurait été idéal d'avoir un format plus adapté au traitement de données pour ce fichier mais ce n'était pas de notre ressort. Notre chaine va aller chercher ce fichier, travailler dessus jusqu'à valoriser celui-ci sous la forme de notre matrice de confusion. Si on imagine que notre chaine prend un certain temps, il n'est pas inutile d'écrire des données intermédiaires. Pour faire cela, puisque nous avons la main, autant choisir un format adapté, à savoir le format `Parquet`.

Cette application va se dérouler en trois temps:

1. _Upload_ de notre source brute (CSV) sur S3
2. Illustration de l'usage des librairies _cloud native_ pour lire celle-ci
3. Partage public de cette donnée pour la rendre accessible de manière plus simple à nos futures applications.

{{< include "./applications/_appli8.qmd" >}}


# Partie 2bis: packagisation de son projet (optionnel)

Cette série d'actions n'est pas forcément pertinente pour tous
les projets. Elle fait un peu la transition entre la modularité
et la portabilité.

## Étape 1 : proposer des tests unitaires (optionnel)

Notre code comporte un certain nombre de fonctions génériques.
On peut vouloir tester leur usage sur des données standardisées,
différentes de celles du Titanic.

Même si la notion de tests unitaires
prend plus de sens dans un _package_, nous pouvons proposer
dans le projet des exemples d'utilisation de la fonction, ceci peut être pédagogique.

Nous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)
pour effectuer des tests unitaires. Cette approche nécessite quelques notions
de programmation orientée objet ou une bonne discussion avec `ChatGPT`.

{{< include "./applications/_appli9.qmd" >}}


::: {.callout-note}

Lorsqu'on effectue des tests unitaires, on cherche généralement
à tester le plus de lignes possibles de son code. On parle de
__taux de couverture__ (_coverage rate_) pour désigner
la statistique mesurant cela.

Cela peut s'effectuer de la manière suivante avec le package
[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):

```{.bash filename="terminal"}
coverage run -m unittest tests/test_create_variable_title.py
coverage report -m
```

```{.python}
Name                                  Stmts   Miss  Cover   Missing
-------------------------------------------------------------------
src/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113
tests/test_create_variable_title.py      21      1    95%   54
-------------------------------------------------------------------
TOTAL                                    55     22    60%
```

Le taux de couverture est souvent mis en avant par les gros
projets comme indicateur de leur qualité. Il existe d'ailleurs
des badges `Github` dédiés.
:::




## Étape 2 : transformer son projet en package (optionnel)

Notre projet est modulaire, ce qui le rend assez simple à transformer
en _package_, en s'inspirant de la structure du `cookiecutter` adapté, issu
de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

On va créer un _package_ nommé `titanicml` qui encapsule
tout notre code et qui sera appelé
par notre script `main.py`. La structure attendue
est la suivante:

<details>
<summary>Structure visée</summary>

```
ensae-reproductibilite-application
├── docs                                    ┐
│   ├── main.py                             │
│   └── notebooks                           │ Package documentation and examples
│       └── titanic.ipynb                   │
├── configuration                           ┐ Configuration (pas à partager avec Git)
│   └── config.yaml                         ┘
├── README.md
├── pyproject.toml                          ┐
├── requirements.txt                        │
├── titanicml                               │
│   ├── __init__.py                         │ Package source code, metadata
│   ├── data                                │ and build instructions
│   │   ├── import_data.py                  │
│   │   └── test_create_variable_title.py   │
│   ├── features                            │
│   │   └── build_features.py               │
│   └── models                              │
│       └── train_evaluate.py               ┘
└── tests                                   ┐
    └── test_create_variable_title.py       ┘ Package tests
```
</details>

<details>
<summary>Rappel: structure actuelle</summary>

```
ensae-reproductibilite-application
├── notebooks
│   └── titanic.ipynb
├── configuration
│   └── config.yaml
├── main.py
├── README.md
├── requirements.txt
└── src
    ├── data
    │   ├── import_data.py
    │   └── test_create_variable_title.py
    ├── features
    │   └── build_features.py
    └── models
        └── train_evaluate.py
```
</details>

Il existe plusieurs
_frameworks_ pour
construire un _package_. Nous
allons privilégier [`Poetry`](https://python-poetry.org/)
à [`Setuptools`](https://pypi.org/project/setuptools/).


::: {.callout-note}

Pour créer la structure minimale d'un _package_, le plus simple est
d'utiliser le `cookiecutter` adapté,
issu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

Comme on a déjà une structure très modulaire, on va plutôt recréer cette
structure dans notre projet déjà existant. En fait, il ne manque qu'un fichier essentiel,
le principal distinguant un projet classique d'un package : `pyproject.toml`.

```{.bash filename="terminal"}
cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git
```

<details>
<summary>Dérouler pour voir les choix possibles</summary>
```{.python}
author_name [Monty Python]: Daffy Duck
package_name [mypkg]: titanicml
package_short_description []: Impressive Titanic survival analysis
package_version [0.1.0]:
python_version [3.9]:
Select open_source_license:
1 - MIT
2 - Apache License 2.0
3 - GNU General Public License v3.0
4 - Creative Commons Attribution 4.0
5 - BSD 3-Clause
6 - Proprietary
7 - None
Choose from 1, 2, 3, 4, 5, 6 [1]:
Select include_github_actions:
1 - no
2 - ci
3 - ci+cd
Choose from 1, 2, 3 [1]:
```
</details>

:::

{{< include "./applications/_appli10.qmd" >}}

# Partie 3 : construction d'un projet portable et reproductible {#partie3}

{{< checkpoint appli8 "Reprendre à partir d'ici" "true" "pre-appli10" >}}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues
dans les chapitres [Qualité du code](/chapters/code-quality.html)
et [Structure des projets](/chapters/projects-architecture.html)
tout au long du cours.

Ce faisant, on s'est déjà considérablement rapprochés d'une
possible mise en production : le code est lisible,
la structure du projet est normalisée et évolutive,
et le code est proprement versionné sur un
dépôt `GitHub` {{< fa brands github >}}.


<details>
<summary>
Illustration de l'état actuel du projet
</summary>
![](/chapters/applications/figures/_pipeline_avant_partie3.png)

</details>



A présent, nous avons une version du projet qui est largement partageable.
Du moins en théorie, car la pratique est souvent plus compliquée :
il y a fort à parier que si vous essayez d'exécuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),
les choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'état, le projet n'est pas portable : il n'est pas possible, sans modifications coûteuses, de l'exécuter dans un environnement différent de celui dans lequel il a été développé**.

Dans cette troisème partie de notre travail vers la mise en production,
nous allons voir
comment **normaliser l'environnement d'exécution afin de produire un projet portable**.
Autrement dit, nous n'allons plus nous contenter de modularité mais allons rechercher
la portabilité.
On sera alors tout proche de pouvoir mettre le projet en production.

On progressera dans l'échelle de la reproductibilité
de la manière suivante:

1. [**Environnements virtuels**](#anaconda) ;
2. Créer un [script shell](#shell) qui permet, depuis un environnement minimal, de construire l'application de A à Z ;
3. [**Images et conteneurs `Docker`**](#docker).


Nous allons repartir de l'application 8, c'est-à-dire d'un projet
modulaire mais qui n'est pas, à strictement parler, un _package_
(objet des applications optionnelles suivantes 9 et 10).

{{< checkpoint appli8 "Reprendre à partir d'ici" "true" "pre-appli10" >}}


## Étape 1 : un environnement pour rendre le projet portable {#anaconda}

Pour qu'un projet soit portable, il doit remplir deux conditions:

- Ne pas nécessiter de dépendance
qui ne soient pas renseignées quelque part ;
- Ne pas proposer des dépendances inutiles, qui ne
sont pas utilisées dans le cadre du projet.

Le prochain exercice vise à mettre ceci en oeuvre.
Comme expliqué dans le [chapitre portabilité](/chapters/portability.qmd),
le choix du gestionnaire d'environnement est laissé
libre. Il est recommandé de privilégier `venv` si vous découvrez
la problématique de la portabilité.

::: {.panel-tabset group="language"}

## Environnement virtuel `venv`

L'approche la plus légère est l'environnement virtuel.
Nous avons en fait implicitement déjà commencé à aller vers
cette direction
en créant un fichier `requirements.txt`.

{{< include "./applications/_appli11a.qmd" >}}


## Environnement `conda`

Les environnements `conda` sont plus lourds à mettre en oeuvre que les
environnements virtuels mais peuvent permettre un contrôle
plus formel des dépendances.

{{< include "./applications/_appli11b.qmd" >}}


## Environnement virtuel via `uv`

`uv` est le _new kid in the game_ pour gérer les environnements virtuels avec `Python`.

{{< include "./applications/_appli11c.qmd" >}}


:::


## Étape 2: construire l'environnement de notre application via un script `shell` {#shell}

Les environnements virtuels permettent de mieux spécifier les dépendances de notre projet, mais ne permettent pas de garantir une portabilité optimale. Pour cela, il faut recourir à la technologie des conteneurs. L'idée est de construire une machine, en partant d'une base quasi-vierge, qui permette de construire étape par étape l'environnement nécessaire au bon fonctionnement de notre projet. C'est le principe des conteneurs `Docker` {{< fa brands docker >}}.

Leur méthode de construction étant un peu difficile à prendre en main au début, nous allons passer par une étape intermédiaire afin de bien comprendre le processus de production.

- Nous allons d'abord créer un script `shell`, c'est à dire une suite de commandes `Linux` permettant de construire l'environnement à partir d'une machine vierge ;
- Nous transformerons celui-ci en `Dockerfile` dans un deuxième temps. C'est l'objet de l'étape suivante.

::: {.panel-tabset group="language"}

## Environnement virtuel `venv`

{{< include "./applications/_appli12a.qmd" >}}

## Environnement `conda`

{{< include "./applications/_appli12b.qmd" >}}

:::


## Étape 3: conteneuriser l'application avec `Docker` {#docker}


::: {.callout-note}
Cette application nécessite l'accès à une version interactive de `Docker`.
Il n'y a pas beaucoup d'instances en ligne disponibles.

Nous proposons deux solutions:

- [Installer `Docker`](https://docs.docker.com/get-docker/) sur sa machine ;
- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_

Sinon, elle peut être réalisée en essai-erreur par le biais des services d'intégration continue de `Github` {{< fa brands github >}} ou `Gitlab` {{< fa brands gitlab >}}. Néanmoins, nous présenterons l'utilisation de ces services plus tard, dans la prochaine partie.
:::

Maintenant qu'on sait que ce script préparatoire fonctionne, on va le transformer en `Dockerfile` pour anticiper la mise en production.  Comme la syntaxe `Docker` est légèrement différente de la syntaxe `Linux` classique (voir le [chapitre portabilité](/chapters/portability.qmd)), il va être nécessaire de changer quelques instructions mais ceci sera très léger.

On va tester le `Dockerfile` dans un environnement bac à sable pour ensuite
pouvoir plus facilement automatiser la construction de l'image
`Docker`.

{{< include "./applications/_appli13.qmd" >}}


# Partie 4 : automatisation avec l'intégration continue

{{< checkpoint appli13 "Reprendre à partir d'ici" "true" "pre-appli13" >}}

Imaginez que vous êtes au restaurant
et qu'on ne vous serve pas le plat mais seulement la recette
et que, de plus, on vous demande de préparer le plat
chez vous avec les ingrédients dans votre frigo.
Vous seriez quelque peu déçu. En revanche, si vous avez goûté
au plat, que vous êtes un réel cordon bleu
et qu'on vous donne la recette pour refaire ce plat ultérieurement,
peut-être
que vous appréciriez plus.

Cette analogie illustre l'enjeu de définir
le public cible et ses attentes afin de fournir un livrable adapté.
Une image `Docker` est un livrable qui n'est pas forcément intéressant
pour tous les publics. Certains préféreront avoir un plat bien préparé
qu'une recette ; certains apprécieront avoir une image `Docker` mais
d'autres ne seront pas en mesure de construire celle-ci ou ne sauront
pas la faire fonctionner. Une image `Docker` est plus souvent un
moyen pour faciliter la mise en service d'une production qu'une fin en soi.

Nous allons donc proposer
plusieurs types de livrables plus classiques par la suite. Ceux-ci
correspondront mieux aux attendus des publics utilisateurs de services
construits à partir de techniques de _data science_. `Docker` est néanmoins
un passage obligé car l'ensemble des types de livrables que nous allons
explorer reposent sur la standardisation permise par les conteneurs.

Cette approche nous permettra de quitter le domaine de l'artisanat pour
s'approcher d'une industrialisation de la mise à disposition
de notre projet. Ceci va notamment nous amener à mettre en oeuvre
l'approche pragmatique du `DevOps` qui consiste à intégrer dès la phase de
développement d'un projet les contraintes liées à sa mise à disposition
au public cible (cette approche est détaillée plus
amplement dans le chapitre sur la [mise en production](/chapters/deployment.qmd)).

L'automatisation et la mise à disposition automatisée de nos productions
sera faite progressivement, au cours des prochaines parties. Tous les
projets n'ont pas vocation à aller aussi loin dans ce domaine.
L'opportunité doit être comparée aux coûts humains et financiers
de leur mise en oeuvre et de leur cycle de vie.
Avant de faire une production en série de nos modèles,
nous allons déjà commencer
par automatiser quelques tests de conformité de notre code.
On va ici utiliser l'intégration continue pour deux objectifs distincts:

- la mise à disposition de l'image `Docker` ;
- la mise en place de tests automatisés de la qualité du code
sur le modèle de notre `linter` précédent.

Nous allons utiliser `Github Actions` pour cela. Il s'agit de serveurs
standardisés mis à disposition gratuitement par `Github` {{<fa brands github >}}.
`Gitlab` {{<fa brands gitlab >}}, l'autre principal acteur du domaine,
propose des services similaires. L'implémentation est légèrement différente
mais les principes sont identiques.


{{< checkpoint appli13 "Reprendre à partir d'ici" "true" "pre-appli14" >}}


## Étape 1: mise en place de tests automatisés

Avant d'essayer de mettre en oeuvre la création de notre image
`Docker` de manière automatisée, nous allons présenter la logique
de l'intégration continue en testant de manière automatisée
notre script `main.py`.

Pour cela, nous allons partir de la structure proposée dans l'[action officielle](https://github.com/actions/setup-python).
La documentation associée est [ici](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python).
Des éléments succincts de présentation de la logique déclarative des actions `Github`
sont disponibles dans le chapitre sur la [mise en production](/chapters/deployment.qmd). Néanmoins, la meilleure
école pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et d'observer
les actions `Github` mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !


{{< include "./applications/_appli14.qmd" >}}


Maintenant, nous pouvons observer que l'onglet `Actions`
s'est enrichi. Chaque `commit` va entraîner une série d'actions automatisées.

Si l'une des étapes échoue, ou si la note de notre projet est mauvaise, nous aurons
une croix rouge (et nous recevrons un mail). On pourra ainsi détecter,
en développant son projet, les moments où on dégrade la qualité du script
afin de la rétablir immédiatemment.



## Étape 2: Automatisation de la livraison de l'image `Docker`

Maintenant, nous allons automatiser la mise à disposition de notre image
sur `DockerHub` (le lieu de partage des images `Docker`). Cela facilitera sa réutilisation mais aussi des
valorisations ultérieures.

Là encore, nous allons utiliser une série d'actions pré-configurées.

Pour que `Github` puisse s'authentifier auprès de `DockerHub`, il va
falloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser
un jeton (_token_) `DockerHub` que nous allons mettre dans un espace
sécurisé associé à votre dépôt `Github`.


{{< include "./applications/_appli15a.qmd" >}}


A ce stade, nous avons donné les moyens à `Github` de s'authentifier avec
notre identité sur `Dockerhub`. Il nous reste à mettre en oeuvre l'action
en s'inspirant de la [documentation officielle](https://github.com/docker/build-push-action/#usage).
On ne va modifier que trois éléments dans ce fichier. Effectuer les
actions suivantes:


{{< include "./applications/_appli15b.qmd" >}}



# Partie 5: expérimenter en local des valorisations puis automatiser leur production


Nous avons automatisé les étapes intermédiaires de notre projet.
Néanmoins nous n'avons pas encore réfléchi à la valorisation
à mettre en oeuvre pour notre projet. On va supposer que notre
projet s'adresse à des _data scientists_ mais aussi à une audience
moins technique. Pour ces premiers, nous pourrions nous contenter
de valorisations techniques, comme des API,
mais pour ces derniers il est
conseillé de privilégier des formats plus _user friendly_.

Afin de faire le parallèle avec les parcours possibles pour l'évaluation,
nous allons proposer deux valorisations[^valorisation]:

- Une [API](https://titanic.kub.sspcloud.fr/docs) facilitant la réutilisation du modèle en "production" ;
- Un [site web statique](https://ensae-reproductibilite.github.io/application/) exploitant cette API pour exposer les prédictions
à une audience moins technique.


[^valorisation]: Vous n'êtes pas obligés pour l'évaluation de mettre en oeuvre
les jalons de plusieurs parcours. Néanmoins, vous découvrirez que
chaque nouveau pas en avant est moins coûteux que le
précédent si vous avez mis en oeuvre les réflexes des bonnes
pratiques.



::: {.callout-warning collapse="true"}
## Site statique vs application réactive

La solution que nous allons proposer
pour les sites statiques, `Quarto` associé
à `Github Pages`, peut être utilisée dans le cadre des parcours
_"rapport reproductible"_ ou _"dashboard / application interactive"_.

Pour ce dernier
parcours, d'autres approches techniques sont néanmoins possibles,
comme `Streamlit`. Celles-ci sont plus exigeantes sur le plan technique
puisqu'elles nécessitent de mettre en production sur des serveurs
conteuneurisés (comme la mise en production de l'API)
là où le site statique ne nécessite qu'un serveur web, mis à disposition
gratuitement par `Github`.


La distinction principale entre ces deux approches est qu'elles
s'appuient sur des serveurs différents. Un site statique repose
sur un serveur web là où `Streamlit` s'appuie sur
serveur classique en _backend_. La différence principale
entre ces deux types de serveurs
réside principalement dans leur fonction et leur utilisation:

- Un __serveur web__ est spécifiquement conçu pour stocker, traiter et livrer des pages web aux clients. Cela inclut des fichiers HTML, CSS, JavaScript, images, etc. Les serveurs web écoutent les requêtes HTTP/HTTPS provenant des navigateurs des utilisateurs et y répondent en envoyant les données demandées.
- Un **serveur _backend_** classique est conçu pour effectuer des opérations en réponse à un _front_, en l'occurrence une page web.
Dans le contexte d'une application `Streamlit`, il s'agit d'un serveur avec l'environnement `Python` _ad hoc_ pour
exécuter le code nécessaire à répondre à toute action d'un utilisateur de l'appliacation.

:::


## Étape 1: développer une API en local

Le premier livrable devenu classique dans un projet
impliquant du _machine learning_ est la mise à
disposition d'un modèle par le biais d'une
API (voir chapitre sur la [mise en production](/chapters/deployment.qmd)).
Le _framework_ [`FastAPI`](https://fastapi.tiangolo.com/) va permettre
de rapidement transformer notre application `Python` en une API fonctionnelle.


{{< checkpoint appli15 "Reprendre à partir d'ici" "true" "pre-appli16" >}}


{{< include "./applications/_appli16.qmd" >}}


## Étape 2: déployer l'API de manière manuelle

{{< checkpoint appli16 "Reprendre à partir d'ici" "true" "pre-appli17" >}}


A ce stade, nous avons déployé l'API seulement localement, dans le cadre d'un terminal qui tourne en arrière-plan.
C'est une mise en production manuelle, pas franchement pérenne.
Ce mode de déploiement est très pratique pour la phase de développement, afin de s'assurer que l'API fonctionne comme attendue.
Pour pérenniser la mise en production, on va éliminer l'aspect artisanal de celle-ci.

Il est temps de passer à l'étape de déploiement, qui permettra à notre API d'être accessible, à tout moment, via une URL sur le web
et d'avoir un serveur, en arrière plan, qui effectuera les opérations pour répondre à une
requête. Pour se faire, on va utiliser les possibilités offertes par `Kubernetes`, technologie sur laquelle est basée l'infrastructure [`SSP Cloud`](https://datalab.sspcloud.fr).


::: {.callout-important collapse="true"}
## Et si vous n'utilisez pas le `SSPCloud` ? (une idée saugrenue mais sait-on jamais)

Les exemples à venir peuvent très bien être répliqués sur n'importe quel _cloud provider_ qui propose une solution d'ordonnancement type `Kubernetes`. Il existe également des fournisseurs de services dédiés, généralement associés à une implémentation, par exemple pour [`Streamlit`](https://streamlit.io/gallery). Ces services sont pratiques si on n'a pas le choix mais il faut garder à l'esprit qu'ils peuvent constituer un mur de la production car vous ne contrôlez pas l'environnement en question, qui peut se distinguer de votre environnement de développement.

Et si jamais vous voulez avoir un `SSPCloud` dans votre entreprise c'est possible: le logiciel [`Onyxia`](https://github.com/InseeFrLab/onyxia) sur lequel repose cette infrastructure est _open source_ et est, déjà, réimplémenté par de nombreux acteurs. Pour bénéficier d'un accompagnement dans la création d'une telle infrastructure, rdv sur le `Slack` du projet `Onyxia`:

[![](https://img.shields.io/badge/Slack-4A154B?logo=slack&logoColor=fff)](https://join.slack.com/t/3innovation/shared_invite/zt-31au6d4uu-Ib~AX40ua2BZjgFq~DZeNg)

:::



{{< include "./applications/_appli17.qmd" >}}


Nous avons préparé la mise à disposition de notre API mais à l'heure
actuelle elle n'est pas accessible de manière aisée car il est nécessaire
de lancer manuellement une image `Docker` pour pouvoir y accéder.
Ce type de travail est la spécialité de `Kubernetes` que nous allons
utiliser pour gérer la mise à disposition de notre API.

{{< include "./applications/_appli18.qmd" >}}

::: {.callout-note title="Gérer le CORS" collapse="true"}
Notre API est accessible sans problème depuis `Python` ou notre navigateur.

En revanche, si on désire utiliser `JavaScript` pour créer une application
interactive il est indispensable de mettre
les lignes un peu obscure sur le CORS dans le fichier `ingress.yaml`.

Comme c'est un point technique qui ne concerne pas les compétences
liées à ce cours, nous avons donné directement les lignes correspondantes dans ce fichier.

:::


On peut remarquer quelques voies d'amélioration de notre approche qui
seront ultérieurement traitées:

- L'entraînement du modèle
est ré-effectué à chaque lancement d'un nouveau conteneur.
On relance donc autant de fois un entraînement qu'on déploie
de conteneurs pour répondre à nos utilisateurs. Ce sera
l'objet de la partie MLOps de fiabiliser et optimiser
cette partie du _pipeline_.
- il est nécessaire de (re)lancer manuellement  `kubectl apply -f deployment/`
à chaque changement de notre code. Autrement dit, lors de cette application,
on a amélioré
la fiabilité du lancement de notre API mais un lancement manuel est encore indispensable.
Comme dans le reste de ce cours, on va essayer d'éviter un geste manuel pouvant
être source d'erreur en privilégiant l'automatisation et l'archivage dans des
scripts. C'est l'objet de la prochaine étape.


## Etape 3: automatiser le déploiement (déploiement en continu)

::: {.callout-important collapse="true"}
## Clarification sur la branche de travail, les _tags_ et l'image `Docker` utilisée

A partir de maintenant, il est nécessaire de clarifier la
branche principale sur laquelle nous travaillons. Toutes les prochaines applications supposeront que vous travaillez depuis la branche `main`. Si vous avez changé de branche, vous pouvez fusionner celle-ci à `main`.

Si vous avez utilisé un `tag` pour sauter une ou plusieurs étapes, il va
être nécessaire de se placer sur une branche car vous êtes en _head detached_. Si vous avez utilisé les scripts automatisés de checkpoint, cette gymnastique a été faite pour vous.

Les prochaines applications vont également nécessiter d'utiliser une image `Docker`. Si vous avez suivi de manière linéaire cette application, votre image `Docker` devrait exister depuis l'application 15 si vous avez pushé votre dépôt à ce moment là.

Néanmoins, si vous n'avez pas fait cette application, vous pouvez utiliser le _checkpoint_ de l'application 18 et faire un `git push origin main --force` (à ne pas reproduire sur vos projets!) qui devrait déclencher les opérations côté `Github` pour construire et livrer votre image `Docker`. Cela nécessite quelques opérations de votre côté, notamment la création d'un _token_ `Dockerhub` à renseigner en secret `Github`. Pour vous refraîchir la mémoire sur le sujet, vous pouvez retourner consulter l'application 15.

:::

Qu'est-ce qui peut déclencher une évolution nécessitant de mettre à jour l'ensemble de notre processus de production ?

Regardons à nouveau notre _pipeline_:

![](/drawio/end_point.png)

Les _inputs_ de notre _pipeline_ sont donc:

- La __configuration__. Ici, on peut considérer que notre `.env` de configuration, les secrets renseignés à `Github` ou encore le `requirements.txt` relèvent de cette catégorie  ;
- Les __données__. Nos données sont statiques et n'ont pas vocation à évoluer. Si c'était le cas, il faudrait en tenir compte dans notre automatisation (@nte-versionning-data). ;
- Le __code__. C'est l'élément principal qui évolue chez nous. Idéalement, on veut automatiser le processus au maximum en faisant en sorte qu'à chaque mise à jour de notre code (un _push_ sur `Github`), les étapes ultérieures (production de l'image `Docker`, etc.) se lancent. Néanmoins, on veut aussi éviter qu'une erreur puisse donner lieu à une mise en production non-fonctionnelle, on va donc maintenir une action manuelle minimale comme garde-fou.


::: {#nte-versionning-data .callout-note}
## Et le _versionning_ des données ?

Ici, nous nous plaçons dans le cas simple où les données brutes reçues sont figées. Ce qui peut changer est la manière dont on constitue nos échantillons train/test. Il sera donc utile de logguer les données en question par le biais de `MLFlow`. Mais il n'est pas nécessaire de versionner les données brutes.

Si celles-ci évoluaient, il pourrait être utile de versionner les données, à la manière dont on le fait pour le code. `Git` n'est pas l'outil approprié pour cela. Parmi les outils populaires de versionning de données, bien intégrés avec `S3`, il y a, sur le `SSPCloud`, [`lakefs`](https://lakefs.io/).
:::

Pour automatiser au maximum la mise en production, on va utiliser un nouvel outil : `ArgoCD`. Ainsi, au lieu de devoir appliquer manuellement la commande `kubectl apply` à chaque modification des fichiers de déploiement (présents dans le dossier `kubernetes/`), c'est l'**opérateur** `ArgoCD`, déployé sur le _cluster_, qui va détecter les changements de configuration du déploiement et les appliquer automatiquement.

C'est l'approche dite **GitOps** : le dépôt `Git` du déploiement fait office de **source de vérité unique** de l'état voulu de l'application, tout changement sur ce dernier doit donc se répercuter immédiatement sur le déploiement effectif.


{{< include "./applications/_appli19a.qmd" >}}


A présent, nous avons tous les outils à notre disposition pour construire un vrai **pipeline de CI/CD, automatisé de bout en bout**. Il va nous suffire pour cela de mettre à bout les composants :

- dans la partie 4 de l'application, nous avons construit un **pipeline de CI** : on a donc seulement à faire un commit sur le dépôt de l'application pour lancer l'étape de **build** et de mise à disposition de la nouvelle image sur le `DockerHub` ;

- dans l'application précédente, nous avons construit un **pipeline de CD** : `ArgoCD` suit en permanence l'état du dépôt `GitOps`, tout commit sur ce dernier lancera donc automatiquement un redéploiement de l'application.

Il y a donc un élément qui fait la liaison entre ces deux pipelines et qui nous sert de garde-fou en cas d'erreur : la **version de l'application**.

{{< include "./applications/_appli19b.qmd" >}}


## Etape 4: construire un site web

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli19
git checkout -b dev
git push origin dev
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::

On va proposer un nouveau livrable pour parler à un public plus large.
Pour faire ce site web,
on va utiliser `Quarto` et déployer sur `Github Pages`.

{{< include "./applications/_appli20.qmd" >}}


# Partie 6: adopter une approche MLOps pour améliorer notre modèle


{{< checkpoint appli20 "Reprendre à partir d'ici" "true" "pre-appli21" >}}


Maintenant que nous avons tout préparé pour mettre à disposition rapidement un modèle,
nous pouvons revenir en arrière pour améliorer ce modèle. Pour cela, nous allons mettre en oeuvre une validation croisée.

Le problème que nous allons rencontrer va être que nous voudrions facilement tracer les évolutions de notre modèle, la qualité prédictive de celui-ci dans différentes situations. Il s'agira d'à nouveau mettre en place du _logging_ mais, cette fois, de suivre la qualité du modèle et pas seulement s'il fonctionne. L'outil `MLFlow` va répondre à ce problème et va, au passage, fluidifier la mise à disposition du modèle de production, c'est-à-dire de celui qu'on désire mettre à disposition du public.

## Revenir sur le code d'entraînement du modèle pour faire de la validation croisée

Pour pouvoir faire ceci, il va falloir changer un tout petit peu notre code applicatif dans sa phase d'entraînement.

{{< include "./applications/_appli21.qmd" >}}


## Garder une trace des entraînements de notre modèle grâce au _register_ de `MLFlow`

{{< checkpoint appli21 "Reprendre à partir d'ici" "true" "pre-appli22" >}}


## Enregistrer nos premiers entraînements

{{< include "./applications/_appli22.qmd" >}}

Cette appplication illustre l'un des premiers apports de `MLFlow`: on garde
une trace de nos expérimentations: le modèle est archivé avec les paramètres et des métriques de performance. On peut donc retrouver de plusieurs manières un modèle qui nous avait tapé dans l'oeil.

Néanmoins, persistent un certain nombre de voies d'amélioration dans notre _pipeline_.

- On entraîne le modèle en local, de manière séquentielle, et en lançant nous-mêmes le script `train.py`.
- Pis encore, à l'heure actuelle, cette étape d'estimation n'est pas séparée de la mise à disposition du modèle par le biais de notre API. On archive des modèles mais on les utilise pas ultérieurement.

Les prochaines applications permettront d'améliorer ceci.

## Consommation d'un modèle archivé sur `MLFlow`

A l'heure actuelle, notre _pipeline_ est linéaire:

![](/chapters/applications/figures/pipeline_avant_appli23.png)

Ceci nous gêne pour faire évoluer notre modèle: on ne dissocie pas ce qui relève de l'entraînement du modèle de son utilisation. Un _pipeline_ plus cyclique permettra de mieux dissocier l'expérimentation de la production:

![](/chapters/applications/figures/pipeline_apres_appli23.png)


{{< include "./applications/_appli23.qmd" >}}

A ce stade, nous avons amélioré la fiabilité de notre application car
nous utilisons le meilleur modèle. Néanmoins, nos entraînements sont encore manuels. Là encore il y a des gains possibles car cela paraît pénible à la longue de devoir systématiquement relancer des entraînements manuellement pour tester des variations de tel ou tel paramètre. Heureusement, nous allons pouvoir automatiser ceci également.



## Industrialiser les entraînements de nos modèles

Pour industrialiser nos entraînements, nous allons créer des processus parallèles indépendants pour chaque combinaison de nos hyperparamètres.

Ce travail nous amène de l'approche _pipeline_ à mi chemin entre _data science_ et _data engineering_. Il existe plusieurs outils pour faire ceci, généralement issus de la sphère du _data engineering_. L'outil le plus complet sur le `SSPCloud`, bien intégré à l'écosystème `Kubernetes`, est [`Argo Workflows`](https://argoproj.github.io/workflows/)[^airflow].

[^airflow]: Il existe d'autres outils d'ordonnancement de _pipelines_ très utilisés dans l'industrie, notamment [`Airflow`](https://airflow.apache.org/).

    Ce dernier est plus utilisé, en pratique, qu'`Argo Workflow` mais, même s'il est disponible sur le `SSPCloud` aussi, est moins pensé autour de `Kubernetes` que l'est `Argo`.

    Pour mieux comprendre la différence entre `Argo` et `Airflow`, la philosphie différente de ces deux outils et leurs avantages comparatifs, cette courte vidéo est intéressante:

      {{< video https://www.youtube.com/watch?v=w98X_0LwKWw >}}


Chaque combinaison d'hyperparamètres sera un processus isolé à l'issue duquel sera loggué le résultat dans `MLFlow`. Ces entraînements auront lieu en parallèle.

Nous allons construire, dans les deux prochaines applications, un _pipeline_ simple prenant cette forme[^forme-pipeline]:

::: {#fig-pipeline layout-ncol=2}

![Via `Argo Workflows`](/chapters/applications/figures/pipeline_ml.png){#fig-pipeline-argo fig-align="center"}

![Via `Github Actions`](/chapters/applications/figures/pipeline_github.png){#fig-pipeline-github fig-align="center"}

_Pipeline_ d'entraînement de nos modèles avec deux outils d'automatisation différents
:::

[^forme-pipeline]: Il serait bien sûr possible d'aller beaucoup plus loin dans la définition du _pipeline_.

    Par exemple, il est possible, si le _framework_ utilisé pour la modélisation n'intègre pas la notion de _pipeline_ au niveau de `Python` de faire ceci au niveau d'`Argo`. Cela donnerait un _pipeline_ prenant cette forme:

    ![](https://inseefrlab.github.io/formation-mlops/slides/img/pokemon_workflow.png)

    Néanmoins, ici, nous utilisons `Scikit` qui permet d'intégrer le _preprocessing_ comme une étape de modélisation. Nous n'avons donc pas d'intérêt à définir ceci comme une tâche autonome, raison pour laquelle notre _pipeline_ apparaît plus simple.

L'outil permettant une intégration native de notre _pipeline_ dans l'infrastructure _cloud_ (`SSPCloud`) que nous avons utilisée jusqu'à présent est `Argo Workflows`. Néanmoins, pour illustrer la modularité de notre chaîne, permise par l'adoption de `Docker`, nous allons montrer que les serveurs d'intégration continue de `Github` peuvent très bien servir d'environnement d'exécution, sans rien perdre de ce que nous avons mis en oeuvre précédemment (_logging_ des modèles dans `MLFlow`, récupération de données depuis `S3`, etc.)


{{< include "./applications/_appli24.qmd" >}}

Nous pouvons maintenant passer à la version `Github`. Celle-ci est optionnelle car elle vient surtout démontrer l'intérêt d'avoir une chaine modulaire et la dissociation que cela permet entre l'environnement d'exécution et les autres environnements nécessaires à notre chaine (notamment le stockage code et le _logging_).

{{< include "./applications/_appli25.qmd" >}}



# Pour aller plus loin

Nous n'avons géré qu'une partie du cycle de vie d'un projet _data_, à savoir l'entraînement et la mise à disposition d'un modèle. Comme nous partions de très long, ce sprint avait plutôt l'allure d'un marathon. Néanmoins, nous avons maintenant un projet très flexible qui pourrait permettre d'aller plus loin et d'intégrer d'autres aspects du cycle de vie d'un projet _data_.

En premier lieu, pour suivre la vie d'un modèle en production (enjeu de l'[observabilité](https://www.giskard.ai/glossary/model-observability)), il faut collecter de nouvelles données annotées. Si cette collecte n'est pas naturelle (de nouveaux enregistrement du _label_ sont automatiques), il faut généralement mettre en oeuvre de l'annotation humaine ou des _feedbacks_. Parmi les outils disponibles sur le `SSPCloud` pour cela, il existe [`Label Studio`](https://labelstud.io/).

En second lieu, on a encore une chaîne de production où peu de profils non _tech_ peuvent s'insérer. On pourrait vouloir la rendre plus accessible: soit en améliorant notre _output_ site web, soit en mettant en oeuvre des _dashboards_ plus orientés _BI_, pensés pour des profils moins techniques mais ayant une place dans la chaîne de valeur _data_ comme les _data analysts_. Pour cela, il existe des outils sur le `SSPCloud` comme [`Apache Superset`](https://superset.apache.org/).

Gardons aussi à l'esprit que notre chaine dépend de données tabulaires simples avec un objectif assez modeste: mettre à disposition un modèle de classification dans un cadre supervisé. Si on avait un _use case_ différent ou des données plus complexes - par exemple des remontées de données en temps réel ou encore des données textuelles - nous adopterions probablement un _pipeline_ ou des briques technologiques différents à certaines étapes. Néanmoins, la philosophie serait probablement la même : avoir une chaine modulaire, avec les outils technologiques les plus efficaces à chaque maillon, et `Python` et `Kubernetes` pour, dans les ténèbres, les lier.


