---
title: "Appliquer les concepts √©tudi√©s √† un projet de data science"
author: "Romain Avouac et Lino Galiana"
draft: false
# layout options: single, single-sidebar
layout: single
from: markdown+emoji
---



L'objectif de cette mise en application est d'**illustrer les diff√©rentes √©tapes qui s√©parent la phase de d√©veloppement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les diff√©rents concepts pr√©sent√©s tout au long du cours.

Nous nous pla√ßons dans une situation initiale correspondant √† la fin de la phase de d√©veloppement d'un projet de data science.
On a un notebook un peu monolithique, qui r√©alise les √©tapes classiques d'un *pipeline* de *machine learning* :

- Import de donn√©es ;
- Statistiques descriptives et visualisations ;
- *Feature engineering* ;
- Entra√Ænement d'un mod√®le ;
- Evaluation du mod√®le

**L'objectif est d'am√©liorer le projet de mani√®re incr√©mentale jusqu'√† pouvoir le mettre en production, en le valorisant sous une forme adapt√©e.** 

::: {.callout-important}
Il est important de bien lire les consignes et d'y aller progressivement.
Certaines √©tapes peuvent √™tre rapides, d'autres plus fastidieuses ;
certaines √™tre assez guid√©es, d'autres vous laisser plus de libert√©.
Si vous n'effectuez pas une √©tape, vous risquez de ne pas pouvoir passer √†
l'√©tape suivante qui en d√©pend.

Bien que l'exercice soit applicable sur toute configuration bien faite, nous 
recommandons de privil√©gier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), o√π tous les 
outils n√©cessaires sont pr√©-install√©s et pr√©-configur√©s. 
:::






# Partie 1 : qualit√© du script

Cette premi√®re partie vise √† **rendre le projet conforme aux bonnes pratiques** pr√©sent√©es dans le cours.

Elle fait intervenir les notions suivantes : 

- Utilisation du **terminal** (voir [Linux 101](/chapters/linux-101.html)) ;
- **Qualit√© du code** (voir [Qualit√© du code](/chapters/code-quality.html)) ;
- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;
- **Contr√¥le de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;
- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).

Le plan de la partie est le suivant :

<!----
0. :zero: _Forker_ le d√©p√¥t et cr√©er une branche de travail
1. :one: S'assurer que le _notebook_ s'ex√©cute correctement
2. :two: Modularisation : mise en fonctions et mise en module
3. :three: Utiliser un `main` script
4. :four:  Appliquer les standards de qualit√© de code
5. :five: Adopter une architecture standardis√©e de projet
6. :six: Fixer l'environnement d'ex√©cution
7. :seven: Stocker les donn√©es de mani√®re externe
8. :eight: Nettoyer le projet `Git`
9. :nine: Ouvrir une *pull request* sur le d√©p√¥t du projet.
------------->

Nous allons partir de ce _Notebook_ `Jupyter`,
que vous pouvez pr√©visualiser voire tester
en cliquant sur l'un des liens suivants:




<a href="https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=false&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fensae-reproductibilite-website%2Fmaster%2Fpreview-notebook.sh%C2%BB" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSPcloud-Tester%20notebook%20sur%20SSP--cloud-informational&amp;color=yellow?logo=Python" alt="Onyxia"></a>
<a href="http://colab.research.google.com/github/linogaliana/ensae-reproductibilite-application/blob/main/titanic.ipynb" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>


## Etape 0: forker le d√©p√¥t d'exemple et cr√©er une branche de travail



- Ouvrir un service `VSCode` sur le [SSP Cloud](https://datalab.sspcloud.fr/home). Vous pouvez aller
dans la page `My Services` et cliquer sur `New service`. Sinon, vous
pouvez lancer le service en cliquant directement [ici](https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=false).

- G√©n√©rer un jeton d'acc√®s (*token*) sur `GitHub` afin de permettre l'authentification en ligne de commande √† votre compte.
La proc√©dure est d√©crite [ici](https://docs.sspcloud.fr/onyxia-guide/controle-de-version#creer-un-jeton-dacces-token). Garder le jeton g√©n√©r√© de c√¥t√©.

- Forker le d√©p√¥t `Github` : https://github.com/linogaliana/ensae-reproductibilite-application

- Cl√¥ner __votre__ d√©p√¥t `Github` en utilisant le
terminal depuis `Visual Studio` (`Terminal > New Terminal`) :

```shell
$ git clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application.git
```

o√π `<TOKEN>` et `<USERNAME>` sont √† remplacer, respectivement, 
par le jeton que vous avez g√©n√©r√© pr√©c√©demment et votre nom d'utilisateur.

- Se placer avec le terminal dans le dossier en question : 

```shell
$ cd ensae-reproductibilite-application
```

- Cr√©ez une branche `nettoyage` :

```shell
$ git checkout -b nettoyage
Switched to a new branch 'nettoyage'
```

## Etape 1 : s'assurer que le script s'ex√©cute correctement

On va partir du fichier `notebook.py` qui reprend le contenu 
du _notebook_[^jupytext] mais dans un script classique.

[^jupytext]: L'export dans un script `.py` a √©t√© fait
        avec [`Jupytext`](https://jupytext.readthedocs.io/en/latest/index.html). Comme
        cela n'est pas vraiment l'objet du cours, nous passons cette √©tape et fournissons
        directement le script. Mais n'oubliez
        pas que cette d√©marche, fr√©quente quand on a d√©marr√© sur un _notebook_ et
        qu'on d√©sire consolider en faisant la transition vers des 
        scripts, n√©cessite d'√™tre attentif pour ne pas risquer de faire une erreur. 

La premi√®re √©tape est simple, mais souvent oubli√©e : **v√©rifier que le code fonctionne correctement**. 


::: {.callout-tip}
## Application 1: corriger les erreurs

- Ouvrir dans `VSCode` le script `titanic.py` ;
- Ex√©cuter le script ligne √† ligne pour d√©tecter les erreurs ;
- Corriger les deux erreurs qui emp√™chent la bonne ex√©cution ;
- V√©rifier le fonctionnement du script en utilisant la ligne de commande

```shell
python titanic.py
```

:::


Il est maintenant temps de *commit* les changements effectu√©s avec `Git`[^2] :

[^2]: Essayez de *commit* vos changements √† chaque √©tape de l'exercice, c'est une bonne habitude √† prendre.

```shell
$ git add titanic.py
$ git commit -m "Corrige l'erreur qui emp√™chait l'ex√©cution"
$ git push
```

::: {.callout-caution collapse="true"}
## Checkpoint

[Script _checkpoint_](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application1/titanic.py)
:::


## Etape 2: utiliser un _linter_ puis un _formatter_

On va maintenant am√©liorer la qualit√© de notre code en appliquant les standards communautaires.
Pour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/). 

::: {.callout-note}
N'h√©sitez pas √† taper un code d'erreur sur un moteur de recherche pour obtenir plus d'informations si jamais le message n'est pas clair !
:::

Pour appliquer le _linter_ √† un script `.py`,
la syntaxe √† entrer dans le terminal est la suivante : 

```shell
$ pylint mon_script.py
```

::: {.callout-important}
[`PyLint`](https://pylint.readthedocs.io/en/latest/) et [`Black`](https://black.readthedocs.io/en/stable/)
sont des _packages_ `Python` qui 
s'utilisent principalement en ligne de commande.

Si vous avez une erreur qui sugg√®re
que votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/)
ou [`Black`](https://black.readthedocs.io/en/stable/),
n'oubliez pas d'ex√©cuter la commande `pip install pylint` ou `pip install black`.
:::


Le _linter_ renvoie alors une s√©rie d'irr√©gularit√©s,
en pr√©cisant √† chaque fois la ligne de l'erreur et le message d'erreur associ√© (ex : mauvaise identation).
Il renvoie finalement une note sur 10,
qui estime la qualit√© du code √† l'aune des standards communautaires √©voqu√©s
dans la partie [Qualit√© du code](/chapters/code-quality.html).


::: {.callout-tip}
## Application 2: rendre lisible le script

- Diagnostiquer et √©valuer la qualit√© de `titanic.py` avec [`PyLint`](https://pylint.readthedocs.io/en/latest/). Regarder la note obtenue.
- Utiliser `black titanic.py --diff --color` pour observer les changements de forme que va induire l'utilisation du _formatter_ [`Black`](https://black.readthedocs.io/en/stable/)
- Appliquer le _formatter_ [`Black`](https://black.readthedocs.io/en/stable/)
- R√©utiliser [`PyLint`](https://pylint.readthedocs.io/en/latest/) pour diagnostiquer l'am√©lioration de la qualit√© du script et le travail qui reste √† faire. 
- Comme la majorit√© du travail restant est √† consacrer aux imports:
    - Mettre tous les _imports_ ensemble en d√©but de script
    - Retirer les _imports_ redondants en s'aidant des diagnostics de votre √©diteur
    - R√©ordonner les _imports_ si [`PyLint`](https://pylint.readthedocs.io/en/latest/) vous indique de le faire
    - Corriger les derni√®res fautes formelles sugg√©r√©es par [`PyLint`](https://pylint.readthedocs.io/en/latest/)
- D√©limiter des parties dans votre code pour rendre sa structure plus lisible 
:::

Le code est maintenant lisible, il obtient √† ce stade une note formelle proche de 10.
Mais il n'est pas encore totalement intelligible ou fiable.
Il y a notamment 
beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. 
N√©anmoins, avant cela, occupons-nous de mieux g√©rer certains param√®tres du script: 
jetons d'API et chemin des fichiers.


::: {.callout-caution collapse="true"}
## Checkpoint

[`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application2/titanic.py)
:::

## Etape 3: gestion des param√®tres

L'ex√©cution du code et les r√©sultats obtenus
d√©pendent de certains param√®tres. L'√©tude de r√©sultats
alternatifs, en jouant sur 
des variantes des param√®tres, est √† ce stade compliqu√©e
car il est n√©cessaire de parcourir le code pour trouver
ces param√®tres. De plus, certains param√®tres personnels
comme des jetons
d'API ou des mots de passe n'ont pas vocation √† 
√™tre pr√©sents dans le code. 

Il est plus judicieux de consid√©rer ces param√®tres comme des
variables d'entr√©e du script. Cela peut √™tre fait de deux
mani√®res:

1. Avec des arguments optionnels appel√©s depuis la ligne de commande.
Cela peut √™tre pratique pour mettre en oeuvre des tests automatis√©s[^noteCI] mais
n'est pas forc√©ment pertinent pour toutes les variables. Nous allons montrer
cet usage avec le nombre d'arbres de notre _random forest_ ;
2. En utilisant un fichier de configuration dont les valeurs sont import√©es dans
le script principal. Nous allons le mettre en oeuvre pour deux types de fichiers:
les √©l√©ments de configuration √† partager et ceux √† conserver pour soi mais 
pouvant servir.

[^noteCI]: Nous le verrons lorsque nous mettrons en oeuvre l'int√©gration continue.

::: {.callout-tip}
## Application 3: Param√©trisation du script

1. En s'inspirant de [cette r√©ponse](https://stackoverflow.com/a/69377311/9197726), 
cr√©er une variable `n_trees` qui peut √©ventuellement √™tre param√©tr√©e en ligne de commande
et dont la valeur par d√©faut est 20.
2. Tester cette param√©trisation en ligne de commande avec la valeur par d√©faut
puis 2, 10 et 50 arbres
3. Rep√©rer le jeton d‚ÄôAPI dans le code. Retirer le jeton d‚ÄôAPI du code
et cr√©er √† la racine du projet un fichier YAML nomm√© `secrets.yaml`
o√π vous √©crivez ce secret sous la forme `key: value`
4. Pour √©viter d'avoir √† le faire plus tard,
cr√©er une fonction `import_yaml_config` qui prend en argument le
chemin d'un fichier `YAML`
et renvoie le contenu de celui-ci en _output_. Vous pouvez suivre
le conseil du chapitre sur la [Qualit√© du code](/chapters/code-quality.html)
en adoptant le _type hinting_.
5. Cr√©er la variable `API_TOKEN` ayant la valeur stock√©e dans `secrets.yaml`[^fileexist].
5. Tester en ligne de commande que l'ex√©cution du fichier est toujours
sans erreur
6. Refaire un diagnostic avec [`PyLint`](https://pylint.readthedocs.io/en/latest/)
et corriger les √©ventuels messages. 
7. Cr√©er un fichier `config.yaml` stockant trois informations: le chemin des donn√©es
d'entra√Ænement, des donn√©es de test et la r√©partition train/test utilis√©e dans le code. 
Cr√©er les variables correspondantes dans le code apr√®s avoir utilis√© `import_yaml_config`
8. Cr√©er un fichier `.gitignore`. Ajouter dans ce fichier `secrets.yaml`
car il ne faut pas committer ce fichier.
8. Cr√©er un fichier `README.md` o√π vous indiquez qu'il faut cr√©er un fichier `secrets.yaml` pour
pouvoir utiliser l'API. 

[^fileexist]: Ici, le jeton d'API n'est pas indispensable pour que le code
    fonctionne. Afin d'√©viter une erreur non n√©cessaire
    lorsqu'on automatisera le processus, on peut
    cr√©er une condition qui v√©rifie la pr√©sence ou non de ce fichier. 
    
    Cela peut √™tre fait avec la fonction `os.path.exists` :

        if os.path.exists('secrets.yaml'):
            secrets = import_yaml_config("secrets.yaml")

    La variable `secrets` n'existera que dans le cas o√π un fichier `secrets.yaml` existe. 
    Le script reste donc reproductible m√™me pour un utilisateur n'ayant pas le fichier
    `secrets.yaml`. 

<details>
<summary>Indice si vous ne trouvez pas comment lire un fichier `YAML`</summary>
Si le fichier s'appelle `toto.yaml`, vous pouvez l'importer de cette mani√®re:
```python
with open("toto.yaml", "r", encoding="utf-8") as stream:
    dict_config = yaml.safe_load(stream)
```
</details>

:::


::: {.callout-caution collapse="true"}
## Checkpoint

- [`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/titanic.py)
- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)


:::


## Etape 4 : Adopter la programmation fonctionnelle

Nous allons **mettre en fonctions les parties importantes de l'analyse, et les mettre dans un module afin de pouvoir les importer directement depuis le notebook**.

Cet exercice √©tant chronophage, il n'est __pas obligatoire de le r√©aliser en entier__. L'important est de
comprendre la d√©marche et d'adopter fr√©quemment une approche fonctionnelle[^POO]. Pour obtenir 
une chaine enti√®rement fonctionnalis√©e, vous pouvez reprendre le _checkpoint_.

[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser
un processus, il serait possible d'adopter le paradigme de la __programmation orient√©e objet (POO)__. Celle-ci est
plus rebutante et demande plus de temps au d√©veloppeur. L'arbitrage co√ªt-avantage est n√©gatif pour notre
exemple, nous proposons donc de nous en passer. N√©anmoins, pour une mise en production r√©elle d'un mod√®le,
il est recommand√© de l'adopter. C'est d'ailleurs obligatoire avec des [_pipelines_ `scikit`](https://pythonds.linogaliana.fr/pipeline-scikit/). 

::: {.callout-tip}
## Application 4: adoption des standards de programmation fonctionnelle 

- Cr√©er une fonction qui importe les donn√©es d'entra√Ænement (`train.csv`) et de test (`test.csv`) et renvoie des `DataFrames` `Pandas` ;
- En fonction du temps disponible, cr√©er plusieurs fonctions pour r√©aliser les √©tapes de *feature engineering*:
    + La cr√©ation de la variable _"Title"_ peut √™tre automatis√©e en vertu du principe _"do not repeat yourself"_[^notepandas].
    + Regrouper ensemble les `fillna` et essayer de cr√©er une fonction g√©n√©ralisant l'op√©ration. 
    + Les _label encoders_ peuvent √™tre transform√©s en deux fonctions: une premi√®re pour encoder une colonne puis une seconde qui utilise
    la premi√®re de mani√®re r√©p√©t√©e pour encoder plusieurs colonnes. _Remarquez les erreurs de copier-coller que cela corrige_
    + Finaliser les derni√®res transformations avec des fonctions
- Cr√©er une fonction qui r√©alise le *split train/test* de validation en fonction d'un param√®tre repr√©sentant la proportion de l'√©chantillon de test.
- Cr√©er une fonction qui entra√Æne et √©value un classifieur `RandomForest`, et qui prend en param√®tre le nombre d'arbres (`n_estimators`). La fonction doit imprimer √† la fin la performance obtenue et la matrice de confusion.
- D√©placer toutes les fonctions ensemble, en d√©but de script.
<!----- plus tard ?
- Mettre ces fonctions dans un module `functions.py`
- importer les fonctions via le module dans le notebook et v√©rifier que l'on retrouve bien les diff√©rents r√©sultats en utilisant les fonctions.
------->
:::

[^notepandas]: Au passage vous pouvez noter que mauvaises pratiques discutables,
    peuvent
    √™tre corrig√©es, notamment l'utilisation excessive de `apply` l√† o√π
    il serait possible d'utiliser des m√©thodes embarqu√©es par `Pandas`.
    Cela est plut√¥t de l'ordre du bon style de programmation que de la
    qualit√© formelle du script. Ce n'est donc pas obligatoire mais c'est mieux. 


::: {.callout-important}
Le fait d'appliquer des fonctions a d√©j√† am√©lior√© la fiabilit√© du processus
en r√©duisant le nombre d'erreurs de copier-coller. N√©anmoins, pour vraiment
fiabiliser le processus, il faudrait utiliser un _pipeline_ de transformations
de donn√©es. 

Ceci n'est pas encore au programme du cours mais le sera dans une prochaine 
version. 
:::

::: {.callout-caution collapse="true"}
## Checkpoint

- [`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application4/titanic.py)

Les autres fichiers inchang√©s:

- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)

:::



# Partie 2 : adoption d'une structure modulaire {#partie2}

Dans la partie pr√©c√©dente,
on a appliqu√© de mani√®re incr√©mentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est d√©j√† consid√©rablement rapproch√©s d'un
possible partage du code : celui-ci est lisible et intelligible. 
Le code est proprement versionn√© sur un
d√©p√¥t `GitHub`.
 
N√©anmoins,
la structure du projet n'est pas encore normalis√©e. 
De plus, 
l'adoption d'une structure plus modulaire facilitera
la compr√©hension de la chaine de traitement.


## Etape 1 : modularisation

Fini le temps de l'exp√©rimentation : on va maintenant essayer de se passer compl√®tement du _notebook_.
Pour cela, on va utiliser un `main` script, c'est √† dire un script qui reproduit l'analyse en important et en ex√©cutant les diff√©rentes fonctions dans l'ordre attendu.


::: {.callout-tip}
## Application 5: modularisation

- D√©placer les fonctions dans une s√©rie de fichiers d√©di√©s:
    +  `import_data.py`: fonctions d'import de donn√©es 
    +  `build_features.py`: fonctions regroupant les √©tapes de _feature engineering_ 
    +  `train_evaluate.py`: fonctions d'entrainement et d'√©valuation du mod√®le
- Sp√©cifier les d√©pendances (i.e. les packages √† importer)
dans les modules pour que ceux-ci puissent s'ex√©cuter ind√©pendamment ;
- Renommer `titanic.py` en `main.py` pour suivre la convention de nommage des projets `Python` ;
- Importer les fonctions n√©cessaires √† partir des modules. ‚ö†Ô∏è Ne pas utiliser `from XXX import *`, ce n'est pas une bonne pratique ! 
- V√©rifier que tout fonctionne bien en ex√©cutant le _script_ `main` √† partir de la ligne de commande :

```shell
$ python main.py
```
:::

On dispose maintenant d'une application `Python` fonctionnelle. 
N√©anmoins, le projet est certes plus fiable mais sa structuration
laisse √† d√©sirer et il serait difficile de rentrer √† nouveau
dans le projet dans quelques temps. 

<details>
<summary>Etat actuel du projet üôà</summary>

```shell
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ train.csv
‚îú‚îÄ‚îÄ test.csv
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ secrets.yaml
‚îú‚îÄ‚îÄ import_data.py
‚îú‚îÄ‚îÄ build_features.py
‚îú‚îÄ‚îÄ train_evaluate.py
‚îî‚îÄ‚îÄmain.py
```

</details>

Comme cela est expliqu√© dans la
partie [Structure des projets](/chapters/projects-architecture.html),
on va adopter une structure certes arbitraire mais qui va 
faciliter l'autodocumentation de notre projet.  

De plus, une telle structure va faciliter des √©volutions optionnelles
comme la packagisation du projet. Passer d'une structure modulaire
bien faite √† un _package_ est quasi-imm√©diat en `Python`. 

::: {.callout-caution collapse="true"}
## Checkpoint

- [`build_features.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/build_features.py)
- [`import_data.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/import_data.py)
- [`train_evaluate.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/train_evaluate.py)
- [`main.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/main.py)

Les autres fichiers inchang√©s:

- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)

:::

## Etape 2 : adopter une architecture standardis√©e de projet

On va maintenant modifier l'architecture de notre projet pour la rendre plus standardis√©e.
Pour cela, on va s'inspirer des structures
[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)
qui g√©n√®rent des _templates_ de projet.

On va s'inspirer de la structure du [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)
d√©velopp√© par la communaut√©.

::: {.callout-note}
L'id√©e de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de b√¢tir √† l'avance une structure √©volutive. La syntaxe √† utiliser dans ce cas est la suivante : 

```shell
$ pip install cookiecutter
$ cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

Ici, on a d√©j√† un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure propos√©e afin de r√©organiser celle de notre projet selon les standards communautaires.
:::

En s'inspirant du _cookiecutter data science_
on va adopter la structure suivante:

```shell
ensae-reproductibilite-application
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îî‚îÄ‚îÄ raw
‚îÇ       ‚îú‚îÄ‚îÄ test.csv
‚îÇ       ‚îî‚îÄ‚îÄ train.csv
‚îú‚îÄ‚îÄ configuration
‚îÇ   ‚îú‚îÄ‚îÄ secrets.yaml
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb
‚îî‚îÄ‚îÄ src
    ‚îú‚îÄ‚îÄ data
    ‚îÇ   ‚îî‚îÄ‚îÄ import_data.py
    ‚îú‚îÄ‚îÄ features
    ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py
    ‚îî‚îÄ‚îÄ models
        ‚îî‚îÄ‚îÄ train_evaluate.py
```

::: {.callout-tip}

## Application 6: adopter une structure lisible

- _(optionnel)_ Analyser et comprendre la [structure de projet](https://drivendata.github.io/cookiecutter-data-science/#directory-structure) propos√©e par le template
- Modifier l'arborescence du projet selon le mod√®le
- Adapter les scripts et les fichiers de configuration √† la nouvelle arborescence
- Ajouter le dossier __pycache__ au `.gitignore`[^pycache] et le dossier `data`
:::

[^pycache]: Il est normal d'avoir des dossiers `__pycache__` qui tra√Ænent : ils se cr√©ent automatiquement √† l'ex√©cution d'un script en `Python`.

::: {.callout-caution collapse="true"}
## Checkpoint

- [`build_features.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/build_features.py)
- [`import_data.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/import_data.py)
- [`train_evaluate.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/train_evaluate.py)
- [`main.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/main.py)

Les autres fichiers sont inchang√©s, √† l'exception de leur emplacement.

:::

### Etape 3: indiquer l'environnement minimal de reproductibilit√©

Le script `main.py` n√©cessite un certain nombre de packages pour
√™tre fonctionnel. Chez vous les packages n√©cessaires sont
bien s√ªr install√©s mais √™tes-vous assur√© que c'est le cas 
chez la personne qui testera votre code ? 

Afin de favoriser la portabilit√© du projet,
il est d'usage de _"fixer l'environnement"_,
c'est-√†-dire d'indiquer dans un fichier toutes les d√©pendances utilis√©es ainsi que leurs version.
Nous proposons de cr√©er un fichier `requirements.txt` minimal, sur lequel nous reviendrons
dans la partie consacr√©e aux environnements reproductibles. 

Le fichier `requirements.txt` est conventionnellement localis√© √† la racine du projet.
Ici on ne va pas fixer les versions, on raffinera ce fichier plus tard.

::: {.callout-tip}

## Application 7: cr√©ation du `requirements.txt`

- Cr√©er un fichier `requirements.txt` avec la liste des packages n√©cessaires
- Ajouter une indication dans `README.md` sur l'installation des _packages_ gr√¢ce au fichier `requirements.txt` 
:::

::: {.callout-caution collapse="true"}
## Checkpoint

- [`requirements.txt`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application7/requirements.txt)
- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application7/README.md)

:::

## Etape 4 : stocker les donn√©es de mani√®re externe {#stockageS3}


::: {.callout-warning}
Pour mettre en oeuvre cette √©tape, il peut √™tre utile de
comprendre un peu comme fonctionne le SSP Cloud.
Vous devrez suivre la [documentation du SSP Cloud](https://docs.sspcloud.fr/onyxia-guide/stockage-de-donnees) pour la r√©aliser. Une aide-m√©moire est √©galement disponible dans le cours
de 2e ann√©e de l'ENSAE [Python pour la data science](https://linogaliana-teaching.netlify.app/reads3/#)
:::

Comme on l'a vu dans le cours ([partie structure des projets](/chapters/project-structure.html)),
les donn√©es ne sont pas cens√©es √™tre versionn√©es sur un projet `Git`.

L'id√©al pour √©viter cela tout en maintenant la reproductibilit√© est d'utiliser une solution de stockage externe.
On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud. 

::: {.callout-tip}

## Application 8: utilisation d'un syst√®me de stockage distant

A partir de la ligne de commande,
utiliser l'utilitaire [MinIO](https://min.io/docs/minio/linux/reference/minio-mc.html)
pour copier les donn√©es `data/raw/train.csv` et `data/raw/test.csv` vers votre
bucket personnel, respectivement dans les dossiers `ensae-reproductibilite/data/raw/train.csv`
et `ensae-reproductibilite/data/raw/test.csv`. 

<details>
<summary>Indice</summary>

Structure √† adopter:

```shell
$ mc cp data/raw/train.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv
$ mc cp data/raw/test.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv
```

en modifiant l'emplacement de votre bucket personnel
</details>

- Pour se simplifier la vie, on va utiliser des URL de t√©l√©chargement des fichiers
(comme si ceux-ci √©taient sur n'importe quel espace de stockage) plut√¥t que d'utiliser
une librairie `S3` compatible comme `boto3` ou `s3fs`. Pour cela, en ligne de
commande, faire:

```shell
mc anonymous set download s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/
```

en modifiant `<BUCKET_PERSONNEL>`. Les URL de t√©l√©chargement seront de la forme 
`https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv`
et `https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv`

- Modifier `configuration.yaml` pour utiliser directement les URL dans l'import 
- Supprimer les fichiers `.csv` du dossier `data` de votre projet, on n'en a plus besoin vu qu'on les importe de l'ext√©rieur
- V√©rifier le bon fonctionnement de votre application
:::

::: {.callout-caution collapse="true"}
## Checkpoint

- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application8/config.yaml)

:::

# Partie 2bis: packagisation de son projet (optionnel)

Cette s√©rie d'actions n'est pas forc√©ment pertinente pour tous
les projets. Elle fait un peu la transition entre la modularit√©
et la portabilit√©. 

## Etape 1 : proposer des tests unitaires (optionnel)

Notre code comporte un certain nombre de fonctions g√©n√©riques.
On peut vouloir tester leur usage sur des donn√©es standardis√©es,
diff√©rentes de celles du Titanic.

M√™me si la notion de tests unitaires
prend plus de sens dans un _package_, nous pouvons proposer
dans le projet des exemples d'utilisation de la fonction, ceci peut √™tre p√©dagogique. 

Nous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)
et `pytest`
pour effectuer des tests unitaires. Cette approche n√©cessite une ma√Ætrise 
de la programmation orient√©e objet.

::: {.callout-tip}

## Application 9: test unitaire _(optionnel)_

Dans le dossier `src/data/`, cr√©er un fichier `test_create_variable_title.py`[^emplacement].

En s'inspirant de l'[exemple de base](https://docs.python.org/3/library/unittest.html#basic-example),
cr√©er une classe `TestCreateVariableTitle` qui effectue les op√©rations suivantes:

- Cr√©ation d'une fonction `test_create_variable_title_default_variable_name` qui permet 
de comparer les objets suivants:

    + Cr√©ation d'un `DataFrame` de test :  

    ```python
    df = pd.DataFrame({
                'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)',
                        'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath (Lily May Peel)',
                        'Allen, Mr. William Henry', 'Moran, Mr. James',
                        'McCarthy, Mr. Timothy J', 'Palsson, Master. Gosta Leonard',
                        'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)',
                        'Nasser, Mrs. Nicholas (Adele Achem)'],
                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],
                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
            })
    ```

    + Utilisation de la fonction `create_variable_title` sur ce `DataFrame`
    + Comparaison au `DataFrame` attendu:

    ```python
    expected_result = pd.DataFrame({
                'Title': ['Mr.', 'Mrs.', 'Miss.', 'Mrs.', 'Mr.', 'Mr.', 'Mr.', 'Master.', 'Mrs.', 'Mrs.'],
                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],
                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
            })
    ```

- Effectuer le test unitaire en ligne de commande avec `unittest`. Corriger le test unitaire en cas d'erreur. 
- Si le temps le permet, proposer des variantes pour tenir compte de param√®tres (comme la variable `variable_name`)
ou d'exceptions (comme la gestion du cas _"Dona"_)
:::

::: {.callout-note}

Lorsqu'on effectue des tests unitaires, on cherche g√©n√©ralement
√† tester le plus de lignes possibles de son code. On parle de
taux de couverture (_coverage rate_) pour d√©signer
la statistique mesurant cela. 

Cela peut s'effectuer de la mani√®re suivante avec le package
[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):

```shell
$ coverage run -m pytest test_create_variable_title.py
$ coverage report -m

Name                            Stmts   Miss  Cover   Missing
-------------------------------------------------------------
import_data.py                     15      6    60%   16-19, 31-34
test_create_variable_title.py      21      1    95%   54
-------------------------------------------------------------
TOTAL                              36      7    81%
```

Le taux de couverture est souvent mis en avant par les gros
projets comme indicateur de leur qualit√©. Il existe d'ailleurs
des badges `Github` d√©di√©s. 
:::

[^emplacement]: L'emplacement de ce fichier est amen√© √† √©voluer dans le cadre
    d'une packagisation. Dans un package, ces tests seront dans un dossier
    sp√©cifique `/tests` car `Python` sait g√©rer de mani√®re plus formelle
    les imports de fonctions depuis des modules. Ici, on est dans une 
    situation transitoire, raison pour laquelle les tests
    sont dans les m√™mes dossiers que les fonctions. 

::: {.callout-caution collapse="true"}
## Checkpoint

- [`test_create_variable_title.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application9/test_create_variable_title.py)

Les autres fichiers sont inchang√©s.

:::


## Etape 2 : transformer son projet en package (optionnel)

Notre projet est modulaire, ce qui le rend assez simple √† transformer
en package, en s'inspirant du `cookiecutter` adapt√©, issu
de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

<details>
<summary>Structure vis√©e</summary>

```shell
ensae-reproductibilite-application
‚îú‚îÄ‚îÄ docs                                    ‚îê 
‚îÇ   ‚îú‚îÄ‚îÄ main.py                             ‚îÇ 
‚îÇ   ‚îî‚îÄ‚îÄ notebooks                           ‚îÇ Package documentation and examples
‚îÇ       ‚îú‚îÄ‚îÄ titanic.ipynb                   ‚îÇ 
‚îú‚îÄ‚îÄ README.md                               ‚îò 
‚îú‚îÄ‚îÄ pyproject.toml                          ‚îê 
‚îú‚îÄ‚îÄ requirements.txt                        ‚îÇ
‚îú‚îÄ‚îÄ src                                     ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ titanicml                           ‚îÇ Package source code, metadata,
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py                     ‚îÇ and build instructions 
‚îÇ       ‚îú‚îÄ‚îÄ config.yaml                     ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ import_data.py                  ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ build_features.py               ‚îÇ 
‚îÇ       ‚îî‚îÄ‚îÄ train_evaluate.py               ‚îò
‚îî‚îÄ‚îÄ tests                                   ‚îê
    ‚îî‚îÄ‚îÄ test_create_variable_title.py       ‚îò Package tests
```
</details>

<details>
<summary>Rappel: structure actuelle</summary>

```shell
ensae-reproductibilite-application
‚îú‚îÄ‚îÄ notebooks                                 
‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb                  
‚îú‚îÄ‚îÄ configuration                                 
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml                  
‚îú‚îÄ‚îÄ main.py                              
‚îú‚îÄ‚îÄ README.md                 
‚îú‚îÄ‚îÄ requirements.txt                      
‚îî‚îÄ‚îÄ src 
    ‚îú‚îÄ‚îÄ data                                
    ‚îÇ   ‚îú‚îÄ‚îÄ import_data.py                    
    ‚îÇ   ‚îî‚îÄ‚îÄ test_create_variable_title.py      
    ‚îú‚îÄ‚îÄ features                           
    ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py      
    ‚îî‚îÄ‚îÄ models                          
        ‚îî‚îÄ‚îÄ train_evaluate.py              
```
</details>

::: {.callout-tip}

## Application 10: packagisation _(optionnel)_

- D√©placer les fichiers dans le dossier `src` pour respecter la nouvelle
arborescence ;
- Dans `src/titanicml`, cr√©er un fichier vide `__init__.py`[^init] ;
- D√©placer le fichier de configuration dans le _package_ (n√©cessaire √† la reproductibilit√©) ;
- Cr√©er le dossier `docs` et mettre les fichiers indiqu√©s dedans
- Modifier `src/titanicml/import_data.py` :
    + Ajouter la variable `config_file = os.path.join(os.path.dirname(__file__), "config.yaml")`. Cela permettra d'utiliser directement le fichier ;
    + Proposer un argument par d√©faut √† la fonction `import_config_yaml` √©gal √† `config_file`
- Cr√©er un fichier `pyproject.toml` √† partir du contenu de [ce mod√®le de `pyproject`](https://github.com/linogaliana/ensae-reproductibilite-application/blob/main/checkpoints/application10/pyproject.toml)[^setuptools]
- Installer le package en local avec `pip install .`
- Modifier le contenu de `docs/main.py` pour importer les fonctions de notre _package_ `titanicml` et tester en 
ligne de commande notre fichier `main.py`
:::

[^init]: Le fichier `__init__.py` indique √† `Python` que le dossier
est un _package_. Il permet de proposer certaines configurations
lors de l'import du _package_. Il permet √©galement de contr√¥ler
les objets export√©s (c'est-√†-dire mis √† disposition de l'utilisateur)
par le _package_ par rapport aux objets internes au _package_. 
En le laissant vide, nous allons utiliser ce fichier 
pour importer l'ensemble des fonctions de nos sous-modules. 
Ce n'est pas la meilleure pratique mais un contr√¥le plus fin des
objets export√©s demanderait un investissement qui ne vaut, ici, pas
le co√ªt. 


[^setuptools]: Ce `pyproject.toml` est un mod√®le qui utilise `setuptools`
    pour _build_ le _package_. C'est l'outil classique. 
    N√©anmoins, pour des usages plus raffin√©s, 
    il peut √™tre utile d'utiliser [`poetry`](https://python-poetry.org/)
    qui propose des fonctionnalit√©s plus compl√®tes.  


::: {.callout-note}

Pour cr√©er la structure minimale d'un _package_, le plus simple est
d'utiliser le `cookiecutter` adapt√©,
issu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

Comme on a d√©j√† une structure tr√®s modulaire, on va plut√¥t recr√©er cette
structure dans notre projet d√©j√† existant. En fait, il ne manque qu'un fichier essentiel, 
le principal distinguant un projet classique d'un package : `pyproject.toml`.

```shell
cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git
```

<details>
<summary>D√©rouler pour voir les choix possibles</summary>
```shell
author_name [Monty Python]: Daffy Duck
package_name [mypkg]: titanicml
package_short_description []: Impressive Titanic survival analysis
package_version [0.1.0]: 
python_version [3.9]: 
Select open_source_license:
1 - MIT
2 - Apache License 2.0
3 - GNU General Public License v3.0
4 - Creative Commons Attribution 4.0
5 - BSD 3-Clause
6 - Proprietary
7 - None
Choose from 1, 2, 3, 4, 5, 6 [1]: 
Select include_github_actions:
1 - no
2 - ci
3 - ci+cd
Choose from 1, 2, 3 [1]:
```
</details>

:::


<!-----
## Etape 9 : ouvrir une *pull request* sur le d√©p√¥t du projet

Enfin termin√© ! Enfin presque... On s'est donn√© beaucoup de mal √† nettoyer ce d√©p√¥t et le mettre aux standards, autant valoriser ce travail. On va pour cela faire une *pull request* sur le [d√©p√¥t du projet initial](https://github.com/avouacr/ensae-reproductibilite-projet), c'est √† dire proposer √† l'auteur d'int√©grer tous les changements que vous avez effectu√© en committant √† chaque √©tape. 

Suivre la proc√©dure d√©crite dans la [documentation GitHub](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) pour cr√©er une *pull request* √† partir de votre *fork*. Pour la branche *upstream* (le d√©p√¥t cible), on va choisir `master`. Par contre, pour la branche locale (celle sur votre d√©p√¥t), on va choisir la branche `nettoyage`.

Si tout s'est bien pass√©, vous devriez √† pr√©sent voir votre *pull request* sur le d√©p√¥t cible ([ici](https://github.com/avouacr/ensae-reproductibilite-projet/pulls)). Bravo, vous venez de faire votre premi√®re contribution √† l'open source !

{{% box status="warning" title="Warning" icon="fa fa-exclamation-triangle" %}}
Faire une *pull request* via la branche `master` d‚Äôun *fork* est tr√®s mal vu. En effet, il faut souvent faire des contorsionnements pour r√©ussir √† faire co√Øncider deux histoires qui n‚Äôont pas de raison de co√Øncider. On s'√©vite beaucoup de probl√®mes en prenant l'habitude de toujours faire ses *pull requests* √† partir d'une autre branche que `master`.
{{% /box %}}
------------------>

# Partie 2 : construction d'un projet portable et reproductible {#partie3}

Dans la partie pr√©c√©dente,
on a appliqu√© de mani√®re incr√©mentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est d√©j√† consid√©rablement rapproch√©s d'une
possible mise en production : le code est lisible,
la structure du projet est normalis√©e et √©volutive,
et le code est proprement versionn√© sur un
d√©p√¥t `GitHub` <i class="fab fa-github"></i>.

A pr√©sent, nous avons une version du projet qui est largement partageable.
Du moins en th√©orie, car la pratique est souvent plus compliqu√©e : il y a fort √† parier que si vous essayez d'ex√©cuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),
les choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'√©tat, le projet n'est pas portable : il n'est pas possible, sans modifications co√ªteuses, de l'ex√©cuter dans un environnement diff√©rent de celui dans lequel il a √©t√© d√©velopp√©**.

Dans cette seconde partie, nous allons voir comment **normaliser l'environnement d'ex√©cution afin de produire un projet portable**. On sera alors tout proche de pouvoir mettre le projet en production.
On progressera dans l'√©chelle de la reproductibilit√© 
de la mani√®re suivante: 
- :one: [**G√©rer des variables d'environnement hors du code**](#configyaml) ;
- :two: [**Environnements virtuels**](#anaconda) ;
- :three: [**Images et conteneurs `Docker`**](#docker).


## Etape 1: cr√©er un r√©pertoire de variables servant d'input {#configyaml}

### Enjeu

Lors de l'[√©tape 7](#stockageS3), nous avons am√©lior√© la qualit√© du script en 
s√©parant stockage et code. Cependant, peut-√™tre avez-vous remarqu√©
que nous avons introduit un nom de _bucket_ personnel dans le script
(voir [le fichier `main.py`](https://github.com/linogaliana/ensae-reproductibilite-projet-1/blob/v7/main.py#L9)).
Il s'agit typiquement du genre de petit vice cach√© d'un script qui peut 
g√©n√©rer une erreur: vous n'avez pas acc√®s au bucket en question donc
si vous essayez de faire tourner ce script en l'√©tat, vous allez rencontrer
une erreur.

Une bonne pratique pour g√©rer ce type de configuration est d'utiliser un 
fichier `YAML` qui stocke de mani√®re hi√©rarchis√©e les variables globales
[^3].

[^3]: Le format `YAML` est un format de fichier o√π les informations sont 
hi√©rarchis√©es. Avec le _package_ `YAML` on peut tr√®s facilement le transformer
en `dict`, ce qui est tr√®s pratique pour acc√©der √† une information.

En l'occurrence, nous n'avons besoin que de deux √©l√©ments pour pouvoir
d√©-personnaliser ce script :

- le nom du bucket
- l'emplacement dans le bucket

### Application

Dans `VSCode`, cr√©er un fichier nomm√© `config.yaml` et le localiser √† la racine
de votre d√©p√¥t. Voici, une proposition de hi√©rarchisation de l'information
que vous devez adapter √† votre nom d'utilisateur :

```yaml
input:
  bucket: "lgaliana"
  path: "ensae-reproductibilite"
```

Dans `main.py`, importer ce fichier et remplacer la ligne pr√©c√©demment
√©voqu√©e par les valeurs du fichier. Tester en faisant tourner `main.py`
<!-----
https://github.com/linogaliana/ensae-reproductibilite-projet-1/commit/4a9d935223b6af366d4cf2a2a208d98a25407fc6
----->

## Etape 2 :  cr√©er un environnement conda √† partir du fichier `environment.yml` {#anaconda}

L'environnement `conda` cr√©√© avec `conda env export` ([√©tape 6](#conda-export))
contient √©norm√©ment
de d√©pendances, dont de nombreuses qui ne nous sont pas n√©cessaires (il 
en serait de m√™me avec `pip freeze`). 
Nous n'avons en effet besoin que des _packages_ pr√©sents dans la
section `import` de nos scripts et les d√©pendances n√©cessaires
pour que ces _packages_ soient fonctionnels.


Vous allez chercher √† obtenir
un `environment.yml` beaucoup plus parcimonieux
que celui g√©n√©r√© par `conda env export`

{{< panelset class="simplification" >}}

{{% panel name="Approche g√©n√©rale :koala: " %}}

Le tableau r√©capitulatif pr√©sent dans
la [partie portabilit√©](/portability/#aide-m√©moire)
peut √™tre utile dans cette partie. L'id√©e est 
de partir _from scratch_ et figer l'environnement qui
permet d'avoir une appli fonctionnelle. 

* Cr√©er un environnement vide avec `Python 3.10`
<!---
conda create -n monenv python=3.10.0
---->

* Activer cet environnement

* Installer en ligne de commande avec `pip` les packages n√©cessaires
pour faire tourner votre code

<!---
pip install pandas PyYAML s3fs scikit-learn
---->

* Faire un `pip freeze > requirements.txt` ou 
`conda env export > environment.yml` (privil√©gier la deuxi√®me option)

* Retirer la section `prefix` (si elle est pr√©sente)
et changer la section `name` en `monenv`


{{% /panel %}}


{{% panel name="Approche fain√©ante :sloth:" %}}

Nous allons g√©n√©rer une version plus minimaliste gr√¢ce √†
l'utilitaire [`pipreqs`](https://github.com/bndr/pipreqs)

* Installer `pipreqs` en `pip install`
* En ligne de commande, depuis la racine du projet, faire `pipreqs`
* Ouvrir le `requirements.txt` automatiquement g√©n√©r√©. Il est beaucoup plus
minimal que celui que vous obtiendriez avec `pip freeze` ou
l'`environment.yml` obtenu √† [l'√©tape 6](#conda-export). 
* Remplacer toute la section `dependencies` du `environment.yml`
par le contenu du `requirements.txt`
(:warning: ne pas oublier l'indentation et le tiret en d√©but de ligne)
* :warning: Modifier le tiret √† `scikit learn`. Il ne faut pas un _underscore_ mais
un tiret
* Ajouter la version de python (par exemple `python=3.10.0`)
au d√©but de la section `dependencies`
* Retirer la section `prefix` du fichier `environment.yml` (si elle est pr√©sente)
et changer le contenu de la section `name` en `monenv`
* Cr√©er l'environnement
([voir le tableau r√©capitulatif dans la partie portabilit√©](/portability/#aide-m√©moire))

<!----
conda env create -f environment.yml
------>


{{% /panel %}}

{{% /panelset %}}


Maintenant, il reste √† tester si tout fonctionne bien dans notre 
environnement plus minimaliste:

* Activer l'environnement
* Tester votre script en ligne de commande
* Faire un `commit` quand vous √™tes contents


## Etape 3: conteneuriser avec Docker <i class="fab fa-docker"></i> {#docker}

### Pr√©liminaire

- Se rendre sur l'environnement bac √† sable [Play with Docker](https://labs.play-with-docker.com)
- Dans le terminal `Linux`, cloner votre d√©p√¥t `Github`  <i class="fab fa-github"></i>
- Cr√©er via la ligne de commande un fichier `Dockerfile`. Il y a plusieurs mani√®res
de proc√©der, en voici un exemple:

```shell
echo "#Dockerfile pour reproduire mon super travail" > Dockerfile
```

- Ouvrir ce fichier via l'√©diteur propos√© par l'environnement bac √† sable. 


### Cr√©ation d'un premier Dockerfile



- :one: Comme couche de d√©part, partir d'une image l√©g√®re comme `ubuntu:20.04`
- :two: Dans une deuxi√®me couche, faire un `apt get -y update` et
installer `wget` qui va √™tre n√©cessaire pour t√©l√©charger `Miniconda`
depuis la ligne
de commande 
- :three: Dans la troisi√®me couche, nous allons installer `Miniconda` :
    + T√©l√©charger la derni√®re version de `Miniconda` avec `wget` depuis
l'url de t√©l√©chargement direct https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
    + Installer `Miniconda` dans le chemin `/home/coder/local/bin/conda`
    + Effacer le fichier d'installation pour lib√©rer de la place sur l'image
- :four: En quatri√®me couche, on va installer `mamba` pour acc√©l√©rer l'installation
des packages dans notre environnement. 
- :five: En cinqui√®me couche, nous allons cr√©er l'environnement `conda`:
    + Utiliser `COPY` pour que `Docker` soit en mesure d'utiliser
le fichier `environment.yml` (sinon `Docker`
renverra une erreur)
    + Cr√©er l'environnement vide `monenv` (pr√©sentant uniquement `Python` 3.10) avec
la commande  `conda` ad√©quate
    + Mettre √† jour l'environnement en utilisant `environment.yml` avec `mamba`
- :six: Utiliser `ENV` pour ajouter l'environnement `monenv` au `PATH` et utiliser le _fix_ suivant:

```python
RUN echo "export PATH=$PATH" >> /home/coder/.bashrc  # Temporary fix while PATH gets overwritten by code-server
```

- :seven: Exposer sur le port `5000`
- :eight: En derni√®re √©tape, utiliser `CMD` pour reproduire le comportement de `python main.py`


{{% box status="hint" title="Hint: `mamba`" icon="fa fa-lightbulb" %}}
`mamba` est une alternative √† `conda` pour installer des _packages_ dans un
environnement `Miniconda`/`Anaconda`. `mamba` n'est pas obligatoire, `conda`
peut suffire. Cependant, `mamba` est beaucoup plus rapide
que `conda` pour installer des packages √† installer ; il s'agit donc
d'un utilitaire tr√®s pratique. 
{{% /box %}}

{{< panelset class="nommage" >}}

{{% panel name="Indications suppl√©mentaires" %}}

Cliquer sur les onglets ci-dessus :point_up_2: pour b√©n√©ficier
d'indications suppl√©mentaires, pour vous aider. Cependant, essayez
de ne pas les consulter imm√©diatement: n'h√©sitez pas √† t√¢tonner. 


{{% /panel %}}

{{% panel name="Installation de Miniconda" %}}

```shell
# INSTALL MINICONDA -------------------------------
ARG CONDA_DIR=/home/coder/local/bin/conda
RUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
RUN bash Miniconda3-latest-Linux-x86_64.sh -b -p $CONDA_DIR
RUN rm -f Miniconda3-latest-Linux-x86_64.sh
```

{{% /panel %}}

{{% panel name="Installation de mamba" %}}

```shell
ENV PATH="/home/coder/local/bin/conda/bin:${PATH}"
RUN conda install mamba -n base -c conda-forge
```

{{% /panel %}}


{{% panel name="Cr√©ation de l'environnement" %}}

```shell
COPY environment.yml .
RUN conda create -n monenv python=3.10
RUN mamba env update -n monenv -f environment.yml
```

{{% /panel %}}


{{< /panelset >}}

### Construire l'image

Maintenant, nous avons d√©fini notre recette. Il nous reste √†
faire notre plat et √† le go√ªter

- Utiliser `docker build` pour cr√©er une image avec le tag `my-python-app`
- V√©rifier les images dont vous disposez. Vous devriez avoir un r√©sultat
proche de celui-ci

<!---
docker build . -t my-python-app
docker images
---->

```shell
REPOSITORY      TAG       IMAGE ID       CREATED         SIZE
my-python-app   latest    c0dfa42d8520   6 minutes ago   2.23GB
ubuntu          20.04     825d55fb6340   6 days ago      72.8MB
```

### Tester l'image: d√©couverte du cache

Il ne reste plus qu'√† go√ªter la recette et voir si le plat est bon. 

Utiliser `docker run` avec l'option `it` pour pouvoir appeler l'image
depuis son tag

<!----
docker run -it my-python-app
---->

:warning: :bomb: :fire: 
`Docker` ne sait pas o√π trouver le fichier `main.py`. D'ailleurs,
il ne connait pas d'autres fichiers de notre application qui sont n√©cessaires
pour faire tourner le code: `config.yaml` et le dossier `src`

- Avant l'√©tape `EXPOSE` utiliser plusieurs `ADD` et/ou `COPY` pour que l'application
dispose de tous les √©l√©ments minimaux pour √™tre en mesure de fonctionner

- Refaire tourner `docker run`
<!---
docker run -it my-python-app
--->

{{% box status="tip" title="Note" icon="fa fa-hint" %}}
Ici, le _cache_ permet d'√©conomiser beaucoup de temps. Par besoin de 
refaire tourner toutes les √©tapes, `Docker` agit de mani√®re intelligente
en faisant tourner uniquement les nouvelles √©tapes.
{{% /box %}}

### Corriger une faille de reproductibilit√©


Vous devriez rencontrer une erreur li√©e √† la variable d'environnement
`AWS_ENDPOINT_URL`. C'est normal, elle est inconnue de cet environnement
minimaliste. D'ailleurs, `Docker` n'a aucune raison de conna√Ætre
votre espace de stockage sur le `S3` du `SSP-Cloud` si vous ne lui dites
pas. 
Donc cet environnement ne sait pas
comment acc√©der aux fichiers pr√©sents dans votre `minio`.

Vous allez r√©gler ce probl√®me avec les √©tapes suivantes, :


- :one: Naviguer dans l'[interface du SSP-Cloud](https://datalab.sspcloud.fr/mes-fichiers)
pour retrouver les liens d'acc√®s direct de vos fichiers
- :two: Dans `VSCode`, les mettre dans `config.yaml` (faire de nouvelles cl√©s)
- :three: Dans `VSCode`, modifier la fonction d'import pour s'adapter √† ce changement.
- :four: Faire un `commit` et pusher les fichiers
- :five: Dans l'environnement bac √† sable, faire un `pull` pour r√©cup√©rer ces
modifications
- :six: Tester √† nouveau le `build` (l√† encore le _cache_ est bien pratique !)

<!---
cf. 
https://github.com/linogaliana/ensae-reproductibilite-projet-1/commit/56946b4c5cb860d50b908d98a87fb549624314a6
----->


:tada: A ce stade, la matrice de confusion doit fonctionner. Vous avez cr√©√©
votre premi√®re application reproductible !

# Partie 3 : mise en production

Une image `Docker` est un livrable qui n'est pas forc√©ment int√©ressant
pour tous les publics. Certains pr√©f√©reront avoir un plat bien pr√©par√©
qu'une recette. Nous allons donc proposer d'aller plus loin en proposant
plusieurs types de livrables. Cela va nous amener √† d√©couvrir les outils
du CI/CD (_Continuous Integration / Continuous Delivery_)
qui sont au coeur de l'approche `DevOps`. Notre approche appliqu√©e
au _machine learning_ va nous entra√Æner plut√¥t du c√¥t√© du `MLOps` qui devient
une approche de plus en plus fr√©quente dans l'industrie de la 
_data science_.

Nous allons am√©liorer notre approche de trois mani√®res:

- Automatisation de la cr√©ation de l'image `Docker` et tests
automatis√©s de la qualit√© du code ;
- Production d'un site _web_ automatis√© permettant de documenter et
valoriser le mod√®le de _Machine Learning_ ;
- Mise √† disposition du mod√®le entra√Æn√© par le biais d'une API pour
ne pas le r√©-entra√Æner √† chaque fois et faciliter sa r√©utilisation ;

A chaque fois, nous allons d'abord tester en local notre travail puis
essayer d'automatiser cela avec les outils de `Github`.

On va ici utiliser l'int√©gration continue pour deux objectifs distincts:

- la mise √† disposition de l'image `Docker` ;
- la mise en place de tests automatis√©s de la qualit√© du code
sur le mod√®le de notre `linter` pr√©c√©dent 

Nous allons utiliser `Github Actions` pour cela. 

## Etape pr√©liminaire

Pour ne pas risquer de tout casser sur notre branche `master`, nous allons 
nous placer sur une branche nomm√©e `dev`:

- si dans l'√©tape suivante vous appliquez la m√©thode la plus simple, vous
allez pouvoir la cr√©er depuis l'interface de `Github` ;
- si vous utilisez l'autre m√©thode, vous allez devoir la cr√©er en local (
via la commande `git checkout -b dev`)

## Etape 1: mise en place de tests automatis√©s

Avant d'essayer de mettre en oeuvre la cr√©ation de notre image
`Docker` de mani√®re automatis√©e, nous allons pr√©senter la logique
de l'int√©gration continue en g√©n√©ralisant les √©valuations de
qualit√© du code avec le `linter`

{{< panelset class="nommage" >}}
{{% panel name="Utilisation d'un _template_ `Github` :cat:" %}}

__Methode la plus simple: utilisation d'un _template_ Github__

Si vous cliquez sur l'onglet `Actions` de votre d√©p√¥t, `Github` vous propose des _workflows_ standardis√©s reli√©s √† `Python`. Choisir l'option `Python Package using Anaconda`.

warning: Nous n'allons modifier que deux √©l√©ments de ce fichier.

:one: La derni√®re √©tape (`Test with pytest`) ne nous est pas n√©cessaire car nous n'avons pas de tests unitaires Nous allons donc remplacer celle-ci par l'utilisation de `pylint` pour avoir une note de qualit√© du package.

+ Utiliser `pylint` √† cette √©tape pour noter les scripts ;
+ Vous pouvez fixer un score minimal √† 5 (option `--fail-under=5`)

:two: Mettre entre guillements la version de `Python` pour que celle-ci soit reconnue.

:three: Enfin, finaliser la cr√©ation de ce script:

- En cliquant sur le bouton `Start Commit`, choisir la m√©thode
`Create a new branch for this commit and start a pull request`
en nommant la branche `dev`
- Cr√©er la `Pull Request` en lui donnant un nom signifiant

{{% /panel %}}

{{% panel name="M√©thode manuelle" %}}

:warning: On est plut√¥t sur une m√©thode de gal√©rien. Il vaut
mieux privil√©gier l'autre approche

On va √©diter
depuis `VisualStudio` nos fichiers.

- Cr√©er une branche `dev` en ligne de commande
- Cr√©er un dossier `.github/workflows` via la ligne de commande ou l'explorateur de fichier 
<!---mkdir .github/workflows -p ---->
- Cr√©er un fichier `.github/workflows/quality.yml`. 


Nous allons construire, par √©tape, une version simplifi√©e du `Dockerfile` pr√©sent
dans [ce post](https://medium.com/swlh/enhancing-code-quality-with-github-actions-67561c6f7063) et
dans [celui-ci](https://autobencoder.com/2020-08-24-conda-actions/)

:one: D'abord, d√©finissons des param√®tres pour indiquer √† `Github`
quand faire tourner notre script:

- Commencez par nommer votre _workflow_ par exemple `Python Linting` avec la cl√© `name`
- Nous allons faire tourner ce _workflow_ dans la branche `master` et dans la branche actuelle (`dev`). Ici, nous laissons de c√¥t√© les autres √©l√©ments (par exemple le fait de faire tourner √† chaque _pull request_). La cl√© `on` est d√©di√©e √† cet usage

:two: Ensuite, d√©fnissons le contexte d'ex√©cution des t√¢ches (`jobs`)
de notre script dans
les options de la partie `build`:

- Utilisons une machine `ubuntu-latest`. Nous verrons plus tard
comment am√©liorer cela. 
    
:three: Nous allons ensuite m√©langer des √©tapes pr√©-d√©finies (des actions du _marketplace_) et des instructions que nous faisons :

- Le _runner_ `Github` doit r√©cup√©rer le contenu de notre d√©p√¥t, pour cela utiliser l'action `checkout`. Par rapport √† l'exemple, il convient d'ajouter, pour le moment, un param√®tre `ref` avec le nom de la branche (par exemple `dev`)
- ~~On installe ensuite `Python` avec l'action `setup-python`~~ Pas besoin d'installer `Python`, on va utiliser l'option `conda-incubator/setup-miniconda@v2`
- Pour installer `Python` et l'environnement `conda`, on va plut√¥t utiliser l'astuce de [ce blog](https://autobencoder.com/2020-08-24-conda-actions/) avec l'option `conda-incubator/setup-miniconda@v2`
- On utilise ensuite `flake8` et `pylint` (option `--fail-under=5`)
pour effectuer des diagnostics de qualit√©

Il ne reste plus qu'√† faire un `commit` et esp√©rer que cela fonctionne.
Cela devrait donner le fichier suivant : 


```yaml
name: Python Linting
on:
  push:
    branches: [master, dev]
jobs:
  build:
    runs-on: ubuntu-latest    
    steps:
      - uses: actions/checkout@v3
        with:
          ref: "dev"
      - uses: conda-incubator/setup-miniconda@v2
        with:
          activate-environment: monenv
          environment-file: environment.yml
          python-version: '3.10'
          auto-activate-base: false
      - shell: bash -l {0}
        run: |
          conda info
          conda list
      - name: Lint with flake8
        run: |
          pip install flake8
          flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 src --count --max-complexity=10 --max-line-length=79 --statistics
      - name: Lint with Pylint
        run: |
          pip install pylint
          pylint src
``` 


{{% /panel %}}


{{< /panelset >}}

 
Maintenant, nous pouvons observer que l'onglet `Actions`
s'est enrichi. Chaque `commit` va entra√Æner une action pour
tester nos scripts.

Si la note est mauvaise, nous aurons
une croix rouge (et nous recevrons un mail). On pourra ainsi d√©tecter,
en d√©veloppant son projet, les moments o√π on d√©grade la qualit√© du script 
afin de la r√©tablir imm√©diatemment. 


{{% box status="hint" title="Un `linter` sous forme de _hook_ pre-commit" icon="fa fa-lightbulb" %}}

`Git` offre une fonctionalit√© int√©ressante lorsqu'on est puriste: les 
_hooks_. Il s'agit de r√®gles qui doivent √™tre satisfaites pour que le 
fichier puisse √™tre committ√©. Cela assurera que chaque `commit` remplisse
des crit√®res de qualit√© afin d'√©viter le probl√®me de la procrastination.

La [documentation de pylint](https://pylint.pycqa.org/en/latest/user_guide/pre-commit-integration.html)
offre des explications suppl√©mentaires. 

{{% /box %}}


## Etape 2: Automatisation de la livraison de l'image `Docker`

Maintenant, nous allons automatiser la mise √† disposition de notre image
sur `DockerHub`. Cela facilitera sa r√©utilisation mais aussi des
valorisations ult√©rieures.

L√† encore, nous allons utiliser une s√©rie d'actions pr√©-configur√©es.

:one: Pour que `Github` puisse s'authentifier aupr√®s de `DockerHub`, il va 
falloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser
un jeton (_token_) `DockerHub` que nous allons mettre dans un espace
s√©curis√© associ√© √† votre d√©p√¥t `Github`. Cette d√©marche sera l√† m√™me
ult√©rieurement lorsque nous connecterons notre d√©p√¥t √† un autre
service tiers, √† savoir `Netlify`:

- Se rendre sur
https://hub.docker.com/ et cr√©er un compte.
- Aller dans les param√®tres (https://hub.docker.com/settings/general)
et cliquer, √† gauche, sur `Security`
- Cr√©er un jeton personnel d'acc√®s, ne fermez pas l'onglet en question,
vous ne pouvez voir sa valeur qu'une fois. 
- Dans votre d√©p√¥t `Github`, cliquer sur l'onglet `Settings` et cliquer,
√† gauche, sur `Actions`. Sur la page qui s'affiche, cliquer sur `New repository secret`
- Donner le nom `DOCKERHUB_TOKEN` √† ce jeton et copier la valeur. Valider
- Cr√©er un deuxi√®me secret nomm√© `DOCKERHUB_USERNAME` ayant comme valeur le nom d'utilisateur
que vous avez cr√©√© sur `Dockerhub`

:two: A ce stade, nous avons donn√© les moyens √† `Github` de s'authentifier avec
notre identit√© sur `Dockerhub`. Il nous reste √† mettre en oeuvre l'action
en s'inspirant de https://github.com/docker/build-push-action/#usage.
On ne va modifier que trois √©l√©ments dans ce fichier. Effectuer les 
actions suivantes:

- Cr√©er depuis `VSCode` un fichier
`.github/workflows/docker.yml` et coller le
contenu du _template_ dedans ; 
- Changer le nom en un titre plus signifiant (par exemple _"Production de l'image Docker"_)
- Ajouter `master` et `dev` √† la liste des branches sur lesquelles tourne
le pipeline ;
- Changer le tag √† la fin pour mettre `<username>/ensae-repro-docker:latest`
o√π `username` est le nom d'utilisateur sur `DockerHub`;
- Faire un `commit` et un `push` de ces fichiers

:four: Comme on est fier de notre travail, on va afficher √ßa avec un badge sur le 
`README`. Pour cela, on se rend dans l'onglet `Actions` et on clique sur
un des scripts en train de tourner. 

- En haut √† droite, on clique sur `...`
- S√©lectionner `Create status badge`
- R√©cup√©rer le code `Markdown` propos√©
- Copier dans le `README` depuis `VSCode`
- Faire de m√™me pour l'autre _workflow_

:five: Maintenant, il nous reste √† tester notre application dans l'espace bac √† sable:

- Se rendre sur l'environnement bac √† sable
- Cr√©er un fichier `Dockerfile` ne contenant que l'import et le d√©ploiement
de l'appli:

```yaml
FROM <username>/ensae-repro-docker:latest

EXPOSE 5000
CMD ["python", "main.py"]
```

- Comme pr√©c√©demment, faire un _build_
- Tester l'image avec `run`

:tada: La matrice de confusion doit s'afficher ! Vous avez grandement
facilit√© la r√©utilisation de votre image. 

## Etape 3: cr√©ation d'un rapport automatique

Maintenant, nous allons cr√©er et d√©ployer un site web pour valoriser notre
travail. Cela va impliquer trois √©tapes:

- Tester en local le logiciel `quarto` et cr√©er un rapport minimal qui sera compil√© par `quarto` ;
- Enrichir l'image docker avec le logiciel `quarto` ;
- Compiler le document en utilisant cette image sur les serveurs de `Github` ;
- D√©ployer ce rapport minimal pour le rendre disponible √† tous sur le _web_.

Le but est de proposer un rapport minimal qui illustre la performance
du mod√®le est la _feature importance_. Pour ce dernier √©l√©ment, le
rapport qui sera propos√© utilise `shap` qui est une librairie d√©di√©e
√† l'interpr√©tabilit√© des mod√®les de _machine learning_

### 1. Rapport minimal en local

:one: La premi√®re √©tape consiste √† installer
`quarto` sur notre machine `Linux` sur laquelle
tourne `VSCode`:

- Dans un terminal, installer `quarto` avec les commandes suivantes:

```shell
QUARTO_VERSION="0.9.287"
wget "https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb"
sudo apt install "./quarto-${QUARTO_VERSION}-linux-amd64.deb"
```

- S'assurer qu'on travaille bien depuis l'environnement `conda` `monenv`. Sinon
l'activer

:two: Il va √™tre n√©cessaire d'enrichir l'environnement `conda`.
Certaines d√©pendances sont n√©cessaires pour que `quarto` fonctionne bien avec
`Python` (`jupyter`, `nbclient`...)
alors que d'autres ne sont n√©cessaires que parce qu'ils sont utilis√©s dans
le document (`seaborn`, `shap`...). Changer la section `dependencies` avec
la liste suivante:

```yaml
dependencies:
  - python=3.10.0
  - ipykernel==6.13.0
  - jupyter==1.0.0
  - matplotlib==3.5.1
  - nbconvert==6.5.0
  - nbclient==0.6.0
  - nbformat==5.3.0
  - pandas==1.4.1
  - PyYAML==6.0
  - s3fs==2022.2.0
  - scikit-learn==1.0.2
  - seaborn==0.11.2
  - shap==0.40.0
```


:three: Cr√©er un fichier nomm√© `report.qmd`

~~~markdown
  ---
  title: "Comprendre les facteurs de survie sur le Titanic"
  subtitle: "Un rapport innovant"
  format:
    html:
      self-contained: true
    ipynb: default
  jupyter: python3
  ---

Voici un rapport pr√©sentant quelques intuitions issues d'un mod√®le 
_random forest_ sur le jeu de donn√©es `Titanic` entra√Æn√© et 
d√©ploy√© de mani√®re automatique. 

Il est possible de t√©l√©charger cette page sous format `Jupyter Notebook` <a href="report.ipynb" download>ici</a>


```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
import main
X_train = main.X_train
y_train = main.y_train
training_data = main.training_data
rdmf = RandomForestClassifier(n_estimators=20)
rdmf.fit(X_train, y_train)
```

# Feature importance

La @fig-feature-importance repr√©sente l'importance des variables :

```python
feature_imp = pd.Series(rdmf.feature_importances_, index=training_data.iloc[:,1:].columns).sort_values(ascending=False)
```

```python
#| label: fig-feature-importance
#| fig-cap: "Feature importance"
plt.figure(figsize=(10,6))
sns.barplot(x=feature_imp, y=feature_imp.index)
# Add labels to your graph
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features")
plt.tight_layout()
plt.show()
```

Celle-ci peut √©galement √™tre obtenue gr√¢ce √† la librairie
`shap`:

```python
#| echo : true
import shap
shap_values = shap.TreeExplainer(rdmf).shap_values(X_train)
shap.summary_plot(shap_values, X_train, plot_type="bar", feature_names = training_data.iloc[:,1:].columns)
```

On peut √©galement utiliser cette librairie pour
interpr√©ter la pr√©diction de notre mod√®le:


```python
# explain all the predictions in the test set
explainer = shap.TreeExplainer(rdmf)
# Calculate Shap values
choosen_instance = main.X_test[15]
shap_values = explainer.shap_values(choosen_instance)
shap.initjs()
shap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance, feature_names = training_data.iloc[:,1:].columns)
```

# Qualit√© pr√©dictive du mod√®le

La matrice de confusion est pr√©sent√©e sur la
@fig-confusion

```python
#| label: fig-confusion
#| fig-cap: "Matrice de confusion"
from sklearn.metrics import confusion_matrix
conf_matrix = confusion_matrix(main.y_test, rdmf.predict(main.X_test))
plt.figure(figsize=(8,5))
sns.heatmap(conf_matrix, annot=True)
plt.title('Confusion Matrix')
plt.tight_layout()
```

Ou, sous forme de tableau:


```python
pd.DataFrame(conf_matrix, columns=['Predicted','Observed'], index = ['Predicted','Observed']).to_html()
```
~~~

:four: On va tenter de compiler ce document

- Le compiler en local avec la commande `quarto render report.qmd`

- Vous devriez rencontrer l'erreur suivante:

```python
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Input In [1], in <cell line: 6>()
      4 from sklearn.ensemble import RandomForestClassifier
      5 import main
----> 6 X_train = main.X_train
      7 y_train = main.y_train
      8 training_data = main.training_data

AttributeError: module 'main' has no attribute 'X_train'
AttributeError: module 'main' has no attribute 'X_train'
```

- Refactoriser `main.py` pour que toutes les op√©rations, √† l'exception
du print de la matrice de confusion ne soient plus dans la section `__main__`
afin qu'ils soient syst√©matiquement ex√©cut√©s. 

- Tenter √† nouveau `quarto render report.qmd`

- Deux fichiers ont √©t√© g√©n√©r√©s: 
    + un `Notebook` que vous pouvez ouvrir et dont vous pouvez ex√©cuter
des cellules
    + un fichier `HTML` que vous pouvez t√©l√©charger et ouvrir

:five: On a d√©j√† un r√©sultat assez esth√©tique en ce qui concerne la page `HTML`.
Cependant, on peut se dire que certains param√®tres par d√©faut, comme l'affichage
des blocs de code, ne conviennent pas au public cibl√©. De m√™me, certains
param√®tres de style, comme l'affichage des tableaux peuvent ne pas convenir
√† notre charte graphique. On va rem√©dier √† cela en deux √©tapes:

- enrichir le _header_ d'options globales contr√¥lant le comportement de `quarto`
- cr√©er un fichier `CSS` pour avoir de beaux tableaux

:six: Changer la section `format` du _header_ avec les options suivantes:

```yaml
format:
  html:
    echo: false
    code-fold: true
    self-contained: true
    code-summary: "Show the code"
    warning: false
    message: false
    theme:
      - cosmo
      - css/custom.scss
  ipynb: default
```

:seven: Cr√©er le fichier `css/custom.scss` avec le contenu suivant:

```css
/*-- scss:rules --*/

table {
    border-collapse: collapse;
    margin: 25px 0;
    font-size: 0.9em;
    font-family: sans-serif;
    min-width: 400px;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);  
}

thead tr {
    background-color: #516db0;
    color: #ffffff;
    text-align: center;
}

th, td {
    padding: 12px 15px;
}

tbody tr {
    border-bottom: 1px solid #dddddd;
}

tbody tr:nth-of-type(even) {
    background-color: #f3f3f3;
}

tbody tr:last-of-type {
    border-bottom: 2px solid #516db0;
}

tbody tr.active-row {
    font-weight: bold;
    color: #009879;
}
```

:eight: Compiler √† nouveau et observer le changement d'esth√©tique du `HTML`

:nine: Commit des nouveaux fichier `report.qmd`, `custom.scss` et des fichiers
d√©j√† existants.

{{% box status="hint" title="Un `linter` sous forme de _hook_ pre-commit" icon="fa fa-lightbulb" %}}

On ne `commit` pas les _output_, ici le notebook et le fichier html.
Les mettre sur le d√©p√¥t `Github` n'est pas la bonne mani√®re de les mettre
√† disposition. On va le voir, on va utiliser l'approche CI/CD pour cela.

Id√©alement, on ajoute au `.gitignore` les fichiers concern√©s, ici `report.ipynb`
et `report.html`

{{% /box %}}

### 3. Enrichir l'image `Docker`

On va vouloir mettre √† jour notre image pour automatiser, √† terme, la production
de nos livrables (le notebook et la page web). 

Pour cela, il est n√©cessaire que notre image int√®gre le logiciel `quarto`.

:one: A partir du script pr√©c√©dent d'installation de `quarto`, enrichir l'image
`Docker`[^1]

<!----
ENV QUARTO_VERSION="0.9.287"
RUN wget "https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb"
RUN apt install "./quarto-${QUARTO_VERSION}-linux-amd64.deb"
----->

[^1]: Le `sudo` n'est pas n√©cessaire puisque vous √™tes d√©j√† en `root`


### 4. Automatisation avec `Github Actions`

:one: Cr√©er un nouveau fichier `.github/workflows.report.yml`


Si les d√©pendances et l'image ont bien √©t√© enrichis, cette √©tape est quasi directe
avec 

{{< panelset class="simplification" >}}

{{% panel name="Version autonome :car: " %}}

- Donner comme nom `Deploy as website`
- Effectuer cette action √† chaque `push` sur les branches `main`, `master` et `dev`
- Le job doit tourner sur une machine `ubuntu`
- Cependant, il convient d'utiliser comme `container` votre image Docker 
- Les `steps`:
    + R√©cup√©rer le contenu du dossier avec `checkout`
    + Faire un `quarto render`
    + R√©cup√©rer le notebook sous forme d'artefact

{{% /panel %}}

{{% panel name="Version guid√©e :map: " %}}

```yaml
name: Deploy as website

on:
  push:
    branches:
      - main
      - master
      - dev

jobs:
  build:
    runs-on: ubuntu-latest
    container: linogaliana/ensae-repro-docker:latest
    steps:
      - uses: actions/checkout@v3
      - name: Render site
        run: quarto render report.qmd
      - uses: actions/upload-artifact@v1
        with:
          name: Report
          path: report.ipynb
```

{{% /panel %}}

{{< /panelset >}}

Si vous √™tes fier de vous, vous pouvez ajouter le badge de ce workflow
sur le `README` :sunglasses:

Cette √©tape nous a permis d'automatiser la construction de nos livrables.
Mais la mise √† disposition de ce livrable est encore assez manuelle: il 
faut aller chercher √† la main la derni√®re version du notebook pour
la partager. 

On va am√©liorer cela en d√©ployant automatiquement un site _web_ pr√©sentant
en page d'accueil notre rapport et permettant le t√©l√©chargement du notebook. 


## Etape 4: D√©ploiement de ce rapport automatique sur le web

:one: Dans un premier temps, nous allons connecter notre d√©p√¥t `Github` au
service tiers `Netlify`

- Aller sur https://www.netlify.com/ et faire `Sign up` (utiliser son compte `Github`)
- Dans la page d'accueil de votre profil, vous pouvez cliquer sur `Add new site > Import an existing project`
- Cliquer sur `Github`. S'il y a des autorisations √† donner, les accorder. Rechercher votre projet dans la liste de vos projets `Github`
- Cliquer sur le nom du projet et laisser les param√®tres par d√©faut (nous allons modifier par la suite)
- Cliquer sur `Deploy site`

:two: A ce stade, votre d√©ploiement devrait √©chouer.
C'est normal, vous essayez de d√©ployer depuis `master` qui ne comporte pas de html.
Mais le rapport n'est pas non plus pr√©sent dans la branche `dev`.
En fait, aucune branche ne comporte le rapport:
celui-ci est g√©n√©r√© dans votre _pipeline_ mais n'est jamais pr√©sent dans le
d√©p√¥t car il s'agit d'un _output_. On va d√©sactiver le d√©ploiement automatique 
pour privil√©gier un d√©ploiement depuis `Github Actions`:

- Aller dans `Site Settings` puis, √† gauche, cliquer sur `Build and Deploy`
- Dans la section `Build settings`, cliquer sur `Stop builds` et valider

On vient de d√©sactiver le d√©ploiement automatique par d√©faut. On va faire
communiquer notre d√©p√¥t `Github` et `Netlify` par le biais de l'int√©gration
continue.

:three: Pour cela, il faut cr√©er un jeton `Netlify` pour que les serveurs
de `Github`, lorsqu'ils disposent d'un rapport, puissent l'envoyer √† `Netlify`
pour la mise sur le _web_. Il va √™tre n√©cessaire de cr√©er deux variables
d'environnement pour connecter `Github` et `Netlify`: l'identifiant du site
et le _token_

- Pour le token : 
    + Cr√©er un jeton en cliquant, en haut √† droite, sur l'icone de votre profil. Aller
dans `User settings`. A gauche, cliquer sur `Applications` et cr√©er un jeton personnel d'acc√®s
avec un nom signifiant (par exemple `PAT_ENSAE_reproductibilite`)
    + Mettre de c√¥t√© (conseil : garder l'onglet ouvert)
- Pour l'identifiant du site:
    + cliquer sur `Site Settings` dans les onglets en haut
    + Garder l'onglet ouvert pour copier la valeur quand n√©cessaire
    

- Il est maintenant n√©cessaire d'aller dans le d√©p√¥t `Github` et de cr√©er 
les secrets (`Settings > Secrets > Actions`):
    + Cr√©er le secret `NETLIFY_AUTH_TOKEN` en collant la valeur du jeton d'authentification `Netlify`
    + Cr√©er le secret `NETLIFY_SITE_ID` en collant l'identifiant du site


:four: Nous avons effectu√© toutes les configurations n√©cessaires. On va
maintenant mettre √† jour l'int√©gration continue afin de mettre √† disposition
sur le _web_ notre rapport. On va utiliser l'interface en ligne de commande
(CLI) de `Netlify`. Celle-ci attend que le site _web_ se trouve dans un
dossier `public` et que la page d'accueil soit nomm√©e `index.html`:

{{< panelset class="simplification" >}}

{{% panel name="Vision d'ensemble " %}}

- une installation de `npm`
- une √©tape de d√©ploiement via la CLI de netlify

```yaml
- name: Install npm
  uses: actions/setup-node@v2
  with:
    node-version: '14'
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
```

{{% /panel %}}

{{% panel name="D√©tails npm " %}}

{{< highlight yaml "hl_lines=1-4" >}}
- name: Install npm
  uses: actions/setup-node@v2
  with:
    node-version: '14'
{{< / highlight >}}

`npm` est le gestionnaire de paquet de JS. Il est n√©cessaire de le configurer,
ce qui est fait automatiquement gr√¢ce √† l'action `actions/setup-node@v2`

{{% /panel %}}

{{% panel name="D√©tails `Netlify CLI`" %}}

- On rappelle √† `Github Actions` nos param√®tres d'authentification
sous forme de variables d'environnement. Cela permet de les garder
secr√®tes

{{< highlight yaml "hl_lines=3-5" >}}
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
{{< / highlight >}}

- On d√©place les rapports de la racine vers le dossier `public`

{{< highlight yaml "hl_lines=7-9" >}}
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
{{< / highlight >}}

- On installe et initialise `Netlify`

{{< highlight yaml "hl_lines=10-11" >}}
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
{{< / highlight >}}

- On d√©ploie sur l'url par d√©faut (`-- prod`) depuis le dossier `public`

{{< highlight yaml "hl_lines=10-12" >}}
- name: Deploy to Netlify
  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets
  env:
    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}
    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}
  run: |
    mkdir -p public
    mv report.html public/index.html
    mv report.ipynb public/report.ipynb
    npm install --unsafe-perm=true netlify-cli -g
    netlify init
    netlify deploy --prod --dir="public" --message "Deploy master"
{{< / highlight >}}
{{% /panel %}}

{{< /panelset >}}


Au bout de quelques minutes, le rapport est disponible en ligne sur
l'URL `Netlify` (par exemple https://spiffy-florentine-c913b9.netlify.app)


