---
title: "Appliquer pas √† pas les concepts √©tudi√©s √† un projet de data science"
author: "Romain Avouac et Lino Galiana"
draft: false
# layout options: single, single-sidebar
layout: single
from: markdown+emoji
---



L'objectif de cette mise en application est d'**illustrer les diff√©rentes √©tapes qui s√©parent la phase de d√©veloppement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les diff√©rents concepts pr√©sent√©s tout au long du cours.

Celle-ci est un tutoriel pas √† pas pour avoir un projet reproductible et disponible sous plusieurs livrables. 
Toutes les √©tapes ne sont pas indispensables √† tous les projets de _data science_. 

Nous nous pla√ßons dans une situation initiale correspondant √† la fin de la phase de d√©veloppement d'un projet de data science.
On a un notebook un peu monolithique, qui r√©alise les √©tapes classiques d'un *pipeline* de *machine learning* :

- Import de donn√©es ;
- Statistiques descriptives et visualisations ;
- *Feature engineering* ;
- Entra√Ænement d'un mod√®le ;
- Evaluation du mod√®le

**L'objectif est d'am√©liorer le projet de mani√®re incr√©mentale jusqu'√† pouvoir le mettre en production, en le valorisant sous une forme adapt√©e.** 

::: {.callout-important}
Il est important de bien lire les consignes et d'y aller progressivement.
Certaines √©tapes peuvent √™tre rapides, d'autres plus fastidieuses ;
certaines √™tre assez guid√©es, d'autres vous laisser plus de libert√©.
Si vous n'effectuez pas une √©tape, vous risquez de ne pas pouvoir passer √†
l'√©tape suivante qui en d√©pend.

Bien que l'exercice soit applicable sur toute configuration bien faite, nous 
recommandons de privil√©gier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), o√π tous les 
outils n√©cessaires sont pr√©-install√©s et pr√©-configur√©s. 
:::


<details>
<summary>
Illustration de notre point de d√©part
</summary>
![](/workflow1.png)
</details>

<details>
<summary>
Illustration de l'horizon vers lequel on se dirige
</summary>
![](/workflow2.png)
</details>


# Partie 0 : initialisation du projet

## Etape 1 : forker le d√©p√¥t d'exemple et cr√©er une branche de travail

- Ouvrir un service `VSCode` sur le [SSP Cloud](https://datalab.sspcloud.fr/home). Vous pouvez aller
dans la page `My Services` et cliquer sur `New service`. Sinon, vous
pouvez lancer le service en cliquant directement [ici](https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=false).

- G√©n√©rer un jeton d'acc√®s (*token*) sur `GitHub` afin de permettre l'authentification en ligne de commande √† votre compte.
La proc√©dure est d√©crite [ici](https://docs.sspcloud.fr/onyxia-guide/controle-de-version#creer-un-jeton-dacces-token). Garder le jeton g√©n√©r√© de c√¥t√©.

- Forker le d√©p√¥t `Github` : [https://github.com/ensae-reproductibilite/application-correction](https://github.com/ensae-reproductibilite/application-correction)

- Cl√¥ner __votre__ d√©p√¥t `Github` en utilisant le
terminal depuis `Visual Studio` (`Terminal > New Terminal`) :

```shell
$ git clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application-correction.git
```

o√π `<TOKEN>` et `<USERNAME>` sont √† remplacer, respectivement, 
par le jeton que vous avez g√©n√©r√© pr√©c√©demment et votre nom d'utilisateur.

- Se placer avec le terminal dans le dossier en question : 

```shell
$ cd ensae-reproductibilite-application-correction
```



# Partie 1 : qualit√© du script

Cette premi√®re partie vise √† **rendre le projet conforme aux bonnes pratiques** pr√©sent√©es dans le cours.

Elle fait intervenir les notions suivantes : 

- Utilisation du **terminal** (voir [Linux 101](/chapters/linux-101.html)) ;
- **Qualit√© du code** (voir [Qualit√© du code](/chapters/code-quality.html)) ;
- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;
- **Contr√¥le de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;
- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).

Le plan de la partie est le suivant :

<!----
0. :zero: _Forker_ le d√©p√¥t et cr√©er une branche de travail
1. :one: S'assurer que le _notebook_ s'ex√©cute correctement
2. :two: Modularisation : mise en fonctions et mise en module
3. :three: Utiliser un `main` script
4. :four:  Appliquer les standards de qualit√© de code
5. :five: Adopter une architecture standardis√©e de projet
6. :six: Fixer l'environnement d'ex√©cution
7. :seven: Stocker les donn√©es de mani√®re externe
8. :eight: Nettoyer le projet `Git`
9. :nine: Ouvrir une *pull request* sur le d√©p√¥t du projet.
------------->

Nous allons partir de ce _Notebook_ `Jupyter`,
que vous pouvez pr√©visualiser voire tester
en cliquant sur l'un des liens suivants:




<a href="https://datalab.sspcloud.fr/launcher/ide/jupyter-python?autoLaunch=false&init.personalInit=%C2%ABhttps%3A%2F%2Fraw.githubusercontent.com%2Flinogaliana%2Fensae-reproductibilite-website%2Fmaster%2Fpreview-notebook.sh%C2%BB" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSPcloud-Tester%20notebook%20sur%20SSP--cloud-informational&amp;color=yellow?logo=Python" alt="Onyxia"></a>
<a href="http://colab.research.google.com/github/linogaliana/ensae-reproductibilite-application/blob/main/titanic.ipynb" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>


## Etape 1 : s'assurer que le script s'ex√©cute correctement

On va partir du fichier `notebook.py` qui reprend le contenu 
du _notebook_[^jupytext] mais dans un script classique.

[^jupytext]: L'export dans un script `.py` a √©t√© fait
        avec [`Jupytext`](https://jupytext.readthedocs.io/en/latest/index.html). Comme
        cela n'est pas vraiment l'objet du cours, nous passons cette √©tape et fournissons
        directement le script. Mais n'oubliez
        pas que cette d√©marche, fr√©quente quand on a d√©marr√© sur un _notebook_ et
        qu'on d√©sire consolider en faisant la transition vers des 
        scripts, n√©cessite d'√™tre attentif pour ne pas risquer de faire une erreur. 

La premi√®re √©tape est simple, mais souvent oubli√©e : **v√©rifier que le code fonctionne correctement**. 


::: {.callout-tip}
## Application 1: corriger les erreurs

- Ouvrir dans `VSCode` le script `titanic.py` ;
- Ex√©cuter le script ligne √† ligne pour d√©tecter les erreurs ;
- Corriger les deux erreurs qui emp√™chent la bonne ex√©cution ;
- V√©rifier le fonctionnement du script en utilisant la ligne de commande

```shell
python titanic.py
```

:::


Il est maintenant temps de *commit* les changements effectu√©s avec `Git`[^2] :

[^2]: Essayez de *commit* vos changements √† chaque √©tape de l'exercice, c'est une bonne habitude √† prendre.

```shell
$ git add titanic.py
$ git commit -m "Corrige l'erreur qui emp√™chait l'ex√©cution"
$ git push
```

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli1
```

ou

[Script _checkpoint_](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application1/titanic.py)
:::


## Etape 2: utiliser un _linter_ puis un _formatter_

On va maintenant am√©liorer la qualit√© de notre code en appliquant les standards communautaires.
Pour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/). 

::: {.callout-note}
N'h√©sitez pas √† taper un code d'erreur sur un moteur de recherche pour obtenir plus d'informations si jamais le message n'est pas clair !
:::

Pour appliquer le _linter_ √† un script `.py`,
la syntaxe √† entrer dans le terminal est la suivante : 

```shell
$ pylint mon_script.py
```

::: {.callout-important}
[`PyLint`](https://pylint.readthedocs.io/en/latest/) et [`Black`](https://black.readthedocs.io/en/stable/)
sont des _packages_ `Python` qui 
s'utilisent principalement en ligne de commande.

Si vous avez une erreur qui sugg√®re
que votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/)
ou [`Black`](https://black.readthedocs.io/en/stable/),
n'oubliez pas d'ex√©cuter la commande `pip install pylint` ou `pip install black`.
:::


Le _linter_ renvoie alors une s√©rie d'irr√©gularit√©s,
en pr√©cisant √† chaque fois la ligne de l'erreur et le message d'erreur associ√© (ex : mauvaise identation).
Il renvoie finalement une note sur 10,
qui estime la qualit√© du code √† l'aune des standards communautaires √©voqu√©s
dans la partie [Qualit√© du code](/chapters/code-quality.html).


::: {.callout-tip}
## Application 2: rendre lisible le script

- Diagnostiquer et √©valuer la qualit√© de `titanic.py` avec [`PyLint`](https://pylint.readthedocs.io/en/latest/). Regarder la note obtenue.
- Utiliser `black titanic.py --diff --color` pour observer les changements de forme que va induire l'utilisation du _formatter_ [`Black`](https://black.readthedocs.io/en/stable/)
- Appliquer le _formatter_ [`Black`](https://black.readthedocs.io/en/stable/)
- R√©utiliser [`PyLint`](https://pylint.readthedocs.io/en/latest/) pour diagnostiquer l'am√©lioration de la qualit√© du script et le travail qui reste √† faire. 
- Comme la majorit√© du travail restant est √† consacrer aux imports:
    - Mettre tous les _imports_ ensemble en d√©but de script
    - Retirer les _imports_ redondants en s'aidant des diagnostics de votre √©diteur
    - R√©ordonner les _imports_ si [`PyLint`](https://pylint.readthedocs.io/en/latest/) vous indique de le faire
    - Corriger les derni√®res fautes formelles sugg√©r√©es par [`PyLint`](https://pylint.readthedocs.io/en/latest/)
- D√©limiter des parties dans votre code pour rendre sa structure plus lisible 
:::

Le code est maintenant lisible, il obtient √† ce stade une note formelle proche de 10.
Mais il n'est pas encore totalement intelligible ou fiable.
Il y a notamment 
beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. 
N√©anmoins, avant cela, occupons-nous de mieux g√©rer certains param√®tres du script: 
jetons d'API et chemin des fichiers.


::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli2
```

ou

[`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application2/titanic.py)
:::

## Etape 3: gestion des param√®tres

L'ex√©cution du code et les r√©sultats obtenus
d√©pendent de certains param√®tres. L'√©tude de r√©sultats
alternatifs, en jouant sur 
des variantes des param√®tres, est √† ce stade compliqu√©e
car il est n√©cessaire de parcourir le code pour trouver
ces param√®tres. De plus, certains param√®tres personnels
comme des jetons
d'API ou des mots de passe n'ont pas vocation √† 
√™tre pr√©sents dans le code. 

Il est plus judicieux de consid√©rer ces param√®tres comme des
variables d'entr√©e du script. Cela peut √™tre fait de deux
mani√®res:

1. Avec des arguments optionnels appel√©s depuis la ligne de commande.
Cela peut √™tre pratique pour mettre en oeuvre des tests automatis√©s[^noteCI] mais
n'est pas forc√©ment pertinent pour toutes les variables. Nous allons montrer
cet usage avec le nombre d'arbres de notre _random forest_ ;
2. En utilisant un fichier de configuration dont les valeurs sont import√©es dans
le script principal. Nous allons le mettre en oeuvre pour deux types de fichiers:
les √©l√©ments de configuration √† partager et ceux √† conserver pour soi mais 
pouvant servir.

[^noteCI]: Nous le verrons lorsque nous mettrons en oeuvre l'int√©gration continue.

::: {.callout-tip}
## Application 3: Param√©trisation du script

1. En s'inspirant de [cette r√©ponse](https://stackoverflow.com/a/69377311/9197726), 
cr√©er une variable `n_trees` qui peut √©ventuellement √™tre param√©tr√©e en ligne de commande
et dont la valeur par d√©faut est 20.
2. Tester cette param√©trisation en ligne de commande avec la valeur par d√©faut
puis 2, 10 et 50 arbres
3. Rep√©rer le jeton d‚ÄôAPI dans le code. Retirer le jeton d‚ÄôAPI du code
et cr√©er √† la racine du projet un fichier YAML nomm√© `secrets.yaml`
o√π vous √©crivez ce secret sous la forme `key: value`
4. Pour √©viter d'avoir √† le faire plus tard,
cr√©er une fonction `import_yaml_config` qui prend en argument le
chemin d'un fichier `YAML`
et renvoie le contenu de celui-ci en _output_. Vous pouvez suivre
le conseil du chapitre sur la [Qualit√© du code](/chapters/code-quality.html)
en adoptant le _type hinting_.
5. Cr√©er la variable `API_TOKEN` ayant la valeur stock√©e dans `secrets.yaml`[^fileexist].
5. Tester en ligne de commande que l'ex√©cution du fichier est toujours
sans erreur
6. Refaire un diagnostic avec [`PyLint`](https://pylint.readthedocs.io/en/latest/)
et corriger les √©ventuels messages. 
7. Cr√©er un fichier `config.yaml` stockant trois informations: le chemin des donn√©es
d'entra√Ænement, des donn√©es de test et la r√©partition train/test utilis√©e dans le code. 
Cr√©er les variables correspondantes dans le code apr√®s avoir utilis√© `import_yaml_config`
8. Cr√©er un fichier `.gitignore`. Ajouter dans ce fichier `secrets.yaml`
car il ne faut pas committer ce fichier.
8. Cr√©er un fichier `README.md` o√π vous indiquez qu'il faut cr√©er un fichier `secrets.yaml` pour
pouvoir utiliser l'API. 

[^fileexist]: Ici, le jeton d'API n'est pas indispensable pour que le code
    fonctionne. Afin d'√©viter une erreur non n√©cessaire
    lorsqu'on automatisera le processus, on peut
    cr√©er une condition qui v√©rifie la pr√©sence ou non de ce fichier. 
    
    Cela peut √™tre fait avec la fonction `os.path.exists` :

        if os.path.exists('secrets.yaml'):
            secrets = import_yaml_config("secrets.yaml")

    La variable `secrets` n'existera que dans le cas o√π un fichier `secrets.yaml` existe. 
    Le script reste donc reproductible m√™me pour un utilisateur n'ayant pas le fichier
    `secrets.yaml`. 

<details>
<summary>Indice si vous ne trouvez pas comment lire un fichier `YAML`</summary>
Si le fichier s'appelle `toto.yaml`, vous pouvez l'importer de cette mani√®re:
```python
with open("toto.yaml", "r", encoding="utf-8") as stream:
    dict_config = yaml.safe_load(stream)
```
</details>

:::


::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli3
```

ou

- [`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/titanic.py)
- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)


:::


## Etape 4 : Adopter la programmation fonctionnelle

Nous allons **mettre en fonctions les parties importantes de l'analyse, et les mettre dans un module afin de pouvoir les importer directement depuis le notebook**.

Cet exercice √©tant chronophage, il n'est __pas obligatoire de le r√©aliser en entier__. L'important est de
comprendre la d√©marche et d'adopter fr√©quemment une approche fonctionnelle[^POO]. Pour obtenir 
une chaine enti√®rement fonctionnalis√©e, vous pouvez reprendre le _checkpoint_.

[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser
un processus, il serait possible d'adopter le paradigme de la __programmation orient√©e objet (POO)__. Celle-ci est
plus rebutante et demande plus de temps au d√©veloppeur. L'arbitrage co√ªt-avantage est n√©gatif pour notre
exemple, nous proposons donc de nous en passer. N√©anmoins, pour une mise en production r√©elle d'un mod√®le,
il est recommand√© de l'adopter. C'est d'ailleurs obligatoire avec des [_pipelines_ `scikit`](https://pythonds.linogaliana.fr/pipeline-scikit/). 

::: {.callout-tip}
## Application 4: adoption des standards de programmation fonctionnelle 

- Cr√©er une fonction qui importe les donn√©es d'entra√Ænement (`train.csv`) et de test (`test.csv`) et renvoie des `DataFrames` `Pandas` ;
- En fonction du temps disponible, cr√©er plusieurs fonctions pour r√©aliser les √©tapes de *feature engineering*:
    + La cr√©ation de la variable _"Title"_ peut √™tre automatis√©e en vertu du principe _"do not repeat yourself"_[^notepandas].
    + Regrouper ensemble les `fillna` et essayer de cr√©er une fonction g√©n√©ralisant l'op√©ration. 
    + Les _label encoders_ peuvent √™tre transform√©s en deux fonctions: une premi√®re pour encoder une colonne puis une seconde qui utilise
    la premi√®re de mani√®re r√©p√©t√©e pour encoder plusieurs colonnes. _Remarquez les erreurs de copier-coller que cela corrige_
    + Finaliser les derni√®res transformations avec des fonctions
- Cr√©er une fonction qui r√©alise le *split train/test* de validation en fonction d'un param√®tre repr√©sentant la proportion de l'√©chantillon de test.
- Cr√©er une fonction qui entra√Æne et √©value un classifieur `RandomForest`, et qui prend en param√®tre le nombre d'arbres (`n_estimators`). La fonction doit imprimer √† la fin la performance obtenue et la matrice de confusion.
- D√©placer toutes les fonctions ensemble, en d√©but de script.
:::

[^notepandas]: Au passage vous pouvez noter que mauvaises pratiques discutables,
    peuvent
    √™tre corrig√©es, notamment l'utilisation excessive de `apply` l√† o√π
    il serait possible d'utiliser des m√©thodes embarqu√©es par `Pandas`.
    Cela est plut√¥t de l'ordre du bon style de programmation que de la
    qualit√© formelle du script. Ce n'est donc pas obligatoire mais c'est mieux. 


::: {.callout-important}
Le fait d'appliquer des fonctions a d√©j√† am√©lior√© la fiabilit√© du processus
en r√©duisant le nombre d'erreurs de copier-coller. N√©anmoins, pour vraiment
fiabiliser le processus, il faudrait utiliser un _pipeline_ de transformations
de donn√©es. 

Ceci n'est pas encore au programme du cours mais le sera dans une prochaine 
version. 
:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli4
```

ou

- [`titanic.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application4/titanic.py)

Les autres fichiers inchang√©s:

- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)

:::



# Partie 2 : adoption d'une structure modulaire {#partie2}

Dans la partie pr√©c√©dente,
on a appliqu√© de mani√®re incr√©mentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est d√©j√† consid√©rablement rapproch√©s d'un
possible partage du code : celui-ci est lisible et intelligible. 
Le code est proprement versionn√© sur un
d√©p√¥t `GitHub`.

<details>
<summary>
Illustration de l'√©tat actuel du projet 
</summary>
![](/schema_post_appli4.png)
</details>


N√©anmoins,
la structure du projet n'est pas encore normalis√©e. 
De plus, 
l'adoption d'une structure plus modulaire facilitera
la compr√©hension de la chaine de traitement.


## Etape 1 : modularisation

Fini le temps de l'exp√©rimentation : on va maintenant essayer de se passer compl√®tement du _notebook_.
Pour cela, on va utiliser un `main` script, c'est √† dire un script qui reproduit l'analyse en important et en ex√©cutant les diff√©rentes fonctions dans l'ordre attendu.


::: {.callout-tip}
## Application 5: modularisation

- D√©placer les fonctions dans une s√©rie de fichiers d√©di√©s:
    +  `import_data.py`: fonctions d'import de donn√©es 
    +  `build_features.py`: fonctions regroupant les √©tapes de _feature engineering_ 
    +  `train_evaluate.py`: fonctions d'entrainement et d'√©valuation du mod√®le
- Sp√©cifier les d√©pendances (i.e. les packages √† importer)
dans les modules pour que ceux-ci puissent s'ex√©cuter ind√©pendamment ;
- Renommer `titanic.py` en `main.py` pour suivre la convention de nommage des projets `Python` ;
- Importer les fonctions n√©cessaires √† partir des modules. ‚ö†Ô∏è Ne pas utiliser `from XXX import *`, ce n'est pas une bonne pratique ! 
- V√©rifier que tout fonctionne bien en ex√©cutant le _script_ `main` √† partir de la ligne de commande :

```shell
$ python main.py
```
:::

On dispose maintenant d'une application `Python` fonctionnelle. 
N√©anmoins, le projet est certes plus fiable mais sa structuration
laisse √† d√©sirer et il serait difficile de rentrer √† nouveau
dans le projet dans quelques temps. 

<details>
<summary>Etat actuel du projet üôà</summary>

```shell
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ train.csv
‚îú‚îÄ‚îÄ test.csv
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ secrets.yaml
‚îú‚îÄ‚îÄ import_data.py
‚îú‚îÄ‚îÄ build_features.py
‚îú‚îÄ‚îÄ train_evaluate.py
‚îî‚îÄ‚îÄmain.py
```

</details>

Comme cela est expliqu√© dans la
partie [Structure des projets](/chapters/projects-architecture.html),
on va adopter une structure certes arbitraire mais qui va 
faciliter l'autodocumentation de notre projet.  

De plus, une telle structure va faciliter des √©volutions optionnelles
comme la packagisation du projet. Passer d'une structure modulaire
bien faite √† un _package_ est quasi-imm√©diat en `Python`. 

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli5
```

ou

- [`build_features.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/build_features.py)
- [`import_data.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/import_data.py)
- [`train_evaluate.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/train_evaluate.py)
- [`main.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/main.py)

Les autres fichiers inchang√©s:

- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/readme.md)
- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/config.yaml)
- [`secrets.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/secrets.yaml)
- [`.gitignore`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application3/.gitignore)

:::

## Etape 2 : adopter une architecture standardis√©e de projet

On va maintenant modifier l'architecture de notre projet pour la rendre plus standardis√©e.
Pour cela, on va s'inspirer des structures
[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)
qui g√©n√®rent des _templates_ de projet.

On va s'inspirer de la structure du [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)
d√©velopp√© par la communaut√©.

::: {.callout-note}
L'id√©e de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de b√¢tir √† l'avance une structure √©volutive. La syntaxe √† utiliser dans ce cas est la suivante : 

```shell
$ pip install cookiecutter
$ cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

Ici, on a d√©j√† un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure propos√©e afin de r√©organiser celle de notre projet selon les standards communautaires.
:::

En s'inspirant du _cookiecutter data science_
on va adopter la structure suivante:

```shell
ensae-reproductibilite-application
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îî‚îÄ‚îÄ raw
‚îÇ       ‚îú‚îÄ‚îÄ test.csv
‚îÇ       ‚îî‚îÄ‚îÄ train.csv
‚îú‚îÄ‚îÄ configuration
‚îÇ   ‚îú‚îÄ‚îÄ secrets.yaml
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml
‚îú‚îÄ‚îÄ notebooks
‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb
‚îî‚îÄ‚îÄ src
    ‚îú‚îÄ‚îÄ data
    ‚îÇ   ‚îî‚îÄ‚îÄ import_data.py
    ‚îú‚îÄ‚îÄ features
    ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py
    ‚îî‚îÄ‚îÄ models
        ‚îî‚îÄ‚îÄ train_evaluate.py
```

::: {.callout-tip}

## Application 6: adopter une structure lisible

- _(optionnel)_ Analyser et comprendre la [structure de projet](https://drivendata.github.io/cookiecutter-data-science/#directory-structure) propos√©e par le template
- Modifier l'arborescence du projet selon le mod√®le
- Adapter les scripts et les fichiers de configuration √† la nouvelle arborescence
- Ajouter le dossier __pycache__ au `.gitignore`[^pycache] et le dossier `data`
:::

[^pycache]: Il est normal d'avoir des dossiers `__pycache__` qui tra√Ænent : ils se cr√©ent automatiquement √† l'ex√©cution d'un script en `Python`.

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli6
```

ou

- [`build_features.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/build_features.py)
- [`import_data.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/import_data.py)
- [`train_evaluate.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/train_evaluate.py)
- [`main.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application5/main.py)

Les autres fichiers sont inchang√©s, √† l'exception de leur emplacement.

:::

### Etape 3: indiquer l'environnement minimal de reproductibilit√©

Le script `main.py` n√©cessite un certain nombre de packages pour
√™tre fonctionnel. Chez vous les packages n√©cessaires sont
bien s√ªr install√©s mais √™tes-vous assur√© que c'est le cas 
chez la personne qui testera votre code ? 

Afin de favoriser la portabilit√© du projet,
il est d'usage de _"fixer l'environnement"_,
c'est-√†-dire d'indiquer dans un fichier toutes les d√©pendances utilis√©es ainsi que leurs version.
Nous proposons de cr√©er un fichier `requirements.txt` minimal, sur lequel nous reviendrons
dans la partie consacr√©e aux environnements reproductibles. 

Le fichier `requirements.txt` est conventionnellement localis√© √† la racine du projet.
Ici on ne va pas fixer les versions, on raffinera ce fichier plus tard.

::: {.callout-tip}

## Application 7: cr√©ation du `requirements.txt`

- Cr√©er un fichier `requirements.txt` avec la liste des packages n√©cessaires
- Ajouter une indication dans `README.md` sur l'installation des _packages_ gr√¢ce au fichier `requirements.txt` 
:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli7
```

ou

- [`requirements.txt`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application7/requirements.txt)
- [`README.md`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application7/README.md)

:::

## Etape 3 : stocker les donn√©es de mani√®re externe {#stockageS3}


::: {.callout-warning}
Pour mettre en oeuvre cette √©tape, il peut √™tre utile de
comprendre un peu comme fonctionne le SSP Cloud.
Vous devrez suivre la [documentation du SSP Cloud](https://docs.sspcloud.fr/onyxia-guide/stockage-de-donnees) pour la r√©aliser. Une aide-m√©moire est √©galement disponible dans le cours
de 2e ann√©e de l'ENSAE [Python pour la data science](https://linogaliana-teaching.netlify.app/reads3/#)
:::

Comme on l'a vu dans le cours ([partie structure des projets](/chapters/project-structure.html)),
les donn√©es ne sont pas cens√©es √™tre versionn√©es sur un projet `Git`.

L'id√©al pour √©viter cela tout en maintenant la reproductibilit√© est d'utiliser une solution de stockage externe.
On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud. 

::: {.callout-tip}

## Application 8: utilisation d'un syst√®me de stockage distant

A partir de la ligne de commande,
utiliser l'utilitaire [MinIO](https://min.io/docs/minio/linux/reference/minio-mc.html)
pour copier les donn√©es `data/raw/train.csv` et `data/raw/test.csv` vers votre
bucket personnel, respectivement dans les dossiers `ensae-reproductibilite/data/raw/train.csv`
et `ensae-reproductibilite/data/raw/test.csv`. 

<details>
<summary>Indice</summary>

Structure √† adopter:

```shell
$ mc cp data/raw/train.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv
$ mc cp data/raw/test.csv s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv
```

en modifiant l'emplacement de votre bucket personnel
</details>

- Pour se simplifier la vie, on va utiliser des URL de t√©l√©chargement des fichiers
(comme si ceux-ci √©taient sur n'importe quel espace de stockage) plut√¥t que d'utiliser
une librairie `S3` compatible comme `boto3` ou `s3fs`. Pour cela, en ligne de
commande, faire:

```shell
mc anonymous set download s3/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/
```

en modifiant `<BUCKET_PERSONNEL>`. Les URL de t√©l√©chargement seront de la forme 
`https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/test.csv`
et `https://minio.lab.sspcloud.fr/<BUCKET_PERSONNEL>/ensae-reproductibilite/data/raw/train.csv`

- Modifier `configuration.yaml` pour utiliser directement les URL dans l'import 
- Supprimer les fichiers `.csv` du dossier `data` de votre projet, on n'en a plus besoin vu qu'on les importe de l'ext√©rieur
- V√©rifier le bon fonctionnement de votre application
:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli8
```

ou

- [`config.yaml`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application8/config.yaml)

:::

# Partie 2bis: packagisation de son projet (optionnel)

Cette s√©rie d'actions n'est pas forc√©ment pertinente pour tous
les projets. Elle fait un peu la transition entre la modularit√©
et la portabilit√©. 

## Etape 1 : proposer des tests unitaires (optionnel)

Notre code comporte un certain nombre de fonctions g√©n√©riques.
On peut vouloir tester leur usage sur des donn√©es standardis√©es,
diff√©rentes de celles du Titanic.

M√™me si la notion de tests unitaires
prend plus de sens dans un _package_, nous pouvons proposer
dans le projet des exemples d'utilisation de la fonction, ceci peut √™tre p√©dagogique. 

Nous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)
pour effectuer des tests unitaires. Cette approche n√©cessite une ma√Ætrise 
de la programmation orient√©e objet.

::: {.callout-tip}

## Application 9: test unitaire _(optionnel)_

Dans le dossier `src/data/`, cr√©er un fichier `test_create_variable_title.py`[^emplacement].

En s'inspirant de l'[exemple de base](https://docs.python.org/3/library/unittest.html#basic-example),
cr√©er une classe `TestCreateVariableTitle` qui effectue les op√©rations suivantes:

- Cr√©ation d'une fonction `test_create_variable_title_default_variable_name` qui permet 
de comparer les objets suivants:

    + Cr√©ation d'un `DataFrame` de test :  

    ```python
    df = pd.DataFrame({
                'Name': ['Braund, Mr. Owen Harris', 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)',
                        'Heikkinen, Miss. Laina', 'Futrelle, Mrs. Jacques Heath (Lily May Peel)',
                        'Allen, Mr. William Henry', 'Moran, Mr. James',
                        'McCarthy, Mr. Timothy J', 'Palsson, Master. Gosta Leonard',
                        'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)',
                        'Nasser, Mrs. Nicholas (Adele Achem)'],
                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],
                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
            })
    ```

    + Utilisation de la fonction `create_variable_title` sur ce `DataFrame`
    + Comparaison au `DataFrame` attendu:

    ```python
    expected_result = pd.DataFrame({
                'Title': ['Mr.', 'Mrs.', 'Miss.', 'Mrs.', 'Mr.', 'Mr.', 'Mr.', 'Master.', 'Mrs.', 'Mrs.'],
                'Age': [22, 38, 26, 35, 35, 27, 54, 2, 27, 14],
                'Survived': [0, 1, 1, 1, 0, 0, 0, 0, 1, 1]
            })
    ```

- Effectuer le test unitaire en ligne de commande avec `unittest`. Corriger le test unitaire en cas d'erreur. 
- Si le temps le permet, proposer des variantes pour tenir compte de param√®tres (comme la variable `variable_name`)
ou d'exceptions (comme la gestion du cas _"Dona"_)
:::

::: {.callout-note}

Lorsqu'on effectue des tests unitaires, on cherche g√©n√©ralement
√† tester le plus de lignes possibles de son code. On parle de
taux de couverture (_coverage rate_) pour d√©signer
la statistique mesurant cela. 

Cela peut s'effectuer de la mani√®re suivante avec le package
[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):

```shell
$ coverage run -m pytest test_create_variable_title.py
$ coverage report -m

Name                            Stmts   Miss  Cover   Missing
-------------------------------------------------------------
import_data.py                     15      6    60%   16-19, 31-34
test_create_variable_title.py      21      1    95%   54
-------------------------------------------------------------
TOTAL                              36      7    81%
```

Le taux de couverture est souvent mis en avant par les gros
projets comme indicateur de leur qualit√©. Il existe d'ailleurs
des badges `Github` d√©di√©s. 
:::

[^emplacement]: L'emplacement de ce fichier est amen√© √† √©voluer dans le cadre
    d'une packagisation. Dans un package, ces tests seront dans un dossier
    sp√©cifique `/tests` car `Python` sait g√©rer de mani√®re plus formelle
    les imports de fonctions depuis des modules. Ici, on est dans une 
    situation transitoire, raison pour laquelle les tests
    sont dans les m√™mes dossiers que les fonctions. 

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli9
```

ou

- [`test_create_variable_title.py`](https://raw.githubusercontent.com/linogaliana/ensae-reproductibilite-application/main/checkpoints/application9/test_create_variable_title.py)

Les autres fichiers sont inchang√©s.

:::


## Etape 2 : transformer son projet en package (optionnel)

Notre projet est modulaire, ce qui le rend assez simple √† transformer
en package, en s'inspirant du `cookiecutter` adapt√©, issu
de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

<details>
<summary>Structure vis√©e</summary>

```shell
ensae-reproductibilite-application
‚îú‚îÄ‚îÄ docs                                    ‚îê 
‚îÇ   ‚îú‚îÄ‚îÄ main.py                             ‚îÇ 
‚îÇ   ‚îî‚îÄ‚îÄ notebooks                           ‚îÇ Package documentation and examples
‚îÇ       ‚îú‚îÄ‚îÄ titanic.ipynb                   ‚îÇ 
‚îú‚îÄ‚îÄ README.md                               ‚îò 
‚îú‚îÄ‚îÄ pyproject.toml                          ‚îê 
‚îú‚îÄ‚îÄ requirements.txt                        ‚îÇ
‚îú‚îÄ‚îÄ src                                     ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ titanicml                           ‚îÇ Package source code, metadata,
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py                     ‚îÇ and build instructions 
‚îÇ       ‚îú‚îÄ‚îÄ config.yaml                     ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ import_data.py                  ‚îÇ
‚îÇ       ‚îú‚îÄ‚îÄ build_features.py               ‚îÇ 
‚îÇ       ‚îî‚îÄ‚îÄ train_evaluate.py               ‚îò
‚îî‚îÄ‚îÄ tests                                   ‚îê
    ‚îî‚îÄ‚îÄ test_create_variable_title.py       ‚îò Package tests
```
</details>

<details>
<summary>Rappel: structure actuelle</summary>

```shell
ensae-reproductibilite-application
‚îú‚îÄ‚îÄ notebooks                                 
‚îÇ   ‚îî‚îÄ‚îÄ titanic.ipynb                  
‚îú‚îÄ‚îÄ configuration                                 
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml                  
‚îú‚îÄ‚îÄ main.py                              
‚îú‚îÄ‚îÄ README.md                 
‚îú‚îÄ‚îÄ requirements.txt                      
‚îî‚îÄ‚îÄ src 
    ‚îú‚îÄ‚îÄ data                                
    ‚îÇ   ‚îú‚îÄ‚îÄ import_data.py                    
    ‚îÇ   ‚îî‚îÄ‚îÄ test_create_variable_title.py      
    ‚îú‚îÄ‚îÄ features                           
    ‚îÇ   ‚îî‚îÄ‚îÄ build_features.py      
    ‚îî‚îÄ‚îÄ models                          
        ‚îî‚îÄ‚îÄ train_evaluate.py              
```
</details>

::: {.callout-tip}

## Application 10: packagisation _(optionnel)_

- D√©placer les fichiers dans le dossier `src` pour respecter la nouvelle
arborescence ;
- Dans `src/titanicml`, cr√©er un fichier vide `__init__.py`[^init] ;
- D√©placer le fichier de configuration dans le _package_ (n√©cessaire √† la reproductibilit√©) ;
- Cr√©er le dossier `docs` et mettre les fichiers indiqu√©s dedans
- Modifier `src/titanicml/import_data.py` :
    + Ajouter la variable `config_file = os.path.join(os.path.dirname(__file__), "config.yaml")`. Cela permettra d'utiliser directement le fichier ;
    + Proposer un argument par d√©faut √† la fonction `import_config_yaml` √©gal √† `config_file`
- Cr√©er un fichier `pyproject.toml` √† partir du contenu de [ce mod√®le de `pyproject`](https://github.com/linogaliana/ensae-reproductibilite-application/blob/main/checkpoints/application10/pyproject.toml)[^setuptools]
- Installer le package en local avec `pip install .`
- Modifier le contenu de `docs/main.py` pour importer les fonctions de notre _package_ `titanicml` et tester en 
ligne de commande notre fichier `main.py`
:::

[^init]: Le fichier `__init__.py` indique √† `Python` que le dossier
est un _package_. Il permet de proposer certaines configurations
lors de l'import du _package_. Il permet √©galement de contr√¥ler
les objets export√©s (c'est-√†-dire mis √† disposition de l'utilisateur)
par le _package_ par rapport aux objets internes au _package_. 
En le laissant vide, nous allons utiliser ce fichier 
pour importer l'ensemble des fonctions de nos sous-modules. 
Ce n'est pas la meilleure pratique mais un contr√¥le plus fin des
objets export√©s demanderait un investissement qui ne vaut, ici, pas
le co√ªt. 


[^setuptools]: Ce `pyproject.toml` est un mod√®le qui utilise `setuptools`
    pour _build_ le _package_. C'est l'outil classique. 
    N√©anmoins, pour des usages plus raffin√©s, 
    il peut √™tre utile d'utiliser [`poetry`](https://python-poetry.org/)
    qui propose des fonctionnalit√©s plus compl√®tes.  


::: {.callout-note}

Pour cr√©er la structure minimale d'un _package_, le plus simple est
d'utiliser le `cookiecutter` adapt√©,
issu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

Comme on a d√©j√† une structure tr√®s modulaire, on va plut√¥t recr√©er cette
structure dans notre projet d√©j√† existant. En fait, il ne manque qu'un fichier essentiel, 
le principal distinguant un projet classique d'un package : `pyproject.toml`.

```shell
cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git
```

<details>
<summary>D√©rouler pour voir les choix possibles</summary>
```shell
author_name [Monty Python]: Daffy Duck
package_name [mypkg]: titanicml
package_short_description []: Impressive Titanic survival analysis
package_version [0.1.0]: 
python_version [3.9]: 
Select open_source_license:
1 - MIT
2 - Apache License 2.0
3 - GNU General Public License v3.0
4 - Creative Commons Attribution 4.0
5 - BSD 3-Clause
6 - Proprietary
7 - None
Choose from 1, 2, 3, 4, 5, 6 [1]: 
Select include_github_actions:
1 - no
2 - ci
3 - ci+cd
Choose from 1, 2, 3 [1]:
```
</details>

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli10
```

:::



# Partie 3 : construction d'un projet portable et reproductible {#partie3}

Dans la partie pr√©c√©dente,
on a appliqu√© de mani√®re incr√©mentale de nombreuses bonnes pratiques vues
dans les chapitres [Qualit√© du code](/chapters/code-quality.html)
et [Structure des projets](/chapters/projects-architecture.html)
tout au long du cours.

Ce faisant, on s'est d√©j√† consid√©rablement rapproch√©s d'une
possible mise en production : le code est lisible,
la structure du projet est normalis√©e et √©volutive,
et le code est proprement versionn√© sur un
d√©p√¥t `GitHub`.


<details>
<summary>
Illustration de l'√©tat actuel du projet 
</summary>
![](/schema_post_appli8.png)
</details>



A pr√©sent, nous avons une version du projet qui est largement partageable.
Du moins en th√©orie, car la pratique est souvent plus compliqu√©e :
il y a fort √† parier que si vous essayez d'ex√©cuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),
les choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'√©tat, le projet n'est pas portable : il n'est pas possible, sans modifications co√ªteuses, de l'ex√©cuter dans un environnement diff√©rent de celui dans lequel il a √©t√© d√©velopp√©**.

Dans cette seconde partie, nous allons voir 
comment **normaliser l'environnement d'ex√©cution afin de produire un projet portable**.
Autrement dit, nous n'allons plus nous contenter de modularit√© mais allons rechercher
la portabilit√©.
On sera alors tout proche de pouvoir mettre le projet en production.
On progressera dans l'√©chelle de la reproductibilit√© 
de la mani√®re suivante: 

- :one: [**Environnements virtuels**](#anaconda) ;
- :two: Cr√©er un script shell qui permet, depuis un environnement minimal, de construire l'application de A √† Z ;
- :three: [**Images et conteneurs `Docker`**](#docker).


Nous allons repartir de l'application 8, c'est-√†-dire d'un projet
modulaire mais qui n'est pas, √† strictement parler, un package
(objet des applications optionnelles suivantes 9 et 10). 

Pour se replacer dans l'√©tat du projet √† ce niveau,
il est possible d'utiliser le _tag_ _ad hoc_.

```shell
git checkout appli8
```


## Etape 1 : un environnement pour rendre le projet portable {#anaconda}

Pour qu'un projet soit portable, il doit remplir deux conditions:

- Ne pas n√©cessiter de d√©pendance
qui ne soient pas renseign√©es quelque part
- Ne pas proposer des d√©pendances inutiles, qui ne
sont pas utilis√©es dans le cadre du projet. 

::: {.panel-tabset}

## Environnement virtuel

L'approche la plus l√©g√®re est l'environnement virtuel. 
Nous avons en fait implicitement d√©j√† commenc√© √† aller vers
cette direction
en cr√©ant un fichier `requirements.txt`. 

:::: {.callout-tip}

## Application 11a: environnement virtuel `venv` 

1. Ex√©cuter `pip freeze` en ligne de commande et observer la (tr√®s) longue
liste de package
2. Cr√©er l'environnement virtuel `titanic` en s'inspirant de [la documentation officielle](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/)[^pythonversion]
3. Utiliser `ls` pour observer et comprendre le contenu du dossier `titanic/bin` install√©
4. Activer l'environnement et v√©rifier l'installation de `Python` maintenant utilis√©e par votre machine <!---source titanic/bin/activate && which python---->
5. V√©rifier directement depuis la ligne de commande que `Python` ex√©cute bien une commande avec:

    ```shell
    python -c "print('Hello')"
    ```

6. Faire la m√™me chose mais avec `import pandas as pd`
7. Installer les packages √† partir du `requirements.txt`. Tester √† nouveau `import pandas as pd` pour comprendre la diff√©rence. 
8. Ex√©cuter `pip freeze` et comprendre la diff√©rence avec la situation pr√©c√©dente.
9. V√©rifier que le script `main.py` fonctionne bien. Sinon ajouter les _packages_ manquants dans le `requirements.txt` et reprendre de mani√®re it√©rative √† partir de la question 7
10. Ajouter le dossier `titanic/` au `.gitignore` pour ne pas ajouter ce dossier √† `Git`
::::

[^pythonversion]: Si vous d√©sirez aussi contr√¥ler la version de `Python`, ce qui peut √™tre important
dans une perspective de portabilit√©, vous pouvez ajouter une option, par exemple `-p python3.9`. 

:::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli11a
```

::::

## Environnement conda

Les environnements `conda` sont plus lourds √† mettre en oeuvre que les 
environnements virtuels mais peuvent permettre un contr√¥le
plus formel des d√©pendances. 

`conda` est √† la fois un gestionnaire de packages (alternative √† `pip`)
et d'environnements virtuels. L'inconv√©nient de l'utilisation de `conda`
pour g√©rer les environnements virtuels est que cet outil est assez lent 
car l'algorithme de v√©rification des conflits de version n'est pas
extr√™mement rapide.

Pour cette raison, nous allons
utiliser [`mamba`](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html),
un utilitaire de gestion des environnements `conda` qui est plus rapide. 

:::: {.callout-tip}

## Application 11b: environnement `conda` 

1. Ex√©cuter `conda env export` en ligne de commande et observer la (tr√®s) longue
liste de package
2. Tester l'utilisation d'un package qu'on n'utilise pas dans notre chaine de
production, par exemple `seaborn`:

    ```shell
    python -c "import seaborn as sns"
    ```

3. Cr√©er un environnement `titanic`
avec [`mamba create`](https://mamba.readthedocs.io/en/latest/user_guide/mamba.html#quickstart)
en listant les packages que vous aviez mis dans le `requirements.txt` et en ajoutant
l'option `-c conda-forge` √† la fin pour utiliser [la _conda forge_](https://conda-forge.org/) 
4. Activer l'environnement et v√©rifier l'installation de `Python` maintenant utilis√©e par votre machine <!---mamba activate titanic && which python---->
5. _(optionnel)_ Utiliser `ls` dans le dossier parent de `Python`
pour observer et comprendre le contenu de celui-ci

6. V√©rifier que cette fois `seaborn` n'est pas install√© dans l'environnement :

    ```shell
    python -c "import seaborn as sns"
    ```

7. Ex√©cuter √† nouveau `conda env export` et comprendre la diff√©rence avec la situation pr√©c√©dente[^splitscreen].
8. V√©rifier que le script `main.py` fonctionne bien. Sinon utiliser `mamba install` avec les _packages_ manquants jusqu'√† ce 
que la chaine de production fonctionne
9. Cr√©er le fichier `environment.yaml` √† partir de `conda env export`:

    ```shell
    conda env export > environment.yaml
    ```

10. Ajouter le dossier `titanic/` au `.gitignore` pour ne pas ajouter ce dossier √† `Git`

::::

[^splitscreen]: Pour comparer les deux listes, vous pouvez utiliser la fonctionnalit√© de _split_ 
du terminal sur `VSCode` pour comparer les outputs de `conda env export` en les mettant 
en face √† face. 

:::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli11b
```

::::


:::

## Etape 2: construire l'environnement de notre application via un script `shell`

Les environnements virtuels permettent de mieux sp√©cifier les d√©pendances de notre projet, mais ne permettent pas de garantir une portabilit√© optimale. Pour cela, il faut recourir √† la technologie des conteneurs. L'id√©e est de construire une machine, en partant d'une base quasi-vierge, qui permette de construire √©tape par √©tape l'environnement n√©cessaire au bon fonctionnement de notre projet. C'est le principe des conteneurs `Docker`.

Leur m√©thode de construction √©tant un peu difficile √† prendre en main au d√©but, nous allons passer par une √©tape interm√©diaire afin de bien comprendre le processus de production. 

- Nous allons d'abord cr√©er un script `shell`, c'est √† dire une suite de commandes `Linux` permettant de construire l'environnement √† partir d'une machine vierge ;
- Nous transformerons celui-ci en `Dockerfile` dans un deuxi√®me temps. C'est l'objet de l'√©tape suivante. 

::: {.panel-tabset}

## Environnement virtuel

:::: {.callout-tip}

## Application 12a : cr√©er un fichier d'installation de A √† Z

1. Cr√©er un service `ubuntu` sur le SSP Cloud
2. Ouvrir un terminal
3. Cloner le d√©p√¥t 
4. Se placer dans le dossier du projet avec `cd`
5. Se placer au niveau du checkpoint 11a avec `git checkout appli11a`
6. Via l'explorateur de fichiers, cr√©er le fichier `install.sh` √† la racine du projet avec le contenu suivant:

<details>
<summary>Script √† cr√©er sous le nom `install.sh` </summary>
```shell
#!/bin/bash

# Install Python
apt-get -y update
apt-get install -y python3-pip python3-venv

# Create empty virtual environment
python3 -m venv titanic
source titanic/bin/activate

# Install project dependencies
pip install -r requirements.txt
```
</details>

6. Changer les permissions sur le script pour le rendre ex√©cutable

```shell
chmod +x install.sh
```

7. Ex√©cuter le script depuis la ligne de commande avec des droits de super-utilisateur (n√©cessaires pour installer des *packages* via `apt`)

```shell
sudo ./install.sh
```

8. V√©rifier que le script `main.py` fonctionne correctement dans l'environnement virtuel cr√©√© 

```shell
source titanic/bin/activate
python3 main.py
```

::::

:::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli12a
```

::::


## Environnement `conda`

:::: {.callout-tip}

## Application 12b : cr√©er un fichier d'installation de A √† Z

1. Cr√©er un service `ubuntu` sur le SSP Cloud en cliquant sur [ce lien](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/ubuntu?autoLaunch=false&git.cache=%C2%AB36000%C2%BB&git.repository=%C2%ABhttps%3A%2F%2Fgithub.com%2Flinogaliana%2Fensae-reproductibilite-application-correction.git%C2%BB)
2. Cloner le d√©p√¥t et se placer au niveau du checkpoint 11b avec `git checkout appli11b`
3. Se placer dans le dossier du projet avec `cd`
4. On va se placer en super-utilisateur dans la ligne de commande en tapant

```shell
sudo bash
```

5. Cr√©er le fichier `install.sh` avec le contenu suivant:

<details>
<summary>Script √† cr√©er sous le nom `install.sh` </summary>
```shell
apt-get -y update && apt-get -y install wget

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \
    bash Miniconda3-latest-Linux-x86_64.sh -b -p /miniconda && \
    rm -f Miniconda3-latest-Linux-x86_64.sh

PATH="/miniconda/bin:${PATH}"

# Create environment
conda install mamba -c conda-forge
mamba create -n titanic pandas PyYAML scikit-learn -c conda-forge
mamba activate titanic

PATH="/miniconda/envs/titanic/bin:${PATH}"

python main.py
```
</details>

6. Changer les permissions sur le fichier 

```shell
chmod u+x ./install.sh
```

7. Ex√©cuter le script depuis la ligne de commande

::::

:::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli12b
```

::::

:::


## Etape 3: conteneuriser l'application avec `Docker` {#docker}


::: {.callout-note}
Cette application n√©cessite l'acc√®s √† une version interactive de `Docker`.
Il n'y a pas beaucoup d'instances en ligne disponibles.

Nous proposons deux solutions:

- [Installer `Docker`](https://docs.docker.com/get-docker/) sur sa machine ;
- Se rendre sur l'environnement bac √† sable _[Play with Docker](https://labs.play-with-docker.com)_
:::

Maintenant qu'on sait que ce script pr√©paratoire fonctionne, on va le transformer en `Dockerfile` (la syntaxe `Docker` est l√©g√®rement diff√©rente de la syntaxe `Linux` classique).
Puis on va le tester dans un environnement bac √† sable (pour ensuite
pouvoir plus facilement automatiser la construction de l'image
`Docker` par la suite).


::: {.callout-tip}

## Application 13: cr√©ation de l'image `Docker` 

Se placer dans un environnement avec `Docker`

- Dans le terminal `Linux`, cloner votre d√©p√¥t `Github` 
- Cr√©er via la ligne de commande un fichier texte vierge nomm√© `Dockerfile` (la majuscule au d√©but du mot est importante)

<details><summary>Commande pour cr√©er un `Dockerfile` vierge depuis la ligne de commande</summary>
```shell
touch Dockerfile
```
</details>

- Ouvrir ce fichier via un √©diteur de texte et copier le contenu suivant dedans:

<details><summary>Premier `Dockerfile`</summary>
```shell
FROM ubuntu:22.04

WORKDIR ${HOME}/titanic

# Install Python
RUN apt-get -y update && \
    apt-get install -y python3-pip

# Install project dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

CMD ["python3", "main.py"]
```
</details>

### Construire l'image

Le `Dockerfile` est la recette de construction de l'image. La construction effective de l'image √† partir de cette recette s'appelle l'√©tape de `build`.

- Utiliser `docker build` pour cr√©er une image avec le tag `my-python-app`

```shell
docker build . -t my-python-app
```

- V√©rifier les images dont vous disposez. Vous devriez avoir un r√©sultat proche de celui-ci :

```shell
$ docker images

REPOSITORY      TAG       IMAGE ID       CREATED         SIZE
my-python-app   latest    c0dfa42d8520   6 minutes ago   836MB
ubuntu          25.04     825d55fb6340   6 days ago      77.8MB
```

### Tester l'image: d√©couverte du cache

L'√©tape de `build` a fonctionn√©: une image a √©t√© construite.

Mais fait-elle effectivement ce que l'on attend d'elle ?

Pour le savoir, il faut passer √† l'√©tape suivante, l'√©tape de `run`.

```shell
$ docker run -it my-python-app

python3: can't open file '/~/titanic/main.py': [Errno 2] No such file or directory
```

Le message d'erreur est clair : `Docker` ne sait pas o√π trouver le fichier `main.py`. D'ailleurs, il ne connait pas non plus les autres fichiers de notre application qui sont n√©cessaires pour faire tourner le code: `config.yaml` et le dossier `src`.

- Avant l'√©tape `CMD`, copier les fichiers n√©cessaires sur l'image afin que l'application dispose de tous les √©l√©ments n√©cessaires pour √™tre en mesure de fonctionner.

<details>
<summary>Nouveau `Dockerfile` </summary>
```shell
FROM ubuntu:22.04

WORKDIR ${HOME}/titanic

# Install Python
RUN apt-get -y update && \
    apt-get install -y python3-pip

# Install project dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY main.py .
COPY src ./src
COPY configuration ./configuration
CMD ["python3", "main.py"]
```
</details>

- Refaire tourner l'√©tape de `build`

- Refaire tourner l'√©tape de `run`. A ce stade, la matrice de confusion doit fonctionner üéâ.
Vous avez cr√©√© votre premi√®re application reproductible !

:::

::: {.callout-note}

Ici, le _cache_ permet d'√©conomiser beaucoup de temps. Par besoin de 
refaire tourner toutes les √©tapes, `Docker` agit de mani√®re intelligente
en faisant tourner uniquement les √©tapes qui ont chang√©.

:::


::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli13
```

:::

# Partie 4 : automatisation avec l'int√©gration continue

Une image `Docker` est un livrable qui n'est pas forc√©ment int√©ressant
pour tous les publics. Certains pr√©f√©reront avoir un plat bien pr√©par√©
qu'une recette. Nous allons donc proposer d'aller plus loin en proposant
plusieurs types de livrables.

Cela va nous amener √† d√©couvrir les outils
du `CI/CD` (_Continuous Integration / Continuous Delivery_)
qui sont au coeur de l'approche `DevOps`.

Notre approche appliqu√©e
au _machine learning_ va nous entra√Æner plut√¥t du c√¥t√© du `MLOps` qui devient
une approche de plus en plus fr√©quente dans l'industrie de la 
_data science_.

Nous allons am√©liorer notre approche de trois mani√®res:

- Automatisation de la cr√©ation de l'image `Docker` et tests
automatis√©s de la qualit√© du code ;
- Production d'un site _web_ automatis√© permettant de documenter et
valoriser le mod√®le de _Machine Learning_ ;
- Mise √† disposition du mod√®le entra√Æn√© par le biais d'une API pour
ne pas le r√©-entra√Æner √† chaque fois et faciliter sa r√©utilisation ;

A chaque fois, nous allons d'abord tester en local notre travail puis
essayer d'automatiser cela avec les outils de `Github`.

On va ici utiliser l'int√©gration continue pour deux objectifs distincts:

- la mise √† disposition de l'image `Docker` ;
- la mise en place de tests automatis√©s de la qualit√© du code
sur le mod√®le de notre `linter` pr√©c√©dent 

Nous allons utiliser `Github Actions` pour cela. 


## Etape 1: mise en place de tests automatis√©s

Avant d'essayer de mettre en oeuvre la cr√©ation de notre image
`Docker` de mani√®re automatis√©e, nous allons pr√©senter la logique
de l'int√©gration continue en testant de mani√®re automatis√©e
notre script `main.py`.

Pour cela, nous allons partir de la structure propos√©e dans l'[action officielle](https://github.com/actions/setup-python). 
La documentation associ√©e est [ici](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)


::: {.callout-tip}

## Application 14: premier script d'int√©gration continue

A partir de l'exemple pr√©sent
dans la [documentation officielle](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)
de `Github`, on a d√©j√† une base de d√©part qui peut √™tre modifi√©e:

1. Cr√©er un fichier `.github/workflows/ci.yaml` avec le contenu de l'exemple de la documentation
2. Retirer la `strategy matrix` et ne tester qu'avec la version `3.10` de `Python`
3. Utiliser le fichier `requirements.txt` pour installer les d√©pendances. 
4. Remplacer `russ` par `pylint` pour v√©rifier la qualit√© du code. Ajouter l'argument `--fail-under=6` pour
renvoyer une erreur en cas de note trop basse[^hook]
5. Plut√¥t que `pytest`, utiliser `python main.py` pour tester que la matrice de confusion s'affiche bien.

[^hook]: Il existe une approche alternative pour faire des tests
    r√©guliers: les _hooks_ `Git`.
    Il s'agit de r√®gles qui doivent √™tre satisfaites pour que le 
    fichier puisse √™tre committ√©. Cela assurera que chaque `commit` remplisse
    des crit√®res de qualit√© afin d'√©viter le probl√®me de la procrastination.
    
    La [documentation de pylint](https://pylint.pycqa.org/en/latest/user_guide/pre-commit-integration.html) offre des explications suppl√©mentaires. 


<details>
<summary>Fichier `.github/workflows/ci.yaml` obtenu</summary>

```yaml
name: Build, test and push

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pylint
          pip install -r requirements.txt
      - name: Lint
        run: |
          pylint src --fail-under=6
      - name: Test workflow
        run: |
          python main.py
```
</details>

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli14
```

:::

 
Maintenant, nous pouvons observer que l'onglet `Actions`
s'est enrichi. Chaque `commit` va entra√Æner une action pour
tester nos scripts.

Si la note est mauvaise, nous aurons
une croix rouge (et nous recevrons un mail). On pourra ainsi d√©tecter,
en d√©veloppant son projet, les moments o√π on d√©grade la qualit√© du script 
afin de la r√©tablir imm√©diatemment. 




## Etape 2: Automatisation de la livraison de l'image `Docker`

Maintenant, nous allons automatiser la mise √† disposition de notre image
sur `DockerHub`. Cela facilitera sa r√©utilisation mais aussi des
valorisations ult√©rieures.

L√† encore, nous allons utiliser une s√©rie d'actions pr√©-configur√©es.

Pour que `Github` puisse s'authentifier aupr√®s de `DockerHub`, il va 
falloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser
un jeton (_token_) `DockerHub` que nous allons mettre dans un espace
s√©curis√© associ√© √† votre d√©p√¥t `Github`.

::: {.callout-tip}

## Application 15a: configuration

- Se rendre sur
https://hub.docker.com/ et cr√©er un compte.
- Cr√©er un d√©p√¥t public `application-correction`
- Aller dans les param√®tres (https://hub.docker.com/settings/general)
et cliquer, √† gauche, sur `Security`
- Cr√©er un jeton personnel d'acc√®s, ne fermez pas l'onglet en question,
vous ne pouvez voir sa valeur qu'une fois. 
- Dans votre d√©p√¥t `Github`, cliquer sur l'onglet `Settings` et cliquer,
√† gauche, sur `Actions`. Sur la page qui s'affiche, cliquer sur `New repository secret`
- Donner le nom `DOCKERHUB_TOKEN` √† ce jeton et copier la valeur. Valider
- Cr√©er un deuxi√®me secret nomm√© `DOCKERHUB_USERNAME` ayant comme valeur le nom d'utilisateur
que vous avez cr√©√© sur `Dockerhub`
:::


A ce stade, nous avons donn√© les moyens √† `Github` de s'authentifier avec
notre identit√© sur `Dockerhub`. Il nous reste √† mettre en oeuvre l'action
en s'inspirant de https://github.com/docker/build-push-action/#usage.
On ne va modifier que trois √©l√©ments dans ce fichier. Effectuer les 
actions suivantes:

::: {.callout-tip}

## Application 15b: automatisation de l'image `Docker`

- En s'inspirant de ce [_template_](https://github.com/marketplace/actions/build-and-push-docker-images), ajouter un nouveau job `docker` dans le fichier `ci.yaml` qui va *build* et *push* l'image sur le `DockerHub`
- Changer le tag √† la fin pour mettre `<username>/application-correction:latest`
o√π `username` est le nom d'utilisateur sur `DockerHub`;

<details>
<summary>Fichier `.github/workflows/ci.yaml` obtenu</summary>

```yaml
name: Build, test and push

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pylint
          pip install -r requirements.txt
      - name: Lint
        run: |
          pylint src --fail-under=6
      - name: Test workflow
        run: |
          python main.py
  docker:
    runs-on: ubuntu-latest
    steps:
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          push: true
          tags: linogaliana/application-correction:latest
```
</details>

- Faire un `commit` et un `push` de ces fichiers
:::

Comme on est fier de notre travail, on va afficher √ßa avec un badge sur le 
`README`. 

::: {.callout-tip}

## Application 15c: Afficher un badge dans le `README`

- Se rendre dans l'onglet `Actions` et cliquer sur un des scripts en train de tourner. 
- En haut √† droite, cliquer sur `...`
- S√©lectionner `Create status badge`
- R√©cup√©rer le code `Markdown` propos√©
- Copier dans le `README` depuis `VSCode`
- Faire de m√™me pour l'autre _workflow_

:::

Maintenant, il nous reste √† tester notre application dans l'espace bac √† sable
ou en local, si `Docker` est install√©.


::: {.callout-tip}

## Application 15d: Tester l'application

- Se rendre sur l'environnement bac √† sable _[Play with Docker](https://labs.play-with-docker.com)_
- R√©cup√©rer l'image :

```yaml
docker pull <username_dockerhub>/application-correction:latest
```

- Tester le bon fonctionnement de l'image

```yaml
docker run -it <username_dockerhub>/application-correction:latest
```

:tada: La matrice de confusion doit s'afficher ! Vous avez grandement
facilit√© la r√©utilisation de votre image. 

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli15
```

:::


# Partie 5: mise en production d'une API servant un mod√®le de machine learning

## Etape 1: cr√©ation d'un pipeline `scikit`


Notre code respecte des bonnes pratiques formelles. Cependant, la mise en
production n√©cessite d'√™tre exigeant sur la mise en oeuvre op√©rationnelle
de notre _pipeline_. 

Quand  on utilise `scikit`, la bonne pratique est d'utiliser
les [_pipelines_](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
qui s√©curisent les √©tapes de _feature engineering_ avant la mise en oeuvre d'un mod√®le (que
ce soit pour l'entra√Ænement ou le test sur un nouveau jeu de donn√©es). 

On va donc devoir refactoriser notre application pour utiliser un _pipeline_ `scikit`. 
Les raisons sont expliqu√©es [ici](https://scikit-learn.org/stable/common_pitfalls.html).
Cela aura
aussi l'avantage de rendre les √©tapes plus lisibles. 


::: {.callout-tip}

## Application 16: Un _pipeline_ de _machine learning_

- Refactoriser le code de `random_forest_titanic` pour cr√©er 
un vrai pipeline de _preprocessing_ avant la mod√©lisation

- Simplifier la fonction `split_train_test_titanic`
en la r√©duisant au d√©coupage train/test

- Modifier `main.py` pour que ce soit √† ce niveau
qu'a lieu le d√©coupage en train/test, l'entrainement
et l'√©valuation
du mod√®le

:::


::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli16
```

:::

## Etape 2: d√©velopper une API en local

::: {.callout-tip}

## Application 17: Mise √† disposition sous forme d'API

- Cr√©er un nouveau service `SSPCloud` en param√©trant dans l'onglet
`Networking` le port 5000 ;
- Cloner le d√©p√¥t et se placer au niveau de l'application pr√©c√©dente (`git checkout appli16`)
- Installer `fastAPI` et `uvicorn` puis les ajouter au `requirements.txt`
- Renommer le fichier `main.py` en `train.py` et ins√©rer le contenu suivant dedans :

<details>
<summary>
Fichier `train.py`
</summary>
R√©cup√©rer le contenu sur [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli17/train.py)
</details>

- Cr√©er le fichier `main.py` permettant d'initialiser l'API:

<details>
<summary>
Fichier `main.py`
</summary>
R√©cup√©rer le contenu sur [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli17/main.py)
</details>

- Ex√©cuter `train.py` pour stocker en local le mod√®le entra√Æn√©
- Ajouter `model.joblib` au `.gitignore`
- D√©ployer en local l'API avec la commande

```shell
uvicorn main:app --reload --host "0.0.0.0" --port 5000
```

- A partir du `README` du service, se rendre sur l'URL de d√©ploiement, 
ajouter `/docs/` √† celui-ci et observer la documentation de l'API 
- Se servir de la documentation pour tester les requ√™tes `/predict`
- R√©cup√©rer l'URL d'une des requ√™tes propos√©es. La tester dans le navigateur
et depuis `Python` avec `requests` (`requests.get(url).json()`)

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli17
```

:::


# Partie 6: un workflow complet de MLOps

Ce sera l'an prochain, d√©sol√© !

# Partie 7: livrer un site web de mani√®re automatis√©e

On va proposer un nouveau livrable pour parler √† un public plus large.
Pour cela, on va d√©ployer un site web statique qui permet de visualiser
rapidement les r√©sultats du mod√®le.

On propose de cr√©er un site web qui permet de comprendre, avec l'appui
des [valeurs de Shapley](https://christophm.github.io/interpretable-ml-book/shapley.html),
les facteurs qui auraient pu nous mettre la puce
√† l'oreille sur les destins de Jake et de Rose. 

Pour faire ce site web,
on va utiliser `Quarto` et d√©ployer sur `Github Pages`.
Des √©tapes pr√©liminaires sont r√©alis√©es en `Python` 
puis l'affichage interactif 
sera contr√¥l√© par du `JavaScript` gr√¢ce
√† des [blocs `Observable`](https://quarto.org/docs/interactive/ojs/). 


::: {.callout-tip}

## Application 19: D√©ploiement automatis√© d'un site web

Dans un premier temps, on va cr√©er un projet `Quarto`
au sein de notre d√©p√¥t: 

- Installer `Quarto` dans votre environnement local (s'il n'est pas d√©j√† disponible) ;
- Dans le projet, utiliser la commande `quarto create-project` pour initialiser le projet `Quarto` ;
- Supprimer le fichier automatiquement g√©n√©r√© avec l'extension `.qmd` ;
- R√©cup√©rer le contenu du mod√®le de fichier `Quarto Markdown` [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/index.qmd). Celui-ci permet de g√©n√©rer la page d'accueil de notre site. Enregistrer dans un fichier nomm√© `index.qmd`

On teste ensuite la compilation en local du fichier:

- Modifier le fichier `train.py` √† partir de [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/train.py) pour √™tre en mesure de compiler le fichier 
- Ex√©cuter le fichier `train.py`
- En ligne de commande, faire `quarto preview` (ajouter les arguments `--port 5000 --host 0.0.0.0` si vous passez par le `SSPCloud`)
- Observer le site web g√©n√©r√© en local

Enfin, on va construire et d√©ployer automatiquement ce site web gr√¢ce au
combo `Github Actions` et `Github Pages`:

- Cr√©er une branche `gh-pages` √† partir du contenu de [cette page](https://quarto.org/docs/publishing/github-pages.html)
- Cr√©er un fichier `.github/workflows/website.yaml` avec le contenu de [ce fichier](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/.github/workflows/publish.yaml)

:::

::: {.callout-note}

On doit dans cette application modifier le fichier `train.py`
pour enregistrer en local une duplication du mod√®le
de _machine learning_ et de l'ensemble d'entra√Ænement
car pour ces deux √©l√©ments
on n'est pas all√© au bout de la d√©marche MLOps
d'enregistrement dans un _model registry_ et un
_feature store_.

Dans la prochaine version de ce cours, qui
int√®grera `MLFlow`, on aura une d√©marche plus 
propre car on utilisera bien le mod√®le de production
et le jeu d'entrainement associ√©. 
:::


::: {.callout-caution collapse="true"}
## Checkpoint

```shell
git checkout appli18
```

:::