---
title: "Application"
image: images/rocket.png
description: |
  Une application fil rouge pour illustrer l'intérêt d'appliquer graduellement les bonnes pratiques dans une optique de mise en production d'une application de data science.
order: 9
href: chapters/application.html
---

<details>
<summary>
Dérouler les _slides_ ci-dessous ou [cliquer ici](https://ensae-reproductibilite.github.io/slides/#/title-slide)
pour afficher les slides en plein écran.
</summary>


<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://ensae-reproductibilite.github.io/slides/#/title-slide"></iframe></div>

</details>

L'objectif de cette mise en application est d'**illustrer les différentes étapes qui séparent la phase de développement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.

Celle-ci est un tutoriel pas à pas pour avoir un projet reproductible et disponible sous plusieurs livrables. 
Toutes les étapes ne sont pas indispensables à tous les projets de _data science_. 

Nous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d'un projet de data science.
On a un _notebook_ un peu monolithique, qui réalise les étapes classiques d'un *pipeline* de *machine learning* :

- Import de données ;
- Statistiques descriptives et visualisations ;
- *Feature engineering* ;
- Entraînement d'un modèle ;
- Evaluation du modèle.

**L'objectif est d'améliorer le projet de manière incrémentale jusqu'à pouvoir le mettre en production, en le valorisant sous une forme adaptée.** 


<details>
<summary>
Illustration de notre point de départ
</summary>
![](/workflow1.png)
</details>

<details>
<summary>
Illustration de l'horizon vers lequel on se dirige
</summary>
![](/workflow2.png)
</details>

::: {.callout-important}
Il est important de bien lire les consignes et d'y aller progressivement.
Certaines étapes peuvent être rapides, d'autres plus fastidieuses ;
certaines être assez guidées, d'autres vous laisser plus de liberté.
Si vous n'effectuez pas une étape, vous risquez de ne pas pouvoir passer à
l'étape suivante qui en dépend.

Bien que l'exercice soit applicable sur toute configuration bien faite, nous 
recommandons de privilégier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), où tous les 
outils nécessaires sont pré-installés et pré-configurés. Le service `VSCode`
ne sera en effet que le point d'entrée pour l'utilisation d'outils plus exigeants
sur le plan de l'infrastructure: _Argo_, _MLFLow_, etc.
:::


# Partie 0 : initialisation du projet

::: {.callout-tip}
## Application préliminaire: forker le dépôt d'exemple

Les premières étapes consistent à mettre en place son environnement de travail sur `Github`:

- Générer un jeton d'accès (*token*) sur `GitHub` afin de permettre l'authentification en ligne de commande à votre compte.
La procédure est décrite [ici](https://docs.sspcloud.fr/onyxia-guide/controle-de-version#creer-un-jeton-dacces-token). 
__Vous ne voyez ce jeton qu'une fois, ne fermez pas la page de suite__. 

- Mettez de côté ce jeton en l'enregistrant dans un gestionnaire de mot de passe ou dans 
l'espace _["Mon compte"](https://datalab.sspcloud.fr/account/third-party-integration)_
du `SSP Cloud`. 

- Forker le dépôt `Github` : [https://github.com/ensae-reproductibilite/application-correction](https://github.com/ensae-reproductibilite/application-correction) en faisant attention à deux choses:
    + Renommer le dépôt en `ensae-reproductibilite-application-correction.git` ;
    + Décocher la case _"Copy the `main` branch only"_ afin de copier également les _tags_ `Git` qui nous permettront de faire les _checkpoint_


<details>

<summary>
Ce que vous devriez voir sur la page de création du _fork_
</summary>

![](/fork-example.png)

</details>

Il est maintenant possible de ce lancer dans la création de l'environnement de travail:

- Ouvrir un service `VSCode` sur le [SSP Cloud](https://datalab.sspcloud.fr/home). Vous pouvez aller
dans la page `My Services` et cliquer sur `New service`. Sinon, vous
pouvez initialiser la création du service en cliquant directement [ici](https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=false). __Modifier les options suivantes__:
    + Dans l'onglet `Kubernetes`, sélectionner le rôle `Admin` ;
    + Dans l'onglet `Networking`, cliquer sur "Enable a custom service port" et laisser la valeur par défaut 5000 pour le numéro du port

- Clôner __votre__ dépôt `Github` en utilisant le
terminal depuis `Visual Studio` (`Terminal > New Terminal`) et
en passant directement le token dans l'URL selon cette structure:

```{.bash filename="terminal"}
git clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application-correction.git
```

où `<TOKEN>` et `<USERNAME>` sont à remplacer, respectivement, 
par le jeton que vous avez généré précédemment et votre nom d'utilisateur.

- Se placer avec le terminal dans le dossier en question : 

```{.bash filename="terminal"}
cd ensae-reproductibilite-application-correction
```

- Se placer sur une branche de travail en faisant:

```{.bash filename="terminal"}
git checkout -b dev
```

:::


# Partie 1 : qualité du script

Cette première partie vise à **rendre le projet conforme aux bonnes pratiques** présentées dans le cours.

Elle fait intervenir les notions suivantes : 

- Utilisation du **terminal** (voir [Linux 101](/chapters/linux-101.qmd)) ;
- **Qualité du code** (voir [Qualité du code](/chapters/code-quality.qmd)) ;
- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;
- **Contrôle de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;
- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).

Nous allons partir de ce _Notebook_ `Jupyter`,
que vous pouvez prévisualiser voire tester
en cliquant sur l'un des liens suivants:

_to do bouton onyxia_
<a href="https://github.com/ensae-reproductibilite/application-correction/blob/main/titanic.ipynb" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>

Le plan de la partie est le suivant :

1. S'assurer que le script fonctionne ;
2. Nettoyer le code des scories formelles avec un _linter_ et un _formatter_ ;
3. Paramétrisation du script ;
4. Utilisation de fonctions.


## Étape 1 : s'assurer que le script s'exécute correctement

On va partir du fichier `notebook.py` qui reprend le contenu 
du _notebook_[^jupytext] mais dans un script classique.
Le travail de nettoyage en sera facilité. 

[^jupytext]: L'export dans un script `.py` a été fait
        directement depuis `VSCode`. Comme
        cela n'est pas vraiment l'objet du cours, nous passons cette étape et fournissons
        directement le script expurgé du texte intermédiaire. Mais n'oubliez
        pas que cette démarche, fréquente quand on a démarré sur un _notebook_ et
        qu'on désire consolider en faisant la transition vers des 
        scripts, nécessite d'être attentif pour ne pas risquer de faire une erreur. 

La première étape est simple, mais souvent oubliée : **vérifier que le code fonctionne correctement**. 
Pour cela, nous recommandons de faire un aller-retour entre le script ouvert dans `VSCode`
et un terminal pour le lancer. 


{{< include "./applications/_appli1.qmd" >}}


## Étape 2: utiliser un _linter_ puis un _formatter_

On va maintenant améliorer la qualité de notre code en appliquant les standards communautaires.
Pour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/)
et le _formatter_ [`Black`](https://github.com/psf/black).

::: {.callout-important}
[`PyLint`](https://pylint.readthedocs.io/en/latest/) et [`Black`](https://black.readthedocs.io/en/stable/)
sont des _packages_ `Python` qui 
s'utilisent principalement en ligne de commande.

Si vous avez une erreur qui suggère
que votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/)
ou [`Black`](https://black.readthedocs.io/en/stable/),
n'oubliez pas d'exécuter la commande `pip install pylint` ou `pip install black`.
:::


Le _linter_ renvoie alors une série d'irrégularités,
en précisant à chaque fois la ligne de l'erreur et le message d'erreur associé (ex : mauvaise identation).
Il renvoie finalement une note sur 10,
qui estime la qualité du code à l'aune des standards communautaires évoqués
dans la partie [Qualité du code](/chapters/code-quality.html).

{{< include "./applications/_appli2.qmd" >}}

Le code est maintenant lisible, il obtient à ce stade une note formelle proche de 10.
Mais il n'est pas encore totalement intelligible ou fiable.
Il y a notamment 
beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. 
Néanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script: 
jetons d'API et chemin des fichiers.


## Étape 3: gestion des paramètres

L'exécution du code et les résultats obtenus
dépendent de certains paramètres définis dans le code. L'étude de résultats
alternatifs, en jouant sur 
des variantes des (hyper)paramètres, est à ce stade compliquée
car il est nécessaire de parcourir le code pour trouver
ces paramètres. De plus, certains paramètres personnels
comme des jetons
d'API ou des mots de passe n'ont pas vocation à 
être présents dans le code. 

Il est plus judicieux de considérer ces paramètres comme des
variables d'entrée du script. Cela peut être fait de deux
manières:

1. Avec des __arguments optionnels__ appelés depuis la ligne de commande _(Application 3a)_.
Cela peut être pratique pour mettre en oeuvre des tests automatisés mais
n'est pas forcément pertinent pour toutes les variables. Nous allons montrer
cet usage avec le nombre d'arbres de notre _random forest_ ;
2. En utilisant un __fichier de configuration__ dont les valeurs sont importées dans
le script principal _(Application 3b)_. 


<details>
<summary>
Un exemple de définition d'un argument pour l'utilisation en ligne de commande
</summary>

```{.python filename="prenom.py"}
import argparse
parser = argparse.ArgumentParser(description="Qui êtes-vous?")
parser.add_argument(
    "--prenom", type=str, default="Toto", help="Un prénom à afficher"
)
args = parser.parse_args()
print(args.prenom)
```

Exemples d'utilisations en ligne de commande

```{.bash filename="terminal"}
python prenom.py
python prenom.py --prenom "Zinedine"
```

</details>

{{< include "./applications/_appli3.qmd" >}}


## Étape 4 : Privilégier la programmation fonctionnelle

Nous allons **mettre en fonctions les parties importantes de l'analyse**. 
Ceci facilitera l'étape ultérieure de modularisation de notre projet. 

Cet exercice étant chronophage, il n'est __pas obligatoire de le réaliser en entier__. L'important est de
comprendre la démarche et d'adopter fréquemment une approche fonctionnelle[^POO]. Pour obtenir 
une chaine entièrement fonctionnalisée, vous pouvez reprendre le _checkpoint_.

[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser
un processus, il serait possible d'adopter le paradigme de la __programmation orientée objet (POO)__. Celle-ci est
plus rebutante et demande plus de temps au développeur. L'arbitrage coût-avantage est négatif pour notre
exemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d'un modèle,
il est recommandé de l'adopter. C'est d'ailleurs obligatoire avec des [_pipelines_ `scikit`](https://pythonds.linogaliana.fr/pipeline-scikit/). 

{{< include "./applications/_appli4.qmd" >}}

Cela ne se remarque pas encore vraiment car nous avons de nombreuses définitions de fonctions
mais notre chaine de production est beaucoup plus
concise (le script fait environ 300 lignes dont 250 de définitions de fonctions génériques).
Cette auto-discipline facilitera grandement
les étapes ultérieures. Cela aurait été néanmoins beaucoup moins coûteux en temps d'adopter
ces bons gestes de manière plus précoce. 


# Partie 2 : adoption d'une structure modulaire {#partie2}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est déjà considérablement rapprochés d'un
possible partage du code : celui-ci est lisible et intelligible. 
Le code est proprement versionné sur un
dépôt `GitHub`.
Cependant, le projet est encore perfectible: il est encore difficile de rentrer
dedans si on ne sait pas exactement ce qu'on recherche. L'objectif de cette partie
est d'isoler les différentes étapes de notre _pipeline_. 
Outre le gain de clarté pour notre projet, nous économiserons beaucoup de peines
pour la mise en production ultérieure de notre modèle. 

<details>
<summary>
Illustration de l'état actuel du projet 
</summary>
![](/schema_post_appli4.png)
</details>

Dans cette partie nous allons continuer les améliorations
incrémentales de notre projet avec les étapes suivantes:

1. Modularisation du code `Python` pour séparer les différentes
étapes de notre _pipeline_ ; 
2. Adopter une structure standardisée pour notre projet afin
d'autodocumenter l'organisation de celui-ci ; 
3. Documenter les _packages_ indispensables à l'exécution du code ;
4. Stocker les données dans un environnement adéquat
afin de continuer la démarche de séparer conceptuellement les données du code en de la configuration.


## Étape 1 : modularisation

Nous allons profiter de la modularisation pour adopter une structure
applicative pour notre code. Celui-ci n'étant en effet plus lancé
que depuis la ligne de commande, on peut considérer qu'on construit
une application générique où un script principal (`main.py`)
encapsule des éléments issus d'autres scripts `Python`. 


{{< include "./applications/_appli5.qmd" >}}


## Étape 2 : adopter une architecture standardisée de projet

On dispose maintenant d'une application `Python` fonctionnelle. 
Néanmoins, le projet est certes plus fiable mais sa structuration
laisse à désirer et il serait difficile de rentrer à nouveau
dans le projet dans quelques temps. 

<details>
<summary>Etat actuel du projet 🙈</summary>

```
├── README.md
├── train.csv
├── test.csv
├── .gitignore
├── config.yaml
├── import_data.py
├── build_features.py
├── train_evaluate.py
├── titanic.ipynb
└── main.py
```

</details>

Comme cela est expliqué dans la
partie [Structure des projets](/chapters/projects-architecture.html),
on va adopter une structure certes arbitraire mais qui va 
faciliter l'autodocumentation de notre projet. De plus, une telle structure va faciliter des évolutions optionnelles
comme la _packagisation_ du projet. Passer d'une structure modulaire
bien faite à un _package_ est quasi-immédiat en `Python`. 

On va donc modifier l'architecture de notre projet pour la rendre plus standardisée.
Pour cela, on va s'inspirer des structures
[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)
qui génèrent des _templates_ de projet. En l'occurrence
notre source d'inspiration sera le [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)
issu d'un effort communautaire.

::: {.callout-note}
L'idée de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de bâtir à l'avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante : 

```{.bash filename="terminal"}
pip install cookiecutter
cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

Ici, on a déjà un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.
:::

En s'inspirant du _cookiecutter data science_
on va adopter la structure suivante:

<details>
<summary>
Structure recommandée
</summary>

```
ensae-reproductibilite-application
├── main.py
├── README.md
├── data
│   └── raw
│       ├── test.csv
│       └── train.csv
├── configuration
│   └── config.yaml
├── notebooks
│   └── titanic.ipynb
└── src
    ├── data
    │   └── import_data.py
    ├── features
    │   └── build_features.py
    └── models
        └── train_evaluate.py
```

</details>

{{< include "./applications/_appli6.qmd" >}}


## Étape 3: indiquer l'environnement minimal de reproductibilité

Le script `main.py` nécessite un certain nombre de packages pour
être fonctionnel. Chez vous les packages nécessaires sont
bien sûr installés mais êtes-vous assuré que c'est le cas 
chez la personne qui testera votre code ? 

Afin de favoriser la portabilité du projet,
il est d'usage de _"fixer l'environnement"_,
c'est-à-dire d'indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version.
Nous proposons de créer un fichier `requirements.txt` minimal, sur lequel nous reviendrons
dans la partie consacrée aux environnements reproductibles. 

Le fichier `requirements.txt` est conventionnellement localisé à la racine du projet.
Ici on ne va pas fixer les versions, on raffinera ce fichier ultérieurement.

{{< include "./applications/_appli7.qmd" >}}

## Étape 4 : stocker les données de manière externe {#stockageS3}

::: {.callout-warning collapse="true"}
## Pour en savoir plus sur le système de stockage `S3`

Pour mettre en oeuvre cette étape, il peut être utile de
comprendre un peu comme fonctionne le SSP Cloud.
Vous devrez suivre la [documentation du SSP Cloud](https://inseefrlab.github.io/docs.sspcloud.fr/docs/fr/storage.html) pour la réaliser. Une aide-mémoire est également disponible dans le cours
de 2e année de l'ENSAE [Python pour la _data science_](https://linogaliana-teaching.netlify.app/reads3/#).
:::


Le chapitre sur la [structure des projets](/chapters/projects-architecture.qmd)
développe l'idée qu'il est recommandé de converger vers un modèle
où environnements d'exécution, de stockage du code et des données sont conceptuellement
séparés. Ce haut niveau d'exigence est un gain de temps important 
lors de la mise en production car au cours de cette dernière, le projet
est amené à être exécuté sur une infrastructure informatique dédiée
qu'il est bon d'anticiper. 

A l'heure actuelle, les données sont stockées dans le dépôt. C'est une
mauvaise pratique. En premier lieu, `Git` n'est techniquement
pas bien adapté au stockage de données. Ici ce n'est pas très grave
car il ne s'agit pas de données volumineuses et ces dernières ne sont
pas modifiées au cours de notre chaine de traitement. 
La raison principale
est que les données traitées par les _data scientists_ 
sont généralement soumises à des clauses de
confidentialités ([RGPD](https://www.cnil.fr/fr/rgpd-de-quoi-parle-t-on), [secret statistique](https://www.insee.fr/fr/information/1300624)...). Mettre ces données sous contrôle de version
c'est prendre le risque de les divulguer à un public non habilité. 
Il est donc recommandé de privilégier des outils techniques adaptés au
stockage de données.

L'idéal, dans notre cas, est d'utiliser une solution de stockage externe. 
On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud. 
Cela nous permettra de supprimer les données de `Github` tout en maintenant la reproductibilité 
de notre projet [^history].

[^history]: Attention, les données ont été _committées_ au moins une fois. Les supprimer
du dépôt ne les efface pas de l'historique. Si cette erreur arrive, le mieux est de supprimer
le dépôt en ligne, créer un nouvel historique `Git` et partir de celui-ci pour des publications
ultérieures sur `Github`. Néanmoins l'idéal serait de ne pas s'exposer à cela. C'est justement
l'objet des bonnes pratiques de ce cours: un `.gitignore` bien construit et une séparation des
environnements de stockage du code et
des données seront bien plus efficaces pour vous éviter ces problèmes que tout les conseils de 
vigilance que vous pourrez trouver ailleurs. 

{{< include "./applications/_appli8.qmd" >}}

# Partie 2bis: packagisation de son projet (optionnel)

Cette série d'actions n'est pas forcément pertinente pour tous
les projets. Elle fait un peu la transition entre la modularité
et la portabilité. 

## Étape 1 : proposer des tests unitaires (optionnel)

Notre code comporte un certain nombre de fonctions génériques.
On peut vouloir tester leur usage sur des données standardisées,
différentes de celles du Titanic.

Même si la notion de tests unitaires
prend plus de sens dans un _package_, nous pouvons proposer
dans le projet des exemples d'utilisation de la fonction, ceci peut être pédagogique. 

Nous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)
pour effectuer des tests unitaires. Cette approche nécessite quelques notions
de programmation orientée objet ou une bonne discussion avec `ChatGPT`.

{{< include "./applications/_appli9.qmd" >}}


::: {.callout-note}

Lorsqu'on effectue des tests unitaires, on cherche généralement
à tester le plus de lignes possibles de son code. On parle de
__taux de couverture__ (_coverage rate_) pour désigner
la statistique mesurant cela. 

Cela peut s'effectuer de la manière suivante avec le package
[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):

```{.bash filename="terminal"}
coverage run -m unittest tests/test_create_variable_title.py
coverage report -m
```

```{.python}
Name                                  Stmts   Miss  Cover   Missing
-------------------------------------------------------------------
src/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113
tests/test_create_variable_title.py      21      1    95%   54
-------------------------------------------------------------------
TOTAL                                    55     22    60%
```

Le taux de couverture est souvent mis en avant par les gros
projets comme indicateur de leur qualité. Il existe d'ailleurs
des badges `Github` dédiés. 
:::




## Étape 2 : transformer son projet en package (optionnel)

Notre projet est modulaire, ce qui le rend assez simple à transformer
en _package_, en s'inspirant de la structure du `cookiecutter` adapté, issu
de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

On va créer un _package_ nommé `titanicml` qui encapsule
tout notre code et qui sera appelé
par notre script `main.py`. La structure attendue
est la suivante:

<details>
<summary>Structure visée</summary>

```
ensae-reproductibilite-application
├── docs                                    ┐ 
│   ├── main.py                             │ 
│   └── notebooks                           │ Package documentation and examples
│       └── titanic.ipynb                   │ 
├── configuration                           ┐ Configuration (pas à partager avec Git)
│   └── config.yaml                         ┘ 
├── README.md                                
├── pyproject.toml                          ┐ 
├── requirements.txt                        │
├── titanicml                               │                
│   ├── __init__.py                         │ Package source code, metadata
│   ├── data                                │ and build instructions 
│   │   ├── import_data.py                  │  
│   │   └── test_create_variable_title.py   │   
│   ├── features                            │
│   │   └── build_features.py               │
│   └── models                              │
│       └── train_evaluate.py               ┘
└── tests                                   ┐
    └── test_create_variable_title.py       ┘ Package tests
```
</details>

<details>
<summary>Rappel: structure actuelle</summary>

```
ensae-reproductibilite-application
├── notebooks                                 
│   └── titanic.ipynb                  
├── configuration                                 
│   └── config.yaml                  
├── main.py                              
├── README.md                 
├── requirements.txt                      
└── src 
    ├── data                                
    │   ├── import_data.py                    
    │   └── test_create_variable_title.py      
    ├── features                           
    │   └── build_features.py      
    └── models                          
        └── train_evaluate.py              
```
</details>

Il existe plusieurs 
_frameworks_ pour
construire un _package_. Nous
allons privilégier [`Poetry`](https://python-poetry.org/)
à [`Setuptools`](https://pypi.org/project/setuptools/). 


::: {.callout-note}

Pour créer la structure minimale d'un _package_, le plus simple est
d'utiliser le `cookiecutter` adapté,
issu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

Comme on a déjà une structure très modulaire, on va plutôt recréer cette
structure dans notre projet déjà existant. En fait, il ne manque qu'un fichier essentiel, 
le principal distinguant un projet classique d'un package : `pyproject.toml`.

```{.bash filename="terminal"}
cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git
```

<details>
<summary>Dérouler pour voir les choix possibles</summary>
```{.python}
author_name [Monty Python]: Daffy Duck
package_name [mypkg]: titanicml
package_short_description []: Impressive Titanic survival analysis
package_version [0.1.0]: 
python_version [3.9]: 
Select open_source_license:
1 - MIT
2 - Apache License 2.0
3 - GNU General Public License v3.0
4 - Creative Commons Attribution 4.0
5 - BSD 3-Clause
6 - Proprietary
7 - None
Choose from 1, 2, 3, 4, 5, 6 [1]: 
Select include_github_actions:
1 - no
2 - ci
3 - ci+cd
Choose from 1, 2, 3 [1]:
```
</details>

:::

{{< include "./applications/_appli10.qmd" >}}

# Partie 3 : construction d'un projet portable et reproductible {#partie3}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues
dans les chapitres [Qualité du code](/chapters/code-quality.html)
et [Structure des projets](/chapters/projects-architecture.html)
tout au long du cours.

Ce faisant, on s'est déjà considérablement rapprochés d'une
possible mise en production : le code est lisible,
la structure du projet est normalisée et évolutive,
et le code est proprement versionné sur un
dépôt `GitHub` {{< fa brands github >}}.


<details>
<summary>
Illustration de l'état actuel du projet 
</summary>
![](/schema_post_appli8.png)
</details>



A présent, nous avons une version du projet qui est largement partageable.
Du moins en théorie, car la pratique est souvent plus compliquée :
il y a fort à parier que si vous essayez d'exécuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),
les choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'état, le projet n'est pas portable : il n'est pas possible, sans modifications coûteuses, de l'exécuter dans un environnement différent de celui dans lequel il a été développé**.

Dans cette troisème partie de notre travail vers la mise en production,
nous allons voir 
comment **normaliser l'environnement d'exécution afin de produire un projet portable**.
Autrement dit, nous n'allons plus nous contenter de modularité mais allons rechercher
la portabilité.
On sera alors tout proche de pouvoir mettre le projet en production.

On progressera dans l'échelle de la reproductibilité 
de la manière suivante: 

1. [**Environnements virtuels**](#anaconda) ;
2. Créer un [script shell](#shell) qui permet, depuis un environnement minimal, de construire l'application de A à Z ;
3. [**Images et conteneurs `Docker`**](#docker).


Nous allons repartir de l'application 8, c'est-à-dire d'un projet
modulaire mais qui n'est pas, à strictement parler, un _package_
(objet des applications optionnelles suivantes 9 et 10). 

Pour se replacer dans l'état du projet à ce niveau,
il est possible d'utiliser le _tag_ _ad hoc_.

```{.bash filename="terminal"}
git checkout appli8
```


## Étape 1 : un environnement pour rendre le projet portable {#anaconda}

Pour qu'un projet soit portable, il doit remplir deux conditions:

- Ne pas nécessiter de dépendance
qui ne soient pas renseignées quelque part ;
- Ne pas proposer des dépendances inutiles, qui ne
sont pas utilisées dans le cadre du projet. 

Le prochain exercice vise à mettre ceci en oeuvre.
Comme expliqué dans le [chapitre portabilité](/chapters/portability.qmd),
le choix du gestionnaire d'environnement est laissé
libre. Il est recommandé de privilégier `venv` si vous découvrez
la problématique de la portabilité. 

::: {.panel-tabset group="language"}

## Environnement virtuel `venv`

L'approche la plus légère est l'environnement virtuel. 
Nous avons en fait implicitement déjà commencé à aller vers
cette direction
en créant un fichier `requirements.txt`. 

{{< include "./applications/_appli11a.qmd" >}}


## Environnement `conda`

Les environnements `conda` sont plus lourds à mettre en oeuvre que les 
environnements virtuels mais peuvent permettre un contrôle
plus formel des dépendances. 

{{< include "./applications/_appli11b.qmd" >}}

:::


## Étape 2: construire l'environnement de notre application via un script `shell` {#shell}

Les environnements virtuels permettent de mieux spécifier les dépendances de notre projet, mais ne permettent pas de garantir une portabilité optimale. Pour cela, il faut recourir à la technologie des conteneurs. L'idée est de construire une machine, en partant d'une base quasi-vierge, qui permette de construire étape par étape l'environnement nécessaire au bon fonctionnement de notre projet. C'est le principe des conteneurs `Docker` {{< fa brands docker >}}.

Leur méthode de construction étant un peu difficile à prendre en main au début, nous allons passer par une étape intermédiaire afin de bien comprendre le processus de production. 

- Nous allons d'abord créer un script `shell`, c'est à dire une suite de commandes `Linux` permettant de construire l'environnement à partir d'une machine vierge ;
- Nous transformerons celui-ci en `Dockerfile` dans un deuxième temps. C'est l'objet de l'étape suivante. 

::: {.panel-tabset group="language"}

## Environnement virtuel `venv`

{{< include "./applications/_appli12a.qmd" >}}

## Environnement `conda`

{{< include "./applications/_appli12b.qmd" >}}

:::


## Étape 3: conteneuriser l'application avec `Docker` {#docker}


::: {.callout-note}
Cette application nécessite l'accès à une version interactive de `Docker`.
Il n'y a pas beaucoup d'instances en ligne disponibles.

Nous proposons deux solutions:

- [Installer `Docker`](https://docs.docker.com/get-docker/) sur sa machine ;
- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_

Sinon, elle peut être réalisée en essai-erreur par le biais des services d'intégration continue de `Github` {{< fa brands github >}} ou `Gitlab` {{< fa brands gitlab >}}. Néanmoins, nous présenterons l'utilisation de ces services plus tard, dans la prochaine partie. 
:::

Maintenant qu'on sait que ce script préparatoire fonctionne, on va le transformer en `Dockerfile` pour anticiper la mise en production.  Comme la syntaxe `Docker` est légèrement différente de la syntaxe `Linux` classique (voir le [chapitre portabilité](/chapters/portability.qmd)), il va être nécessaire de changer quelques instructions mais ceci sera très léger.

On va tester le `Dockerfile` dans un environnement bac à sable pour ensuite
pouvoir plus facilement automatiser la construction de l'image
`Docker`.

{{< include "./applications/_appli13.qmd" >}}


# Partie 4 : automatisation avec l'intégration continue


Imaginez que vous êtes au restaurant
et qu'on ne vous serve pas le plat mais seulement la recette
et que, de plus, on vous demande de préparer le plat
chez vous avec les ingrédients dans votre frigo.
Vous seriez quelque peu déçu. En revanche, si vous avez goûté
au plat, que vous êtes un réel cordon bleu
et qu'on vous donne la recette pour refaire ce plat ultérieurement,
peut-être
que vous appréciriez plus. 

Cette analogie illustre l'enjeu de définir
le public cible et ses attentes afin de fournir un livrable adapté. 
Une image `Docker` est un livrable qui n'est pas forcément intéressant
pour tous les publics. Certains préféreront avoir un plat bien préparé
qu'une recette ; certains apprécieront avoir une image `Docker` mais
d'autres ne seront pas en mesure de construire celle-ci ou ne sauront
pas la faire fonctionner. Une image `Docker` est plus souvent un 
moyen pour faciliter la mise en service d'une production qu'une fin en soi. 

Nous allons donc proposer
plusieurs types de livrables plus classiques par la suite. Ceux-ci
correspondront mieux aux attendus des publics utilisateurs de services
construits à partir de techniques de _data science_. `Docker` est néanmoins
un passage obligé car l'ensemble des types de livrables que nous allons
explorer reposent sur la standardisation permise par les conteneurs. 

Cette approche nous permettra de quitter le domaine de l'artisanat pour
s'approcher d'une industrialisation de la mise à disposition 
de notre projet. Ceci va notamment nous amener à mettre en oeuvre
l'approche pragmatique du `DevOps` qui consiste à intégrer dès la phase de
développement d'un projet les contraintes liées à sa mise à disposition
au public cible (cette approche est détaillée plus
amplement dans le chapitre sur la [mise en production](/chapters/deployment.qmd)). 

L'automatisation et la mise à disposition automatisée de nos productions
sera faite progressivement, au cours des prochaines parties. Tous les 
projets n'ont pas vocation à aller aussi loin dans ce domaine. 
L'opportunité doit être comparée aux coûts humains et financiers
de leur mise en oeuvre et de leur cycle de vie. 
Avant de faire une production en série de nos modèles,
nous allons déjà commencer
par automatiser quelques tests de conformité de notre code. 
On va ici utiliser l'intégration continue pour deux objectifs distincts:

- la mise à disposition de l'image `Docker` ;
- la mise en place de tests automatisés de la qualité du code
sur le modèle de notre `linter` précédent.

Nous allons utiliser `Github Actions` pour cela. Il s'agit de serveurs
standardisés mis à disposition gratuitement par `Github` {{<fa brands github >}}.
`Gitlab` {{<fa brands gitlab >}}, l'autre principal acteur du domaine,
propose des services similaires. L'implémentation est légèrement différente
mais les principes sont identiques. 


::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli13
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::


## Étape 1: mise en place de tests automatisés

Avant d'essayer de mettre en oeuvre la création de notre image
`Docker` de manière automatisée, nous allons présenter la logique
de l'intégration continue en testant de manière automatisée
notre script `main.py`.

Pour cela, nous allons partir de la structure proposée dans l'[action officielle](https://github.com/actions/setup-python). 
La documentation associée est [ici](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python).
Des éléments succincts de présentation de la logique déclarative des actions `Github` 
sont disponibles dans le chapitre sur la [mise en production](/chapters/deployment.qmd). Néanmoins, la meilleure
école pour comprendre le fonctionnement de celles-ci est de parcourir la documentation du service et d'observer
les actions `Github` mises en oeuvre par vos projets favoris, celles-ci seront fort instructives !


{{< include "./applications/_appli14.qmd" >}}

 
Maintenant, nous pouvons observer que l'onglet `Actions`
s'est enrichi. Chaque `commit` va entraîner une série d'actions automatisées.

Si l'une des étapes échoue, ou si la note de notre projet est mauvaise, nous aurons
une croix rouge (et nous recevrons un mail). On pourra ainsi détecter,
en développant son projet, les moments où on dégrade la qualité du script 
afin de la rétablir immédiatemment. 



## Étape 2: Automatisation de la livraison de l'image `Docker`

Maintenant, nous allons automatiser la mise à disposition de notre image
sur `DockerHub` (le lieu de partage des images `Docker`). Cela facilitera sa réutilisation mais aussi des
valorisations ultérieures.

Là encore, nous allons utiliser une série d'actions pré-configurées.

Pour que `Github` puisse s'authentifier auprès de `DockerHub`, il va 
falloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser
un jeton (_token_) `DockerHub` que nous allons mettre dans un espace
sécurisé associé à votre dépôt `Github`.


{{< include "./applications/_appli15a.qmd" >}}


A ce stade, nous avons donné les moyens à `Github` de s'authentifier avec
notre identité sur `Dockerhub`. Il nous reste à mettre en oeuvre l'action
en s'inspirant de la [documentation officielle](https://github.com/docker/build-push-action/#usage).
On ne va modifier que trois éléments dans ce fichier. Effectuer les 
actions suivantes:


{{< include "./applications/_appli15b.qmd" >}}



# Partie 5: expérimenter en local des valorisations puis automatiser leur production


Nous avons automatisé les étapes intermédiaires de notre projet. 
Néanmoins nous n'avons pas encore réfléchi à la valorisation
à mettre en oeuvre pour notre projet. On va supposer que notre
projet s'adresse à des _data scientists_ mais aussi à une audience
moins technique. Pour ces premiers, nous pourrions nous contenter
de valorisations techniques, comme des API, 
mais pour ces derniers il est
conseillé de privilégier des formats plus _user friendly_. 

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli15
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::


Afin de faire le parallèle avec les parcours possibles pour l'évaluation,
nous allons proposer trois valorisations[^valorisation]:

- Une [API](https://titanic.kub.sspcloud.fr/docs) facilitant la réutilisation du modèle en "production" ;
- Un [site web statique](https://ensae-reproductibilite.github.io/application-correction/) exploitant cette API pour exposer les prédictions
à une audience moins technique.


[^valorisation]: Vous n'êtes pas obligés pour l'évaluation de mettre en oeuvre
les jalons de plusieurs parcours. Néanmoins, vous découvrirez que 
chaque nouveau pas en avant est moins coûteux que le
précédent si vous avez mis en oeuvre les réflexes des bonnes
pratiques.  



::: {.callout-warning collapse="true"}
## Site statique vs application réactive

La solution que nous allons proposer 
pour les sites statiques, `Quarto` associé
à `Github Pages`, peut être utilisée dans le cadre des parcours 
_"rapport reproductible"_ ou _"dashboard / application interactive"_. 

Pour ce dernier
parcours, d'autres approches techniques sont néanmoins possibles,
comme `Streamlit`. Celles-ci sont plus exigeantes sur le plan technique
puisqu'elles nécessitent de mettre en production sur des serveurs
conteuneurisés (comme la mise en production de l'API)
là où le site statique ne nécessite qu'un serveur web, mis à disposition
gratuitement par `Github`. 


La distinction principale entre ces deux approches est qu'elles
s'appuient sur des serveurs différents. Un site statique repose
sur un serveur web là où `Streamlit` s'appuie sur 
serveur classique en _backend_. La différence principale
entre ces deux types de serveurs
réside principalement dans leur fonction et leur utilisation:

- Un __serveur web__ est spécifiquement conçu pour stocker, traiter et livrer des pages web aux clients. Cela inclut des fichiers HTML, CSS, JavaScript, images, etc. Les serveurs web écoutent les requêtes HTTP/HTTPS provenant des navigateurs des utilisateurs et y répondent en envoyant les données demandées.
- Un **serveur _backend_** classique est conçu pour effectuer des opérations en réponse à un _front_, en l'occurrence une page web. 
Dans le contexte d'une application `Streamlit`, il s'agit d'un serveur avec l'environnement `Python` _ad hoc_ pour
exécuter le code nécessaire à répondre à toute action d'un utilisateur de l'appliacation. 

:::


## Étape préliminaire: création d'un _pipeline_ `scikit`

La mise en
production nécessite d'être exigeant sur la mise en oeuvre opérationnelle
de notre _pipeline_. Nous avons néanmoins un _pipeline_ un peu bancal
car il requiert d'être vigilant dans la manière d'enchaîner les
étapes de _preprocessing_, d'entraînement et d'évaluation.

Quand on utilise `scikit`, la bonne pratique est d'utiliser
les [_pipelines_](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
qui sécurisent les étapes de _feature engineering_ nécessaires avant la mise en oeuvre d'un modèle, qu'il
que
ce soit pour l'entraînement ou pour appliquer les mêmes opérations avec les mêmes paramètres sur 
sur un nouveau jeu de données avant de faire un _predict_. 

On va donc devoir refactoriser notre application pour utiliser un _pipeline_ `scikit`. 
Les raisons sont expliquées plus en détail [ici](https://scikit-learn.org/stable/common_pitfalls.html).
Cela aura
également l'avantage de rendre les étapes de notre _pipeline_ plus lisibles lorsqu'on passera à
l'étape d'industrialisation avec `MLFLow`. 

{{< include "./applications/_appli16.qmd" >}}


## Étape 1: développer une API en local

Le premier livrable devenu classique dans un projet
impliquant du _machine learning_ est la mise à
disposition d'un modèle par le biais d'une
API (voir chapitre sur la [mise en production](/chapters/deployment.qmd)).
Le _framework_ [`FastAPI`](https://fastapi.tiangolo.com/) va permettre
de rapidement transformer notre application `Python` en une API fonctionnelle.

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli16
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::


{{< include "./applications/_appli17.qmd" >}}


## Étape 2: déployer l'API de manière manuelle

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli18
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::

A ce stade, nous avons déployé l'API seulement localement, dans le cadre d'un terminal qui tourne en arrière-plan.
C'est une mise en production manuelle, pas franchement pérenne. 
Ce mode de déploiement est très pratique pour la phase de développement, afin de s'assurer que l'API fonctionne comme attendu. 
Pour pérenniser la mise en production, on va éliminer l'aspect artisanal de celle-ci. 

Il est temps de passer à l'étape de déploiement, qui permettra à notre API d'être accessible via une URL sur le web
et d'avoir un serveur, en arrière plan, qui effectuera les opérations pour répondre à une
requête. Pour se faire, on va utiliser les possibilités offertes par `Kubernetes`, sur lequel est basé le [SSP Cloud](https://datalab.sspcloud.fr).


{{< include "./applications/_appli18a.qmd" >}}


Nous avons préparé la mise à disposition de notre API mais à l'heure
actuelle elle n'est pas disponible de manière aisée car il est nécessaire
de lancer manuellement une image `Docker` pour pouvoir y accéder. 
Ce type de travail est la spécialité de `Kubernetes` que nous allons
utiliser pour gérer la mise à disposition de notre API. 

{{< include "./applications/_appli18b.qmd" >}}

On peut remarquer quelques voies d'amélioration de notre approche qui
seront ultérieurement traitées:

- L'entraînement du modèle
est ré-effectué à chaque lancement d'un nouveau conteneur. 
On relance donc autant de fois un entraînement qu'on déploie
de conteneurs pour répondre à nos utilisateurs. Ce sera
l'objet de la partie MLOps de fiabiliser et optimiser
cette partie du _pipeline_. 
- il est nécessaire de (re)lancer manuellement  `kubectl apply -f deployment/`
à chaque changement de notre code. Autrement dit, lors de cette application,
on a amélioré
la fiabilité du lancement de notre API mais un lancement manuel est encore indispensable. 
Comme dans le reste de ce cours, on va essayer d'éviter un geste manuel pouvant 
être source d'erreur en privilégiant l'automatisation et l'archivage dans des
scripts. C'est l'objet de la prochaine étape. 


## Etape 3: automatiser le déploiement (déploiement en continu)

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli19
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::


::: {.callout-important}
## Clarification sur la branche de travail

A partir de maintenant, il est nécessaire de clarifier la
branche principale sur laquelle nous travaillons. De manière
traditionnelle, on utilise la branche `main`. Néanmoins,
pour être cohérent avec les instructions du début, qui étaient 
de créer une branche `dev`, tous les exemples ultérieures
partiront de cette hypothèse. 

Si vous avez fait les applications les unes après les autres, et
que vous vous situez toujours sur `dev`, vous pouvez passer 
aux applications suivantes. Si vous avez changé de branche,
vous pouvez continuer mais en tenir compte dans les exemples ultérieurs.

Si vous avez utilisé un `tag` pour sauter une ou plusieurs étapes, il va
être nécessaire de se placer sur une branche car vous êtes en _head detached_. 
Pour cela, après avoir _committé_ les fichiers que vous désirez garder

```{python}
#| eval: false
#| file: "terminal"
#| filename: "terminal"
$ git branch -D dev #<1>
$ git checkout -b dev
$ git push origin dev
```
1. Pas indispensable, mais permet de supprimer la branche `dev` si elle existe.
:::

Qu'est-ce qui peut déclencher une évolution nécessitant de mettre à 
jour l'ensemble de notre processus de production ? 

Regardons à nouveau notre _pipeline_:

![](/workflow2.png)

Les _inputs_ de notre _pipeline_ sont donc:

- La __configuration__. Ici, on peut considérer que notre YAML de configuration relèvent de cette catégorie  ;
- Les __données__. Nos données sont statiques et n'ont pas vocation à évoluer. Si c'était le cas, il faudrait en tenir compte dans notre automatisation. ;
- Le __code__. C'est l'élément principal qui évolue chez nous. On va donc faire en sorte qu'à chaque mise à jour de notre code (un _push_ sur `Github`), les étapes ultérieures (production de l'image `Docker`, etc.) se mettent à compiler. Néanmoins, il va être nécessaire de se discipliner pour ne pas mettre en production n'importe quel code. 

Grâce à `ArgoCD` il est possible d'automatiser la mise en production de notre
application. Au lieu d'être fait manuellement par l'appel à `kubectl apply`,
cela se fera en modifiant les instructions présentes dans le dossier `kubernetes/`
de notre dépôt. Un changement de ces fichiers
va entraîner le redéploiement automatique en synchronisant avec notre dépôt `Github`.

{{< include "./applications/_appli19a.qmd" >}}

La page n'a pas changé. En pratique, nous n'avons pas vraiment organisé la mise en production
et `ArgoCD` nécessite un certain formalisme. 
Dorénavant, il convient de versionner nos
productions pour déclencher dans `ArgoCD` des opérations. 
Ce versionnement va se faire à deux niveaux[^gitopsrepo]:

- Dans le fichier `deployment.yaml` où on va spécifier 
la version de l'image Docker utilisée en production
- Dans le YAML qui nous permet de _pusher_ une image sur `Dockerhub` (`.github/workflows/prod.yml`),
on va versionner notre image `Docker`. 


[^gitopsrepo]: 
    Techniquement, il serait plus judicieux de séparer notre
    projet en deux dépôts: un dépôt pour le code source et un visant à fournir
    les fichiers YAML pour la mise en production.

    ![](https://inseefrlab.github.io/formation-mlops/slides/img/ci-cd.png)


{{< include "./applications/_appli19b.qmd" >}}


Notre API est accessible sans problème depuis `Python` ou notre navigateur
mais si on désire utiliser `JavaScript` pour créer une application
interactive, on va essuyer un refus à cause du [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS).

Permettre à n'importe quel client de se connecter à 
notre API permettra de faire un site web exploitant notre API. 
Comme c'est un point technique qui ne concerne pas les compétences
liées à ce cours, nous donnons directement les mises à jour nécessaires
du projet:

{{< include "./applications/_cors.qmd" >}}

## Etape 4: construire un site web

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli19
git checkout -b dev
git push origin dev
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::

On va proposer un nouveau livrable pour parler à un public plus large.
Pour faire ce site web,
on va utiliser `Quarto` et déployer sur `Github Pages`.

{{< include "./applications/_appli20.qmd" >}}


# Partie 6: adopter une approche MLOps pour améliorer notre modèle

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli20
git checkout -b dev
git push origin dev
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::

Nous allons dans cette partie faire de la
validation croisée. Pour éviter le problème
du [_data leakage_](https://machinelearningmastery.com/data-leakage-machine-learning/),
nous proposons de revoir notre _pipeline_ pour 
exclure la variable `Title` dont certaines modalités
rares posent problème dans les découpages
multiples d'échantillons lors de la validation
croisée. 

## Restructurer le _pipeline_ pour fluidifier la mise en production


{{< include "./applications/_appli21.qmd" >}}


## Garder une trace des entraînements de notre modèle grâce au _register_ de `MLFlow`

::: {.callout-caution collapse="true"}
## Si vous prenez ce projet fil rouge en cours de route

```{.bash filename="terminal"}
git checkout appli21
git checkout -b dev
git push origin dev
```

![](/checkpoint.jpg){width=80% fig-align="center"}

:::


::: {.callout-tip}
## Application 22 : archiver nos entraînements avec `MLFlow`

1. Lancer `MLFlow` depuis l'onflet [Mes services](https://datalab.sspcloud.fr/catalog/automation) du SSPCloud.
Attendre que le service soit bien lancé. 
Cela créera un service dont l'URL est de la forme `https://user-{username}-{pod_id}.user.lab.sspcloud.fr`,
où `pod_id` est un identifiant aléatoire. Ce service `MLFlow` communiquera avec les `VSCode` que vous
ouvrirez ultérieurement à partir de cet URL ainsi qu'avec le système de stockage `S3`[^tokenMLFlow].

2. Regarder la page `Experiments`. Elle est vide à ce stade, c'est normal

[^tokenMLFlow]: Par conséquent, `MLFLow` bénéficie de l'injection automatique des _tokens_
pour pouvoir lire/écrire sur S3. Ces jetons ont la même durée avant expiration que ceux
de vos services interactifs `VSCode`. Il faut donc supprimer et rouvrir un service `MLFLow`
régulièrement. La manière d'éviter cela
est de créer des _service account_ sur [https://minio-console.lab.sspcloud.fr/](https://minio-console.lab.sspcloud.fr/login)
et de les renseigner sur [la page](https://datalab.sspcloud.fr/project-settings/s3-configs). 


2. Une fois le service `MLFlow` fonctionnel,
lancer un nouveau `VSCode` pour bénéficier de la configuration
automatisée

2. Clôner votre projet, vous situer sur la branche de travail (nous
supposerons qu'il s'agit de `dev`).

4. Depuis un terminal `Python`, lancer les commandes suivantes:

```python
import mlflow
mlflow_experiment_name = "titanicml"
mlflow.set_experiment(experiment_name=mlflow_experiment_name)
```

Retourner sur l'UI et observer la différence, à gauche. 

5. Créer un fichier `src/models/log.py`

<details>
<summary>
Contenu du fichier `src/models/log.py`
</summary>
```{.python include="./applications/code/appli22_log.py" filename="src/models/log.py"}
```

</details>


6. Modifier le fichier `train.py` pour ajouter la ligne

```{.python}
mlog.log_gsvc_to_mlflow(pipe_cross_validation, EXPERIMENT_NAME, APPLI_ID)
```

avec 

```{.python}
import src.models.log as mlog
```

7. Faire tourner avec le paramètre `--appli appli22`:

```{.bash filename="terminal"}
python train.py --appli appli22
```

8. Observer l'évolution de la page `Experiments`. Cliquer sur un des _run_. 
Observer toutes les métadonnées archivées (hyperparamètres, métriques d'évaluation, `requirements.txt` dont `MLFlow` a fait l'inférence, etc.)

9. Observer le code proposé par `MLFlow` pour récupérer le _run_ en question. Modifier le fichier `eval.py` à partir de cet exemple et du modèle suivant pour utiliser un des modèles archivés dans `MLFlow` 

10. Retourner à la liste des _runs_ en cliquant à nouveau sur _"titanicml"_ dans les expérimentations
11. Dans l'onglet `Table`, sélectionner plusieurs expérimentations, cliquer sur `Columns` et ajouter `mean_test_f1`. 
Ajuster la taille des colonnes pour la voir et classer les modèles par score décroissants
12. Cliquer sur `Compare` après en avoir sélectionné plusieurs. Afficher un _scatterplot_ des performances
en fonction du nombre d'estimateurs. Conclure. 

:::


::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git stash #<1>
git checkout appli22
```
1. Pour annuler les modifications depuis le dernier _commit_


![](/checkpoint.jpg){width=80% fig-align="center"}

:::

Cette appplication illustre l'un des premiers apports de `MLFlow`: on garde
une trace de nos expérimentations et on peut déjà mieux comprendre
la manière dont certains paramètres de notre modèle peuvent influencer
la qualité de nos prédictions. 

Néanmoins, persistent un certain nombre de voies d'amélioration:

- On entraîne le modèle en local, de manière séquentielle, et en lançant nous-mêmes le script `train.py`
- On n'archive pas les jeux de données associés à ces modèles (les jeux d'entraînement et de test). On doit alors 
le faire manuellement si on désire évaluer les performances _ex post_, ce qui est pénible.
- On récupère manuellement les modèles ce qui n'est pas très pérenne.
- Notre API n'utilise pas encore l'un des modèles archivé sur `MLFlow`. 

Les prochaines applications permettront d'améliorer ceci.

### Exo modèle en prod

On met un modèle en prod
On l'utilise dans l'API

### Exo entraînements sur MLFLow

y a que ça de vrai
