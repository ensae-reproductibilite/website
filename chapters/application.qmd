---
title: "Application"
subtitle: "Appliquer pas à pas les concepts étudiés à un projet de data science"
author: "Romain Avouac et Lino Galiana"
image: images/rocket.png
description: |
  Une application fil rouge pour illustrer l'intérêt d'appliquer graduellement les bonnes pratiques dans une optique de mise en production d'une application de data science.
order: 8
href: chapters/application.html
---

<details>
<summary>
Dérouler les _slides_ ci-dessous ou [cliquer ici](https://ensae-reproductibilite.github.io/slides/#/title-slide)
pour afficher les slides en plein écran.
</summary>


<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://ensae-reproductibilite.github.io/slides/#/title-slide"></iframe></div>

</details>

L'objectif de cette mise en application est d'**illustrer les différentes étapes qui séparent la phase de développement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.

Celle-ci est un tutoriel pas à pas pour avoir un projet reproductible et disponible sous plusieurs livrables. 
Toutes les étapes ne sont pas indispensables à tous les projets de _data science_. 

Nous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d'un projet de data science.
On a un _notebook_ un peu monolithique, qui réalise les étapes classiques d'un *pipeline* de *machine learning* :

- Import de données ;
- Statistiques descriptives et visualisations ;
- *Feature engineering* ;
- Entraînement d'un modèle ;
- Evaluation du modèle.

**L'objectif est d'améliorer le projet de manière incrémentale jusqu'à pouvoir le mettre en production, en le valorisant sous une forme adaptée.** 


<details>
<summary>
Illustration de notre point de départ
</summary>
![](/workflow1.png)
</details>

<details>
<summary>
Illustration de l'horizon vers lequel on se dirige
</summary>
![](/workflow2.png)
</details>

::: {.callout-important}
Il est important de bien lire les consignes et d'y aller progressivement.
Certaines étapes peuvent être rapides, d'autres plus fastidieuses ;
certaines être assez guidées, d'autres vous laisser plus de liberté.
Si vous n'effectuez pas une étape, vous risquez de ne pas pouvoir passer à
l'étape suivante qui en dépend.

Bien que l'exercice soit applicable sur toute configuration bien faite, nous 
recommandons de privilégier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), où tous les 
outils nécessaires sont pré-installés et pré-configurés. Le service `VSCode`
ne sera en effet que le point d'entrée pour l'utilisation d'outils plus exigeants
sur le plan de l'infrastructure: _Argo_, _MLFLow_, etc.
:::


# Partie 0 : initialisation du projet

::: {.callout-tip}
## Application préliminaire: forker le dépôt d'exemple

Les premières étapes consistent à mettre en place son environnement de travail sur `Github`:

- Générer un jeton d'accès (*token*) sur `GitHub` afin de permettre l'authentification en ligne de commande à votre compte.
La procédure est décrite [ici](https://docs.sspcloud.fr/onyxia-guide/controle-de-version#creer-un-jeton-dacces-token). 
__Vous ne voyez ce jeton qu'une fois, ne fermez pas la page de suite__. 

- Mettez de côté ce jeton en l'enregistrant dans un gestionnaire de mot de passe ou dans 
l'espace _["Mon compte"](https://datalab.sspcloud.fr/account/third-party-integration)_
du `SSP Cloud`. 

- Forker le dépôt `Github` : [https://github.com/ensae-reproductibilite/application-correction](https://github.com/ensae-reproductibilite/application-correction) en faisant attention à deux choses:
    + Renommer le dépôt en `ensae-reproductibilite-application-correction.git` ;
    + Décocher la case _"Copy the `main` branch only"_ afin de copier également les _tags_ `Git` qui nous permettront de faire les _checkpoint_


<details>

<summary>
Ce que vous devriez voir sur la page de création du _fork_
</summary>

![](/fork-example.png)

</details>

Il est maintenant possible de ce lancer dans la création de l'environnement de travail:

- Ouvrir un service `VSCode` sur le [SSP Cloud](https://datalab.sspcloud.fr/home). Vous pouvez aller
dans la page `My Services` et cliquer sur `New service`. Sinon, vous
pouvez initialiser la création du service en cliquant directement [ici](https://datalab.sspcloud.fr/launcher/ide/vscode-python?autoLaunch=false). __Modifier les options suivantes__:
    + Dans l'onglet `Kubernetes`, sélectionner le rôle `Admin` ;
    + Dans l'onglet `Networking`, cliquer sur "Enable a custom service port" et laisser la valeur par défaut 5000 pour le numéro du port

- Clôner __votre__ dépôt `Github` en utilisant le
terminal depuis `Visual Studio` (`Terminal > New Terminal`) et
en passant directement le token dans l'URL selon cette structure:

```{.bash filename="terminal"}
git clone https://<TOKEN>@github.com/<USERNAME>/ensae-reproductibilite-application-correction.git
```

où `<TOKEN>` et `<USERNAME>` sont à remplacer, respectivement, 
par le jeton que vous avez généré précédemment et votre nom d'utilisateur.

- Se placer avec le terminal dans le dossier en question : 

```{.bash filename="terminal"}
cd ensae-reproductibilite-application-correction
```

- Se placer sur une branche de travail en faisant:

```{.bash filename="terminal"}
git checkout -b dev
```

:::


# Partie 1 : qualité du script

Cette première partie vise à **rendre le projet conforme aux bonnes pratiques** présentées dans le cours.

Elle fait intervenir les notions suivantes : 

- Utilisation du **terminal** (voir [Linux 101](/chapters/linux-101.html)) ;
- **Qualité du code** (voir [Qualité du code](/chapters/code-quality.html)) ;
- **Architecture de projets** (voir [Architecture des projets](/chapters/projects-architecture.html)) ;
- **Contrôle de version** avec `Git` (voir [Rappels `Git`](/chapters/git.qmd)) ;
- **Travail collaboratif** avec `Git` et `GitHub` (voir [Rappels `Git`](/chapters/git.qmd)).

Nous allons partir de ce _Notebook_ `Jupyter`,
que vous pouvez prévisualiser voire tester
en cliquant sur l'un des liens suivants:

_to do bouton onyxia_
<a href="https://github.com/ensae-reproductibilite/application-correction/blob/main/titanic.ipynb" target="_blank" rel="noopener"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>

Le plan de la partie est le suivant :

1. S'assurer que le script fonctionne ;
2. Nettoyer le code des scories formelles avec un _linter_ et un _formatter_ ;
3. Paramétrisation du script ;
4. Utilisation de fonctions.


## Étape 1 : s'assurer que le script s'exécute correctement

On va partir du fichier `notebook.py` qui reprend le contenu 
du _notebook_[^jupytext] mais dans un script classique.
Le travail de nettoyage en sera facilité. 

[^jupytext]: L'export dans un script `.py` a été fait
        directement depuis `VSCode`. Comme
        cela n'est pas vraiment l'objet du cours, nous passons cette étape et fournissons
        directement le script expurgé du texte intermédiaire. Mais n'oubliez
        pas que cette démarche, fréquente quand on a démarré sur un _notebook_ et
        qu'on désire consolider en faisant la transition vers des 
        scripts, nécessite d'être attentif pour ne pas risquer de faire une erreur. 

La première étape est simple, mais souvent oubliée : **vérifier que le code fonctionne correctement**. 


{{< include "./applications/_appli1.qmd" >}}


## Étape 2: utiliser un _linter_ puis un _formatter_

On va maintenant améliorer la qualité de notre code en appliquant les standards communautaires.
Pour cela, on va utiliser le *linter* classique [`PyLint`](https://pylint.readthedocs.io/en/latest/)
et le _formatter_ [`Black`](https://github.com/psf/black).

::: {.callout-important}
[`PyLint`](https://pylint.readthedocs.io/en/latest/) et [`Black`](https://black.readthedocs.io/en/stable/)
sont des _packages_ `Python` qui 
s'utilisent principalement en ligne de commande.

Si vous avez une erreur qui suggère
que votre terminal ne connait pas [`PyLint`](https://pylint.readthedocs.io/en/latest/)
ou [`Black`](https://black.readthedocs.io/en/stable/),
n'oubliez pas d'exécuter la commande `pip install pylint` ou `pip install black`.
:::


Le _linter_ renvoie alors une série d'irrégularités,
en précisant à chaque fois la ligne de l'erreur et le message d'erreur associé (ex : mauvaise identation).
Il renvoie finalement une note sur 10,
qui estime la qualité du code à l'aune des standards communautaires évoqués
dans la partie [Qualité du code](/chapters/code-quality.html).

{{< include "./applications/_appli2.qmd" >}}

Le code est maintenant lisible, il obtient à ce stade une note formelle proche de 10.
Mais il n'est pas encore totalement intelligible ou fiable.
Il y a notamment 
beaucoup de redondance de code auxquelles nous allons nous attaquer par la suite. 
Néanmoins, avant cela, occupons-nous de mieux gérer certains paramètres du script: 
jetons d'API et chemin des fichiers.


## Étape 3: gestion des paramètres

L'exécution du code et les résultats obtenus
dépendent de certains paramètres définis dans le code. L'étude de résultats
alternatifs, en jouant sur 
des variantes des (hyper)paramètres, est à ce stade compliquée
car il est nécessaire de parcourir le code pour trouver
ces paramètres. De plus, certains paramètres personnels
comme des jetons
d'API ou des mots de passe n'ont pas vocation à 
être présents dans le code. 

Il est plus judicieux de considérer ces paramètres comme des
variables d'entrée du script. Cela peut être fait de deux
manières:

1. Avec des __arguments optionnels__ appelés depuis la ligne de commande _(Application 3a)_.
Cela peut être pratique pour mettre en oeuvre des tests automatisés[^noteCI] mais
n'est pas forcément pertinent pour toutes les variables. Nous allons montrer
cet usage avec le nombre d'arbres de notre _random forest_ ;
2. En utilisant un __fichier de configuration__ dont les valeurs sont importées dans
le script principal _(Application 3b)_. 


<details>
<summary>
Un exemple de définition d'un argument pour l'utilisation en ligne de commande
</summary>

```{.python filename="prenom.py"}
import argparse
parser = argparse.ArgumentParser(description="Qui êtes-vous?")
parser.add_argument(
    "--prenom", type=str, default="Toto", help="Un prénom à afficher"
)
args = parser.parse_args()
print(args.prenom)
```

Exemples d'utilisations en ligne de commande

```{.bash filename="terminal"}
python prenom.py
python prenom.py --prenom "Zinedine"
```

</details>

{{< include "./applications/_appli3.qmd" >}}


## Étape 4 : Privilégier la programmation fonctionnelle

Nous allons **mettre en fonctions les parties importantes de l'analyse**. 
Ceci facilitera l'étape ultérieure de modularisation de notre projet. 

Cet exercice étant chronophage, il n'est __pas obligatoire de le réaliser en entier__. L'important est de
comprendre la démarche et d'adopter fréquemment une approche fonctionnelle[^POO]. Pour obtenir 
une chaine entièrement fonctionnalisée, vous pouvez reprendre le _checkpoint_.

[^POO]: Nous proposons ici d'adopter le principe de la __programmation fonctionnelle__. Pour encore fiabiliser
un processus, il serait possible d'adopter le paradigme de la __programmation orientée objet (POO)__. Celle-ci est
plus rebutante et demande plus de temps au développeur. L'arbitrage coût-avantage est négatif pour notre
exemple, nous proposons donc de nous en passer. Néanmoins, pour une mise en production réelle d'un modèle,
il est recommandé de l'adopter. C'est d'ailleurs obligatoire avec des [_pipelines_ `scikit`](https://pythonds.linogaliana.fr/pipeline-scikit/). 

{{< include "./applications/_appli4.qmd" >}}

Cela ne se remarque pas encore vraiment car nous avons de nombreuses définitions de fonctions
mais notre chaine de production est beaucoup plus
concise (le script fait environ 300 lignes dont 250 de définitions de fonctions génériques).
Cette auto-discipline facilitera grandement
les étapes ultérieures. Cela aurait été néanmoins beaucoup moins coûteux en temps d'adopter
ces bons gestes de manière plus précoce. 


# Partie 2 : adoption d'une structure modulaire {#partie2}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.
Ce faisant, on s'est déjà considérablement rapprochés d'un
possible partage du code : celui-ci est lisible et intelligible. 
Le code est proprement versionné sur un
dépôt `GitHub`.
Cependant, le projet est encore perfectible: il est encore difficile de rentrer
dedans si on ne sait pas exactement ce qu'on recherche. L'objectif de cette partie
est d'isoler les différentes étapes de notre _pipeline_. 
Outre le gain de clarté pour notre projet, nous économiserons beaucoup de peines
pour la mise en production ultérieure de notre modèle. 

<details>
<summary>
Illustration de l'état actuel du projet 
</summary>
![](/schema_post_appli4.png)
</details>

Dans cette partie nous allons continuer les améliorations
incrémentales de notre projet avec les étapes suivantes:

1. Modularisation du code `Python` pour séparer les différentes
étapes de notre _pipeline_ ; 
2. Adopter une structure standardisée pour notre projet afin
d'autodocumenter l'organisation de celui-ci ; 
3. Documenter les _packages_ indispensables à l'exécution du code ;
4. Stocker les données dans un environnement adéquat
afin de continuer la démarche de séparer conceptuellement les données du code en de la configuration.


## Étape 1 : modularisation

Nous allons profiter de la modularisation pour adopter une structure
applicative pour notre code. Celui-ci n'étant en effet plus lancé
que depuis la ligne de commande, on peut considérer qu'on construit
une application générique où un script principal (`main.py`)
encapsule des éléments issus d'autres scripts. 

{{< include "./applications/_appli5.qmd" >}}


## Étape 2 : adopter une architecture standardisée de projet

On dispose maintenant d'une application `Python` fonctionnelle. 
Néanmoins, le projet est certes plus fiable mais sa structuration
laisse à désirer et il serait difficile de rentrer à nouveau
dans le projet dans quelques temps. 

<details>
<summary>Etat actuel du projet 🙈</summary>

```
├── README.md
├── train.csv
├── test.csv
├── .gitignore
├── config.yaml
├── secrets.yaml
├── import_data.py
├── build_features.py
├── train_evaluate.py
└── main.py
```

</details>

Comme cela est expliqué dans la
partie [Structure des projets](/chapters/projects-architecture.html),
on va adopter une structure certes arbitraire mais qui va 
faciliter l'autodocumentation de notre projet. De plus, une telle structure va faciliter des évolutions optionnelles
comme la _packagisation_ du projet. Passer d'une structure modulaire
bien faite à un _package_ est quasi-immédiat en `Python`. 

On va donc modifier l'architecture de notre projet pour la rendre plus standardisée.
Pour cela, on va s'inspirer des structures
[`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/)
qui génèrent des _templates_ de projet. En l'occurrence
notre source d'inspiration sera le [_template datascience_](https://drivendata.github.io/cookiecutter-data-science/)
issu d'un effort communautaire.

::: {.callout-note}
L'idée de [`cookiecutter`](https://cookiecutter.readthedocs.io/en/stable/) est de proposer des _templates_ que l'on utilise pour __initialiser__ un projet, afin de bâtir à l'avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante : 

```{.bash filename="terminal"}
$ pip install cookiecutter
$ cookiecutter https://github.com/drivendata/cookiecutter-data-science
```

Ici, on a déjà un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.
:::

En s'inspirant du _cookiecutter data science_
on va adopter la structure suivante:

<details>
<summary>
Structure recommandée
</summary>

```
ensae-reproductibilite-application
├── main.py
├── README.md
├── data
│   └── raw
│       ├── test.csv
│       └── train.csv
├── configuration
│   └── config.yaml
├── notebooks
│   └── titanic.ipynb
└── src
    ├── data
    │   └── import_data.py
    ├── features
    │   └── build_features.py
    └── models
        └── train_evaluate.py
```

</details>

{{< include "./applications/_appli6.qmd" >}}


## Étape 3: indiquer l'environnement minimal de reproductibilité

Le script `main.py` nécessite un certain nombre de packages pour
être fonctionnel. Chez vous les packages nécessaires sont
bien sûr installés mais êtes-vous assuré que c'est le cas 
chez la personne qui testera votre code ? 

Afin de favoriser la portabilité du projet,
il est d'usage de _"fixer l'environnement"_,
c'est-à-dire d'indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version.
Nous proposons de créer un fichier `requirements.txt` minimal, sur lequel nous reviendrons
dans la partie consacrée aux environnements reproductibles. 

Le fichier `requirements.txt` est conventionnellement localisé à la racine du projet.
Ici on ne va pas fixer les versions, on raffinera ce fichier ultérieurement.

{{< include "./applications/_appli7.qmd" >}}

## Étape 4 : stocker les données de manière externe {#stockageS3}

::: {.callout-warning collapse="true"}
## Pour en savoir plus sur le système de stockage `S3`

Pour mettre en oeuvre cette étape, il peut être utile de
comprendre un peu comme fonctionne le SSP Cloud.
Vous devrez suivre la [documentation du SSP Cloud](https://inseefrlab.github.io/docs.sspcloud.fr/docs/fr/storage.html) pour la réaliser. Une aide-mémoire est également disponible dans le cours
de 2e année de l'ENSAE [Python pour la data science](https://linogaliana-teaching.netlify.app/reads3/#).
:::


Le chapitre sur la [structure des projets](/chapters/project-structure.qmd)
développe l'idée qu'il est recommandé de converger vers un modèle
où environnements d'exécution, de stockage du code et des données sont conceptuellement
séparés. Ce haut niveau d'exigence est un gain de temps important 
lors de la mise en production car au cours de cette dernière, le projet
est amené à être exécuté sur une infrastructure informatique dédiée
qu'il est bon d'anticiper. 

A l'heure actuelle, les données sont stockées dans le dépôt. C'est une
mauvaise pratique. En premier lieu, `Git` n'est techniquement
pas bien adapté au stockage de données. Ici ce n'est pas très grave
car il ne s'agit pas de données volumineuses et ces dernières ne sont
pas modifiées au cours de notre chaine de traitement. 
La raison principale
est que les données traitées par les _data scientists_ 
sont généralement soumises à des clauses de
confidentialités ([RGPD](https://www.cnil.fr/fr/rgpd-de-quoi-parle-t-on), [secret statistique](https://www.insee.fr/fr/information/1300624)...). Mettre ces données sous contrôle de version
c'est prendre le risque de les divulguer à un public non habilité. 
Il est donc recommandé de privilégier des outils techniques adaptés au
stockage de données.

L'idéal, dans notre cas, est d'utiliser une solution de stockage externe. 
On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud. 
Cela nous permettra de supprimer les données de `Github` tout en maintenant la reproductibilité 
de notre projet [^history].

[^history]: Attention, les données ont été _committées_ au moins une fois. Les supprimer
du dépôt ne les efface pas de l'historique. Si cette erreur arrive, le mieux est de supprimer
le dépôt en ligne, créer un nouvel historique `Git` et partir de celui-ci pour des publications
ultérieures sur `Github`. Néanmoins l'idéal serait de ne pas s'exposer à cela. C'est justement
l'objet des bonnes pratiques de ce cours: un `.gitignore` bien construit et une séparation des
environnements de stockage du code et
des données seront bien plus efficaces pour vous éviter ces problèmes que tout les conseils de 
vigilance que vous pourrez trouver ailleurs. 

{{< include "./applications/_appli8.qmd" >}}

# Partie 2bis: packagisation de son projet (optionnel)

Cette série d'actions n'est pas forcément pertinente pour tous
les projets. Elle fait un peu la transition entre la modularité
et la portabilité. 

## Étape 1 : proposer des tests unitaires (optionnel)

Notre code comporte un certain nombre de fonctions génériques.
On peut vouloir tester leur usage sur des données standardisées,
différentes de celles du Titanic.

Même si la notion de tests unitaires
prend plus de sens dans un _package_, nous pouvons proposer
dans le projet des exemples d'utilisation de la fonction, ceci peut être pédagogique. 

Nous allons utiliser [`unittest`](https://docs.python.org/3/library/unittest.html)
pour effectuer des tests unitaires. Cette approche nécessite quelques notions
de programmation orientée objet ou une bonne discussion avec `ChatGPT`.

{{< include "./applications/_appli9.qmd" >}}


::: {.callout-note}

Lorsqu'on effectue des tests unitaires, on cherche généralement
à tester le plus de lignes possibles de son code. On parle de
__taux de couverture__ (_coverage rate_) pour désigner
la statistique mesurant cela. 

Cela peut s'effectuer de la manière suivante avec le package
[`coverage`](https://coverage.readthedocs.io/en/7.2.2/):

```{.bash filename="terminal"}
coverage run -m unittest tests/test_create_variable_title.py
coverage report -m
```

```{.python}
Name                                  Stmts   Miss  Cover   Missing
-------------------------------------------------------------------
src/features/build_features.py           34     21    38%   35-36, 48-58, 71-74, 85-89, 99-101, 111-113
tests/test_create_variable_title.py      21      1    95%   54
-------------------------------------------------------------------
TOTAL                                    55     22    60%
```

Le taux de couverture est souvent mis en avant par les gros
projets comme indicateur de leur qualité. Il existe d'ailleurs
des badges `Github` dédiés. 
:::




## Étape 2 : transformer son projet en package (optionnel)

Notre projet est modulaire, ce qui le rend assez simple à transformer
en _package_, en s'inspirant de la structure du `cookiecutter` adapté, issu
de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

On va créer un _package_ nommé `titanicml` qui encapsule
tout notre code et qui sera appelé
par notre script `main.py`. La structure attendue
est la suivante:

<details>
<summary>Structure visée</summary>

```
ensae-reproductibilite-application
├── docs                                    ┐ 
│   ├── main.py                             │ 
│   └── notebooks                           │ Package documentation and examples
│       └── titanic.ipynb                   │ 
├── configuration                           ┐ Configuration (pas à partager avec Git)
│   └── config.yaml                         ┘ 
├── README.md                                
├── pyproject.toml                          ┐ 
├── requirements.txt                        │
├── titanicml                               │                
│   ├── __init__.py                         │ Package source code, metadata
│   ├── config.yaml                         │ and build instructions 
│   ├── import_data.py                      │
│   ├── build_features.py                   │ 
│   └── train_evaluate.py                   ┘
└── tests                                   ┐
    └── test_create_variable_title.py       ┘ Package tests
```
</details>

<details>
<summary>Rappel: structure actuelle</summary>

```
ensae-reproductibilite-application
├── notebooks                                 
│   └── titanic.ipynb                  
├── configuration                                 
│   └── config.yaml                  
├── main.py                              
├── README.md                 
├── requirements.txt                      
└── src 
    ├── data                                
    │   ├── import_data.py                    
    │   └── test_create_variable_title.py      
    ├── features                           
    │   └── build_features.py      
    └── models                          
        └── train_evaluate.py              
```
</details>

Il existe plusieurs 
_frameworks_ pour
construire un _package_. Nous
allons privilégier [`Poetry`](https://python-poetry.org/)
à [`Setuptools`](https://pypi.org/project/setuptools/). 


::: {.callout-note}

Pour créer la structure minimale d'un _package_, le plus simple est
d'utiliser le `cookiecutter` adapté,
issu de [cet ouvrage](https://py-pkgs.org/03-how-to-package-a-python#package-structure).

Comme on a déjà une structure très modulaire, on va plutôt recréer cette
structure dans notre projet déjà existant. En fait, il ne manque qu'un fichier essentiel, 
le principal distinguant un projet classique d'un package : `pyproject.toml`.

```{.bash filename="terminal"}
cookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git
```

<details>
<summary>Dérouler pour voir les choix possibles</summary>
```{.python}
author_name [Monty Python]: Daffy Duck
package_name [mypkg]: titanicml
package_short_description []: Impressive Titanic survival analysis
package_version [0.1.0]: 
python_version [3.9]: 
Select open_source_license:
1 - MIT
2 - Apache License 2.0
3 - GNU General Public License v3.0
4 - Creative Commons Attribution 4.0
5 - BSD 3-Clause
6 - Proprietary
7 - None
Choose from 1, 2, 3, 4, 5, 6 [1]: 
Select include_github_actions:
1 - no
2 - ci
3 - ci+cd
Choose from 1, 2, 3 [1]:
```
</details>

:::

{{< include "./applications/_appli10.qmd" >}}

# Partie 3 : construction d'un projet portable et reproductible {#partie3}

Dans la partie précédente,
on a appliqué de manière incrémentale de nombreuses bonnes pratiques vues
dans les chapitres [Qualité du code](/chapters/code-quality.html)
et [Structure des projets](/chapters/projects-architecture.html)
tout au long du cours.

Ce faisant, on s'est déjà considérablement rapprochés d'une
possible mise en production : le code est lisible,
la structure du projet est normalisée et évolutive,
et le code est proprement versionné sur un
dépôt `GitHub` {{< fa brands github >}}.


<details>
<summary>
Illustration de l'état actuel du projet 
</summary>
![](/schema_post_appli8.png)
</details>



A présent, nous avons une version du projet qui est largement partageable.
Du moins en théorie, car la pratique est souvent plus compliquée :
il y a fort à parier que si vous essayez d'exécuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),
les choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'état, le projet n'est pas portable : il n'est pas possible, sans modifications coûteuses, de l'exécuter dans un environnement différent de celui dans lequel il a été développé**.

Dans cette troisème partie de notre travail vers la mise en production,
nous allons voir 
comment **normaliser l'environnement d'exécution afin de produire un projet portable**.
Autrement dit, nous n'allons plus nous contenter de modularité mais allons rechercher
la portabilité.
On sera alors tout proche de pouvoir mettre le projet en production.

On progressera dans l'échelle de la reproductibilité 
de la manière suivante: 

1. [**Environnements virtuels**](#anaconda) ;
2. Créer un [script shell](#shell) qui permet, depuis un environnement minimal, de construire l'application de A à Z ;
3. [**Images et conteneurs `Docker`**](#docker).


Nous allons repartir de l'application 8, c'est-à-dire d'un projet
modulaire mais qui n'est pas, à strictement parler, un _package_
(objet des applications optionnelles suivantes 9 et 10). 

Pour se replacer dans l'état du projet à ce niveau,
il est possible d'utiliser le _tag_ _ad hoc_.

```{.bash filename="terminal"}
git checkout appli8
```


## Étape 1 : un environnement pour rendre le projet portable {#anaconda}

Pour qu'un projet soit portable, il doit remplir deux conditions:

- Ne pas nécessiter de dépendance
qui ne soient pas renseignées quelque part ;
- Ne pas proposer des dépendances inutiles, qui ne
sont pas utilisées dans le cadre du projet. 

Le prochain exercice vise à mettre ceci en oeuvre.
Comme expliqué dans le [chapitre portabilité](/chapters/portabilite.qmd),
le choix du gestionnaire d'environnement est laissé
libre. Il est recommandé de privilégier `venv` si vous découvrez
la problématique de la portabilité. 

::: {.panel-tabset group="language"}

## Environnement virtuel `venv`

L'approche la plus légère est l'environnement virtuel. 
Nous avons en fait implicitement déjà commencé à aller vers
cette direction
en créant un fichier `requirements.txt`. 

{{< include "./applications/_appli11a.qmd" >}}


## Environnement `conda`

Les environnements `conda` sont plus lourds à mettre en oeuvre que les 
environnements virtuels mais peuvent permettre un contrôle
plus formel des dépendances. 

{{< include "./applications/_appli11b.qmd" >}}

:::


## Étape 2: construire l'environnement de notre application via un script `shell` {#shell}

Les environnements virtuels permettent de mieux spécifier les dépendances de notre projet, mais ne permettent pas de garantir une portabilité optimale. Pour cela, il faut recourir à la technologie des conteneurs. L'idée est de construire une machine, en partant d'une base quasi-vierge, qui permette de construire étape par étape l'environnement nécessaire au bon fonctionnement de notre projet. C'est le principe des conteneurs `Docker` {{< fa brands docker >}}.

Leur méthode de construction étant un peu difficile à prendre en main au début, nous allons passer par une étape intermédiaire afin de bien comprendre le processus de production. 

- Nous allons d'abord créer un script `shell`, c'est à dire une suite de commandes `Linux` permettant de construire l'environnement à partir d'une machine vierge ;
- Nous transformerons celui-ci en `Dockerfile` dans un deuxième temps. C'est l'objet de l'étape suivante. 

::: {.panel-tabset group="language"}

## Environnement virtuel `venv`

{{< include "./applications/_appli12a.qmd" >}}

## Environnement `conda`

{{< include "./applications/_appli12b.qmd" >}}

:::


## Étape 3: conteneuriser l'application avec `Docker` {#docker}


::: {.callout-note}
Cette application nécessite l'accès à une version interactive de `Docker`.
Il n'y a pas beaucoup d'instances en ligne disponibles.

Nous proposons deux solutions:

- [Installer `Docker`](https://docs.docker.com/get-docker/) sur sa machine ;
- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_

Sinon, elle peut être réalisée en essai-erreur par le biais des services d'intégration continue de `Github` {{< fa brands github >}} ou `Gitlab` {{< fa brands gitlab >}}. Néanmoins, nous présenterons l'utilisation de ces services plus tard, dans la prochaine partie. 
:::

Maintenant qu'on sait que ce script préparatoire fonctionne, on va le transformer en `Dockerfile` pour anticiper la mise en production.  Comme la syntaxe `Docker` est légèrement différente de la syntaxe `Linux` classique (voir le [chapitre portabilité](/chapters/portabilite.qmd)), il va être nécessaire de changer quelques instructions mais ceci sera très léger.

On va tester le `Dockerfile` dans un environnement bac à sable pour ensuite
pouvoir plus facilement automatiser la construction de l'image
`Docker`.

{{< include "./applications/_appli13.qmd" >}}


# Partie 4 : automatisation avec l'intégration continue

Une image `Docker` est un livrable qui n'est pas forcément intéressant
pour tous les publics. Certains préféreront avoir un plat bien préparé
qu'une recette. Nous allons donc proposer d'aller plus loin en proposant
plusieurs types de livrables.

Cela va nous amener à découvrir les outils
du `CI/CD` (_Continuous Integration / Continuous Delivery_)
qui sont au coeur de l'approche `DevOps`.

Notre approche appliquée
au _machine learning_ va nous entraîner plutôt du côté du `MLOps` qui devient
une approche de plus en plus fréquente dans l'industrie de la 
_data science_.

Nous allons améliorer notre approche de trois manières:

- Automatisation de la création de l'image `Docker` et tests
automatisés de la qualité du code ;
- Production d'un site _web_ automatisé permettant de documenter et
valoriser le modèle de _Machine Learning_ ;
- Mise à disposition du modèle entraîné par le biais d'une API pour
ne pas le ré-entraîner à chaque fois et faciliter sa réutilisation ;

A chaque fois, nous allons d'abord tester en local notre travail puis
essayer d'automatiser cela avec les outils de `Github`.

On va ici utiliser l'intégration continue pour deux objectifs distincts:

- la mise à disposition de l'image `Docker` ;
- la mise en place de tests automatisés de la qualité du code
sur le modèle de notre `linter` précédent 

Nous allons utiliser `Github Actions` pour cela. 


## Étape 1: mise en place de tests automatisés

Avant d'essayer de mettre en oeuvre la création de notre image
`Docker` de manière automatisée, nous allons présenter la logique
de l'intégration continue en testant de manière automatisée
notre script `main.py`.

Pour cela, nous allons partir de la structure proposée dans l'[action officielle](https://github.com/actions/setup-python). 
La documentation associée est [ici](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)


::: {.callout-tip}

## Application 14: premier script d'intégration continue

A partir de l'exemple présent
dans la [documentation officielle](https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python)
de `Github`, on a déjà une base de départ qui peut être modifiée:

1. Créer un fichier `.github/workflows/ci.yaml` avec le contenu de l'exemple de la documentation
2. Retirer la `strategy matrix` et ne tester qu'avec la version `3.10` de `Python`
3. Utiliser le fichier `requirements.txt` pour installer les dépendances. 
4. Remplacer `russ` par `pylint` pour vérifier la qualité du code. Ajouter l'argument `--fail-under=6` pour
renvoyer une erreur en cas de note trop basse[^hook]
5. Plutôt que `pytest`, utiliser `python main.py` pour tester que la matrice de confusion s'affiche bien.

[^hook]: Il existe une approche alternative pour faire des tests
    réguliers: les _hooks_ `Git`.
    Il s'agit de règles qui doivent être satisfaites pour que le 
    fichier puisse être committé. Cela assurera que chaque `commit` remplisse
    des critères de qualité afin d'éviter le problème de la procrastination.
    
    La [documentation de pylint](https://pylint.pycqa.org/en/latest/user_guide/pre-commit-integration.html) offre des explications supplémentaires. 


<details>
<summary>Fichier `.github/workflows/ci.yaml` obtenu</summary>

```yaml
name: Build, test and push

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pylint
          pip install -r requirements.txt
      - name: Lint
        run: |
          pylint src --fail-under=6
      - name: Test workflow
        run: |
          python main.py
```
</details>

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git checkout appli14
```

:::

 
Maintenant, nous pouvons observer que l'onglet `Actions`
s'est enrichi. Chaque `commit` va entraîner une action pour
tester nos scripts.

Si la note est mauvaise, nous aurons
une croix rouge (et nous recevrons un mail). On pourra ainsi détecter,
en développant son projet, les moments où on dégrade la qualité du script 
afin de la rétablir immédiatemment. 




## Étape 2: Automatisation de la livraison de l'image `Docker`

Maintenant, nous allons automatiser la mise à disposition de notre image
sur `DockerHub`. Cela facilitera sa réutilisation mais aussi des
valorisations ultérieures.

Là encore, nous allons utiliser une série d'actions pré-configurées.

Pour que `Github` puisse s'authentifier auprès de `DockerHub`, il va 
falloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser
un jeton (_token_) `DockerHub` que nous allons mettre dans un espace
sécurisé associé à votre dépôt `Github`.

::: {.callout-tip}

## Application 15a: configuration

- Se rendre sur
https://hub.docker.com/ et créer un compte.
- Créer un dépôt public `application-correction`
- Aller dans les paramètres (https://hub.docker.com/settings/general)
et cliquer, à gauche, sur `Security`
- Créer un jeton personnel d'accès, ne fermez pas l'onglet en question,
vous ne pouvez voir sa valeur qu'une fois. 
- Dans votre dépôt `Github`, cliquer sur l'onglet `Settings` et cliquer,
à gauche, sur `Actions`. Sur la page qui s'affiche, cliquer sur `New repository secret`
- Donner le nom `DOCKERHUB_TOKEN` à ce jeton et copier la valeur. Valider
- Créer un deuxième secret nommé `DOCKERHUB_USERNAME` ayant comme valeur le nom d'utilisateur
que vous avez créé sur `Dockerhub`
:::


A ce stade, nous avons donné les moyens à `Github` de s'authentifier avec
notre identité sur `Dockerhub`. Il nous reste à mettre en oeuvre l'action
en s'inspirant de https://github.com/docker/build-push-action/#usage.
On ne va modifier que trois éléments dans ce fichier. Effectuer les 
actions suivantes:

::: {.callout-tip}

## Application 15b: automatisation de l'image `Docker`

- En s'inspirant de ce [_template_](https://github.com/marketplace/actions/build-and-push-docker-images), ajouter un nouveau job `docker` dans le fichier `ci.yaml` qui va *build* et *push* l'image sur le `DockerHub`
- Définir le job `test` comme prérequis du job `docker` en vous référant à cette [documentation](https://docs.github.com/en/actions/using-jobs/using-jobs-in-a-workflow#defining-prerequisite-jobs)
- Changer le tag à la fin pour mettre `<username>/application-correction:latest`
où `username` est le nom d'utilisateur sur `DockerHub`;

<details>
<summary>Fichier `.github/workflows/ci.yaml` obtenu</summary>

```yaml
name: Build, test and push

on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pylint
          pip install -r requirements.txt
      - name: Lint
        run: |
          pylint src --fail-under=6
      - name: Test workflow
        run: |
          python main.py
  docker:
    runs-on: ubuntu-latest
    needs: test
    steps:
      - name: Set up QEMU
        uses: docker/setup-qemu-action@v2
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      - name: Login to Docker Hub
        uses: docker/login-action@v2
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}
      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          push: true
          tags: linogaliana/application-correction:latest
```
</details>

- Faire un `commit` et un `push` de ces fichiers
:::

Comme on est fier de notre travail, on va afficher ça avec un badge sur le 
`README`. 

::: {.callout-tip}

## Application 15c: Afficher un badge dans le `README`

- Se rendre dans l'onglet `Actions` et cliquer sur un des scripts en train de tourner. 
- En haut à droite, cliquer sur `...`
- Sélectionner `Create status badge`
- Récupérer le code `Markdown` proposé
- Copier dans le `README` depuis `VSCode`
- Faire de même pour l'autre _workflow_

:::

Maintenant, il nous reste à tester notre application dans l'espace bac à sable
ou en local, si `Docker` est installé.


::: {.callout-tip}

## Application 15d: Tester l'application

- Se rendre sur l'environnement bac à sable _[Play with Docker](https://labs.play-with-docker.com)_
- Récupérer l'image :

```yaml
docker pull <username_dockerhub>/application-correction:latest
```

- Tester le bon fonctionnement de l'image

```yaml
docker run -it <username_dockerhub>/application-correction:latest
```

:tada: La matrice de confusion doit s'afficher ! Vous avez grandement
facilité la réutilisation de votre image. 

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git checkout appli15
```

:::


# Partie 5: mise en production d'une API servant un modèle de machine learning

## Étape 1: création d'un pipeline `scikit`


Notre code respecte des bonnes pratiques formelles. Cependant, la mise en
production nécessite d'être exigeant sur la mise en oeuvre opérationnelle
de notre _pipeline_. 

Quand  on utilise `scikit`, la bonne pratique est d'utiliser
les [_pipelines_](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
qui sécurisent les étapes de _feature engineering_ avant la mise en oeuvre d'un modèle (que
ce soit pour l'entraînement ou le test sur un nouveau jeu de données). 

On va donc devoir refactoriser notre application pour utiliser un _pipeline_ `scikit`. 
Les raisons sont expliquées [ici](https://scikit-learn.org/stable/common_pitfalls.html).
Cela aura
aussi l'avantage de rendre les étapes plus lisibles. 


::: {.callout-tip}

## Application 16: Un _pipeline_ de _machine learning_

- Refactoriser le code de `random_forest_titanic` pour créer 
un vrai pipeline de _preprocessing_ avant la modélisation

- Simplifier la fonction `split_train_test_titanic`
en la réduisant au découpage train/test

- Modifier `main.py` pour que ce soit à ce niveau
qu'a lieu le découpage en train/test, l'entrainement
et l'évaluation
du modèle

:::


::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git checkout appli16
```

:::

## Étape 2: développer une API en local

::: {.callout-tip}

## Application 17: Mise à disposition sous forme d'API locale

- Créer un nouveau service `SSPCloud` en paramétrant dans l'onglet
`Networking` le port 5000 ;
- Cloner le dépôt et se placer au niveau de l'application précédente (`git checkout appli16`)
- Installer `fastAPI` et `uvicorn` puis les ajouter au `requirements.txt`
- Renommer le fichier `main.py` en `train.py` et insérer le contenu suivant dedans :

<details>
<summary>
Fichier `train.py`
</summary>
Récupérer le contenu sur [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/appli17/train.py)
</details>

- Créer le fichier `api.py` permettant d'initialiser l'API:

<details>
<summary>
Fichier `api.py`
</summary>
Récupérer le contenu sur [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/appli17/api.py)
</details>

- Exécuter `train.py` pour stocker en local le modèle entraîné
- Ajouter `model.joblib` au `.gitignore`
- Déployer en local l'API avec la commande

```{.bash filename="terminal"}
uvicorn api:app --reload --host "0.0.0.0" --port 5000
```

- A partir du `README` du service, se rendre sur l'URL de déploiement, 
ajouter `/docs/` à celui-ci et observer la documentation de l'API 
- Se servir de la documentation pour tester les requêtes `/predict`
- Récupérer l'URL d'une des requêtes proposées. La tester dans le navigateur
et depuis `Python` avec `requests` (`requests.get(url).json()`)

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git checkout appli17
```

:::

## Étape 3: déployer l'API

A ce stade, nous avons déployé l'API seulement localement, dans le cadre d'un service. Ce mode de déploiement est très pratique pour la phase de développement, afin de s'assurer que l'API fonctionne comme attendu. A présent, il est temps de passer à l'étape de déploiement, qui permettra à notre API d'être accessible via une URL sur le web, et donc aux utilisateurs potentiels de la requêter. Pour se faire, on va utiliser les possibilités offertes par `Kubernetes`, sur lequel est basé le [SSP Cloud](https://datalab.sspcloud.fr).

::: {.callout-tip}

## Application 18: Dockeriser l'API

- Modifier le `Dockerfile` pour tenir compte des changements dans les noms de fichier effecutés dans l'application précédente

- Créer un script `run.sh` à la racine du projet qui lance le script `train.py` puis déploie localement l'API 

<details>
<summary>Fichier `run.sh`</summary>

```{.bash filename="terminal"}
#/bin/bash

python3 train.py
uvicorn api:app --reload --host "0.0.0.0" --port 5000
```
</details>

- Donner au script `run.sh` des permissions d'exécution : `chmod +x run.sh`

- Changer l'instruction `CMD` du `Dockerfile` pour exécuter le script `run.sh` au lancement du conteneur

- *Commit* et *push* les changements

- Une fois le CI terminé, récupérer la nouvelle image dans l'environnement de test et vérifier que l'API se déploie correctement

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git checkout appli18
```
:::

::: {.callout-tip}

## Application 19: Déployer l'API

- Créer un dossier `deployment` à la racine du projet qui va contenir les fichiers de configuration nécessaires pour déployer sur un cluster `Kubernetes`

- En vous inspirant de la [documentation](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment), y ajouter un premier fichier `deployment.yaml` qui va spécifier la configuration du *Pod* à lancer sur le cluster

<details>
<summary>Fichier `deployment/deployment.yaml`</summary>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: titanic-deployment
  labels:
    app: titanic
spec:
  replicas: 1
  selector:
    matchLabels:
      app: titanic
  template:
    metadata:
      labels:
        app: titanic
    spec:
      containers:
      - name: titanic
        image: linogaliana/application-correction:latest
        ports:
        - containerPort: 5000
```
</details>

- En vous inspirant de la [documentation](https://kubernetes.io/fr/docs/concepts/services-networking/service/#d%C3%A9finition-d-un-service), y ajouter un second fichier `service.yaml` qui va créer une ressource `Service` permettant de donner une identité fixe au `Pod` précédemment créé au sein du cluster

<details>
<summary>Fichier `deployment/service.yaml`</summary>

```yaml
apiVersion: v1
kind: Service
metadata:
  name: titanic-service
spec:
  selector:
    app: titanic
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5000
```
</details>

- En vous inspirant de la [documentation](https://kubernetes.io/docs/concepts/services-networking/ingress/#the-ingress-resource), y ajouter un troisième fichier `ingress.yaml` qui va créer une ressource `Ingress` permettant d'exposer le service via une URL en dehors du cluster

<details>
<summary>Fichier `deployment/ingress.yaml`</summary>

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: titanic-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - titanic.kub.sspcloud.fr
  rules:
  - host: titanic.kub.sspcloud.fr
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: titanic-service
            port:
              number: 80
```
</details>

- Appliquer ces fichiers de configuration sur le cluster : `kubectl apply -f deployement/`

- Si tout a correctement fonctionné, vous devriez pouvoir accéder à l'API à l'URL spécifiée dans le fichier `deployment/ingress.yaml`

:::

::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git checkout appli19
```

:::


# Partie 6: un workflow complet de MLOps

Ce sera l'an prochain, désolé !

# Partie 7: livrer un site web de manière automatisée

On va proposer un nouveau livrable pour parler à un public plus large.
Pour cela, on va déployer un site web statique qui permet de visualiser
rapidement les résultats du modèle.

On propose de créer un site web qui permet de comprendre, avec l'appui
des [valeurs de Shapley](https://christophm.github.io/interpretable-ml-book/shapley.html),
les facteurs qui auraient pu nous mettre la puce
à l'oreille sur les destins de Jake et de Rose. 

Pour faire ce site web,
on va utiliser `Quarto` et déployer sur `Github Pages`.
Des étapes préliminaires sont réalisées en `Python` 
puis l'affichage interactif 
sera contrôlé par du `JavaScript` grâce
à des [blocs `Observable`](https://quarto.org/docs/interactive/ojs/). 


::: {.callout-tip}

## Application 19: Déploiement automatisé d'un site web

Dans un premier temps, on va créer un projet `Quarto`
au sein de notre dépôt: 

- Installer `Quarto` dans votre environnement local (s'il n'est pas déjà disponible) ;
- Dans le projet, utiliser la commande `quarto create-project` pour initialiser le projet `Quarto` ;
- Supprimer le fichier automatiquement généré avec l'extension `.qmd` ;
- Récupérer le contenu du modèle de fichier `Quarto Markdown` [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/index.qmd). Celui-ci permet de générer la page d'accueil de notre site. Enregistrer dans un fichier nommé `index.qmd`

On teste ensuite la compilation en local du fichier:

- Modifier le fichier `train.py` à partir de [cette page](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/train.py) pour être en mesure de compiler le fichier 
- Exécuter le fichier `train.py`
- En ligne de commande, faire `quarto preview` (ajouter les arguments `--port 5000 --host 0.0.0.0` si vous passez par le `SSPCloud`)
- Observer le site web généré en local

Enfin, on va construire et déployer automatiquement ce site web grâce au
combo `Github Actions` et `Github Pages`:

- Créer une branche `gh-pages` à partir du contenu de [cette page](https://quarto.org/docs/publishing/github-pages.html)
- Créer un fichier `.github/workflows/website.yaml` avec le contenu de [ce fichier](https://raw.githubusercontent.com/ensae-reproductibilite/application-correction/tree/appli19/.github/workflows/publish.yaml)

:::

::: {.callout-note}

On doit dans cette application modifier le fichier `train.py`
pour enregistrer en local une duplication du modèle
de _machine learning_ et de l'ensemble d'entraînement
car pour ces deux éléments
on n'est pas allé au bout de la démarche MLOps
d'enregistrement dans un _model registry_ et un
_feature store_.

Dans la prochaine version de ce cours, qui
intègrera `MLFlow`, on aura une démarche plus 
propre car on utilisera bien le modèle de production
et le jeu d'entrainement associé. 
:::


::: {.callout-caution collapse="true"}
## Checkpoint

```{.bash filename="terminal"}
git checkout appli20
```

:::
