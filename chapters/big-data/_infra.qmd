::: {.content-visible when-profile="fr"}

# Infrastructures

## Evolution des infrastructures de données

Historiquement, les données ont été stockées dans des bases de données, c'est à dire des systèmes de stockage et d'organisation de la donnée. Ces objets ont vu le jour dans les années 1950, et ont connu un essor particulier avec les bases de données relationnelles dans les années 1980. Cette technologie se révélant particulièrement pertinente pour organiser le stockage des données "métier" des entreprises, elle a été à la base des *data warehouses*, qui ont longtemps constitué la référence des infrastructures de stockage de la donnée [@chaudhuri1997overview]. Si leur implémentation technique peut être de nature variée, leur principe est simple : des données de sources multiples et hétérogènes sont intégrées dans un système de bases de données relationnel selon des règles métier grâce à des processus dits *ETL* (*extract-transform-load*), afin de les rendre directement accessibles pour une variété d'usages (analyse statistique, *reporting*, etc.) à l'aide d'un langage normalisé : `SQL` (@fig-datawarehouse).

![Architecture d'un *data warehouse*. Source : [airbyte.com](https://airbyte.com/data-engineering-resources/business-intelligence-data-warehouse)](/datawharehouse.png){#fig-datawarehouse fig-align="center"}

Au début des années 2000, la montée en puissance des usages de nature *big data* dans les entreprises met en lumière les limites des *data warehouses* traditionnels. D'une part, les données traités présentent une diversité croissante de formats (structurés, semi-structurés et non structurés) et un format changeant au gré de l'ajout de fonctionnalités sur les plateformes web collectant des données. Ces éléments rentre difficilement dans le monde ordonné des *data warehouses*, qui nécessite de spécifier *a priori* le schéma des données. Pour pallier ces limites, de nouvelles infrastructures de stockage vont être développées : les *data lakes*, qui permettent la collecte et le stockage de quantités massives de données de nature diverse (@fig-datalake).

![Architecture d'un *data lake*. Source : [cartelis.com](https://www.cartelis.com/blog/data-lake-definition-enjeux/)](/datalake.png){#fig-datalake fig-align="center"}

D'autre part, la taille considérable de ces données rend de plus en plus difficile leur exploitation sur une unique machine. C'est dans ce contexte que Google publie le paradigme `MapReduce` [@ghemawat2003google; @dean2008mapreduce], posant les bases d'une nouvelle génération de systèmes permettant de traiter de larges volumes de données de manière distribuée. Dans les infrastructures traditionnelles, le passage à l'échelle était réalisé selon un principe de scalabilité verticale, c'est à dire en augmentant la puissance d'une machine de calcul ou en choisissant une machine plus performante. Cette approche devient néanmoins rapidement très coûteuse et se heurte aux limites physiques des composants. A l'inverse, les architectures distribuées adoptent le principe de scalabité horizontale : en installant des serveurs — chacun d'une puissance limitée — en parallèle et en adaptant les algorithmes à cette logique distribuée, on parvient à traiter des données massives avec du matériel standard. Dans la lignée de ces travaux, émerge l'écosystème `Hadoop` qui offre une combinaison de technologies complémentaires : un *data lake* (`HDFS` - *Hadoop Distributed File System*), un moteur de calcul distribué (`MapReduce`) et des outils d'intégration et de transformation de la donnée (@fig-hadoop). Cet éco-système est progressivement complété par des outils qui vont démocratiser la capacité à traiter des données *big data* : `Hive`, qui convertit des requêtes `SQL` en traitements `MapReduce` distribués, puis `Spark`, qui lève certaines limites techniques de `MapReduce` et fournit des API dans plusieurs langages (`Java`, `Scala`, `Python`, etc.). Le succès de l'éco-système `Hadoop` dans les entreprises est considérable dans la mesure où il permet de traiter des volumes de données sans précédent — jusqu'au péta-octet — et des vélocités considérables — jusqu'au temps réel — à l'aide de langages de programmation non réservés aux seuls informaticiens.

C'est ce mouvement technologique qui a permis l'engouement pour le _big data_, offrant les moyens techniques pour répondre à de nouvelles questions à partir de sources de données volumineuses. Au delà du sujet technique, ces changements ont entraîné une révolution paradigmatique dans le domaine de la _data_. Plutôt que de collecter un volume limité de données correspondant à quelques besoins bien identifiés, il est plus simple d'empiler la donnée dans des entrepôts et, seulement ensuite, lors du traitement, se poser la question de la valeur de celles-ci. Cette philosophie est typique des environnements reposant sur l'approche [NoSQL](https://en.wikipedia.org/wiki/NoSQL) (_"Not only SQL"_) où les données sont, comme dans l'écosystème SQL, enregistrées à chaque événement transactionnel, mais où celles-ci sont empilées dans des formats plus flexibles que dans les bases de données traditionnelles. Parmi les formats de prédilection de ce domaine,  le JSON, issu de transactions web, tient la dragée haute. Selon la nature plus ou moins structurée des données, il existe des outils différents pour les requêter: `ElasticSearch` ou `MongoDB` pour des données textuelles, `Spark` pour des données tabulaires, etc. Le trait commun entre ces outils et qu'ils sont très scalables horizontalement, ce qui les rend idéaux pour être utilisés dans des fermes de serveurs.

![Représentation schématique d'une architecture `Hadoop`. La donnée volumineuse est découpée en blocs, et le stockage ainsi que le traitement des différents blocs sont distribués sur plusieurs nœuds de calcul. Les algorithmes utilisés pour traiter la donnée sont adaptés au mode distribué via le paradigme `MapReduce` : on applique à chaque bloc une fonction (étape "*map*" ; par exemple, compter les fréquences des mots qui apparaissent dans les documents d'un bloc), puis on effectue une étape d'agrégation (étape "*reduce*" ; par exemple, agréger les fréquences des différents blocs pour obtenir des fréquences agrégées sur l'ensemble des données initiales). Les données en sortie de ces deux étapes sont généralement de taille bien inférieure à celle des données source, et peuvent donc être rapatriées en local pour des traitements ultérieurs (ex : faire de la visualisation de données). Source : [glennklockwood.com](https://www.glennklockwood.com/data-intensive/hadoop/overview.html)](/mapreduce-hdfs.png){#fig-hadoop fig-align="center" height=350}

À la fin des années 2010, les architectures basées sur `Hadoop` connaissent néanmoins un net déclin de popularité. Dans les environnements `Hadoop` traditionnels, le stockage et le calcul sont co-localisés par construction : si les données à traiter sont réparties sur plusieurs serveurs, chaque section des données est directement traitée sur la machine hébergeant cette section, afin d'éviter les transferts réseau entre serveurs. Dans ce paradigme, la mise à l'échelle de l'architecture implique une augmentation linéaire à la fois des capacités de calcul et de stockage, indépendamment de la demande réelle. Dans un article volontairement provocateur et intitulé "*Big Data is Dead*" [@Tigani_2023], Jordan Tigani, l'un des ingénieurs fondateurs de Google BigQuery, explique pourquoi ce modèle ne correspond plus à la réalité de la plupart des organisations exploitant intensivement de la donnée. Premièrement, parce que "dans la pratique, la taille des données augmente beaucoup plus rapidement que les besoins en calcul". Même si la quantité de données générées et nécessitant donc d'être stockées croît de manière rapide au fil du temps, il n'est généralement pas nécessaire d'interroger l'ensemble des données stockées mais seulement les portions les plus récentes, ou seulement certaines colonnes et/ou groupes de lignes. Par ailleurs, Tigani souligne que "la frontière du *big data* ne cesse de reculer" : les avancées dans les capacités des serveurs et la baisse des coûts du matériel signifient que le nombre de charges de travail ne tenant pas sur une seule machine — une définition simple mais efficace du *big data* — a diminué de manière continue (@fig-bigdata-frontier). L'apparition de nouveaux formats de données rendant plus efficients à la fois le stockage et le traitement de la donnée en mémoire participent également à cette dynamique (voir @sec-new-formats). En conséquence, en séparant correctement les fonctions de stockage et de calcul, même les traitements de données substantiels peuvent finir par utiliser "beaucoup moins de calcul que prévu [...] et pourraient même ne pas avoir besoin d'un traitement distribué du tout". Ces enseignements plaident donc de manière générale pour le choix d'infrastructures dans lesquelles ressources de calcul et de stockage sont le plus faiblement couplées possibles.

!["*The big data frontier keeps receding*" : la part des traitements de données ne pouvant être réalisés sur une seule machine a continuellement diminué au cours de la dernière décennie. Source : [motherduck.com](https://motherduck.com/blog/big-data-is-dead/)](/bigdatafrontier.jpg){#fig-bigdata-frontier fig-align="center" height=350}

## L'apport des technologies *cloud*

Dans la lignée des observations de Tigani, on observe ces dernières années une transition marquée des organisations vers des architectures plus flexibles et faiblement couplées. L'avènement des technologies *cloud* a joué un rôle déterminant dans cette transition, et ce pour plusieurs raisons. D'abord, une raison technique : par rapport à l'époque où `Hadoop` constituait l'infrastructure *big data* de référence, la latence des flux réseaux est devenue une préoccupation bien moindre, rendant le modèle de co-localisation du stockage et des ressources de calcul sur de mêmes machines moins pertinent. Ensuite, une raison liée aux usages : si le volume des données générées continue de croître, c'est surtout la diversification des données exploitées qui marque l'évolution récente de l'éco-système. Les infrastructures modernes doivent doivent non seulement être capables de traiter de grands volumes, mais aussi être adaptables sur de multiples dimensions. Elles doivent pouvoir prendre en charge diverses structures de données (allant des formats structurés et tabulaires aux formats non structurés comme le texte, les images, le son et la vidéo) et permettre une large gamme de techniques computationnelles, du calcul parallèle aux modèles d'apprentissage profond qui nécessitent des GPU, ainsi que le déploiement et la gestion d'applications [@li2020big].

Ces dernières années, deux technologies intimement liée au *cloud* — justifiant leur qualificatif de technologies *cloud-native* — ont émergé comme des solutions essentielles pour atteindre ce besoin d'environnements de calcul plus flexibles : la conteneurisation et le stockage objet. La conteneurisation est une technologie centrale dès lors qu'on aborde le sujet de la mise en production, dans la mesure où elle permet de garantir la reproductibilité et la portabilité des projets, c'est à dire leur capacité à fonctionner correctement dans différents environnements de traitement. Par conséquent, la technologie des conteneurs sera présentée en détail dans les chapitres concernant la [Portabilité](/chapters/portability.html) et le [Déploiement](/chapters/deployment.html). Nous nous concentrons dans cette section sur la deuxième technologie *cloud-native* devenue un standard dans les infrastructures de données modernes : le stockage objet.

Les conteneurs étant par construction sans état (*stateless*), il est nécessaire dans une infrastructure conteneurisée de prévoir une couche de persistence pour stocker à la fois les données brutes en entrée des traitements et les données transformées en sortie de ces derniers (@fig-types-storage). Dans l'écosystème des infrastructures de données conteneurisées, le stockage dit "objet" s'est progressivement imposé comme référence, largement popularisée par l'implémentation `S3` (*Amazon Simple Storage Service*) d'Amazon [@mesnier2003object; @samundiswary2017object]. Afin de comprendre cette prédominance, il est utile de comparer ce mode de stockage aux autres modes existants.

Schématiquement, on peut distinguer trois grandes approches en matière de stockage : le stockage de fichiers (*filesystem*), le stockage par bloc (*block storage*) et le stockage d'objets (*object storage*) (@fig-types-storage). Le stockage de fichiers est le plus intuitif : les données sont organisées sous forme d'une structure hiérarchique de répertoires et de fichiers — comme sur un ordinateur personnel. Facile d'utilisation et adapté aux environnements traditionnels, ce mode de stockage passe difficilement à l'échelle et requiert des interventions manuelles pour monter et gérer les accès aux fichiers, ce qui restreint l'autonomie des utilisateurs et n'est pas adapté aux environnements de traitement éphémères comme les conteneurs. Le stockage par bloc propose un accès de bas niveau aux données sous forme de blocs contigus — à l'image du stockage sur un disque dur — garantissant des performances élevées et une faible latence. Il s'avère donc très pertinent pour des applications qui exigent un accès rapide aux données stockées, comme une base de données. En revanche, il passe là encore difficilement à l'échelle du fait du coût de la technologie et de la difficulté à faire croître horizontalement ce type de stockage. Enfin, le stockage objet divise quant à lui les fichiers de données en morceaux appelés "objets" qui sont ensuite stockés dans un référentiel unique, qui peut être distribué sur plusieurs machines. Chaque objet se voit attribuer un certain nombre de métadonnées (nom de l'objet, taille, date de création, etc.) dont un identifiant unique qui permet au système de retrouver l'objet sans la nécessité d'une structure hiérarchique comme celle d'un *filesystem*, ce qui réduit drastiquement le coût du stockage.

![Comparaison des principaux systèmes de stockage de la donnée. Source : [bytebytego.com](https://blog.bytebytego.com/p/storage-systems-overview)](/types-storage.png){#fig-types-storage fig-align="center" height=250}

Les différentes propriétés du stockage objet le rendent particulièrement pertinent pour construire une infrastructure conteneurisée pour la *data science*. D'abord, il est optimisé pour la scalabilité : les objets stockés ne sont pas limités en taille et la technologie sous-jacente permet un stockage efficient de fichiers potentiellement très volumineux, si besoin en les distribuant horizontalement. Ensuite, il est source d'autonomie pour les utilisateurs : en stockant les données sous forme d'objets enrichis de métadonnées et accessibles via l'API standardisée `S3` d'Amazon (@fig-types-storage), il permet aux utilisateurs d'interagir directement avec le stockage via leur code applicatif (en `R`, `Python`, etc.) tout en offrant une gestion très fine des permissions — jusqu'aux droits sur un fichier — vie des jetons d'accès, garantissant ainsi une traçabilité accrue des opérations effectuées. Enfin, le stockage objet joue un rôle clé dans l'objectif de construction d'une infrastructure découplée comme celle évoquée précédemment. Dans la mesure où les dépôts de données — appelés *"buckets"* — sont interrogeables via des requêtes `HTTP` standards, les environnements de calcul peuvent importer par le biais du réseau les données nécessaires aux traitements réalisés. Ainsi, les ressources de stockage et de calcul n'ont plus besoin d'être sur les mêmes machines ni même nécessairement dans le même lieu, et peuvent ainsi évoluer indépendamment en fonction des besoins spécifiques de l'organisation.

![Dans une infrastructure basée sur des conteneurs — *stateless* par construction — le stockage objet permet de fournir la couche de persistence. MinIO est une solution open-source de stockage objet qui s'intègre nativement avec Kubernetes. Par ailleurs, elle est compatible avec l'API S3, qui est devenu le standard dominant pour l'interaction avec des systèmes de stockage objet. Ce système de stockage est donc par construction interopérable avec différents environnements de calcul. Source : [lemondeinformatique.fr](https://www.lemondeinformatique.fr/actualites/lire-les-donnees-sont-plus-importantes-que-le-cloud--assure-minio%C2%A0%C2%A0-77730.html)](/minio-s3.png){#fig-minio-s3 fig-align="center" height=300}

:::

::: {.content-visible when-profile="en"}

# Infrastructures

## Evolution of Data Infrastructures

Historically, data has been stored in databases, systems designed to store and organize information. These systems emerged in the 1950s and saw significant growth with relational databases in the 1980s. This technology proved especially effective for organizing corporate “business” data and served as the foundation of *data warehouses*, long considered the standard for data storage infrastructure [@chaudhuri1997overview]. While technical implementations can vary, their core idea is simple: data from various heterogeneous sources is integrated into a relational database system according to business rules via *ETL* (extract-transform-load) processes, making it accessible for a range of uses (statistical analysis, reporting, etc.) using the standardized `SQL` language (@fig-datawarehouse).

![Architecture of a data warehouse. Source: [airbyte.com](https://airbyte.com/data-engineering-resources/business-intelligence-data-warehouse)](/datawharehouse.png){#fig-datawarehouse fig-align="center"}

In the early 2000s, the growing adoption of *big data* practices exposed the limitations of traditional data warehouses. On one hand, data increasingly came in diverse formats (structured, semi-structured, and unstructured), often evolving as new features were added to data collection platforms. These dynamic, heterogeneous formats fit poorly with the ordered nature of data warehouses, which require schemas to be defined *a priori*. To address this, *data lakes* were developed—systems that allow for the collection and storage of large volumes of diverse data types (@fig-datalake).

![Architecture of a data lake. Source: [cartelis.com](https://www.cartelis.com/blog/data-lake-definition-enjeux/)](/datalake.png){#fig-datalake fig-align="center"}

Additionally, the enormous size of these data sets made it increasingly difficult to process them on a single machine. This is when Google introduced the `MapReduce` paradigm [@ghemawat2003google; @dean2008mapreduce], which laid the foundation for a new generation of distributed data processing systems. Traditional infrastructures used vertical scalability—adding more powerful or additional resources to a single machine. However, this quickly became expensive and hit hardware limits. Distributed architectures use horizontal scalability: by using many parallel, lower-powered servers and adapting algorithms to this distributed logic, massive datasets can be processed using commodity hardware. This led to the emergence of the `Hadoop` ecosystem, combining complementary technologies: a data lake (`HDFS` - Hadoop Distributed File System), a distributed processing engine (`MapReduce`), and tools for data integration and transformation (@fig-hadoop). This ecosystem expanded with tools like `Hive` (which converts `SQL` queries into distributed `MapReduce` tasks) and `Spark` (which overcomes certain technical limitations of `MapReduce` and provides APIs in multiple languages, including `Java`, `Scala`, and `Python`). Hadoop’s success was profound—it enabled organizations to process petabyte-scale datasets in real-time using widely accessible programming languages.

This technological shift fueled the *big data* revolution, enabling new types of questions to be answered using vast datasets. Philosophically, it marked a shift from collecting only the data needed for known purposes, to storing as much data as possible and evaluating its usefulness later during analysis. This approach is typical of [NoSQL](https://en.wikipedia.org/wiki/NoSQL) environments ("Not only SQL"), where data is stored at each transactional event but in more flexible formats than traditional databases. JSON, derived from web transactions, is especially prominent. Depending on the structure of the data, different tools are used to query it: `ElasticSearch` or `MongoDB` for text data, `Spark` for tabular data, and so on. All these tools share a common trait: they are highly horizontally scalable, making them ideal for server farms.

![Schematic of a Hadoop architecture. Large datasets are split into blocks, and both storage and processing are distributed across multiple compute nodes. Algorithms are adapted to this distributed setup via `MapReduce`: first, a "map" function is applied to each block (e.g., count word frequencies), then a "reduce" step aggregates these results (e.g., compute total frequencies across blocks). Output data is often much smaller than input data and can be brought back locally for further tasks like visualization. Source: [glennklockwood.com](https://www.glennklockwood.com/data-intensive/hadoop/overview.html)](/mapreduce-hdfs.png){#fig-hadoop fig-align="center" height=350}

By the late 2010s, `Hadoop` architectures began to decline in popularity. In traditional Hadoop setups, storage and compute are co-located by design: data segments are processed on the servers where they are stored, avoiding network traffic. This architecture scales linearly, increasing both storage and compute capacity—even if only one is needed. In a provocative article titled "*Big Data is Dead*" [@Tigani_2023], Jordan Tigani (one of the founding engineers of Google BigQuery) argues that this model no longer suits modern data workloads. First, he explains, “in practice, data size grows much faster than compute needs.” Most use cases don’t require querying all stored data—just recent subsets or specific columns. Second, "the big data frontier keeps receding": thanks to more powerful and cheaper servers, fewer workloads require distributed systems (@fig-bigdata-frontier). Additionally, new storage formats (see @sec-new-formats) make data handling more efficient. As a result, properly decoupling storage from compute often leads to simpler, more efficient infrastructures.

!["The big data frontier keeps receding": the share of data workloads that cannot be handled by a single machine has steadily declined. Source: [motherduck.com](https://motherduck.com/blog/big-data-is-dead/)](/bigdatafrontier.jpg){#fig-bigdata-frontier fig-align="center" height=350}

## The Role of *Cloud* Technologies

Building on Tigani’s analysis, we observe a growing shift toward more flexible, loosely coupled architectures. The rise of *cloud* technologies has been pivotal in this transition, for several reasons. Technically, network latency is no longer the bottleneck it was during Hadoop’s heyday, making the co-location of compute and storage less necessary. In terms of usage, it’s not just that data volumes are growing—it’s also the **diversity** of data and processing needs that is expanding. Modern infrastructures must support various data formats (from structured tables to unstructured media) and a wide range of compute requirements—from parallel data processing to deep learning on GPUs [@li2020big].

Two cloud-native technologies have become central to this modern flexibility: **containerization** and **object storage**. Containerization ensures reproducibility and portability—crucial for production environments—and will be discussed in the [Portability](/chapters/portability.html) and [Deployment](/chapters/deployment.html) chapters. In this section, we focus on **object storage**, the default standard in modern data infrastructures.

Since containers are *stateless* by nature, a persistent storage layer is needed to store input and output data across computations (@fig-types-storage). In container-based infrastructures, **object storage** has become dominant—popularized by Amazon’s `S3` (Simple Storage Service) [@mesnier2003object; @samundiswary2017object]. To understand its popularity, it's helpful to contrast object storage with other storage types.

There are three main storage models: file systems, block storage, and object storage (@fig-types-storage). File systems organize data in a hierarchical structure—like a traditional desktop environment—but they don’t scale well and require manual access management. Block storage, like that on hard drives, offers fast low-latency access—ideal for databases—but also struggles with scalability and cost. Object storage, on the other hand, breaks data into "objects" stored in a flat namespace and assigned unique IDs and metadata. It removes the need for hierarchical structures, lowering storage costs.

![Comparison of storage types. Source: [bytebytego.com](https://blog.bytebytego.com/p/storage-systems-overview)](/types-storage.png){#fig-types-storage fig-align="center" height=250}

Object storage’s characteristics make it ideal for containerized *data science* infrastructures. It’s highly scalable, supports large files, and works well with distributed systems. It also enhances user autonomy by exposing data via APIs like Amazon’s `S3`, allowing direct interaction from code (`R`, `Python`, etc.) and fine-grained access control via tokens. Most importantly, object storage supports **decoupled architectures**, where compute and storage are independent and remotely accessible. This improves flexibility and efficiency.

![In container-based infrastructure (which is stateless by nature), object storage provides the persistence layer. MinIO is an open-source object storage solution that integrates natively with Kubernetes and supports the S3 API—now the industry standard—ensuring compatibility across environments. Source: [lemondeinformatique.fr](https://www.lemondeinformatique.fr/actualites/lire-les-donnees-sont-plus-importantes-que-le-cloud--assure-minio%C2%A0%C2%A0-77730.html)](/minio-s3.png){#fig-minio-s3 fig-align="center" height=300}

:::
