:::: {.content-visible when-profile="fr"}

Le phénomène du *big data* est maintenant bien documenté : la génération et la collecte de données par une multitude de sources (capteurs IoT, interactions quotidiennes sur les réseaux sociaux, transactions en ligne, dispositifs mobiles, etc.) démultiplie les volumes de données disponibles pour l'analyse. Les raisons pouvant mener à s'intéresser à de telles données pour des projets de data science sont nombreuses : haute disponibilité, plus grande finesse des phénomènes observés, bases pour l'entraînement de modèles de ML de plus en plus gourmands (comme les LLM), etc.

On définit souvent le _big data_ comme étant la situation où les données sont volumineuses au point qu'on ne soit plus en mesure de les traiter sur une unique machine. Cette définition relativiste peut sembler réductrice mais présente néanmoins l'intérêt de souligner qu'une source de données peut, selon les époques et les environnements, faire appel à des compétences différentes. Le fait de passer à des données _big data_ n'est en effet pas seulement un changement de volume: on change souvent de nature d'infrastructure informatique et cela a des implications fortes sur les compétences à mettre en oeuvre pour traiter des données et l'évolutivité des chaines de production en question.

En contrepartie, le traitement de ces données génère de nouveaux défis. Ces derniers peuvent être résumés par les **"trois V"**, une manière désormais admise de caractériser ces nouvelles sources de données [@sagiroglu2013big] :

- **Volume** : des **volumes massifs de données**, souvent à une échelle bien supérieure à ce que les systèmes traditionnels peuvent traiter efficacement. Par conséquent, de nouvelles architectures de stockage et de traitement sont nécessaires afin de gérer de telles volumétries.

- **Vitesse** : ces données sont généralement produites à **haute fréquence, jusqu’au temps réel**. Là encore, cela nécessite des ajustements dans les architectures et les méthodes de traitement, comme les méthodes de *stream processing* par exemple.

- **Variété** : ces données présentent **des sources et des formats très variés**, allant des bases structurées (données tabulaires) aux données non-structurées (texte, image, vidéo, etc.). Là encore, les techniques de traitement adaptées vont dépendre des formes prises par les données. Dans ce cours, nous nous concentrerons sur des données relativement structurées, dans la mesure où ces dernières restent centrales dans les projets analytiques de data science.

![Les "3 V" du *big data*. Source : [AI with Python](https://www.packtpub.com/product/artificial-intelligence-with-python-second-edition/9781839219535)](/vvv.png){fig-align="center" height=350}

Lorsque l'on envisage de passer en production un projet de data science basé sur des données volumineuses, **l'adoption de bonnes pratiques de développement n'est pas seulement recommandée, elle est indispensable**. En effet, les données volumineuses voire massives introduisent une complexité significative dans tous les aspects du cycle de vie d'un projet de data science, de la collecte et du stockage des données à leur traitement et analyse. Les systèmes doivent être conçus pour non seulement gérer le volume actuel de données, mais aussi pour être **évolutifs** face à une croissance future inévitable. Les bonnes pratiques de développement facilitent cette évolutivité en promouvant des **architectures modulaires**, des codes réutilisables, et des technologies adaptées au traitement des grandes quantités de données.

Face à ces enjeux, le choix des technologies est primordial. Dans ce cours, nous présenterons trois axes principaux qui peuvent guider ces choix : **l'infrastructure informatique**, des **formats de données** adaptés aux données volumineuses, et des ***frameworks*** (solutions logicielles et leur écosystème) utilisés pour le traitement de la donnée.

::::

:::: {.content-visible when-profile="en"}

The *big data* phenomenon is now well-documented: the generation and collection of data from a multitude of sources (IoT sensors, daily interactions on social media, online transactions, mobile devices, etc.) drastically increases the volume of data available for analysis. There are many reasons to focus on such data in data science projects: high availability, greater granularity of observed phenomena, and large datasets needed for training increasingly data-hungry models (like LLMs), among others.

Big data is often defined as a situation where the data volume is so large that it can no longer be processed on a single machine. This relative definition may seem reductive but has the advantage of highlighting that a data source, depending on the time and environment, may require different skill sets. Moving to *big data* is not just a matter of scale—it often involves a fundamental shift in the computing infrastructure, with strong implications for the expertise required and the scalability of the data pipelines.

Processing such data introduces new challenges, often summarized as the **"three Vs"**, a widely accepted way to characterize these data sources [@sagiroglu2013big]:

- **Volume**: Massive volumes of data, often far beyond the capabilities of traditional systems. New storage and processing architectures are needed to handle such scales.

- **Velocity**: These data are typically produced at **high frequency, even in real time**. This demands adjustments to architecture and processing methods, such as stream processing.

- **Variety**: These data come from **a wide range of sources and formats**, from structured tables to unstructured data like text, images, or video. The appropriate processing techniques depend heavily on the data type. In this course, we focus on relatively structured data, as they remain central to analytical data science projects.

![The "3 Vs" of *big data*. Source: [AI with Python](https://www.packtpub.com/product/artificial-intelligence-with-python-second-edition/9781839219535)](/vvv.png){fig-align="center" height=350}

When considering putting a data science project based on large datasets into production, **adopting good development practices is not just recommended—it is essential**. Massive datasets introduce significant complexity at every stage of the data science project lifecycle, from collection and storage to processing and analysis. Systems must be designed not only to handle the current data load but also to be **scalable** for future growth. Good practices enable this scalability by promoting **modular architectures**, reusable code, and technologies suited for large-scale data processing.

To address these challenges, technology choices are crucial. In this course, we will focus on three main aspects to guide those choices: **computing infrastructure**, **data formats** suited for high volumes, and ***frameworks*** (software solutions and their ecosystems) used for data processing.

::::
