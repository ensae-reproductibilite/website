---
title: "Déployer et valoriser son projet de data science"
author: "Romain Avouac et Lino Galiana"
description: |
  <br>
  Présentation des techniques de déploiement et de valorisation des projets de data science.
image: images/whale.png
order: 7
href: chapters/deployment.html
---

<details>
<summary>
Dérouler les _slides_ ci-dessous ou [cliquer ici](https://ensae-reproductibilite.github.io/slides/#/vers-la-mise-en-production)
pour afficher les slides en plein écran.
</summary>


<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><iframe class="sourceCode yaml code-with-copy" src="https://ensae-reproductibilite.github.io/slides/#/vers-la-mise-en-production"></iframe></div>

</details>

# La notion de mise en production

Les chapitres précédents nous ont permis de construire un projet de data science à la fois conforme aux bonnes pratiques de développement et portable. Ce faisant, nous avons déjà largement réduit le coût de transition entre notre environnement de développement et un environnement de production. La notion de mise en production désigne précisément cette transition vers un environnement de production, c'est à dire un environnement dans lequel l'application sera **déployée** et donc accessible à ses utilisateurs potentiels, avec une haute disponibilité — idéalement, 24h sur 24.

Réaliser une mise en production nécessite d'effectuer un certain nombre de choix, plus ou moins contraints. D'abord, il est nécessaire de bien comprendre le besoin métier — qui seront les utilisateurs potentiels de notre application et quels sont leurs besoins — pour déterminer le format de valorisation pertinent pour celle-ci. Par exemple, le format adapté pour mettre à disposition un modèle entraîné de *machine learning* ne sera pas le même que l'on s'adresse au grand public — auquel cas on s'orientera certainement vers une application interactive simple d'utilisation[^1] — ou à une équipe interne de notre organisation qui voudrait effectuer de la prédiction à partir du modèle — cas dans lequel une API requêtable via un langage de programmation s'avèrera sans doute plus pertinente. Un second choix à effectuer concerne l'infrastructure de production sur laquelle va être déployée l'application. Si des considérations techniques entrent en compte pour déterminer l'infrastructure pertinente, ce choix est en général contraint pour le *data scientist* par ce qu'il a à sa disposition au sein de son organisation. Enfin, il est nécessaire de choisir les outils qui vont permettre l'**industrialisation** du projet : automatisation du déploiement, orchestration des traitements, *monitoring* de la solution en production, etc.

[^1]: Pensons au cas récent de `ChatGPT`, qui doit en partie son succès au fait d'avoir réussi à exposer le modèle entraîné via une interface intuitive et réactive.

La diversité des réponses qui peuvent être apportées à chacun de ces choix illustre la complexité du sujet de la mise en production. Il serait bien illusoire de prétendre traiter ici ce sujet avec exhaustivité. Afin de restreindre cette complexité, nous allons effectuer à chaque étape des choix raisonnables, c'est à dire qui représentent des pratiques courantes dans le domaine de la *data science*. Ainsi, nous allons nous restreindre au cas du déploiement d'un modèle de *machine learning*, exposé via une API, car il s'agit d'une tâche fréquemment effectuée dans le monde de la *data science* et sur laquelle il est donc bon d'avoir quelques notions. Comme nous allons effectuer le déploiement sur le [SSP Cloud](https://datalab.sspcloud.fr), l'infrastructure de production sera un cluster `Kubernetes`. Là encore, cette solution constitue un standard actuel de l'industrie. Enfin, pour ce qui est de l'industrialisation de notre projet, nous choisirons des solutions standards et *open-source* qui permettent d'implémenter l'**approche CI/CD** (intégration continue / déploiement continu), qui fait référence en la matière. Dans tous les cas, l'enjeu de ce cours est de transmettre les concepts sous-jacents à la mise en production plus que de former à telle ou telle technologie.

Par rapport au précédent, ce chapitre est assez technique et repose sur une vaste palette d'outils. Notre propos n'est cependant pas de dire que le *data scientist* doit maîtriser l'ensemble des concepts et outils présentés. Au contraire : avec la complexification des projets et des infrastructures, un ensemble de professions ont émergé — *date engineer*, *dev ops*, *machine learning engineer*, etc. — dont les rôles sont plus spécifiquement centrés sur l'industrialisation des projets. Néanmoins, nous sommes convaincus qu'il est essentiel que le *data scientist* possède des notions sur le sujet afin de pouvoir jouer le **rôle d'interface** entre le métier et les équipes techniques. Ce chapitre est une tentative de condenser ces différentes notions sous une forme la plus accessible possible.

Ce chapitre nous amènera à explorer plusieurs écosystèmes, pour lesquels 
on retrouve quelques _buzz-words_ dont voici les définitions :

| Terme | Définition|
|------|--------|
| [`DevOps`](https://fr.wikipedia.org/wiki/Devops) | mouvement en ingénierie informatique et une pratique technique visant à l'unification du développement logiciel (dev) et de l'administration des infrastructures informatiques (ops) |
| [`MLOps`](https://fr.wikipedia.org/wiki/MLOps) | ensemble de pratiques qui vise à déployer et maintenir des modèles de machine learning en production de manière fiable et efficace |
| `CI/CD` | combinaison des pratiques d'intégration continue et de livraison continue ou de déploiement continu |


# L'approche CI/CD


## Définition

> L’intégration continue est un ensemble de pratiques utilisées en génie logiciel consistant à vérifier à chaque modification de code source que le résultat des modifications ne produit pas de régression dans l’application développée. […] Elle permet d’automatiser l’exécution des suites de tests et de voir l’évolution du développement du logiciel.

> La livraison continue est une approche d’ingénierie logicielle dans laquelle les équipes produisent des logiciels dans des cycles courts, ce qui permet de le mettre à disposition à n’importe quel moment. Le but est de construire, tester et diffuser un logiciel plus rapidement.

![](/cicd_exemple.png)

L'approche CI/CD garantit une automatisation et une surveillance continues
tout au long du cycle de vie d'un projet. 

Cela présente de nombreux avantages:

- on peut anticiper les contraintes de la mise en production grâce à des environnements
normalisés partant d'image docker standardisées
- on peut tester les changements apportés à un livrable par un nouveau prototype 
- on peut déterminer très rapidement l'introduction de bugs dans un projet.


## Intégration continue

- Automatisation des phases de build et de test (data validation, model validation..)
- Mise à disposition des artifacts (forme "morte")

### Mise en oeuvre : `GitHub Actions`

Les actions `Github` sont un ensemble de règles qui se suivent au 
format `YAML`. Cela permet de définir différentes étapes du processus
avec, pour chaque étape, des éléments de configuration. 
Elles permettent de partir d'une configuration minimale
(ici une machine `ubuntu`) et d'appliquer des instructions à la suite,
par étape. 

<details>
<summary>Exemple d'action impliquant exclusivement de l'intégration continue ([application 14](https://linogaliana.github.io/ensae-reproductibilite-website/chapters/application.html#etape-1-mise-en-place-de-tests-automatis%C3%A9s))</summary>
```yaml
name: Python package

on: [push]

jobs:
  build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11"]

    steps:
      - uses: actions/checkout@v3
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pylint
          pip install -r requirements.txt
      - name: Lint
        run: |
          pylint src --fail-under=6
      - name: Test workflow
        run: |
          python main.py
```
</details>


::: {.callout-note}
L'intégration continue est aussi au coeur des fonctionnalités
de `Gitlab`. Il s'agissait même d'une fonctionnalité disponible
des années avant `Github`. 

La syntaxe est différente mais le principe est le même : on part
d'une machine à la configuration minimale depuis des serveurs
mis à disposition et on teste la reproductibilité de son projet.  
:::


## Déploiement

- Avantages de déployer sur une infra Cloud
- Fonctionnement basique de Kubernetes
- GitOps
- Implémentation : Kubernetes

## Orchestration

- Modélisation d'un pipeline data sous forme de DAG
- Implémentation : argo-workflow

Une chaine de production implique plusieurs étapes qui peuvent éventuellement 
nécessiter plusieurs langages. Ces étapes peuvent être vues comme une suite de 
transformations.

La représentation de ces étapes peut être faite à l'aide des diagrammes
acycliques dirigés (DAG):

![](https://miro.medium.com/max/1400/1*grWvT-3jUcrnbTrsVtRHAg.png)

Un _workflow_ complet sera ainsi reproductible si on peut, en ayant accès
aux _inputs_ et à l'ensemble des règles de transformation, reproduire
exactement les outputs. 
Si les inputs ou le code changent, on peut être en mesure de mettre à jour
les outputs, si possible sans faire retourner les parties du projet non
concernés. 

Une première manière de développer est l'approche manuelle, qui est une tâche
digne de Sisyphe:

1. Ecriture du code
2. Exécution du code jusqu'à sa fin
3. Découverte d'une erreur ou mise à jour du code ou des données
4. Relance le code dans son ensemble

Pour éviter ce cycle interminable, on est tenté d'écrire des bases
intermédiaires et de ne faire tourner qu'une partie du code. 
Cette approche, si elle a l'avantage de faire gagner du temps, est 
néanmoins dangereuse car on peut facilement oublier de mettre à jour
une base intermédiaire qui a changé ou au contraire refaire tourner
une partie du code qui n'a pas été mise à jour. 

Il existe des méthodes plus fiables pour éviter ces gestes manuels. 
Celles-ci sont inspirées de [GNU Make](https://www.gnu.org/software/make/)
et consistent à créer le chemin de dépendance de la chaine de production 
(lister l'environnement, les inputs et les outputs à produire), à déterminer
les chemins affectés par un changement de code ou de données pour
ne faire tourner à nouveau que les étapes nécessaires. 

# Application au ML : le MLOps

- Implémentation : MLFlow

## Spécificités liées à la mise en prod de modèles de ML

- Entrainement : batch ou online
- Batch vs real-time prediction 
- Feedback loops expérimentation/déploiement/monitoring
- Exposer le modèle via une API

## Notions d'observabilité

- Drifts
- Quelles métriques monitorer

## Amélioration continue

- Réentraînement : périodique vs continu





<!-- # Qu'est-ce que la mise en production ?

Dans les chapitres précédents, nous avons exploré la manière
dont une structure de projet et de code adéquate facilite
la réutilisation d'un projet. Cependant, le code est rarement,
en soi, le produit final mais un moyen. Le code peut
servir à mettre en oeuvre une application, à effectuer
des traitements sur une base de données pour un papier ou 
un rapport, etc. La fréquence de ré-utilisation du code peut
elle-même être variable: certains projets vont être utilisés
quotidiennement alors que d'autres ne le seront qu'à des
échéances diverses. 

Comme tout produit, un projet a un cycle de vie. Pour faire
simple, on peut séparer celui-ci en trois phases:

- la phase de développement correspond à l'écriture du code et
à des exploitations exploratoires ;
- la phase de mise en production correspond à l'adaptation du prototype
à des contraintes nécessaires pour qu'un projet produise un _output_ à
la demande ;
- la phase de maintenance correspond à la situation où on ne met plus
en oeuvre de nouvelles fonctionalités mais où on s'assure qu'un projet
informatique continue de produire les _output_ désirés malgré l'évolution
du contexte. 

En pratique, la distinction entre ces moments d'un projet peut être 
floue. Par exemple, grâce à `Git`, on peut ainsi mettre en oeuvre de 
nouvelles fonctionalités (protypage correspond à la
phase de développement) parallèles à celles déjà existantes (phase 
de maintenance). Néanmoins, ces différences conceptuelles sont intéressantes
pour appréhender les contraintes différentes de ces phases
d'un projet. 


## Importance des bonnes pratiques

Les bonnes pratiques mises en oeuvre jusqu'à présent avaient pour
objectif de faciliter la compréhension et la réutilisation d'un 
projet. Elles sont donc particulièrement appropriées pour réduire
le coût en temps de la mise en production et de la maintenance
d'un projet. Ces coûts pourraient amener un projet, même utile,
à être abandonné. 


## Environnements de production

De même, l'emphase du chapitre précédent sur la portabilité
vise à faciliter la mise en production. En effet, en créant
un environnement normalisé qui créé des conditions simples
pour reproduire certains _output_, on évite un hiatus 
entre le protypage et la mise en production. 


# Différents types de livrable

L'écosystème de la _data-science_ est florissant et permet
une grande variété d'_output_ qui vont toucher des publics
différents. Selon le type de projet, on peut retrouver
plusieurs livrables: 

- base de données ;
- API pour mettre à disposition un modèle  ;
- application  ;
- rapport automatisé ;
- site internet  ;
- ...


L'objectif du présent chapitre est de proposer des
méthodes pour anticiper la mise en production d'un
projet en intégrant, dès la phase de prototypage,
la production de livrables valorisant un projet
de _data-science_. 

Ces méthodes nous amèneront à explorer plusieurs écosystèmes, pour lesquels 
on retrouve quelques _buzz-words_ dont voici les définitions :

| Terme | Définition|
|------|--------|
| [`devops`](https://fr.wikipedia.org/wiki/Devops) | mouvement en ingénierie informatique et une pratique technique visant à l'unification du développement logiciel (dev) et de l'administration des infrastructures informatiques (ops) |
| [`MLOps`](https://fr.wikipedia.org/wiki/MLOps) | ensemble de pratiques qui vise à déployer et maintenir des modèles de machine learning en production de manière fiable et efficace |
| `CI/CD` | combinaison des pratiques d'intégration continue et de livraison continue ou de déploiement continu |



# Approches

## Pipelines de données

Une chaine de production implique plusieurs étapes qui peuvent éventuellement 
nécessiter plusieurs langages. Ces étapes peuvent être vues comme des 
transformations à la chaine d'un ou plusieurs inputs afin de produire
un ou plusieurs output.

La représentation de ces étapes peut être faite à l'aide des diagrammes
acycliques dirigés (DAG):

![](https://miro.medium.com/max/1400/1*grWvT-3jUcrnbTrsVtRHAg.png)

Un _workflow_ complet sera ainsi reproductible si on peut, en ayant accès
aux _inputs_ et à l'ensemble des règles de transformation reproduire
exactement les outputs. 
Si les inputs ou le code change, on peut être en mesure de mettre à jour
les outputs, si possible sans faire retourner les parties du projet non
concernés. 

Une première manière de développer est l'approche manuelle, qui est une tâche
digne de Sisyphe:

1. Ecriture du code
2. Exécution du code jusqu'à sa fin
3. Découverte d'une erreur ou mise à jour du code ou des données
4. Relance le code dans son ensemble

Pour éviter ce cycle interminable, on est tenté d'écrire des bases
intermédiaires et de ne faire tourner qu'une partie du code. 
Cette approche, si elle a l'avantage de faire gagner du temps, est 
néanmoins dangereuse car on peut facilement oublier de mettre à jour
une base intermédiaire qui a changé ou au contraire refaire tourner
une partie du code qui n'a pas été mise à jour. 

Il existe des méthodes plus fiables pour éviter ces gestes manuels. 
Celles-ci sont inspirées de [GNU Make](https://www.gnu.org/software/make/)
et consistent à créer le chemin de dépendance de la chaine de production 
(lister l'environnement, les inputs et les outputs à produire), à déterminer
les chemins affectés par un changement de code ou de données pour
ne faire tourner à nouveau que les étapes nécessaires. 

Les implémentations en `Python` et `R` sont nombreuses. Parmi celles-ci, on
peut mettre en valeur

- [`snakemake`](https://snakemake.readthedocs.io/en/stable/) pour `Python`
- [`targets`](https://books.ropensci.org/targets/) pour `R`


## Orchestration

Certains outils vont plus loin:

- argo
- mlflow
- airflow

# CI/CD

## Définition

> L’intégration continue est un ensemble de pratiques utilisées en génie logiciel consistant à vérifier à chaque modification de code source que le résultat des modifications ne produit pas de régression dans l’application développée. […] Elle permet d’automatiser l’exécution des suites de tests et de voir l’évolution du développement du logiciel.

> La livraison continue est une approche d’ingénierie logicielle dans laquelle les équipes produisent des logiciels dans des cycles courts, ce qui permet de le mettre à disposition à n’importe quel moment. Le but est de construire, tester et diffuser un logiciel plus rapidement.

![](/cicd_exemple.png)

## Avantages

L'approche CI/CD garantit une automatisation et une surveillance continues
tout au long du cycle de vie d'un projet. 

Cela présente de nombreux avantages:

- on peut anticiper les contraintes de la mise en production grâce à des environnements
normalisés partant d'image docker standardisées
- on peut tester les changements apportés à un livrable par un nouveau prototype 
- on peut déterminer très rapidement l'introduction de bugs dans un projet

## Mise en oeuvre

L'idée de l'approche CI/CD est ainsi d'associer chaque changement de 
code (`commit`) à l'exécution de scripts automatisés.
Bien que mis en oeuvre de manière différente, `Gitlab` et `Github`
proposent tous les deux ce type de fonctionalités.

Les actions `Github` sont un ensemble de règles qui se suivent au 
format `YAML`. Cela permet de définir différentes étapes du processus
avec, pour chaque étape, des éléments de configuration. 

Voici, par exemple, l'action qui sert de modèle à modifier dans l'exercice
d'application 

```yaml
name: Python Package using Conda

on: [push]

jobs:
  build-linux:
    runs-on: ubuntu-latest
    strategy:
      max-parallel: 5

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: 3.10
    - name: Add conda to system path
      run: |
        # $CONDA is an environment variable pointing to the root of the miniconda directory
        echo $CONDA/bin >> $GITHUB_PATH
    - name: Install dependencies
      run: |
        conda env update --file environment.yml --name base
    - name: Lint with flake8
      run: |
        conda install flake8
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Test with pytest
      run: |
        conda install pytest
        pytest
```

# Valoriser son projet avec un site web automatisé


Un code ou une API intéressent des publics très ciblés. D'autres
publics seront intéressés par une autre valorisation
d'un projet: les chercheurs apprécieront un papier académique, 
d'autres personnes préfèreront un site web ergonomique...

## Qu'est-ce qu'un site _web_ ?

Pour simplifier,
on peut voir un site _web_ comme la combinaison de trois éléments:

- une arborescence de fichiers `HTML` qui présentent le contenu du site
dans un balisage lourd
- des fichiers `CSS` qui gèrent la mise en forme[^1]
- des fonctions `javascript`

[^1]: Cette séparation entre le fond et la forme renvoie au paradigme du
_What you see is what you mean_ (WYSIWYM) dont l'un des
logiciels les plus connus est `LaTeX` et s'oppose au principe
du _What you see is what you get_ (WYSIWYG) des éditeurs de texte.

Il existe énormément d'outils aujourd'hui qui permettent, sans connaissance
en HTML, CSS ou JS, de créer un site _web_. Dans le domaine de la
_data-science_, le format `Markdown` (fichiers `.md`) s'est imposé. 

`Markdown` est un système d’édition doté d’une syntaxe simplifiée souvent
utilisé pour faire de la documentation de projet.
Le format est utilisé sur de nombreux sites internet,
notamment `Gitlab` et `Stackoverflow`.
L’extension de ce type de fichier est `.md`.
`Markdown` présente plusieurs avantages: 

- il est facile d’inclure des blocs de code informatique et des équations mathématiques dans un document Markdown ; 
- le formatage de blocs de texte ou de code est simple et très bien fait (et beaucoup plus léger qu’en LaTeX par exemple) ;
- il existe des outils de conversion de `Markdown` en `HTML` très bien faits.

Plus d'éléments sur la logique de `Markdown` et ses intérêts
sont disponibles dans le
chapitre [R Markdown de la documentation  `utilitR`](https://www.book.utilitr.org/rmarkdown.html)

## Comment faire un site _web_ ? 

Il existe historiquement plusieurs approches dans l'écosystème de la
_data-science_, selon le langage utilisé et l'output désiré. 

Si on part de fichiers qui présentent des blocs de
code qui ne nécessitent pas d'être exécutés, l'écosystème le plus riche
est [Hugo](https://gohugo.io/content-management/formats/). Celui-ci permet
de générer des sites web à l'architecture complexe à partir d'une
arborescence de `.md`. C'est l'approche adoptée 
pour ce site _web_ 

Cependant, il est souvent nécessaire d'exécuter des bouts de code pour tester
les exemples présentés, ou générer des tableaux ou sorties graphiques.
Pour cela, historiquement, il existe deux paradigmes: 

- [`JupyterBook`](https://jupyterbook.org/intro.html) qui vise à générer des
sites web et des notebooks à partir de fichiers `markdown`. Le notebook n'est
donc pas le produit de départ (cf. XXXX) mais un livrable. 
- [`R Markdown`](https://rmarkdown.rstudio.com/): l'écosystème le plus riche
avec de nombreux modèles de documents customisables (documents HTML ou articles PDF, sites web de documentation
avec `bookdown`, blogs avec `blogdown`,  _dashboards_, etc.) Le principe de `R Markdown`
est d'offrir une surcouche à `Markdown` pour que les blocs de code soient exécutés afin de créer
un _output_ reproductible.

`R Markdown` est un système d'une grande richesse. Initialement pensé pour les
utilisateurs de `R`, ce paradigme permet maintenant de créer des documents
executant d'autres langages, notamment
du `Python` ([le cours de python de 2e année](https://pythonds.linogaliana.fr/)
de l'ENSAE est testé et construit grâce à `R Markdown`).

`Quarto` est le petit nouveau dans cet écosystème, amené à devenir un 
outil standard des _data-scientists_ comme peuvent l'être, aujourd'hui,
les Notebooks `Jupyter`.
Successeur de `R Markdown`,
il vise à améliorer l'aspect universel des documents produits en n'obligeant
plus à utiliser `R` pour compiler le document qui ne le nécessitent pas. 
C'est un outil particulièrement adapté aux utilisateurs de `Python`

![](/quarto.png)

Un document `quarto` hérite des principes de base d'un document `R Markdown`, notamment
la structure. Il comporte ainsi deux parties principales :

- L’en-tête (`YAML` header) qui gère les éléments de style et les paramètres globaux;
- Le contenu qui gère le fond et permet d’alterner librement texte et code :
    + Les __blocs de texte brut__ mis en forme selon la syntaxe markdown 
    + Les __blocs de code__ sont présentés dans des _chunks_ identifiés par un
langage (ici `python`) qui seront exécutés de manière linéaire et produiront
l'output désiré (ici une figure `matplotlib`).  

## Comment mettre à disposition un site _web_ ?

Mettre à jour manuellement un site _web_ après l'avoir compilé
est une tâche pénible et source d'erreur dans un projet actif de
_data-science_.
L'automatisation issue de l'approche CI/CD permet un gain de confort 
aussi dans ce domaine.

Supposons qu'on ait mis en oeuvre une routine pour automatiser la construction
d'un site _web_ à partir de fichiers _Markdown_. Comment les mettre à
disposition ?

Il existe plusieurs manières de déployer automatiquement un site web : 

- [Gitlab pages](https://docs.gitlab.com/ee/user/project/pages/) ;
- [Github pages](https://pages.github.com/) ;
- [Netlify](https://www.netlify.com/).

Ces trois services sont gratuits. Ils consistent à mettre 
à disposition un DNS sur lequel des fichiers HTML peuvent être mis à
disposition automatiquement pour assurer que chaque `commit` 

L'[exercice d'application](../application) supposant l'utilisation
de `Github`, il présentera `Netlify` qui est
plus pratique que `Github Pages`[^2].
Le résultat de cette chaine de production d'un site
web reproductible est accessible à l'url
https://spiffy-florentine-c913b9.netlify.app/

[^2]: `Gitlab Pages` permet une mise en lien plus directe entre les
output de l'intégration continue et le déploiement. Cependant, avec
les scripts présentés dans l'exercice d'application on peut
avoir une intégration très flexible de `Netlify` dans un pipeline
plus riche.  -->
