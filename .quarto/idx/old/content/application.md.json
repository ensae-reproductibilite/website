{"title":"Comprendre les facteurs de survie sur le Titanic","markdown":{"yaml":{"title":"Appliquer les concepts étudiés à un projet de data science","date":"2022-03-03","author":"Romain Avouac et Lino Galiana","draft":false,"layout":"single"},"headingText":"Partie 1 : application des bonnes pratiques","containsRefs":false,"markdown":"\n\n\n\nL'objectif de cette mise en application est d'**illustrer les différentes étapes qui séparent la phase de développement d'un projet de celle de la mise en production**. Elle permettra de mettre en pratique les différents concepts présentés tout au long du cours.\n\nNous nous plaçons dans une situation initiale correspondant à la fin de la phase de développement d'un projet de data science. On a un notebook un peu monolithique, qui réalise les étapes classiques d'un *pipeline* de *machine learning* :\n- import de données\n- statistiques descriptives et visualisations\n- *feature engineering*\n- entraînement d'un modèle\n- évaluation du modèle\n\n**L'objectif est d'améliorer le projet de manière incrémentale jusqu'à pouvoir le mettre en production, en le valorisant sous une forme adaptée.** \n\n{{% box status=\"warning\" title=\"Warning\" icon=\"fa fa-exclamation-triangle\" %}}\nIl est important de bien lire les consignes et d'y aller progressivement.\nCertaines étapes peuvent être rapides, d'autres plus fastidieuses ;\ncertaines être assez guidées, d'autres vous laisser plus de liberté.\nSi vous n'effectuez pas une étape, vous risquez de ne pas pouvoir passer à\nl'étape suivante qui en dépend.\n\nBien que l'exercice soit applicable sur toute configuration bien faite, nous \nrecommandons de privilégier l'utilisation du [SSP Cloud](https://datalab.sspcloud.fr/home), où tous les \noutils nécessaires sont pré-installés et pré-configurés. \n{{% /box %}}\n\n\n\n\n\nCette première partie vise à **rendre le projet conforme aux bonnes pratiques** présentées dans le cours. Elle fait intervenir les notions suivantes : \n- utilisation du **terminal**\n- **qualité du code**\n- **architecture de projets**\n- **contrôle de version** avec Git\n- **travail collaboratif** avec Git et GitHub\n\nLe plan de la partie est le suivant :\n\n0. :zero: Forker le dépôt et créer une branche de travail\n1. :one: S'assurer que le notebook s'exécute correctement\n2. :two: Modularisation : mise en fonctions et mise en module\n3. :three: Utiliser un `main` script\n4. :four:  Appliquer les standards de qualité de code\n5. :five: Adopter une architecture standardisée de projet\n6. :six: Fixer l'environnement d'exécution\n7. :seven: Stocker les données de manière externe\n8. :eight: Nettoyer le projet Git\n9. :nine: Ouvrir une *pull request* sur le dépôt du projet.\n\n\n## Etape 0: forker le dépôt d'exemple et créer une branche de travail\n\n- Ouvrir un service `vscode` sur le [SSP Cloud](https://datalab.sspcloud.fr/home). Vous pouvez aller\ndans la page `My Services` et cliquer sur `New service`. Sinon, vous\npouvez lancer le service en cliquant directement [ici](https://datalab.sspcloud.fr/launcher/inseefrlab-helm-charts-datascience/vscode?autoLaunch=false).\n\n- Générer un jeton d'accès (*token*) sur GitHub afin de permettre l'authentification en ligne de commande à votre compte. La procédure est décrite [ici](https://docs.sspcloud.fr/onyxia-guide/controle-de-version#creer-un-jeton-dacces-token). Garder le jeton généré de côté.\n\n- Forker le dépôt `Github` <i class=\"fab fa-github\"></i> https://github.com/avouacr/ensae-reproductibilite-projet\n\n- Clôner __votre__ dépôt `Github` <i class=\"fab fa-github\"></i> en utilisant le\nterminal depuis `Visual Studio` (`Terminal > New Terminal`) :\n\n```shell\n$ git clone https://<TOKEN>@github.com/avouacr/ensae-reproductibilite-projet.git\n```\n\noù `<TOKEN>` est à remplacer par le jeton que vous avez généré précédemment.\n\n- Se placer avec le terminal dans le dossier en question : \n\n```shell\n$ cd ensae-reproductibilite-projet\n```\n\n- Créez une branche `nettoyage` :\n\n```shell\n$ git checkout -b nettoyage\nSwitched to a new branch 'nettoyage'\n```\n\n## Etape 1 : s'assurer que le notebook s'exécute correctement\n\nLa première étape est simple, mais souvent oubliée : **vérifier que le code fonctionne correctement**. \n\n- Ouvrir dans VSCode le notebook `titanic.ipynb`, et choisir comme kernel `basesspcloud`\n- Exécuter le notebook en entier pour vérifier s'il fonctionne\n- Corriger l'erreur qui empêche la bonne exécution.\n\nIl est maintenant temps de *commit* les changements effectués avec Git :\n\n```shell\n$ git add titanic.ipynb\n$ git commit -m \"Corrige l'erreur qui empêchait l'exécution\"\n$ git push\n```\n\nEssayez de *commit* vos changements à chaque étape de l'exercice, c'est une bonne habitude à prendre.\n\n## Etape 2 : Modularisation - mise en fonctions et mise en module\n\nNous allons **mettre en fonctions les parties importantes de l'analyse, et les mettre dans un module afin de pouvoir les importer directement depuis le notebook**. En reformattant le code présent dans le notebook :\n\n- créer une fonction qui importe les données d'entraînement (`train.csv`) et de test (`test.csv`) et renvoie des `DataFrames` pandas\n- créer une (ou plusieurs) fonction(s) pour réaliser les étapes de *feature engineering*\n- créer une fonction qui réalise le *split train/test* de validation\n- créer une fonction qui entraîne et évalue un classifieur `RandomForest`, et qui prend en paramètre le nombre d'arbres (`n_estimators`). La fonction doit imprimer à la fin la performance obtenue et la matrice de confusion.\n- mettre ces fonctions dans un module `functions.py`\n- importer les fonctions via le module dans le notebook et vérifier que l'on retrouve bien les différents résultats en utilisant les fonctions.\n\n{{% box status=\"warning\" title=\"Warning\" icon=\"fa fa-exclamation-triangle\" %}}\nAttention à bien **spécifier les dépendances** (packages à importer) dans le module pour que les fonctions puissent faire leur travail indépendamment du notebook !\n{{% /box %}}\n\n## Etape 3 : utiliser un `main` script\n\nFini le temps de l'expérimentation : on va maintenant essayer de se passer complètement du notebook. Pour cela, on va utiliser un `main` script, c'est à dire un script qui reproduit l'analyse en important et en exécutant les différentes fonctions dans l'ordre attendu.\n\n- créer un script `main.py` (convention de nommage pour les `main` scripts en Python)\n- importer les fonctions nécessaires à partir du module `functions.py`. Ne pas faire d' `import *`, ce n'est pas une bonne pratique ! Appeler les fonctions une par une en les séparant par des virgules\n- programmer leur exécution dans l'ordre attendu dans le script\n- vérifier que tout fonctionne bien en exécutant le `main` script à partir de l'exécutable Python :\n\n```shell\n$ python main.py\n```\n\nSi tout a correctement fonctionné, la performance du `RandomForest` et la matrice de confusion devraient s'afficher dans la console.\n\n## Etape 4 : appliquer les standards de qualité de code\n\nOn va maintenant améliorer la qualité de notre code en appliquant les standards communautaires. Pour cela, on va utiliser le *linter* classique `PyLint`. \n\nPour appliquer le linter à un script `.py`, la syntaxe à entrer dans le terminal est la suivante : \n```shell\n$ pylint mon_script.py\n```\nLe linter renvoie alors une série d'irrégularités, en précisant à chaque fois la ligne de l'erreur et le message d'erreur associé (ex : mauvaise identation). Il renvoie finalement une note sur 10, qui estime la qualité du code à l'aune des standards communautaires (PEP8 et PEP257).\n\n- appliquer une première fois le linter, respectivement aux scripts `functions.py` et `main.py`. Noter les notes obtenues.\n- à partir des codes d'erreur, modifier le code pour résoudre les différents problèmes un par un\n- viser une note minimale de 9/10 pour `main.py` et 6/10 pour `functions.py`.\n\n{{% box status=\"tip\" title=\"Note\" icon=\"fa fa-hint\" %}}\nN'hésitez pas à taper un code d'erreur sur un moteur de recherche pour obtenir plus d'informations si jamais le message n'est pas clair !\n{{% /box %}}\n\n## Etape 5 : adopter une architecture standardisée de projet\n\nOn va maintenant modifier l'architecture de notre projet pour la rendre plus standardisée. Pour cela, on va utiliser le package *cookiecutter* qui génère des templates de projet. En particulier, on va choisir le [template datascience](https://drivendata.github.io/cookiecutter-data-science/) développé par la communauté pour s'inspirer de sa structure.\n\n{{% box status=\"tip\" title=\"Note\" icon=\"fa fa-hint\" %}}\nL'idée de *cookiecutter* est de proposer des templates que l'on utilise pour initialiser un projet, afin de bâtir à l'avance une structure évolutive. La syntaxe à utiliser dans ce cas est la suivante : \n```shell\n$ pip install cookiecutter\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science\n```\n\nIci, on a déjà un projet, on va donc faire les choses dans l'autre sens : on va s'inspirer de la structure proposée afin de réorganiser celle de notre projet selon les standards communautaires.\n{{% /box %}}\n\n- analyser et comprendre la [structure de projet](https://drivendata.github.io/cookiecutter-data-science/#directory-structure) proposée par le template\n- créer les dossiers qui vous semblent pertinents pour contenir les différents éléments de notre projet selon le modèle\n- vous aller devoir séparer le module `functions.py` en différents modules afin de pouvoir entrer dans la structure suggérée dans le dossier `src` (le dossier destiné à contenir le code source de votre package)\n- vous devriez arriver à une structure semblable à celle-ci :\n\n```shell\nensae-reproductibilite-projet\n├── data\n│   └── raw\n│       ├── test.csv\n│       └── train.csv\n├── main.py\n├── notebooks\n│   └── titanic.ipynb\n├── README.md\n└── src\n    ├── data\n    │   ├── import_data.py\n    │   └── train_test_split.py\n    ├── features\n    │   └── build_features.py\n    └── models\n        └── train_evaluate.py\n```\n\n{{% box status=\"tip\" title=\"Note\" icon=\"fa fa-hint\" %}}\nIl est normal d'avoir des dossiers `__pycache__` qui traînent : ils se créent automatiquement à l'exécution d'un script en Python. On verra comment les supprimer définitivement à l'étape 8.\n{{% /box %}}\n\n## Etape 6 : fixer l'environnement d'exécution {#conda-export}\n\nAfin de favoriser la portabilité du projet, il est d'usage de \"fixer l'environnement\", c'est à dire d'indiquer dans un fichier toutes les dépendances utilisées ainsi que leurs version. Il est conventionnellement localisé à la racine du projet.\n\nSur le VSCode du SSP Cloud, on se situe dans un environnement `conda`.\nLa commande pour exporter un environnement `conda` est la suivante : \n\n```shell\n$ conda env export > environment.yml\n```\n\nVous devriez à présent avoir un fichier `environement.yml` à la racine de votre projet, qui contient les dépendances et leurs versions.\n\n{{% box status=\"tip\" title=\"Note\" icon=\"fa fa-hint\" %}}\nEn réalité, on aun peu triché : on a exporté l'environnement de base du VSCode SSP Cloud, qui contient beaucoup plus de packages que ceux utilisés par notre projet. On verra dans la [Partie 2](#partie2) de l'application comment fixer proprement les dépendances de notre projet.\n{{% /box %}}\n\n## Etape 7 : stocker les données de manière externe {#stockageS3}\n\n{{% box status=\"warning\" title=\"Warning\" icon=\"fa fa-exclamation-triangle\" %}}\nCette étape n'est pas facile. Vous devrez suivre la [documentation du SSP Cloud](https://docs.sspcloud.fr/onyxia-guide/stockage-de-donnees) pour la réaliser. Une aide-mémoire est également disponible dans le cours\nde [Python pour les data-scientists](https://linogaliana-teaching.netlify.app/reads3/#)\n{{% /box %}}\n\nComme on l'a vu dans le cours, les données ne sont pas censées être versionnées sur un projet Git. L'idéal pour éviter cela tout en maintenant la reproductibilité est d'utiliser une solution de stockage externe. On va utiliser pour cela `MinIO`, la solution de stockage de type `S3` offerte par le SSP Cloud. \n\n- créer un dossier `ensae-reproductibilite` dans votre bucket personnel via l'[interface utilisateur](https://datalab.sspcloud.fr/mes-fichiers)\n- modifier votre fonction d'import des données pour qu'elle récupère les données à partir de MinIO. Elle devra prendre en paramètres le nom du bucket et le dossier dans lequel sont contenues les données sur MinIO.\n- modifier le `main` script pour appeler la fonction avec les paramètres propres à votre compte\n- supprimer les fichiers `.csv` du dossier `data` de votre projet, on n'en a plus besoin vu qu'on les importe de l'extérieur\n- vérifier le bon fonctionnement de votre application\n\n<!-----\nmc cp train.csv s3/lgaliana/ensae-reproductibilite/train.csv\nmc cp test.csv s3/lgaliana/ensae-reproductibilite/test.csv\n----->\n\n## Etape 8 : nettoyer le dépôt Git\n\nDes dossiers parasites `__pycache__` se sont glissés dans notre projet. Ils se créent automatiquement à l'exécution d'un script en Python, afin de rendre plus rapide les exécutions ultérieures. Ils n'ont cependant pas de raison d'être versionnés, vu que ce sont des fichiers locaux (spécifiques à un environnement d'exécution donné).\n\n- supprimer les différents dossiers `__pycache__` du projet\n- ajouter le [fichier .gitignore adapté à Python](https://github.com/github/gitignore/blob/main/Python.gitignore) à la racine du projet\n- ajouter le dossier `data/` au `.gitignore` pour éviter tout ajout involontaire de données au dépôt Git\n\n{{% box status=\"tip\" title=\"Note\" icon=\"fa fa-hint\" %}}\nEn pratique, mieux vaut adopter l'habitude de toujours mettre un `.gitignore`, pertinent selon le langage du projet, dès le début du projet. `GitHub` offre cette option à l'initialisation d'un projet. Le site\n[gitignore.io](https://www.toptal.com/developers/gitignore) propose des modèles\nselon le langage que vous utilisez qui peuvent être utiles.\n{{% /box %}}\n\n## Etape 9 : ouvrir une *pull request* sur le dépôt du projet\n\nEnfin terminé ! Enfin presque... On s'est donné beaucoup de mal à nettoyer ce dépôt et le mettre aux standards, autant valoriser ce travail. On va pour cela faire une *pull request* sur le [dépôt du projet initial](https://github.com/avouacr/ensae-reproductibilite-projet), c'est à dire proposer à l'auteur d'intégrer tous les changements que vous avez effectué en committant à chaque étape. \n\nSuivre la procédure décrite dans la [documentation GitHub](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request-from-a-fork) pour créer une *pull request* à partir de votre *fork*. Pour la branche *upstream* (le dépôt cible), on va choisir `master`. Par contre, pour la branche locale (celle sur votre dépôt), on va choisir la branche `nettoyage`.\n\nSi tout s'est bien passé, vous devriez à présent voir votre *pull request* sur le dépôt cible ([ici](https://github.com/avouacr/ensae-reproductibilite-projet/pulls)). Bravo, vous venez de faire votre première contribution à l'open source !\n\n{{% box status=\"warning\" title=\"Warning\" icon=\"fa fa-exclamation-triangle\" %}}\nFaire une *pull request* via la branche `master` d’un *fork* est très mal vu. En effet, il faut souvent faire des contorsionnements pour réussir à faire coïncider deux histoires qui n’ont pas de raison de coïncider. On s'évite beaucoup de problèmes en prenant l'habitude de toujours faire ses *pull requests* à partir d'une autre branche que `master`.\n{{% /box %}}\n\n# Partie 2 : construction d'un projet portable et reproductible {#partie2}\n\nDans la partie précédente,\non a appliqué de manière incrémentale de nombreuses bonnes pratiques vues tout au long du cours.\nCe faisant, on s'est déjà considérablement rapprochés d'une\npossible mise en production : le code est lisible,\nla structure du projet est normalisée et évolutive,\net le code est proprement versionné sur un\ndépôt `GitHub` <i class=\"fab fa-github\"></i>.\n\nA présent, nous avons une version du projet qui est largement partageable.\nDu moins en théorie, car la pratique est souvent plus compliquée : il y a fort à parier que si vous essayez d'exécuter votre projet sur un autre environnement (typiquement, votre ordinateur personnel),\nles choses ne se passent pas du tout comme attendu. Cela signifie qu'**en l'état, le projet n'est pas portable : il n'est pas possible, sans modifications coûteuses, de l'exécuter dans un environnement différent de celui dans lequel il a été développé**.\n\nDans cette seconde partie, nous allons voir comment **normaliser l'environnement d'exécution afin de produire un projet portable**. On sera alors tout proche de pouvoir mettre le projet en production.\nOn progressera dans l'échelle de la reproductibilité \nde la manière suivante: \n- :one: [**Gérer des variables d'environnement hors du code**](#configyaml) ;\n- :two: [**Environnements virtuels**](#anaconda) ;\n- :three: [**Images et conteneurs `Docker`**](#docker).\n\n\n## Etape 1: créer un répertoire de variables servant d'input {#configyaml}\n\n### Enjeu\n\nLors de l'[étape 7](#stockageS3), nous avons amélioré la qualité du script en \nséparant stockage et code. Cependant, peut-être avez-vous remarqué\nque nous avons introduit un nom de _bucket_ personnel dans le script\n(voir [le fichier `main.py`](https://github.com/linogaliana/ensae-reproductibilite-projet-1/blob/v7/main.py#L9)).\nIl s'agit typiquement du genre de petit vice caché d'un script qui peut \ngénérer une erreur: vous n'avez pas accès au bucket en question donc\nsi vous essayez de faire tourner ce script en l'état, vous allez rencontrer\nune erreur.\n\nUne bonne pratique pour gérer ce type de configuration est d'utiliser un \nfichier `YAML` qui stocke de manière hiérarchisée les variables globales\n[^3].\n\n[^3]: Le format `YAML` est un format de fichier où les informations sont \nhiérarchisées. Avec le _package_ `YAML` on peut très facilement le transformer\nen `dict`, ce qui est très pratique pour accéder à une information.\n\nEn l'occurrence, nous n'avons besoin que de deux éléments pour pouvoir\ndé-personnaliser ce script :\n\n- le nom du bucket\n- l'emplacement dans le bucket\n\n### Application\n\nDans `VSCode`, créer un fichier nommé `config.yaml` et le localiser à la racine\nde votre dépôt. Voici, une proposition de hiérarchisation de l'information\nque vous devez adapter à votre nom d'utilisateur :\n\n```yaml\ninput:\n  bucket: \"lgaliana\"\n  path: \"ensae-reproductibilite\"\n```\n\nDans `main.py`, importer ce fichier et remplacer la ligne précédemment\névoquée par les valeurs du fichier. Tester en faisant tourner `main.py`\n<!-----\nhttps://github.com/linogaliana/ensae-reproductibilite-projet-1/commit/4a9d935223b6af366d4cf2a2a208d98a25407fc6\n----->\n\n## Etape 2 :  créer un environnement conda à partir du fichier `environment.yml` {#anaconda}\n\nL'environnement `conda` créé avec `conda env export` ([étape 6](#conda-export))\ncontient énormément\nde dépendances, dont de nombreuses qui ne nous sont pas nécessaires (il \nen serait de même avec `pip freeze`). \nNous n'avons en effet besoin que des _packages_ présents dans la\nsection `import` de nos scripts et les dépendances nécessaires\npour que ces _packages_ soient fonctionnels.\n\n\nVous allez chercher à obtenir\nun `environment.yml` beaucoup plus parcimonieux\nque celui généré par `conda env export`\n\n{{< panelset class=\"simplification\" >}}\n\n{{% panel name=\"Approche générale :koala: \" %}}\n\nLe tableau récapitulatif présent dans\nla [partie portabilité](/portability/#aide-mémoire)\npeut être utile dans cette partie. L'idée est \nde partir _from scratch_ et figer l'environnement qui\npermet d'avoir une appli fonctionnelle. \n\n* Créer un environnement vide avec `Python 3.10`\n<!---\nconda create -n monenv python=3.10.0\n---->\n\n* Activer cet environnement\n\n* Installer en ligne de commande avec `pip` les packages nécessaires\npour faire tourner votre code\n\n<!---\npip install pandas PyYAML s3fs scikit-learn\n---->\n\n* Faire un `pip freeze > requirements.txt` ou \n`conda env export > environment.yml` (privilégier la deuxième option)\n\n* Retirer la section `prefix` (si elle est présente)\net changer la section `name` en `monenv`\n\n\n{{% /panel %}}\n\n\n{{% panel name=\"Approche fainéante :sloth:\" %}}\n\nNous allons générer une version plus minimaliste grâce à\nl'utilitaire [`pipreqs`](https://github.com/bndr/pipreqs)\n\n* Installer `pipreqs` en `pip install`\n* En ligne de commande, depuis la racine du projet, faire `pipreqs`\n* Ouvrir le `requirements.txt` automatiquement généré. Il est beaucoup plus\nminimal que celui que vous obtiendriez avec `pip freeze` ou\nl'`environment.yml` obtenu à [l'étape 6](#conda-export). \n* Remplacer toute la section `dependencies` du `environment.yml`\npar le contenu du `requirements.txt`\n(:warning: ne pas oublier l'indentation et le tiret en début de ligne)\n* :warning: Modifier le tiret à `scikit learn`. Il ne faut pas un _underscore_ mais\nun tiret\n* Ajouter la version de python (par exemple `python=3.10.0`)\nau début de la section `dependencies`\n* Retirer la section `prefix` du fichier `environment.yml` (si elle est présente)\net changer le contenu de la section `name` en `monenv`\n* Créer l'environnement\n([voir le tableau récapitulatif dans la partie portabilité](/portability/#aide-mémoire))\n\n<!----\nconda env create -f environment.yml\n------>\n\n\n{{% /panel %}}\n\n{{% /panelset %}}\n\n\nMaintenant, il reste à tester si tout fonctionne bien dans notre \nenvironnement plus minimaliste:\n\n* Activer l'environnement\n* Tester votre script en ligne de commande\n* Faire un `commit` quand vous êtes contents\n\n\n## Etape 3: conteneuriser avec Docker <i class=\"fab fa-docker\"></i> {#docker}\n\n### Préliminaire\n\n- Se rendre sur l'environnement bac à sable [Play with Docker](https://labs.play-with-docker.com)\n- Dans le terminal `Linux`, cloner votre dépôt `Github`  <i class=\"fab fa-github\"></i>\n- Créer via la ligne de commande un fichier `Dockerfile`. Il y a plusieurs manières\nde procéder, en voici un exemple:\n\n```shell\necho \"#Dockerfile pour reproduire mon super travail\" > Dockerfile\n```\n\n- Ouvrir ce fichier via l'éditeur proposé par l'environnement bac à sable. \n\n\n### Création d'un premier Dockerfile\n\n\n\n- :one: Comme couche de départ, partir d'une image légère comme `ubuntu:20.04`\n- :two: Dans une deuxième couche, faire un `apt get -y update` et\ninstaller `wget` qui va être nécessaire pour télécharger `Miniconda`\ndepuis la ligne\nde commande \n- :three: Dans la troisième couche, nous allons installer `Miniconda` :\n    + Télécharger la dernière version de `Miniconda` avec `wget` depuis\nl'url de téléchargement direct https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n    + Installer `Miniconda` dans le chemin `/home/coder/local/bin/conda`\n    + Effacer le fichier d'installation pour libérer de la place sur l'image\n- :four: En quatrième couche, on va installer `mamba` pour accélérer l'installation\ndes packages dans notre environnement. \n- :five: En cinquième couche, nous allons créer l'environnement `conda`:\n    + Utiliser `COPY` pour que `Docker` soit en mesure d'utiliser\nle fichier `environment.yml` (sinon `Docker`\nrenverra une erreur)\n    + Créer l'environnement vide `monenv` (présentant uniquement `Python` 3.10) avec\nla commande  `conda` adéquate\n    + Mettre à jour l'environnement en utilisant `environment.yml` avec `mamba`\n- :six: Utiliser `ENV` pour ajouter l'environnement `monenv` au `PATH` et utiliser le _fix_ suivant:\n\n```python\nRUN echo \"export PATH=$PATH\" >> /home/coder/.bashrc  # Temporary fix while PATH gets overwritten by code-server\n```\n\n- :seven: Exposer sur le port `5000`\n- :eight: En dernière étape, utiliser `CMD` pour reproduire le comportement de `python main.py`\n\n\n{{% box status=\"hint\" title=\"Hint: `mamba`\" icon=\"fa fa-lightbulb\" %}}\n`mamba` est une alternative à `conda` pour installer des _packages_ dans un\nenvironnement `Miniconda`/`Anaconda`. `mamba` n'est pas obligatoire, `conda`\npeut suffire. Cependant, `mamba` est beaucoup plus rapide\nque `conda` pour installer des packages à installer ; il s'agit donc\nd'un utilitaire très pratique. \n{{% /box %}}\n\n{{< panelset class=\"nommage\" >}}\n\n{{% panel name=\"Indications supplémentaires\" %}}\n\nCliquer sur les onglets ci-dessus :point_up_2: pour bénéficier\nd'indications supplémentaires, pour vous aider. Cependant, essayez\nde ne pas les consulter immédiatement: n'hésitez pas à tâtonner. \n\n\n{{% /panel %}}\n\n{{% panel name=\"Installation de Miniconda\" %}}\n\n```shell\n# INSTALL MINICONDA -------------------------------\nARG CONDA_DIR=/home/coder/local/bin/conda\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nRUN bash Miniconda3-latest-Linux-x86_64.sh -b -p $CONDA_DIR\nRUN rm -f Miniconda3-latest-Linux-x86_64.sh\n```\n\n{{% /panel %}}\n\n{{% panel name=\"Installation de mamba\" %}}\n\n```shell\nENV PATH=\"/home/coder/local/bin/conda/bin:${PATH}\"\nRUN conda install mamba -n base -c conda-forge\n```\n\n{{% /panel %}}\n\n\n{{% panel name=\"Création de l'environnement\" %}}\n\n```shell\nCOPY environment.yml .\nRUN conda create -n monenv python=3.10\nRUN mamba env update -n monenv -f environment.yml\n```\n\n{{% /panel %}}\n\n\n{{< /panelset >}}\n\n### Construire l'image\n\nMaintenant, nous avons défini notre recette. Il nous reste à\nfaire notre plat et à le goûter\n\n- Utiliser `docker build` pour créer une image avec le tag `my-python-app`\n- Vérifier les images dont vous disposez. Vous devriez avoir un résultat\nproche de celui-ci\n\n<!---\ndocker build . -t my-python-app\ndocker images\n---->\n\n```shell\nREPOSITORY      TAG       IMAGE ID       CREATED         SIZE\nmy-python-app   latest    c0dfa42d8520   6 minutes ago   2.23GB\nubuntu          20.04     825d55fb6340   6 days ago      72.8MB\n```\n\n### Tester l'image: découverte du cache\n\nIl ne reste plus qu'à goûter la recette et voir si le plat est bon. \n\nUtiliser `docker run` avec l'option `it` pour pouvoir appeler l'image\ndepuis son tag\n\n<!----\ndocker run -it my-python-app\n---->\n\n:warning: :bomb: :fire: \n`Docker` ne sait pas où trouver le fichier `main.py`. D'ailleurs,\nil ne connait pas d'autres fichiers de notre application qui sont nécessaires\npour faire tourner le code: `config.yaml` et le dossier `src`\n\n- Avant l'étape `EXPOSE` utiliser plusieurs `ADD` et/ou `COPY` pour que l'application\ndispose de tous les éléments minimaux pour être en mesure de fonctionner\n\n- Refaire tourner `docker run`\n<!---\ndocker run -it my-python-app\n--->\n\n{{% box status=\"tip\" title=\"Note\" icon=\"fa fa-hint\" %}}\nIci, le _cache_ permet d'économiser beaucoup de temps. Par besoin de \nrefaire tourner toutes les étapes, `Docker` agit de manière intelligente\nen faisant tourner uniquement les nouvelles étapes.\n{{% /box %}}\n\n### Corriger une faille de reproductibilité\n\n\nVous devriez rencontrer une erreur liée à la variable d'environnement\n`AWS_ENDPOINT_URL`. C'est normal, elle est inconnue de cet environnement\nminimaliste. D'ailleurs, `Docker` n'a aucune raison de connaître\nvotre espace de stockage sur le `S3` du `SSP-Cloud` si vous ne lui dites\npas. \nDonc cet environnement ne sait pas\ncomment accéder aux fichiers présents dans votre `minio`.\n\nVous allez régler ce problème avec les étapes suivantes, :\n\n\n- :one: Naviguer dans l'[interface du SSP-Cloud](https://datalab.sspcloud.fr/mes-fichiers)\npour retrouver les liens d'accès direct de vos fichiers\n- :two: Dans `VSCode`, les mettre dans `config.yaml` (faire de nouvelles clés)\n- :three: Dans `VSCode`, modifier la fonction d'import pour s'adapter à ce changement.\n- :four: Faire un `commit` et pusher les fichiers\n- :five: Dans l'environnement bac à sable, faire un `pull` pour récupérer ces\nmodifications\n- :six: Tester à nouveau le `build` (là encore le _cache_ est bien pratique !)\n\n<!---\ncf. \nhttps://github.com/linogaliana/ensae-reproductibilite-projet-1/commit/56946b4c5cb860d50b908d98a87fb549624314a6\n----->\n\n\n:tada: A ce stade, la matrice de confusion doit fonctionner. Vous avez créé\nvotre première application reproductible !\n\n# Partie 3 : mise en production\n\nUne image `Docker` est un livrable qui n'est pas forcément intéressant\npour tous les publics. Certains préféreront avoir un plat bien préparé\nqu'une recette. Nous allons donc proposer d'aller plus loin en proposant\nplusieurs types de livrables. Cela va nous amener à découvrir les outils\ndu CI/CD (_Continuous Integration / Continuous Delivery_)\nqui sont au coeur de l'approche `DevOps`. Notre approche appliquée\nau _machine learning_ va nous entraîner plutôt du côté du `MLOps` qui devient\nune approche de plus en plus fréquente dans l'industrie de la \n_data science_.\n\nNous allons améliorer notre approche de trois manières:\n\n- Automatisation de la création de l'image `Docker` et tests\nautomatisés de la qualité du code ;\n- Production d'un site _web_ automatisé permettant de documenter et\nvaloriser le modèle de _Machine Learning_ ;\n- Mise à disposition du modèle entraîné par le biais d'une API pour\nne pas le ré-entraîner à chaque fois et faciliter sa réutilisation ;\n\nA chaque fois, nous allons d'abord tester en local notre travail puis\nessayer d'automatiser cela avec les outils de `Github`.\n\nOn va ici utiliser l'intégration continue pour deux objectifs distincts:\n\n- la mise à disposition de l'image `Docker` ;\n- la mise en place de tests automatisés de la qualité du code\nsur le modèle de notre `linter` précédent \n\nNous allons utiliser `Github Actions` pour cela. \n\n## Etape préliminaire\n\nPour ne pas risquer de tout casser sur notre branche `master`, nous allons \nnous placer sur une branche nommée `dev`:\n\n- si dans l'étape suivante vous appliquez la méthode la plus simple, vous\nallez pouvoir la créer depuis l'interface de `Github` ;\n- si vous utilisez l'autre méthode, vous allez devoir la créer en local (\nvia la commande `git checkout -b dev`)\n\n## Etape 1: mise en place de tests automatisés\n\nAvant d'essayer de mettre en oeuvre la création de notre image\n`Docker` de manière automatisée, nous allons présenter la logique\nde l'intégration continue en généralisant les évaluations de\nqualité du code avec le `linter`\n\n{{< panelset class=\"nommage\" >}}\n{{% panel name=\"Utilisation d'un _template_ `Github` :cat:\" %}}\n\n__Methode la plus simple: utilisation d'un _template_ Github__\n\nSi vous cliquez sur l'onglet `Actions` de votre dépôt, `Github` vous propose des _workflows_ standardisés reliés à `Python`. Choisir l'option `Python Package using Anaconda`.\n\nwarning: Nous n'allons modifier que deux éléments de ce fichier.\n\n:one: La dernière étape (`Test with pytest`) ne nous est pas nécessaire car nous n'avons pas de tests unitaires Nous allons donc remplacer celle-ci par l'utilisation de `pylint` pour avoir une note de qualité du package.\n\n+ Utiliser `pylint` à cette étape pour noter les scripts ;\n+ Vous pouvez fixer un score minimal à 5 (option `--fail-under=5`)\n\n:two: Mettre entre guillements la version de `Python` pour que celle-ci soit reconnue.\n\n:three: Enfin, finaliser la création de ce script:\n\n- En cliquant sur le bouton `Start Commit`, choisir la méthode\n`Create a new branch for this commit and start a pull request`\nen nommant la branche `dev`\n- Créer la `Pull Request` en lui donnant un nom signifiant\n\n{{% /panel %}}\n\n{{% panel name=\"Méthode manuelle\" %}}\n\n:warning: On est plutôt sur une méthode de galérien. Il vaut\nmieux privilégier l'autre approche\n\nOn va éditer\ndepuis `VisualStudio` nos fichiers.\n\n- Créer une branche `dev` en ligne de commande\n- Créer un dossier `.github/workflows` via la ligne de commande ou l'explorateur de fichier \n<!---mkdir .github/workflows -p ---->\n- Créer un fichier `.github/workflows/quality.yml`. \n\n\nNous allons construire, par étape, une version simplifiée du `Dockerfile` présent\ndans [ce post](https://medium.com/swlh/enhancing-code-quality-with-github-actions-67561c6f7063) et\ndans [celui-ci](https://autobencoder.com/2020-08-24-conda-actions/)\n\n:one: D'abord, définissons des paramètres pour indiquer à `Github`\nquand faire tourner notre script:\n\n- Commencez par nommer votre _workflow_ par exemple `Python Linting` avec la clé `name`\n- Nous allons faire tourner ce _workflow_ dans la branche `master` et dans la branche actuelle (`dev`). Ici, nous laissons de côté les autres éléments (par exemple le fait de faire tourner à chaque _pull request_). La clé `on` est dédiée à cet usage\n\n:two: Ensuite, défnissons le contexte d'exécution des tâches (`jobs`)\nde notre script dans\nles options de la partie `build`:\n\n- Utilisons une machine `ubuntu-latest`. Nous verrons plus tard\ncomment améliorer cela. \n    \n:three: Nous allons ensuite mélanger des étapes pré-définies (des actions du _marketplace_) et des instructions que nous faisons :\n\n- Le _runner_ `Github` doit récupérer le contenu de notre dépôt, pour cela utiliser l'action `checkout`. Par rapport à l'exemple, il convient d'ajouter, pour le moment, un paramètre `ref` avec le nom de la branche (par exemple `dev`)\n- ~~On installe ensuite `Python` avec l'action `setup-python`~~ Pas besoin d'installer `Python`, on va utiliser l'option `conda-incubator/setup-miniconda@v2`\n- Pour installer `Python` et l'environnement `conda`, on va plutôt utiliser l'astuce de [ce blog](https://autobencoder.com/2020-08-24-conda-actions/) avec l'option `conda-incubator/setup-miniconda@v2`\n- On utilise ensuite `flake8` et `pylint` (option `--fail-under=5`)\npour effectuer des diagnostics de qualité\n\nIl ne reste plus qu'à faire un `commit` et espérer que cela fonctionne.\nCela devrait donner le fichier suivant : \n\n\n```yaml\nname: Python Linting\non:\n  push:\n    branches: [master, dev]\njobs:\n  build:\n    runs-on: ubuntu-latest    \n    steps:\n      - uses: actions/checkout@v3\n        with:\n          ref: \"dev\"\n      - uses: conda-incubator/setup-miniconda@v2\n        with:\n          activate-environment: monenv\n          environment-file: environment.yml\n          python-version: '3.10'\n          auto-activate-base: false\n      - shell: bash -l {0}\n        run: |\n          conda info\n          conda list\n      - name: Lint with flake8\n        run: |\n          pip install flake8\n          flake8 src --count --select=E9,F63,F7,F82 --show-source --statistics\n          flake8 src --count --max-complexity=10 --max-line-length=79 --statistics\n      - name: Lint with Pylint\n        run: |\n          pip install pylint\n          pylint src\n``` \n\n\n{{% /panel %}}\n\n\n{{< /panelset >}}\n\n \nMaintenant, nous pouvons observer que l'onglet `Actions`\ns'est enrichi. Chaque `commit` va entraîner une action pour\ntester nos scripts.\n\nSi la note est mauvaise, nous aurons\nune croix rouge (et nous recevrons un mail). On pourra ainsi détecter,\nen développant son projet, les moments où on dégrade la qualité du script \nafin de la rétablir immédiatemment. \n\n\n{{% box status=\"hint\" title=\"Un `linter` sous forme de _hook_ pre-commit\" icon=\"fa fa-lightbulb\" %}}\n\n`Git` offre une fonctionalité intéressante lorsqu'on est puriste: les \n_hooks_. Il s'agit de règles qui doivent être satisfaites pour que le \nfichier puisse être committé. Cela assurera que chaque `commit` remplisse\ndes critères de qualité afin d'éviter le problème de la procrastination.\n\nLa [documentation de pylint](https://pylint.pycqa.org/en/latest/user_guide/pre-commit-integration.html)\noffre des explications supplémentaires. \n\n{{% /box %}}\n\n\n## Etape 2: Automatisation de la livraison de l'image `Docker`\n\nMaintenant, nous allons automatiser la mise à disposition de notre image\nsur `DockerHub`. Cela facilitera sa réutilisation mais aussi des\nvalorisations ultérieures.\n\nLà encore, nous allons utiliser une série d'actions pré-configurées.\n\n:one: Pour que `Github` puisse s'authentifier auprès de `DockerHub`, il va \nfalloir d'abord interfacer les deux plateformes. Pour cela, nous allons utiliser\nun jeton (_token_) `DockerHub` que nous allons mettre dans un espace\nsécurisé associé à votre dépôt `Github`. Cette démarche sera là même\nultérieurement lorsque nous connecterons notre dépôt à un autre\nservice tiers, à savoir `Netlify`:\n\n- Se rendre sur\nhttps://hub.docker.com/ et créer un compte.\n- Aller dans les paramètres (https://hub.docker.com/settings/general)\net cliquer, à gauche, sur `Security`\n- Créer un jeton personnel d'accès, ne fermez pas l'onglet en question,\nvous ne pouvez voir sa valeur qu'une fois. \n- Dans votre dépôt `Github`, cliquer sur l'onglet `Settings` et cliquer,\nà gauche, sur `Actions`. Sur la page qui s'affiche, cliquer sur `New repository secret`\n- Donner le nom `DOCKERHUB_TOKEN` à ce jeton et copier la valeur. Valider\n- Créer un deuxième secret nommé `DOCKERHUB_USERNAME` ayant comme valeur le nom d'utilisateur\nque vous avez créé sur `Dockerhub`\n\n:two: A ce stade, nous avons donné les moyens à `Github` de s'authentifier avec\nnotre identité sur `Dockerhub`. Il nous reste à mettre en oeuvre l'action\nen s'inspirant de https://github.com/docker/build-push-action/#usage.\nOn ne va modifier que trois éléments dans ce fichier. Effectuer les \nactions suivantes:\n\n- Créer depuis `VSCode` un fichier\n`.github/workflows/docker.yml` et coller le\ncontenu du _template_ dedans ; \n- Changer le nom en un titre plus signifiant (par exemple _\"Production de l'image Docker\"_)\n- Ajouter `master` et `dev` à la liste des branches sur lesquelles tourne\nle pipeline ;\n- Changer le tag à la fin pour mettre `<username>/ensae-repro-docker:latest`\noù `username` est le nom d'utilisateur sur `DockerHub`;\n- Faire un `commit` et un `push` de ces fichiers\n\n:four: Comme on est fier de notre travail, on va afficher ça avec un badge sur le \n`README`. Pour cela, on se rend dans l'onglet `Actions` et on clique sur\nun des scripts en train de tourner. \n\n- En haut à droite, on clique sur `...`\n- Sélectionner `Create status badge`\n- Récupérer le code `Markdown` proposé\n- Copier dans le `README` depuis `VSCode`\n- Faire de même pour l'autre _workflow_\n\n:five: Maintenant, il nous reste à tester notre application dans l'espace bac à sable:\n\n- Se rendre sur l'environnement bac à sable\n- Créer un fichier `Dockerfile` ne contenant que l'import et le déploiement\nde l'appli:\n\n```yaml\nFROM <username>/ensae-repro-docker:latest\n\nEXPOSE 5000\nCMD [\"python\", \"main.py\"]\n```\n\n- Comme précédemment, faire un _build_\n- Tester l'image avec `run`\n\n:tada: La matrice de confusion doit s'afficher ! Vous avez grandement\nfacilité la réutilisation de votre image. \n\n## Etape 3: création d'un rapport automatique\n\nMaintenant, nous allons créer et déployer un site web pour valoriser notre\ntravail. Cela va impliquer trois étapes:\n\n- Tester en local le logiciel `quarto` et créer un rapport minimal qui sera compilé par `quarto` ;\n- Enrichir l'image docker avec le logiciel `quarto` ;\n- Compiler le document en utilisant cette image sur les serveurs de `Github` ;\n- Déployer ce rapport minimal pour le rendre disponible à tous sur le _web_.\n\nLe but est de proposer un rapport minimal qui illustre la performance\ndu modèle est la _feature importance_. Pour ce dernier élément, le\nrapport qui sera proposé utilise `shap` qui est une librairie dédiée\nà l'interprétabilité des modèles de _machine learning_\n\n### 1. Rapport minimal en local\n\n:one: La première étape consiste à installer\n`quarto` sur notre machine `Linux` sur laquelle\ntourne `VSCode`:\n\n- Dans un terminal, installer `quarto` avec les commandes suivantes:\n\n```shell\nQUARTO_VERSION=\"0.9.287\"\nwget \"https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb\"\nsudo apt install \"./quarto-${QUARTO_VERSION}-linux-amd64.deb\"\n```\n\n- S'assurer qu'on travaille bien depuis l'environnement `conda` `monenv`. Sinon\nl'activer\n\n:two: Il va être nécessaire d'enrichir l'environnement `conda`.\nCertaines dépendances sont nécessaires pour que `quarto` fonctionne bien avec\n`Python` (`jupyter`, `nbclient`...)\nalors que d'autres ne sont nécessaires que parce qu'ils sont utilisés dans\nle document (`seaborn`, `shap`...). Changer la section `dependencies` avec\nla liste suivante:\n\n```yaml\ndependencies:\n  - python=3.10.0\n  - ipykernel==6.13.0\n  - jupyter==1.0.0\n  - matplotlib==3.5.1\n  - nbconvert==6.5.0\n  - nbclient==0.6.0\n  - nbformat==5.3.0\n  - pandas==1.4.1\n  - PyYAML==6.0\n  - s3fs==2022.2.0\n  - scikit-learn==1.0.2\n  - seaborn==0.11.2\n  - shap==0.40.0\n```\n\n\n:three: Créer un fichier nommé `report.qmd`\n\n~~~markdown\n---\ntitle: \"Comprendre les facteurs de survie sur le Titanic\"\nsubtitle: \"Un rapport innovant\"\nformat:\n  html:\n    self-contained: true\n  ipynb: default\njupyter: python3\n---\n\nVoici un rapport présentant quelques intuitions issues d'un modèle \n_random forest_ sur le jeu de données `Titanic` entraîné et \ndéployé de manière automatique. \n\nIl est possible de télécharger cette page sous format `Jupyter Notebook` <a href=\"report.ipynb\" download>ici</a>\n\n\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nimport main\nX_train = main.X_train\ny_train = main.y_train\ntraining_data = main.training_data\nrdmf = RandomForestClassifier(n_estimators=20)\nrdmf.fit(X_train, y_train)\n```\n\n# Feature importance\n\nLa @fig-feature-importance représente l'importance des variables :\n\n```{python}\nfeature_imp = pd.Series(rdmf.feature_importances_, index=training_data.iloc[:,1:].columns).sort_values(ascending=False)\n```\n\n```{python}\n#| label: fig-feature-importance\n#| fig-cap: \"Feature importance\"\nplt.figure(figsize=(10,6))\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.tight_layout()\nplt.show()\n```\n\nCelle-ci peut également être obtenue grâce à la librairie\n`shap`:\n\n```{python}\n#| echo : true\nimport shap\nshap_values = shap.TreeExplainer(rdmf).shap_values(X_train)\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\", feature_names = training_data.iloc[:,1:].columns)\n```\n\nOn peut également utiliser cette librairie pour\ninterpréter la prédiction de notre modèle:\n\n```{python}\n# explain all the predictions in the test set\nexplainer = shap.TreeExplainer(rdmf)\n# Calculate Shap values\nchoosen_instance = main.X_test[15]\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance, feature_names = training_data.iloc[:,1:].columns)\n```\n\n# Qualité prédictive du modèle\n\nLa matrice de confusion est présentée sur la\n@fig-confusion\n\n```{python}\n#| label: fig-confusion\n#| fig-cap: \"Matrice de confusion\"\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(main.y_test, rdmf.predict(main.X_test))\nplt.figure(figsize=(8,5))\nsns.heatmap(conf_matrix, annot=True)\nplt.title('Confusion Matrix')\nplt.tight_layout()\n```\n\nOu, sous forme de tableau:\n\n```{python}\npd.DataFrame(conf_matrix, columns=['Predicted','Observed'], index = ['Predicted','Observed']).to_html()\n```\n~~~\n\n:four: On va tenter de compiler ce document\n\n- Le compiler en local avec la commande `quarto render report.qmd`\n\n- Vous devriez rencontrer l'erreur suivante:\n\n```python\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nInput In [1], in <cell line: 6>()\n      4 from sklearn.ensemble import RandomForestClassifier\n      5 import main\n----> 6 X_train = main.X_train\n      7 y_train = main.y_train\n      8 training_data = main.training_data\n\nAttributeError: module 'main' has no attribute 'X_train'\nAttributeError: module 'main' has no attribute 'X_train'\n```\n\n- Refactoriser `main.py` pour que toutes les opérations, à l'exception\ndu print de la matrice de confusion ne soient plus dans la section `__main__`\nafin qu'ils soient systématiquement exécutés. \n\n- Tenter à nouveau `quarto render report.qmd`\n\n- Deux fichiers ont été générés: \n    + un `Notebook` que vous pouvez ouvrir et dont vous pouvez exécuter\ndes cellules\n    + un fichier `HTML` que vous pouvez télécharger et ouvrir\n\n:five: On a déjà un résultat assez esthétique en ce qui concerne la page `HTML`.\nCependant, on peut se dire que certains paramètres par défaut, comme l'affichage\ndes blocs de code, ne conviennent pas au public ciblé. De même, certains\nparamètres de style, comme l'affichage des tableaux peuvent ne pas convenir\nà notre charte graphique. On va remédier à cela en deux étapes:\n\n- enrichir le _header_ d'options globales contrôlant le comportement de `quarto`\n- créer un fichier `CSS` pour avoir de beaux tableaux\n\n:six: Changer la section `format` du _header_ avec les options suivantes:\n\n```yaml\nformat:\n  html:\n    echo: false\n    code-fold: true\n    self-contained: true\n    code-summary: \"Show the code\"\n    warning: false\n    message: false\n    theme:\n      - cosmo\n      - css/custom.scss\n  ipynb: default\n```\n\n:seven: Créer le fichier `css/custom.scss` avec le contenu suivant:\n\n```css\n/*-- scss:rules --*/\n\ntable {\n    border-collapse: collapse;\n    margin: 25px 0;\n    font-size: 0.9em;\n    font-family: sans-serif;\n    min-width: 400px;\n    box-shadow: 0 0 20px rgba(0, 0, 0, 0.15);  \n}\n\nthead tr {\n    background-color: #516db0;\n    color: #ffffff;\n    text-align: center;\n}\n\nth, td {\n    padding: 12px 15px;\n}\n\ntbody tr {\n    border-bottom: 1px solid #dddddd;\n}\n\ntbody tr:nth-of-type(even) {\n    background-color: #f3f3f3;\n}\n\ntbody tr:last-of-type {\n    border-bottom: 2px solid #516db0;\n}\n\ntbody tr.active-row {\n    font-weight: bold;\n    color: #009879;\n}\n```\n\n:eight: Compiler à nouveau et observer le changement d'esthétique du `HTML`\n\n:nine: Commit des nouveaux fichier `report.qmd`, `custom.scss` et des fichiers\ndéjà existants.\n\n{{% box status=\"hint\" title=\"Un `linter` sous forme de _hook_ pre-commit\" icon=\"fa fa-lightbulb\" %}}\n\nOn ne `commit` pas les _output_, ici le notebook et le fichier html.\nLes mettre sur le dépôt `Github` n'est pas la bonne manière de les mettre\nà disposition. On va le voir, on va utiliser l'approche CI/CD pour cela.\n\nIdéalement, on ajoute au `.gitignore` les fichiers concernés, ici `report.ipynb`\net `report.html`\n\n{{% /box %}}\n\n### 3. Enrichir l'image `Docker`\n\nOn va vouloir mettre à jour notre image pour automatiser, à terme, la production\nde nos livrables (le notebook et la page web). \n\nPour cela, il est nécessaire que notre image intègre le logiciel `quarto`.\n\n:one: A partir du script précédent d'installation de `quarto`, enrichir l'image\n`Docker`[^1]\n\n<!----\nENV QUARTO_VERSION=\"0.9.287\"\nRUN wget \"https://github.com/quarto-dev/quarto-cli/releases/download/v${QUARTO_VERSION}/quarto-${QUARTO_VERSION}-linux-amd64.deb\"\nRUN apt install \"./quarto-${QUARTO_VERSION}-linux-amd64.deb\"\n----->\n\n[^1]: Le `sudo` n'est pas nécessaire puisque vous êtes déjà en `root`\n\n\n### 4. Automatisation avec `Github Actions`\n\n:one: Créer un nouveau fichier `.github/workflows.report.yml`\n\n\nSi les dépendances et l'image ont bien été enrichis, cette étape est quasi directe\navec \n\n{{< panelset class=\"simplification\" >}}\n\n{{% panel name=\"Version autonome :car: \" %}}\n\n- Donner comme nom `Deploy as website`\n- Effectuer cette action à chaque `push` sur les branches `main`, `master` et `dev`\n- Le job doit tourner sur une machine `ubuntu`\n- Cependant, il convient d'utiliser comme `container` votre image Docker \n- Les `steps`:\n    + Récupérer le contenu du dossier avec `checkout`\n    + Faire un `quarto render`\n    + Récupérer le notebook sous forme d'artefact\n\n{{% /panel %}}\n\n{{% panel name=\"Version guidée :map: \" %}}\n\n```yaml\nname: Deploy as website\n\non:\n  push:\n    branches:\n      - main\n      - master\n      - dev\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    container: linogaliana/ensae-repro-docker:latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Render site\n        run: quarto render report.qmd\n      - uses: actions/upload-artifact@v1\n        with:\n          name: Report\n          path: report.ipynb\n```\n\n{{% /panel %}}\n\n{{< /panelset >}}\n\nSi vous êtes fier de vous, vous pouvez ajouter le badge de ce workflow\nsur le `README` :sunglasses:\n\nCette étape nous a permis d'automatiser la construction de nos livrables.\nMais la mise à disposition de ce livrable est encore assez manuelle: il \nfaut aller chercher à la main la dernière version du notebook pour\nla partager. \n\nOn va améliorer cela en déployant automatiquement un site _web_ présentant\nen page d'accueil notre rapport et permettant le téléchargement du notebook. \n\n\n## Etape 4: Déploiement de ce rapport automatique sur le web\n\n:one: Dans un premier temps, nous allons connecter notre dépôt `Github` au\nservice tiers `Netlify`\n\n- Aller sur https://www.netlify.com/ et faire `Sign up` (utiliser son compte `Github`)\n- Dans la page d'accueil de votre profil, vous pouvez cliquer sur `Add new site > Import an existing project`\n- Cliquer sur `Github`. S'il y a des autorisations à donner, les accorder. Rechercher votre projet dans la liste de vos projets `Github`\n- Cliquer sur le nom du projet et laisser les paramètres par défaut (nous allons modifier par la suite)\n- Cliquer sur `Deploy site`\n\n:two: A ce stade, votre déploiement devrait échouer.\nC'est normal, vous essayez de déployer depuis `master` qui ne comporte pas de html.\nMais le rapport n'est pas non plus présent dans la branche `dev`.\nEn fait, aucune branche ne comporte le rapport:\ncelui-ci est généré dans votre _pipeline_ mais n'est jamais présent dans le\ndépôt car il s'agit d'un _output_. On va désactiver le déploiement automatique \npour privilégier un déploiement depuis `Github Actions`:\n\n- Aller dans `Site Settings` puis, à gauche, cliquer sur `Build and Deploy`\n- Dans la section `Build settings`, cliquer sur `Stop builds` et valider\n\nOn vient de désactiver le déploiement automatique par défaut. On va faire\ncommuniquer notre dépôt `Github` et `Netlify` par le biais de l'intégration\ncontinue.\n\n:three: Pour cela, il faut créer un jeton `Netlify` pour que les serveurs\nde `Github`, lorsqu'ils disposent d'un rapport, puissent l'envoyer à `Netlify`\npour la mise sur le _web_. Il va être nécessaire de créer deux variables\nd'environnement pour connecter `Github` et `Netlify`: l'identifiant du site\net le _token_\n\n- Pour le token : \n    + Créer un jeton en cliquant, en haut à droite, sur l'icone de votre profil. Aller\ndans `User settings`. A gauche, cliquer sur `Applications` et créer un jeton personnel d'accès\navec un nom signifiant (par exemple `PAT_ENSAE_reproductibilite`)\n    + Mettre de côté (conseil : garder l'onglet ouvert)\n- Pour l'identifiant du site:\n    + cliquer sur `Site Settings` dans les onglets en haut\n    + Garder l'onglet ouvert pour copier la valeur quand nécessaire\n    \n\n- Il est maintenant nécessaire d'aller dans le dépôt `Github` et de créer \nles secrets (`Settings > Secrets > Actions`):\n    + Créer le secret `NETLIFY_AUTH_TOKEN` en collant la valeur du jeton d'authentification `Netlify`\n    + Créer le secret `NETLIFY_SITE_ID` en collant l'identifiant du site\n\n\n:four: Nous avons effectué toutes les configurations nécessaires. On va\nmaintenant mettre à jour l'intégration continue afin de mettre à disposition\nsur le _web_ notre rapport. On va utiliser l'interface en ligne de commande\n(CLI) de `Netlify`. Celle-ci attend que le site _web_ se trouve dans un\ndossier `public` et que la page d'accueil soit nommée `index.html`:\n\n{{< panelset class=\"simplification\" >}}\n\n{{% panel name=\"Vision d'ensemble \" %}}\n\n- une installation de `npm`\n- une étape de déploiement via la CLI de netlify\n\n```yaml\n- name: Install npm\n  uses: actions/setup-node@v2\n  with:\n    node-version: '14'\n- name: Deploy to Netlify\n  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets\n  env:\n    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n  run: |\n    mkdir -p public\n    mv report.html public/index.html\n    mv report.ipynb public/report.ipynb\n    npm install --unsafe-perm=true netlify-cli -g\n    netlify init\n    netlify deploy --prod --dir=\"public\" --message \"Deploy master\"\n```\n\n{{% /panel %}}\n\n{{% panel name=\"Détails npm \" %}}\n\n{{< highlight yaml \"hl_lines=1-4\" >}}\n- name: Install npm\n  uses: actions/setup-node@v2\n  with:\n    node-version: '14'\n{{< / highlight >}}\n\n`npm` est le gestionnaire de paquet de JS. Il est nécessaire de le configurer,\nce qui est fait automatiquement grâce à l'action `actions/setup-node@v2`\n\n{{% /panel %}}\n\n{{% panel name=\"Détails `Netlify CLI`\" %}}\n\n- On rappelle à `Github Actions` nos paramètres d'authentification\nsous forme de variables d'environnement. Cela permet de les garder\nsecrètes\n\n{{< highlight yaml \"hl_lines=3-5\" >}}\n- name: Deploy to Netlify\n  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets\n  env:\n    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n  run: |\n    mkdir -p public\n    mv report.html public/index.html\n    mv report.ipynb public/report.ipynb\n    npm install --unsafe-perm=true netlify-cli -g\n    netlify init\n    netlify deploy --prod --dir=\"public\" --message \"Deploy master\"\n{{< / highlight >}}\n\n- On déplace les rapports de la racine vers le dossier `public`\n\n{{< highlight yaml \"hl_lines=7-9\" >}}\n- name: Deploy to Netlify\n  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets\n  env:\n    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n  run: |\n    mkdir -p public\n    mv report.html public/index.html\n    mv report.ipynb public/report.ipynb\n    npm install --unsafe-perm=true netlify-cli -g\n    netlify init\n    netlify deploy --prod --dir=\"public\" --message \"Deploy master\"\n{{< / highlight >}}\n\n- On installe et initialise `Netlify`\n\n{{< highlight yaml \"hl_lines=10-11\" >}}\n- name: Deploy to Netlify\n  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets\n  env:\n    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n  run: |\n    mkdir -p public\n    mv report.html public/index.html\n    mv report.ipynb public/report.ipynb\n    npm install --unsafe-perm=true netlify-cli -g\n    netlify init\n    netlify deploy --prod --dir=\"public\" --message \"Deploy master\"\n{{< / highlight >}}\n\n- On déploie sur l'url par défaut (`-- prod`) depuis le dossier `public`\n\n{{< highlight yaml \"hl_lines=10-12\" >}}\n- name: Deploy to Netlify\n  # NETLIFY_AUTH_TOKEN and NETLIFY_SITE_ID added in the repo's secrets\n  env:\n    NETLIFY_AUTH_TOKEN: ${{ secrets.NETLIFY_AUTH_TOKEN }}\n    NETLIFY_SITE_ID: ${{ secrets.NETLIFY_SITE_ID }}\n  run: |\n    mkdir -p public\n    mv report.html public/index.html\n    mv report.ipynb public/report.ipynb\n    npm install --unsafe-perm=true netlify-cli -g\n    netlify init\n    netlify deploy --prod --dir=\"public\" --message \"Deploy master\"\n{{< / highlight >}}\n{{% /panel %}}\n\n{{< /panelset >}}\n\n\nAu bout de quelques minutes, le rapport est disponible en ligne sur\nl'URL `Netlify` (par exemple https://spiffy-florentine-c913b9.netlify.app)\n\n\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"self-contained":true,"output-file":"application.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"cosmo","title":"Comprendre les facteurs de survie sur le Titanic","date":"2022-03-03","author":"Romain Avouac et Lino Galiana","draft":false,"layout":"single","subtitle":"Un rapport innovant","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}},"ipynb":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"ipynb","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"default-image-extension":"png","to":"ipynb","output-file":"application.ipynb"},"language":{},"metadata":{"title":"Comprendre les facteurs de survie sur le Titanic","date":"2022-03-03","author":"Romain Avouac et Lino Galiana","draft":false,"layout":"single","subtitle":"Un rapport innovant","jupyter":"python3"}}}}